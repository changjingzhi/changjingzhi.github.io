<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>《定风波》</title>
    <link href="/2024/04/25/juzhi2/"/>
    <url>/2024/04/25/juzhi2/</url>
    
    <content type="html"><![CDATA[<center> <font size = 6>《定风波》 <font></center>  三月七日，沙湖道中遇雨。雨具先去，同行皆狼狈，余独不觉。已而遂晴，故作此词。  莫听穿林打叶声，何妨吟啸且徐行。竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生。  料峭春风吹酒醒，微冷，山头斜照却相迎。回首向来萧瑟处，归去，也无风雨也无晴。<p><font size = 3><font>译文：三月七日，在沙湖道上赶上了下雨。雨具先前被带走了，同行的人都觉得很狼狈，只有我不这么觉得。过了一会儿天晴了，就创作了这首词。不用注意那穿林打叶的雨声，不妨一边吟咏长啸着，一边悠然地行走。竹杖和草鞋轻捷得胜过骑马，有什么可怕的？一身蓑衣任凭风吹雨打，照样过我的一生。春风微凉，将我的酒意吹醒，寒意初上，山头初晴的斜阳却应时相迎。回头望一眼走过来遇到风雨的地方，回去吧，对我来说，既无所谓风雨，也无所谓天晴。</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>喜欢的一些句子</title>
    <link href="/2024/04/25/juzhi1/"/>
    <url>/2024/04/25/juzhi1/</url>
    
    <content type="html"><![CDATA[<ol><li><p>于浩歌狂热之际中寒，于天上看见深渊，于一切眼中看见无所有，于无所希望中得救 —— 鲁迅《墓碣文》</p></li><li><p>但是太阳，他每时每刻都是夕阳也都是旭日。 当他熄灭着走下山去收尽苍凉残照之际，正是他在另一面，燃烧着爬上山巅布散烈烈朝辉之时。那一天，我也将沉静着走下山去，扶着我的拐杖。有一天，在某一处山洼里，势必会跑上来一个欢蹦的孩子，抱着他的玩具。——史铁生</p></li><li><p>生命就是这样一个过程，一个不断超越自身局限的过程，这就是命运，任何人都是一样。在这过程中，我们遭遇痛苦、超越局限、从而感受幸福。所以一切人都是平等的，我们毫不特殊。——史铁生 《病隙碎笔》</p></li><li><p>只要你不停的向上走，一级级楼梯就没有尽头，在你向上走的脚下，它们也在向上长。——卡夫卡《律师》</p></li><li><p>找到属于自己的意义，赋予生命目的，每一天都像向日葵朝向太阳一样，充满方向和意义的活，是人类能活出的最好样子，它治愈我们的根本恐惧。</p></li><li><p>成功就是用自己喜欢的方式过一生。这句话分三部分。首先要知道自己喜欢什么，其次要有追逐它的勇气，追到了，还需要一生不渝的毅力。</p></li><li><p>“我来到这个世界，不是为了繁衍后代，而是来看花怎么开，水怎么流，太阳怎么升起，夕阳如何落下。我活在世上，无非是想要明白些道理，遇见有趣的事。生命是一场偶然，我在其中寻找因果。”生命对于每个人，它的意义是不一样的，每个人都是宇宙中一个独特的存在。</p></li><li><p>世界上只有一种英雄主义，那就是认清生活的真相后依旧热爱生活。</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔11</title>
    <link href="/2024/04/25/ganwu11/"/>
    <url>/2024/04/25/ganwu11/</url>
    
    <content type="html"><![CDATA[<p>“人生的意义是什么？”<br>回答：“每个人都有不同的答案。”</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文思路</title>
    <link href="/2024/04/25/paper_idear/"/>
    <url>/2024/04/25/paper_idear/</url>
    
    <content type="html"><![CDATA[<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><ol><li>傅里叶变换 （时间域变换为频域）</li><li>归一化（零归一化，批归一化，层归一化）</li></ol><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ol><li>startRule</li><li>ReLU</li></ol><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><ol><li>编码器——解码器</li><li>残差连接（基于ResNet）</li><li>MLP</li><li></li></ol><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ol><li>交叉熵损失函数</li></ol><h2 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h2><ol><li>SGD</li><li>RMSprop</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python自定义包的层级引用</title>
    <link href="/2024/04/24/tiankeng3/"/>
    <url>/2024/04/24/tiankeng3/</url>
    
    <content type="html"><![CDATA[<h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><p>今天debug的时候自定义了一个函数，使用了start主函数来引用processing函数，processing函数引用了同级文件夹中的python文件中的dataset函数，在运行processing的时候，test是通过的，但是在使用start函数来调用processing函数，processing函数函数调用dataset函数时就出现了报错，提示找不到这个包。（注：这里需要指明的是start函数放置在根文件夹中，processing函数放置在processing文件夹中）问题就在于python文件的文件运行路径的出错。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs stylus">│  start<span class="hljs-selector-class">.py</span><br>│<br>├─static<br>│  │  __init__<span class="hljs-selector-class">.py</span><br>│  │<br>│  ├─model<br>│  │      MLPForMer<span class="hljs-selector-class">.pth</span><br>│  │<br>│  ├─processing<br>│  │  │  dataset<span class="hljs-selector-class">.py</span><br>│  │  │  net<span class="hljs-selector-class">.py</span><br>│  │  │  processing<span class="hljs-selector-class">.py</span><br>│  │  │  __init__<span class="hljs-selector-class">.py</span><br>│  │  │<br>│  │  └─__pycache__<br>│  │          dataset<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │          net<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │          processing<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │          __init__<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │<br>│  ├─result<br>│  │      average_probabilities<span class="hljs-selector-class">.csv</span><br>│  │      average_probabilities<span class="hljs-selector-class">.png</span><br>│  │<br>│  ├─tmp<br>│  │      <span class="hljs-number">1</span><span class="hljs-selector-class">.edf</span><br>│  │<br>│  └─__pycache__<br>│          __init__<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│<br>└─templates<br>        upload.html<br></code></pre></td></tr></table></figure><h2 id="填坑"><a href="#填坑" class="headerlink" title="填坑"></a>填坑</h2><p>对待这种问题目前我知道的有两种方法</p><ol><li>第一种方法在processing文件中明确的所以绝对引用的方法,因为问题是出现在processing中的。</li></ol><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> <span class="hljs-keyword">static</span>.processing.net <span class="hljs-keyword">import</span> * <span class="hljs-comment"># 这里引用是相对于start函数的位置</span><br><br></code></pre></td></tr></table></figure><ol start="2"><li>第二种方法，在__init__文件中给出直接引用<br>1.相对引用package需要采用from 相对位置 import package_name的方式。因为相对位置只能写在from和import中间。<br>2.from . import * 只会检索当前目录下的module，而不会导入package。</li></ol><h3 id="挖坑-1"><a href="#挖坑-1" class="headerlink" title="挖坑"></a>挖坑</h3><h3 id="windown怎么打印树状图？"><a href="#windown怎么打印树状图？" class="headerlink" title="windown怎么打印树状图？"></a>windown怎么打印树状图？</h3><p>使用<code>tree</code>来打印文件夹<br>使用<code>tree /f</code>来打印文件目录，如上面的文件目录结构。</p><h3 id="init-文件的作用是什么？"><a href="#init-文件的作用是什么？" class="headerlink" title="__init__文件的作用是什么？"></a>__init__文件的作用是什么？</h3><p>作为包的标识：</p><ol><li>当一个目录包含__init__.py文件时，Python会将该目录视为一个包，而不仅仅是一个普通的目录。这使得包内的模块可以被正确导入和使用。</li><li><strong>init</strong>.py文件可以是一个空文件，也可以包含初始化包的代码，比如设置包的属性、导入子模块等。</li></ol><p>初始化包：</p><ol><li>在包被导入时，<strong>init</strong>.py文件会在包内的其他模块之前被执行。这使得可以在__init__.py中执行一些初始化操作，比如设置包级别的变量、执行必要的初始化代码等。</li><li>这也可以用于在导入包时自动执行一些操作，比如注册插件、加载配置等。·</li></ol><h2 id="填坑-1"><a href="#填坑-1" class="headerlink" title="填坑"></a>填坑</h2><h3 id="居中显示"><a href="#居中显示" class="headerlink" title="居中显示"></a>居中显示</h3><p>可以使用center标签，或者使用div标签，或者使用p标签，或者h标签都是可以的</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">center</span>&gt;</span> <span class="hljs-tag">&lt;&gt;</span>数据结构和算法是居中展示，使用center标签<span class="hljs-tag">&lt;/<span class="hljs-name">center</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">center</span>&gt;</span>数据结构和算法是居中展示，使用div标签<span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">align</span>=<span class="hljs-string">&quot;center&quot;</span>&gt;</span>数据结构和算法是居中展示，使用p标签<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">h5</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;text-align:center&quot;</span>&gt;</span>数据结构和算法是居中展示，使用h标签<span class="hljs-tag">&lt;/<span class="hljs-name">h5</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="给改文字大小"><a href="#给改文字大小" class="headerlink" title="给改文字大小"></a>给改文字大小</h3><p>使用font标签，字体使用face，颜色使用color，尺寸使用size。<br>颜色可以使用字母比如red，black，blue，yellow等，也可以是十六进制表示比如#0000ff或者#F025AB等等<br>size 是从1到7，数字越小字体越小，浏览器默认是3<br>这几个属性可以都设置，也可以只设置其中的1到2个</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">&lt;font <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;黑体&quot;</span>&gt;我是黑体字体&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;微软雅黑&quot;</span>&gt;我是微软雅黑字体&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;STCAIYUN&quot;</span>&gt;我是华文彩字体云&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">color</span>=red <span class="hljs-attribute">size</span>=3 <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;黑体&quot;</span>&gt;我是红色，黑色字体，大小是3&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">color</span>=#F025AB <span class="hljs-attribute">size</span>=5&gt;我的颜色是#F025AB，大小是5&lt;/font&gt;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔10</title>
    <link href="/2024/04/24/ganwu10/"/>
    <url>/2024/04/24/ganwu10/</url>
    
    <content type="html"><![CDATA[<h3 id="怎么才能快乐？"><a href="#怎么才能快乐？" class="headerlink" title="怎么才能快乐？"></a>怎么才能快乐？</h3><p>不以物喜，不以己悲。因为外物而带来的快乐是会失去的，喜悦要来源于自己的内心。对我而言，战胜一个又一个困难的过程是有意思的，可能这件事我不是很感兴趣，但是令我开心的是解决问题的过程。我知道自己现在无法解决这个问题，但是慢慢的去做，在做的过程中我发现自己爱上了这个感觉，爱上了解决问题的过程，就像米哈里所说的心流状态，即使是一点点进步我就会产生一点点发自内心的喜悦。</p><p>一呼一吸，一言一行。花开花落，云卷云舒。感受过程，提升自己。一句我从小听到的话，一句很普通的话，隐含着巨大的道理——“每天进步一点点”,这句话在我曾经就读的小学校园的门口就能看到。进步是令人快乐的，这种快乐不是来源于外物，而是来源于自己的内心。胡适先生为“中国科学社”写社歌，最后几句歌词就是:我们唱天行有常，我们唱致知穷理。怕什么真理无穷，进一寸有一寸的欢喜。1934年，他写《“九·一八”的第三周年纪念告全国的青年》。其中说:“努力一分，就有一分的效果。努力百分，就有百分的效果。”</p><p>以勇气来迎接人生的每一个挑战。这个挑战不一定很宏大，可能它就是今天我要8点起床，晚上11点睡觉，可能就是我今天要做一个俯卧撑，跑一圈操场，一个普普通通的挑战。改变总是开始于微小的，即使是写一个project也是从新建文件开始的。积极向上，把每一次挑战看作一次进步的机会，即使失败了又有什么问题，我想这个过程中一定是快乐的。苦难不值得被歌颂，认清苦难的现实和战胜苦难的勇气才值得被歌颂。</p><p>踏上自我成长的道路，每一个过程都是令人快乐的。</p><h3 id="今天冲浪的感受。"><a href="#今天冲浪的感受。" class="headerlink" title="今天冲浪的感受。"></a>今天冲浪的感受。</h3>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像处理-数据预处理</title>
    <link href="/2024/04/23/deeplearnbook3/"/>
    <url>/2024/04/23/deeplearnbook3/</url>
    
    <content type="html"><![CDATA[<h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><p>在深度学习中，图像数据通常以多维数组（在Python中通常使用Numpy数组）的形式表示，这个数组的形状（shape）取决于图像的维度和颜色通道数。<br>灰度图像：对于灰度图像（也就是黑白图像），shape通常是两维的，表示图像的高度和宽度。例如，一个256x256像素的灰度图像的shape将是(256, 256)。灰度图像的像素值通常在0到255之间，其中0表示黑色，255表示白色，中间的值表示不同的灰度级别。这是因为每个像素通常由8位（一个字节）表示，所以可以有256（即$2^8$）个不同的可能值。然而，这并不是唯一的表示方式。有时，为了方便计算，我们可能会将像素值归一化到0到1之间。在这种情况下，0仍然表示黑色，1表示白色，中间的值表示不同的灰度级别。<br>彩色图像：对于彩色图像，通常使用RGB（红，绿，蓝）三个颜色通道，所以shape是三维的。例如，一个256x256像素的RGB彩色图像的shape将是(256, 256, 3)。这里的3代表三个颜色通道。彩色图像通常由三个颜色通道组成：红色（R），绿色（G）和蓝色（B）。每个通道的像素值通常在0到255之间，其中0表示该颜色的完全缺失，255表示该颜色的最大强度。所以，一个RGB颜色图像的像素值范围在理论上是0到255的三维空间，即(0,0,0)到(255,255,255)。同样，有时我们也会将每个颜色通道的像素值归一化到0到1之间。在这种情况下，(0,0,0)表示黑色，(1,1,1)表示白色，其他值表示不同的颜色。需要注意的是，虽然RGB是最常用的颜色空间，但也有其他的颜色空间，如HSV（色相，饱和度，亮度）或者CMYK（青色，品红，黄色，黑色），它们的取值范围可能会有所不同。<br>图像批量：在深度学习中，我们通常会一次处理多个图像，这就是所谓的批量（batch）。在这种情况下，图像数据的shape将是四维的：(批量大小, 高度, 宽度, 颜色通道数)。例如，如果我们有32个256x256像素的RGB图像，那么这个批量的shape将是(32, 256, 256, 3)。</p><h2 id="显示彩色图像"><a href="#显示彩色图像" class="headerlink" title="显示彩色图像"></a>显示彩色图像</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> cv2 # opencv中按BGR排布，蓝绿红<br><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">img</span> = np.zeros((<span class="hljs-number">5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>),dtype=np.uint8)<br><br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>) <br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>) #中间的白色区块。 <br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">4</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>)<br><br><span class="hljs-attribute">cv2</span>.namedWindow(&#x27;img&#x27;,cv2.WINDOW_NORMAL)<br><span class="hljs-attribute">cv2</span>.resizeWindow(&#x27;img&#x27;,<span class="hljs-number">500</span>,<span class="hljs-number">500</span>)<br><span class="hljs-attribute">cv2</span>.imshow(&#x27;img&#x27;,img)<br><span class="hljs-attribute">cv2</span>.waitKey()<br><span class="hljs-attribute">cv2</span>.destroyAllWindows()<br></code></pre></td></tr></table></figure><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><ol><li>二值化处理：这是最基本的阈值处理方法。对于每个像素，我们选择一个阈值。如果像素值大于阈值，我们将其设置为一个值（通常是白色），如果像素值小于或等于阈值，我们将其设置为另一个值（通常是黑色）。这样我们就得到了一个二值图像。</li><li>反二值化处理：这是二值化处理的反向操作。如果像素值大于阈值，我们将其设置为一个值（通常是黑色），如果像素值小于或等于阈值，我们将其设置为另一个值（通常是白色）。</li><li>截断阈值处理：对于每个像素，如果其值大于阈值，我们将其设置为阈值。如果像素值小于或等于阈值，我们保持其原值不变。</li><li>超阈值零处理：对于每个像素，如果其值大于阈值，我们保持其原值不变。如果像素值小于或等于阈值，我们将其设置为零。</li><li>低阈值零处理：这是超阈值零处理的反向操作。如果像素值大于阈值，我们将其设置为零。如果像素值小于或等于阈值，我们保持其原值不变。</li><li>自适应阈值处理：这是一种更复杂的方法，它不使用固定的阈值。相反，它根据像素周围的小区域计算阈值。因此，对于同一张图片上的不同区域，我们可以有不同的阈值。这对于当图像的光照条件变化很大时，例如，一半是明亮的，一半是暗淡的图像，非常有用。</li></ol><h2 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h2><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs smali">import cv2<br><br><span class="hljs-comment"># 读取图像</span><br>image = cv2.imread(&#x27;e1.jpg&#x27;, cv2.IMREAD_GRAYSCALE)<br><br><span class="hljs-comment"># 二值化处理</span><br>_, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)<br><br><span class="hljs-comment"># 反二值化处理</span><br>_, binary_inv_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)<br><br><span class="hljs-comment"># 截断阈值处理</span><br>_, trunc_image = cv2.threshold(image, 127, 255, cv2.THRESH_TRUNC)<br><br><span class="hljs-comment"># 超阈值零处理</span><br>_, tozero_inv_image = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO_INV)<br><br><span class="hljs-comment"># 低阈值零处理</span><br>_, tozero_image = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO)<br><br><span class="hljs-comment"># 自适应阈值处理</span><br>adaptive_image = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)<br><br><span class="hljs-comment"># 保存处理后的图像</span><br>cv2.imwrite(&#x27;binary_image.jpg&#x27;, binary_image)<br>cv2.imwrite(&#x27;binary_inv_image.jpg&#x27;, binary_inv_image)<br>cv2.imwrite(&#x27;trunc_image.jpg&#x27;, trunc_image)<br>cv2.imwrite(&#x27;tozero_inv_image.jpg&#x27;, tozero_inv_image)<br>cv2.imwrite(&#x27;tozero_image.jpg&#x27;, tozero_image)<br>cv2.imwrite(&#x27;adaptive_image.jpg&#x27;, adaptive_image)<br><br></code></pre></td></tr></table></figure><p><img src="/pic/e1.jpg" alt="原图"><br><img src="/pic/binary_image.jpg" alt="二值化处理图像"><br><img src="/pic/binary_inv_image.jpg" alt="反二值化处理图像"><br><img src="/pic/trunc_image.jpg" alt="截断阈值处理图像"><br><img src="/pic/tozero_inv_image.jpg" alt="超阈值处理图像"><br><img src="/pic/tozero_image.jpg" alt="低阈值零处理图像"><br><img src="/pic/adaptive_image.jpg" alt="自适应阈值处理图像"></p><h3 id="挖更大的坑，opencv库。"><a href="#挖更大的坑，opencv库。" class="headerlink" title="挖更大的坑，opencv库。"></a>挖更大的坑，opencv库。</h3><h3 id="彩色图像怎么转换为二维图像的？"><a href="#彩色图像怎么转换为二维图像的？" class="headerlink" title="彩色图像怎么转换为二维图像的？"></a>彩色图像怎么转换为二维图像的？</h3><p>首先灰度图像中的一个像素点的范围为0-255，彩色图像可以理解为3个灰度图重合。</p><h3 id="需要深度解析代码中的含义，比如一个参数有什么用处。"><a href="#需要深度解析代码中的含义，比如一个参数有什么用处。" class="headerlink" title="需要深度解析代码中的含义，比如一个参数有什么用处。"></a>需要深度解析代码中的含义，比如一个参数有什么用处。</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑——经典网络结构——AlexNet</title>
    <link href="/2024/04/23/tiankeng2/"/>
    <url>/2024/04/23/tiankeng2/</url>
    
    <content type="html"><![CDATA[<h2 id="问题1，卷积是什么？作用什么？"><a href="#问题1，卷积是什么？作用什么？" class="headerlink" title="问题1，卷积是什么？作用什么？"></a>问题1，卷积是什么？作用什么？</h2><p>卷积（Convolution）是一种数学运算，常用于信号处理和图像处理领域。在信号处理中，卷积用于将输入信号与卷积核（也称为滤波器）进行运算，产生输出信号。<br>卷积的作用有以下几个方面：</p><ol><li>信号滤波：卷积可以用于信号滤波，通过将输入信号与合适的卷积核进行卷积运算，可以实现对信号的滤波操作。滤波可以用于去除信号中的噪声、平滑信号、强调信号中的某些频率成分等。</li><li>特征提取：在图像处理中，卷积可以用于特征提取。通过将图像与不同的卷积核进行卷积运算，可以提取出图像中的不同特征，例如边缘、纹理、角点等。这些特征可以用于图像识别、目标检测和图像处理中的其他任务。</li><li>信号压缩：卷积可以用于信号压缩。通过将输入信号与适当的卷积核进行卷积运算，可以将信号表示转换为另一种表示形式，通常具有更紧凑的表示。这种表示形式可以用于信号压缩和数据压缩。</li><li>卷积神经网络：卷积神经网络（Convolutional Neural Network，CNN）是一种基于卷积运算的深度学习模型，广泛应用于图像识别、计算机视觉和自然语言处理等领域。卷积在 CNN 中用于提取图像或文本的特征，并通过多层卷积和池化操作来实现对输入数据的高级表示和分类。如果输入数据为图片，那么卷积层的作用就是提取图片中的信息，这些信息被称为图像特征，这些特征是由图像中的每个像素通过组合或者独立的方式所体现，比如图片的纹理特征、颜色特征、空间特征。</li></ol><p>关于卷积其实还有很多问题，比如说输入一张（3x255x255）的图片，输入后经过卷积后输出的特征图大小为什么shape。1x1卷积为什么可以实现升维和降维。）</p><h2 id="问题2，池化是什么？作用是什么？"><a href="#问题2，池化是什么？作用是什么？" class="headerlink" title="问题2，池化是什么？作用是什么？"></a>问题2，池化是什么？作用是什么？</h2><p>池化（Pooling）是一种常用的操作，通常与卷积神经网络（CNN）结合使用。池化操作通过对输入数据的局部区域进行聚合或采样来减小数据的空间尺寸，从而减少参数数量、降低计算量，并提取出输入数据的重要特征。</p><p>池化的作用有以下几个方面</p><ol><li>降采样：池化操作可以减小输入数据的空间尺寸，从而降低后续层的计算复杂度。通过降低数据的维度，池化可以在保留重要特征的同时减少冗余信息，提高计算效率。</li><li>平移不变性：池化操作具有一定的平移不变性。在图像处理中，通过对局部区域进行池化操作，可以使得输入图像在平移、旋转和缩放等变换下具有一定的不变性。这对于图像识别和目标检测等任务是有益的。</li><li>特征提取：池化操作可以提取输入数据的重要特征。通过对局部区域进行池化，池化操作会选择区域中的最大值（最大池化）或平均值（平均池化）作为输出值，从而提取出输入数据的显著特征。这有助于减少数据的维度，并保留重要的特征信息。</li><li>减少过拟合：池化操作可以在一定程度上减少过拟合。通过减小数据的空间尺寸，池化操作可以降低模型的参数数量，从而减少过拟合的风险。此外，池化操作还可以通过丢弃一些冗余信息来提高模型的泛化能力。</li></ol><p>池化的种类</p><ol><li>最大池化（Max Pooling）：最大池化是一种常见的池化操作。在最大池化中，输入数据的局部区域被分割成不重叠的块，然后在每个块中选择最大值作为输出。最大池化可以提取出输入数据的显著特征，同时减小数据的空间尺寸。</li><li>平均池化（Average Pooling）：平均池化是另一种常见的池化操作。在平均池化中，输入数据的局部区域被分割成不重叠的块，然后计算每个块中元素的平均值作为输出。平均池化可以平滑输入数据并减小数据的空间尺寸。</li><li>自适应池化（Adaptive Pooling）：自适应池化是一种具有灵活性的池化操作。与最大池化和平均池化不同，自适应池化不需要指定池化窗口的大小，而是根据输入数据的尺寸自动调整池化窗口的大小。这使得自适应池化可以适应不同尺寸的输入数据。</li><li>全局池化（Global Pooling）：全局池化是一种特殊的池化操作，它将整个输入数据的空间尺寸缩减为一个单一的值或向量。全局池化可以通过对输入数据的所有位置进行池化操作，从而提取出输入数据的全局特征。常见的全局池化有全局平均池化（Global Average Pooling）和全局最大池化（Global Max Pooling）。</li></ol><h2 id="问题3，全连接是什么？作用是什么？"><a href="#问题3，全连接是什么？作用是什么？" class="headerlink" title="问题3，全连接是什么？作用是什么？"></a>问题3，全连接是什么？作用是什么？</h2><h2 id="问题4，AlexNet论文使用的loss函数是什么？"><a href="#问题4，AlexNet论文使用的loss函数是什么？" class="headerlink" title="问题4，AlexNet论文使用的loss函数是什么？"></a>问题4，AlexNet论文使用的loss函数是什么？</h2><h2 id="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"><a href="#问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？" class="headerlink" title="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"></a>问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？</h2><h2 id="问题6，AlexNet论文中使用的评价指标是什么？"><a href="#问题6，AlexNet论文中使用的评价指标是什么？" class="headerlink" title="问题6，AlexNet论文中使用的评价指标是什么？"></a>问题6，AlexNet论文中使用的评价指标是什么？</h2><h2 id="问题7，AlexNet中的创新点是什么？"><a href="#问题7，AlexNet中的创新点是什么？" class="headerlink" title="问题7，AlexNet中的创新点是什么？"></a>问题7，AlexNet中的创新点是什么？</h2><ol><li>ReLU激活函数的引入，采样非线性单元（ReLU）的深度卷积神经网络训练时间要比tanh单元要快几倍。而时间开销是进行模型训练过程中的很重要的因数。同时ReLU有效的防止了过拟合的现象。</li><li>层叠池化操作，以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize&#x3D;2×2，stride&#x3D;2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize &gt; stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。</li><li>Dropout操作， Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。</li><li>网络层数更深，与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。当然，这种思考的重要节点出现在VGG网络（下一篇博文VGG论文中将会讲到）。</li></ol><h2 id="问题8，优化函数的具体实现是什么？"><a href="#问题8，优化函数的具体实现是什么？" class="headerlink" title="问题8，优化函数的具体实现是什么？"></a>问题8，优化函数的具体实现是什么？</h2><h2 id="问题9，关于卷积后特征图应该怎么计算？"><a href="#问题9，关于卷积后特征图应该怎么计算？" class="headerlink" title="问题9，关于卷积后特征图应该怎么计算？"></a>问题9，关于卷积后特征图应该怎么计算？</h2><h2 id="问题10，什么是过拟合合和欠拟合？"><a href="#问题10，什么是过拟合合和欠拟合？" class="headerlink" title="问题10，什么是过拟合合和欠拟合？"></a>问题10，什么是过拟合合和欠拟合？</h2><h2 id="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"><a href="#问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？" class="headerlink" title="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"></a>问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？</h2><h2 id="问题12，-神经元数量和参数量的计算方法是什么？"><a href="#问题12，-神经元数量和参数量的计算方法是什么？" class="headerlink" title="问题12， 神经元数量和参数量的计算方法是什么？"></a>问题12， 神经元数量和参数量的计算方法是什么？</h2><h2 id="问题13，-softMax的机制是怎么样的？"><a href="#问题13，-softMax的机制是怎么样的？" class="headerlink" title="问题13， softMax的机制是怎么样的？"></a>问题13， softMax的机制是怎么样的？</h2><h2 id="问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？"><a href="#问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？" class="headerlink" title="问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？"></a>问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？</h2><h2 id="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？"><a href="#问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？" class="headerlink" title="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？"></a>问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？</h2><h2 id="问题16，-什么是超参数？"><a href="#问题16，-什么是超参数？" class="headerlink" title="问题16， 什么是超参数？"></a>问题16， 什么是超参数？</h2><h2 id="问题17，-什么是监督学习和无监督学习，半监督学习？"><a href="#问题17，-什么是监督学习和无监督学习，半监督学习？" class="headerlink" title="问题17， 什么是监督学习和无监督学习，半监督学习？"></a>问题17， 什么是监督学习和无监督学习，半监督学习？</h2>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑-PyTorch基础——Numpy</title>
    <link href="/2024/04/23/tiankeng1/"/>
    <url>/2024/04/23/tiankeng1/</url>
    
    <content type="html"><![CDATA[<h3 id="向量和数组之间的关系是什么？向量的定义是什么？"><a href="#向量和数组之间的关系是什么？向量的定义是什么？" class="headerlink" title="向量和数组之间的关系是什么？向量的定义是什么？"></a>向量和数组之间的关系是什么？向量的定义是什么？</h3><p>在数学科物理中，向量被定义为具有大小和方向量。例如速度是一个向量，因为它不仅有大小（数独），还有方向（行进的方向）。<br>数组是编程中的一种基本数据结构，用于存储一组有序的元素。这些元素可以是任何类型，如整形、浮点数、字符串等。<br>标量（scalar）是零维只有大小，没有方向的量，如1，2，3<br>向量（Vector）是一维只有大小和方向的量，如（1，2）。（计算方向的公式为：）<br>矩阵（Matrix）是二维的向量，[[1, 2], [2, 3]]<br>张量（Tensor） 按照任意维排列的一堆数字的推广。矩阵不过是三维张量下的一个二维切面。要在三维张量下找到零维张量需要三个维度的坐标来定位。（注：张量可以是多维的）</p><h3 id="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"><a href="#矩阵是什么，作用是什么？如何实现矩阵的加减乘除" class="headerlink" title="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"></a>矩阵是什么，作用是什么？如何实现矩阵的加减乘除</h3><ol><li>矩阵是一个二维数组，由行和列的元素组成。在数学中，矩阵通常用大写字母表示，如 A，B 等，矩阵中的元素通常用小写字母表示，如aij​，表示矩阵 A 的第 i 行第 j 列的元素。</li><li>矩阵可以用来表示线性变换，解决线性方程组，或者表示图形的变换。在数据科学和机器学习中，矩阵通常用于存储和操作大量的数据。</li></ol><h4 id="实现矩阵的加减乘除。"><a href="#实现矩阵的加减乘除。" class="headerlink" title="实现矩阵的加减乘除。"></a>实现矩阵的加减乘除。</h4><p>加法：两个矩阵相加，只有在它们的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相加的结果。例如，如果A &#x3D; aij 和B &#x3D; bij 是同样大小的矩阵，那么它们的和C &#x3D; [ cij ]是矩阵 ,其中cij &#x3D; aij + bij。对应相加<br>减法：矩阵的减法与加法类似，只有在两个矩阵的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相减的结果。<br>乘法：矩阵的乘法比较复杂。如果A 是一个 m×n 的矩阵，B 是一个n×p 的矩阵，那么它们的乘积 AB 是一个 m×p 的矩阵，其元素由A 的行和 B 的列的对应元素的乘积之和给出。<br>除法：在矩阵中，通常不直接定义除法。但是，我们可以通过乘以逆矩阵来实现类似的效果。如果A是一个可逆的（也就是说，存在一个矩阵 （A-1）使得，A（A-1） &#x3D; （A-1）A &#x3D; I其中 𝐼I 是单位矩阵），那么我们可以定义B&#x2F;A为（BA-1），即是B矩阵除以A矩阵等于B乘以A矩阵的转置。但是，请注意，不是所有的矩阵都是可逆的。 </p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs makefile">import numpy as np<br><br><span class="hljs-comment"># 创建两个矩阵</span><br>A = np.array([[1, 2], [3, 4]])<br>B = np.array([[5, 6], [7, 8]])<br><br><span class="hljs-comment"># 矩阵加法</span><br>C = A + B<br><br><span class="hljs-comment"># 矩阵减法</span><br>D = A - B<br><br><span class="hljs-comment"># 矩阵乘法</span><br>E = np.dot(A, B)<br><br><span class="hljs-comment"># 矩阵除法（通过乘以逆矩阵）</span><br>F = np.dot(A, np.linalg.inv(B)) <br><br></code></pre></td></tr></table></figure><h3 id="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"><a href="#傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）" class="headerlink" title="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"></a>傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）</h3><h4 id="基本介绍。"><a href="#基本介绍。" class="headerlink" title="基本介绍。"></a>基本介绍。</h4><p>傅里叶变换是一种在数学、物理和工程中广泛使用的数学变换，它可以将一个函数或信号从其原始的时间或空间表示转换为频率表示。这对于许多应用都非常有用，因为它可以揭示信号的频率成分，这在原始的时间或空间表示中可能不明显。<br>傅里叶变换的基本思想是，任何函数都可以表示为一系列正弦波和余弦波的叠加。换句话说，我们可以将一个复杂的信号分解为一系列更简单的正弦波和余弦波。</p><h4 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h4><p>傅里叶变换的基本原理是将一个函数或信号从其原始的时间或空间表示转换为频率表示。这是通过将函数表示为一系列正弦波和余弦波的叠加来实现的。<br><img src="/pic/fly1.jpg" alt="傅里叶变换示意图"></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># 创建一个简单的信号</span><br><span class="hljs-attribute">t</span> = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">500</span>)<br><span class="hljs-attribute">f</span> = np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">50</span> * t) + <span class="hljs-number">0</span>.<span class="hljs-number">5</span> * np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">120</span> * t)<br><br><span class="hljs-comment"># 绘制原始信号</span><br><span class="hljs-attribute">plt</span>.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-attribute">plt</span>.plot(t, f)<br><span class="hljs-attribute">plt</span>.title(&#x27;Original Signal&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Time&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Amplitude&#x27;)<br><br><span class="hljs-comment"># 计算傅里叶变换</span><br><span class="hljs-attribute">F</span> = np.fft.fft(f)<br><br><span class="hljs-comment"># 计算频率</span><br><span class="hljs-attribute">freq</span> = np.fft.fftfreq(t.shape[-<span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 绘制频谱</span><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-attribute">plt</span>.plot(freq, np.abs(F))<br><span class="hljs-attribute">plt</span>.title(&#x27;Frequency Spectrum&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Frequency&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Magnitude&#x27;)<br><br><span class="hljs-attribute">plt</span>.tight_layout()<br><span class="hljs-attribute">plt</span>.show()<br><br></code></pre></td></tr></table></figure><h3 id="什么是对象？-封装，继承，多态是什么？"><a href="#什么是对象？-封装，继承，多态是什么？" class="headerlink" title="什么是对象？ 封装，继承，多态是什么？"></a>什么是对象？ 封装，继承，多态是什么？</h3><p>什么是对象？<br>在面向对象编程（Object-Oriented Programming，OOP）中，对象是类的实例。类是一种抽象的概念，用于描述具有相似属性和行为的对象的集合。对象是类的具体实现，它具有类定义的属性和方法。<br>对象可以看作是现实世界中的实体或概念在程序中的表示。每个对象都有自己的状态（属性）和行为（方法），并且可以与其他对象进行交互。</p><p>封装<br>封装是面向对象编程的一种重要概念，它将数据和操作数据的方法捆绑在一起，形成一个称为类的单个实体。封装隐藏了数据的内部实现细节，只暴露对外部可见的接口。这样可以保护数据的完整性，并提供更好的代码组织和维护性。<br>通过封装，对象的内部状态可以被保护起来，只能通过公共接口进行访问和修改。这样可以防止对数据的不合理访问和修改，增加了代码的安全性和可靠性。</p><p>继承<br>继承是面向对象编程中的另一个重要概念，它允许一个类继承另一个类的属性和方法。继承创建了一个类的层次结构，其中一个类（称为子类或派生类）可以从另一个类（称为父类或基类）继承属性和方法。<br>通过继承，子类可以继承父类的特性，并且可以添加自己的特定特性。这样可以实现代码的重用和扩展，减少了重复编写代码的工作量。</p><p>多态<br>多态是面向对象编程中的另一个重要概念，它允许使用统一的接口来处理不同的对象类型。多态性允许同一个方法在不同的对象上产生不同的行为。<br>通过多态，可以编写通用的代码，可以处理多个不同类型的对象，而无需针对每种类型编写特定的代码。这提高了代码的灵活性和可扩展性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 封装示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model</span>):<br>        self.brand = brand<br>        self.model = model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 Car 对象并访问其信息</span><br>my_car = Car(<span class="hljs-string">&quot;Toyota&quot;</span>, <span class="hljs-string">&quot;Corolla&quot;</span>)<br>my_car.display_info()<br><br><span class="hljs-comment"># 继承示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ElectricCar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model, battery_capacity</span>):<br>        <span class="hljs-built_in">super</span>().__init__(brand, model)<br>        self.battery_capacity = battery_capacity<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Electric Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>, Battery Capacity: <span class="hljs-subst">&#123;self.battery_capacity&#125;</span> kWh&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 ElectricCar 对象并访问其信息</span><br>my_electric_car = ElectricCar(<span class="hljs-string">&quot;Tesla&quot;</span>, <span class="hljs-string">&quot;Model S&quot;</span>, <span class="hljs-number">100</span>)<br>my_electric_car.display_info()<br><br><span class="hljs-comment"># 多态示例</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_car_info</span>(<span class="hljs-params">car</span>):<br>    car.display_info()<br><br><span class="hljs-comment"># 使用 show_car_info 函数展示不同类型的车辆信息</span><br>show_car_info(my_car)<br>show_car_info(my_electric_car)<br><br></code></pre></td></tr></table></figure><h3 id="python中的不同代码高亮表示什么？"><a href="#python中的不同代码高亮表示什么？" class="headerlink" title="python中的不同代码高亮表示什么？"></a>python中的不同代码高亮表示什么？</h3><p>在Python的IDLE编程环境中，不同颜色的文本表示不同的含义。以下是IDLE中常见的颜色及其含义：<br>黑色：普通的代码文本。<br>蓝色：关键字，例如if、else、for、while等。<br>绿色：字符串文本。<br>红色：语法错误或代码中的错误。<br>紫色：函数和方法的名称。<br>棕色：数字。<br>橙色：内置函数和模块的名称。<br>灰色：注释。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——VGG</title>
    <link href="/2024/04/22/deeplearnpaper2/"/>
    <url>/2024/04/22/deeplearnpaper2/</url>
    
    <content type="html"><![CDATA[<h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="使用VGG来实现垃圾的40分类"><a href="#使用VGG来实现垃圾的40分类" class="headerlink" title="使用VGG来实现垃圾的40分类"></a>使用VGG来实现垃圾的40分类</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑博客总目录</title>
    <link href="/2024/04/22/tiankeng/"/>
    <url>/2024/04/22/tiankeng/</url>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="PyTorch基础——Numpy"><a href="#PyTorch基础——Numpy" class="headerlink" title="PyTorch基础——Numpy"></a>PyTorch基础——Numpy</h2><h2 id="经典网络结构——AlexNet"><a href="#经典网络结构——AlexNet" class="headerlink" title="经典网络结构——AlexNet"></a>经典网络结构——AlexNet</h2>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔9</title>
    <link href="/2024/04/22/ganwu9/"/>
    <url>/2024/04/22/ganwu9/</url>
    
    <content type="html"><![CDATA[<h1 id="夜晚思考"><a href="#夜晚思考" class="headerlink" title="夜晚思考"></a>夜晚思考</h1><p>作为一个大学生，住在寝室是很正常的。2024&#x2F;4&#x2F;21的夜晚不是很寻常，床下的键盘声和电脑的光亮让我难以入睡。思绪浮想联翩，世界毁灭了，我要毁灭了。情绪在波动，心脏在疼痛。我该怎么去停止这键盘声和光亮从而让我安静的入眠。<br>人总是以为自己是站在道德的高点，很不幸的告诉我自己，当自以为在道德高点时，我其实已经没有了道德。以自己最大的恶意去揣测他人，已经不道德了。键盘声和光亮真的不能让我入睡吗？还是自己不让自己睡觉，自己的情绪，自己禁锢自己。意识和情绪不是一体的，控制自己的情绪。<br>我总是有两个自己，一个以恶意揣测别人，一个则想怎么去解决这个问题。逃离，争吵，苦恼，毁灭世界。思想斗争吧，预演所有情况吧，一个小时后，问题不能被解决，反而越来越难受。沟通一下吧。起身，正坐。问道：“兄弟，你有什么重要的事情需要去完成吗？” 答曰：“作业没有写完，正在写作业。”听之，甚觉羞愧，焕然冰释。<br>自己禁锢自己，被情绪裹挟，人啊人啊。<br><img src="/pic/ganwu9.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔8</title>
    <link href="/2024/04/21/ganwu8/"/>
    <url>/2024/04/21/ganwu8/</url>
    
    <content type="html"><![CDATA[<h1 id="做事中什么最重要？"><a href="#做事中什么最重要？" class="headerlink" title="做事中什么最重要？"></a>做事中什么最重要？</h1><p>明白自己在干什么，明白此时此刻我在干什么。为什么说这是最重要的?拿做数学题来讲吧，要做一道数学题，必须先看题，了解题目的内容，获取前提条件。获取了前提条件之后，有两种可能，会做和不会做，会做的过程中，最好的感觉是，看了一眼题目之后就胸有成竹了，虽然不知道这道题的答案是什么，但是清楚这道题应该怎么去解答，应该在什么部分注意。这是最好的状态，很清晰的明白自己在做什么的状态。对于第二种，不知道该怎么解决的，首先得清楚自己不会做，然后去分析自己哪里不会，而不是自己不会就不会，这样的心态是大忌，不要傲慢，没有人生下来就会所有事情，都是从不会到会的，对待自己会别人不会的题，不要傲慢，不要鄙视他人，因为我也是从不会到会的。了解自己哪里不会了之后，去找到解决方法，补足不会的点，然后就能解决这道数学题了。这是第二个情况的解决方法，在整个流程中，我都清晰自己在干什么，而不是迷迷糊糊的不知道自己在干什么。<br>再举一个例子，拿科研来讲，一篇论文需要注意什么？第一，创新点。第二，实验设计。第三，文章表述。这三点的重要性不分先后。我想的，我做的，我写出来的是三种东西。在这个过程中需要清晰的认识自己在做什么，如果很清晰的知道，胸有成竹的，就可以去做，如果是第二种情况，那就慢慢来补充自己欠缺的知识。需要说明的是，胸有成竹和存在不足的情况可能会周期交互，一段时间的胸有成竹和一段时间的不足。但是在解决不足的过程却是胸有成竹的，清楚的明白自己需要干什么。</p><h2 id="《胸有成竹》-苏轼"><a href="#《胸有成竹》-苏轼" class="headerlink" title="《胸有成竹》-苏轼"></a>《胸有成竹》-苏轼</h2><p>竹之始生，一寸之萌耳，而节叶具焉；自蜩蝮蛇蚹，以至于剑拔十寻者，生而有之也。<br>今画者乃节节而为之，叶叶而累之，岂复有竹乎？故画竹必先得成竹于胸中，执笔熟视，乃见其所欲画者，急起从之，振笔直遂，以追其所见，如兔起鹘落，少纵则逝矣。与可之教予如此。予不能然也，而心识其所以然。夫既心识其所以然，而不能然者，内外不一，心手不相应，不学之过也。故凡有见于中，而操之不熟者，平居自视了然，而临时忽焉丧之，岂独竹乎？子由为《墨竹赋》以遗与可曰：“庖丁，解牛者也，而养生者取之；轮扁，斫轮者也，而读书者与之。今夫夫子之托于斯竹也，而予以为有道者则非耶？”子由未尝画也，故得其意而已。若予者，岂独得其意，并得其法。</p><p>译文：竹子开始生出时，只是一寸高的萌芽而已，但节、叶都具备了。从蝉破壳而出、蛇长出鳞一样的状态，直至像剑拔出鞘一样长到八丈高，都是一生长出来就有的。如今画竹的人都是一节节地画它，一叶叶地堆积它，这样哪里还会有完整的、活生生的竹子呢？所以画竹必定要心里先有完整的竹子形象，拿起笔来仔细看去，就看到了自己所想画的竹子，急速起身跟住它，动手作画，一气呵成，以追上自己所见到的，如兔子跃起奔跑、隼俯冲下搏，稍一放松就消失了。与可告诉我的是如此。我不能做到这样，但心里明白这样做的道理。既然心里明白这样做的道理，但不能做到这样，是由于内外不一，心与手不相适应，没有学习的过错。所以凡是在心中有了构思，但是做起来不熟练的，平常自己认为很清楚，可事到临头忽然又忘记了，这种现象难道仅仅是画竹有吗？ 　子由写了篇《墨竹赋》，把它送给与可，说：“丁厨子，是杀牛的，但讲求养生的人从他的行动中悟出了道理；轮匠扁，是造车轮的，但读书的人赞成他讲的道理。如今您寄托意蕴在这幅竹画上，我认为您是深知道理的人，难道不是吗？”子由没有作过画，所以只得到了他的意蕴。象我这样的人，哪里仅仅是得到与可的意蕴，并且也得到了与可的方法。</p><p>自注： 我觉得苏轼这篇说说很好，但是需要补充的是，这里以做事的态度讨论，其他角度碍于自己的层次有限暂时不讨论，胸有成竹是最好的做事状态。竹子一开始是具备了节和叶，但是只是一寸高的萌芽的，要从一寸长的萌芽成长到高耸，需要很多条件都满足，竹子的成长也是一点点，一节一节来长得。很多时候做事都不是胸有成竹的状态， 一开始都是懵懂不知道的，只有在做的得心应手时才有胸有成竹的状态，在做事之前已作好充分准备，对事情的成功已有了十分的把握；最开始的竹子都是矮小的，高耸的竹子都是一点点成长的。苏轼说的没错的是故画竹必先得成竹于胸中，执笔熟视，乃见其所欲画者，急起从之，振笔直遂，以追其所见，如兔起鹘落，少纵则逝矣。但是他没有说的是这是大佬做事的境界，小白不都是从今画者乃节节而为之，叶叶而累之吗？<br>故凡有见于中，而操之不熟者，平居自视了然，而临时忽焉丧之，岂独竹乎？这句叫人不要傲慢，知行合一。<br>子由为《墨竹赋》以遗与可曰：“庖丁，解牛者也，而养生者取之；轮扁，斫轮者也，而读书者与之。今夫夫子之托于斯竹也，而予以为有道者则非耶？”子由未尝画也，故得其意而已。若予者，岂独得其意，并得其法。这句法则都是相同的，方法是法则的具体体现。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——AlexNet</title>
    <link href="/2024/04/21/deeplearnpaper/"/>
    <url>/2024/04/21/deeplearnpaper/</url>
    
    <content type="html"><![CDATA[<p>我给自己挖了很多坑没有去填，只能慢慢填了，今天先填第一个坑。<br><a href="https://blog.csdn.net/guzhao9901/article/details/118552085">本人参考博客1-</a><br><a href="https://zhuanlan.zhihu.com/p/618545757">本人参考博客2-</a><br><a href="https://blog.csdn.net/hongbin_xu/article/details/80271291">本人参考博客3-AlexNet的翻译</a><br><a href="https://blog.csdn.net/ARYAD/article/details/107687362">本人参考的博客-模型结构发展简史</a></p><h1 id="AlexNet-介绍"><a href="#AlexNet-介绍" class="headerlink" title="AlexNet 介绍"></a>AlexNet 介绍</h1><p><a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">论文原文链接</a><br>AlexNet是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年ImageNet图像分类竞赛中提出的一种经典的卷积神经网络。AlexNet在 ImageNet 大规模视觉识别竞赛中取得了优异的成绩，把深度学习模型在比赛中的正确率提升到一个前所未有的高度。因此，它的出现对深度学习发展具有里程碑式的意义。<br><a href="https://github.com/aaron-xichen/pytorch-playground">可以参考的github仓库</a></p><ol><li>AlexNet的输入为RGB三通道大小的图像，图像的shape可以表述为（227x227x3）。AlexNet共包含5个卷积层（包含3个池化）和3个全连接层。其中每个卷积层都包含卷积核、偏置项、ReLU激活函数和局部响应归一化（LRN）模块。第1，2，5个卷积层后面都跟着一个最大池化层，后三个层为全连接层。最终的输出层为softmax（这里有一个很有意思的知识，softmax怎么将网络输出转化为概率值，后面再说。）</li></ol><p><img src="/pic/paper_Alex_1.png" alt="AlexNet模型结构图"><br>这里需要指出的是，在网络设计上并非上图所示，上图包含了GPU通信的部分。这是因为当时的GPU内存的限制引起的，作者使用了两块GPU进行计算<br>废话不多说，直接上代码。代码来源为《动手深度学习》</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn, optim<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-meta"># 上面的部分为引入包操作，介绍一下上面引入的包的作用。</span><br><span class="hljs-meta"># time：这是python中的内置的模块，用于处理时间相关的操作。可以用来获取当前的时间，或者在程序中添加延迟。</span><br><span class="hljs-meta"># torch：这是pytorch库的主要部分，一个用于机器学习和深度学习的开源库。提高高效的张量（多维数组）计算（类似于Numpy）的方式，同时支持GPU计算（基于CUDA和CUDNN）</span><br><span class="hljs-meta"># torch.nn 是pytorch中的一个子模块，提供构建神经网络所需要的各种工具和组件。</span><br><span class="hljs-meta"># torch.optim也是pytorch中的一个子模块，提供各种优化算法，比如SGD，Adam和RMSProp等（这里给自己挖个坑）</span><br><span class="hljs-meta"># torch.torchvision，一个与PyTorch关联的库，专门用于处理图像和视频的计算机视觉任务。它提供许多预训练的模型，如ResNet，VGG和AlexNet等，同时还有常见的数据集，如ImageNet，CIFAR10/100，MNIST等。</span><br><br><span class="hljs-title">device</span> = torch.device(&#x27;cuda&#x27; <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> &#x27;cpu&#x27;)<br><span class="hljs-meta"># 这一句的作用是选取GPU训练还是CPU训练。</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">AlexNet</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">AlexNet</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.conv = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding（输出通道数，输出通道数，卷积核大小，步长，填充，这里又有坑，关于卷积后特征图应该怎么计算？）</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(), # <span class="hljs-type">ReLU</span>激活函数</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2), # kernel_size, stride 最大池化，3x3的池化层，步长为2.意思是一个3x3的二维矩阵，按照最大值来输出最大特征。</span><br><span class="hljs-class">            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(96, 256, 5, 1, 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2),</span><br><span class="hljs-class">            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span><br><span class="hljs-class">            # 前两个卷积层后不使用池化层来减小输入的高和宽</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(256, 384, 3, 1, 1), # 第三个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(384, 384, 3, 1, 1), # 第四个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(384, 256, 3, 1, 1), # 第五个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2)</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">         # 这里全连接层的输出个数比<span class="hljs-type">LeNet</span>中的大数倍。使用丢弃层来缓解过拟合</span><br><span class="hljs-class">        self.fc = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Linear</span>(256*5*5, 4096), # 线性层，256*5*5为输入大小，4096为输出大小。</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout</span>(0.5), # 随机失活，<span class="hljs-type">AlexNet</span>的主要创新点之一。这里失活率为0.5</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(4096, 4096),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout</span>(0.5),</span><br><span class="hljs-class">            # 输出层。由于这里使用<span class="hljs-type">Fashion</span>-<span class="hljs-type">MNIST</span>，所以用类别数为10，而非论文中的1000</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(4096, 10), # 输出类为10.</span><br><span class="hljs-class">        )</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-title">img</span>): # 前向传播，forward在代码中需要自定义。在这里可以加入残差等操作。</span><br><span class="hljs-class">        feature = self.conv(<span class="hljs-title">img</span>)</span><br><span class="hljs-class">        output = self.fc(<span class="hljs-title">feature</span>.<span class="hljs-title">view</span>(<span class="hljs-title">img</span>.<span class="hljs-title">shape</span>[0], -1))</span><br><span class="hljs-class">        return output</span><br></code></pre></td></tr></table></figure><ol start="2"><li>背景介绍，在AlexNet网络问世之前，大量的学者在进行图像分类、分割、识别的操作时，主要是通过对图像提取特征或特征+机器学习的方法，手工提取特征是非常难的事情，即特征工程。为了提升准确率或减少人工复杂度等种种原因。因此，学界一直认为，特征是不是可以进行学习？如果可以学习，特征之间的表示方法是什么？例如第一层为线或是点特征，第二层为线与点组成的初步特征，第三层为局部特征）？从这一思想出发，特征可学习且自动组合并给出结果，这是典型的“end-to-end” 。</li></ol><h1 id="论文阅读："><a href="#论文阅读：" class="headerlink" title="论文阅读："></a>论文阅读：</h1><p>先阐述一下论文结构</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">0</span>. 标题（title）<br><span class="hljs-attribute">0</span>.<span class="hljs-number">5</span>. 摘要（Abstract）<br><span class="hljs-attribute">1</span>. 介绍（Introduction）<br><span class="hljs-attribute">2</span>. 数据集（The Dataset）<br><span class="hljs-attribute">3</span>. 网络结构（The Architecture）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">1</span> ReLU非线性单元（ReLU Nonlinearity）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">2</span> 多GPU训练（Training <span class="hljs-literal">on</span> Multiple GPU）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">3</span> 局部响应和归一化（Local Response Normalization）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">4</span> 层叠池化（Overlapping Pooling）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">5</span> 整体结构（Overall Architecture）<br><span class="hljs-attribute">4</span>. 减少过拟合（Reducing Overfitting） <br><span class="hljs-attribute">4</span>.<span class="hljs-number">1</span> 数据增强 （Data Augmentation）<br><span class="hljs-attribute">4</span>.<span class="hljs-number">2</span> 随机失活 （Dropout）<br><span class="hljs-attribute">5</span>. 学习细节 （Details of learning）<br><span class="hljs-attribute">6</span>. 结果 （Results）<br><span class="hljs-attribute">6</span>.<span class="hljs-number">1</span> 定性评估（Qualitative Evacuation）<br><span class="hljs-attribute">6</span>.<span class="hljs-number">2</span> 讨论（Discussion） <br></code></pre></td></tr></table></figure><p>这里需要说明的是由于markdown的限制和本人技术能力的欠缺。在这篇博文中不放公式，如果想看公式，请去看原论文，数学的公式才是最简洁的表达方式，前提是能够看懂，看懂了之后就像打开新世界的大门。感觉就像我有一双滑板鞋，我走到那就穿到哪。<br>0. 标题论文标题为<br>ImageNet Classification with Deep Convolutional Neural Networks<br>摘要： 我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。（开门见山，直接说干了什么。）在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率（这里使用的是错误率的评估指标，和我目前使用的Acc，Presion，Recall，召回率评估指标不一样。），这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成（这里挖一个坑，参数量和神经元数量的评估指标不一样。），其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。（这里挖个坑softMax的机制是怎么样的？）为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法（dropout，随机丢弃的机制是什么？），该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模式的一个变种，取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。</p><ol><li><p>介绍：目前，机器学习方法对物体识别非常重要。为了改善他们的表现（前提条件就是之前的表现不是很好），我们可以收集更大的数据集，训练更强大的模型，并使用更好的技术来防止过拟合。（这里指出减少过拟合的方法有增大数据集，更改模型结构，使用更好的优化技术。）直到最近，标记好图像的数据集相对还较小——大约上万的数量级（例如，NORB，Caltech-101&#x2F;256和CIFAR-10&#x2F;100）。使用这种规模的数据集可以很好地解决简单的识别任务，特别是如果他们增加了保留标签转换（label-preserving transformations）。例如，目前MNIST数字识别任务的最低错误率（&lt;0.3％）基本达到了人类的识别水平。但是物体在现实环境中可能表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。事实上，小图像数据集的缺点已是众所周知（例如，Pinto），但直到最近才可以收集到数百万的标记数据集。新的大型数据集包括LabelMe ，其中包含数十万个完全分割的图像，以及ImageNet ，其中包含超过15,000万个超过22,000个类别的高分辨率图像。（目前的研究的对象，研究现状。）<br>要从数百万图像中学习数千个类别，我们需要一个具有强大学习能力的模型。（这里暗示这篇文章的模型大小非常大，但是现在看来入门级把，毕竟是12年前的文章了，开山鼻祖了。）然而，物体识别任务的巨大复杂性意味着即使是像ImageNet这样大的数据集也不能完美地解决这个问题，所以我们的模型也需要使用很多先验知识来弥补我们数据集不足的问题。卷积神经网络（CNN）就构成了一类这样的模型（卷积神经网络可以获取先验的知识，来弥补数据集不足的问题，后面是卷积神经网络为什么能够实现获取先验知识。）。它们的容量可以通过改变它们的深度和宽度来控制，并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。<br>尽管CNN具有很好的质量，并且尽管其局部结构的效率相对较高，但将它们大规模应用于高分辨率图像时仍然显得非常昂贵。幸运的是，当前的GPU可以用于高度优化的二维卷积，能够加速许多大型CNN的训练，并且最近的数据集（如ImageNet）包含足够多的标记样本来训练此类模型，而不会出现严重的过度拟合。<br>本文的具体贡献如下：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。（codeing能力还是有的，我的目标就是能够实现自己的想法，Talk is cheap. Show me the code.这句话真的是令人兴奋）我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3章节。我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。（这点可能验证了特征是层级表示的，）最后，网络的规模主要受限于目前GPU上可用的内存量以及我们可接受的训练时间。我们的网络需要在两块GTX 580 3GB GPU上花费五到六天的时间来训练。我们所有的实验都表明，通过等待更快的GPU和更大的数据集出现，我们的结果可以进一步完善。</p></li><li><p>数据集：ImageNet是一个拥有超过1500万个已标记高分辨率图像的数据集，大概有22,000个类别。（对数据集有一个基本介绍，保证权威性，说明没有造假）图像都是从网上收集，并使用Amazon-Mechanical Turk群智工具人工标记（人工智能，人工越多越智能，找不到工作就去打标签，打标签的特点就是不费脑子，一坐坐一天。ImageNet是李飞飞<a href="https://baike.baidu.com/item/%E6%9D%8E%E9%A3%9E%E9%A3%9E/7448630">放个连接</a>。从2010年起，作为Pascal视觉对象挑战赛的一部分，这是每年举办一次的名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。 ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。<br>ILSVRC-2010是ILSVRC中的唯一可以使用测试集标签的版本，因此这也正是我们进行大部分实验的版本。由于我们也在ILSVRC-2012比赛中引入了我们的模型，因此在第6部分中，我们也会给出此版本数据集的结果，尽管这个版本的测试集标签不可用。在ImageNet上，习惯上使用两种错误率：top-1和top-5，其中top-5错误率是正确标签不在被模型认为最可能的五个标签之中的测试图像的百分率。（原来错误率来源于这个比赛）<br>ImageNet由可变分辨率的图像组成，而我们的系统需要固定的输入尺寸。因此，我们将图像下采样到256×256的固定分辨率。给定一个矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心的256×256的图片。除了将每个像素中减去训练集的像素均值之外，我们没有以任何其他方式对图像进行预处理。所以我们在像素的（中心）原始RGB值上训练了我们的网络。</p></li><li><p>图（前文放的模型结构图）概括了我们所提出网络的结构。它包含八个学习层——五个卷积层和三个全连接层。下面，我们将描述一些所提出网络框架中新颖或不寻常的地方。 3.1-3.4节按照我们对它们重要性的估计进行排序，其中最重要的是第一个。<br>RelU：对一个神经元模型的输出的常规套路是，给他接上一个激活函数：（tanh（x）的公式，）就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数（ReLU的公式f(x)&#x3D;max(0,x)注：因为ReLU的公式比较简单所以这里放一下)如慢得多。根据Nair和Hinton的说法[20]（这篇论文相当于为ReLU背书了，就相当于我的理论依据），我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。这个结果从图1中可以看出来（实验证明来了，填坑，使用了实验证明），该图展示了对于一个特定的四层CNN，CIFAR-10数据集训练中的误差率达到25%所需要的迭代次数。从这张图的结果可以看出，如果我们使用传统的饱和神经元模型来训练CNN，那么我们将无法为这项工作训练如此大型的神经网络。我们并不是第一个考虑在CNN中替换掉传统神经元模型的(继续理论证明，巨大的论文阅读量，)。例如，Jarrett等人[11]声称，非线性函数在他们的对比度归一化问题上，再接上局部均值池化单元，在Caltech-101数据集上表现的非常好。然而，在这个数据集中，主要担心的还是防止过拟合，所以他们观察到的效果与我们在使用ReLU时观察到的训练集的加速能力还是不一样。加快训练速度对大型数据集上训练的大型模型的性能有很大的影响。<br>在多个GPU上训练：单个GTX 580 GPU只有3GB内存（当时GPU的内存确实小，不过也挺厉害了。有时候真觉得自己跟不上时代了，对时间没有一点感觉，12年24年对我有什么区别？），这限制了可以在其上训练的网络的最大尺寸。事实证明，120万个训练样本足以训练那些因规模太大而不适合使用一个GPU训练的网络。因此，我们将网络分布在两个GPU上。目前的GPU很适合于跨GPU并行化操作，因为它们能够直接读写对方的内存，而无需通过主机内存。我们采用的并行化方案基本上将半个内核（或神经元）放在各个GPU上，（有种左右脑的感觉）——另外还有一个技巧：GPU只在某些层间进行通信。这意味着，例如，第3层的内核从第2层的所有内核映射（kernel maps）中获取输入。然而，第4层中的内核又仅从位于同一GPU上的第3层中的那些内核映射获取输入。选择连接模式对于交叉验证是一个不小的问题，但这使得我们能够精确调整通信量，直到它的计算量的达到可接受的程度。由此产生的架构有点类似于Cire¸san等人使用的“柱状”CNN[5]，除了我们的每列不是独立的之外（见图2）。与一个GPU上训练的每个卷积层只有一半的内核数量的网络相比，该方案分别将我们的top-1和top-5错误率分别降低了1.7％和1.2％。双GPU网络的训练时间比单GPU网络更少。<br>局部响应归一化：ReLU具有理想的属性，它们不需要对输入进行归一化来防止它们饱和。如果至少有一些训练实例为ReLU产生了正的输入，那么这个神经元就会学习。然而，我们还是发现下面的这种归一化方法有助于泛化。设aix,y表示第i个内核计算(x,y)位置的ReLU非线性单元的输出，而响应归一化（Local Response Normalization）的输出值定义为bix,y其中，（公式）求和部分公式中的 n表示同一个位置下与该位置相邻的内核映射的数量，而N表示这一层所有的内核数（即通道数）。内核映射的顺序当然是任意的，并且在训练之前就已经定好了。这种响应归一化实现了一种模仿真实神经元的横向抑制，从而在使用不同内核计算的神经元输出之间产生较大的竞争。常数k都是超参数（hyper-parameters），它们的值都由验证集决定。我们取 k&#x3D;2。我们在某些层的应用ReLU后再使用这种归一化方法（参见第3.5节）。这个方案与Jarrett等人[11]的局部对比归一化方案有些相似之处，但我们的被更准确地称为“亮度归一化”，因为我们没有减去均值。响应归一化将我们的top-1和top-5的错误率分别降低了1.4％和1.2％。我们还验证了这种方案在CIFAR-10数据集上的有效性：没有进行归一化的四层CNN实现了13％的测试错误率，而进行了归一化的则为11％。<br>层叠池化：CNN中的池化层汇集了相同内核映射中相邻神经元组的输出。在传统方法中，相邻池化单元之间互不重叠（例如[17,11,4]）。更准确地说，一个池化层可以被认为是由一些间隔为s个像素的池化单元组成的网格，每个都表示了一个以池化单元的位置为中心的大小为z×z的邻域。如果我们令s &#x3D; z，我们就可以得到CNN中常用的传统的局部池化。<br>整体结构：现在我们已经准备好描述CNN的整体架构了。如图2所示，这个网络包含了八层权重;前五个是卷积层，其余三个为全连接层。最后的全连接层的输出被送到1000维的softmax函数，其产生1000个类的预测。我们的网络最大化多项逻辑回归目标，这相当于在预测的分布下最大化训练样本中正确标签对数概率的平均值。第二，第四和第五个卷积层的内核仅与上一层存放在同一GPU上的内核映射相连（见图2）。第三个卷积层的内核连接到第二层中的所有内核映射。全连接层中的神经元连接到前一层中的所有神经元。响应归一化层紧接着第一个和第二个卷积层。 在3.4节中介绍的最大池化层，后面连接响应归一化层以及第五个卷积层。将ReLU应用于每个卷积层和全连接层的输出。第一个卷积层的输入为224×224×3的图像，对其使用96个大小为11×11×3、步长为4（步长表示内核映射中相邻神经元感受野中心之间的距离）的内核来处理输入图像。第二个卷积层将第一个卷积层的输出（响应归一化以及池化）作为输入，并使用256个内核处理图像，每个内核大小为5×5×48。第三个、第四个和第五个卷积层彼此连接而中间没有任何池化或归一化层。第三个卷积层有384个内核，每个的大小为3×3×256，其输入为第二个卷积层的输出。第四个卷积层有384个内核，每个内核大小为3×3×192。第五个卷积层有256个内核，每个内核大小为3×3×192。全连接层各有4096个神经元。</p></li><li><p>减少过拟合。我们的神经网络架构拥有6000万个参数。尽管ILSVRC的1000个类别使得每个训练样本从图像到标签的映射被限制在了10 bit之内，但这不足以保证训练这么多参数而不出现过拟合。下面，我们将介绍对付过度拟合的两个方法。<br>数据增强： 减小过拟合的最简单且最常用的方法就是，使用标签保留转换（label-preserving transformations，例如[25,4,5]），人为地放大数据集。我们采用两种不同形式的数据增强方法，它们都允许通过很少的计算就能从原始图像中生成转换图像，所以转换后的图像不需要存储在硬盘上。在我们实现过程中，转换后的图像是使用CPU上的Python代码生成的，在生成这些转换图像的同时，GPU还在训练上一批图像数据。所以这些数据增强方案实际上是很高效的。<br>数据增强的第一种形式包括平移图像和水平映射。我们通过从256×256图像中随机提取224×224的图像块（及其水平映射）并在这些提取的图像块上训练我们的网络来做到这一点。这使我们的训练集的规模增加了2048倍，尽管由此产生的训练样本当然还是高度相互依赖的。如果没有这个方案，我们的网络就可能会遭受大量的的过拟合，可能会迫使我们不得不使用更小的网络。在测试时，网络通过提取5个224×224的图像块（四个角块和中心块）以及它们的水平映射（因此总共包括10个块）来进行预测，并求网络的softmax层的上的十个预测结果的均值。第二种形式的数据增强包括改变训练图像中RGB通道的灰度。具体而言，我们在整个ImageNet训练集的图像的RGB像素值上使用PCA。对于每个训练图像，我们添加多个通过PCA找到的主成分，大小与相应的特征值成比例，乘以一个随机值，该随机值属于均值为0、标准差为0.1的高斯分布。因此，对于每个图像的RGB像素有：Ixy&#x3D;[IRxy IGxy IBxy]T（自己去看论文中的公式），我们加入如下的值：[p1 p2 p3] [α1λ1 α2λ2 α3λ3]T其中， pi和 λi分别是3x3的RGB协方差矩阵的第 i个特征向量和第i个的特征值，而 αi是前面所说的随机值。对于一张特定图像中的所有像素，每个 αi只会被抽取一次，知道这张图片再次用于训练时，才会重新提取随机变量。这个方案近似地捕捉原始图像的一些重要属性，对象的身份不受光照的强度和颜色变化影响。这个方案将top-1错误率降低了1％以上。<br>Dropout： 结合许多不同模型的预测结果是减少测试错误率的一种非常成功的方法[1,3]，但对于已经花费数天时间训练的大型神经网络来说，它似乎成本太高了。然而，有一种非常有效的模型组合方法，在训练期间，只需要消耗1&#x2F;2的参数。这个新发现的技术叫做“Dropout”[10]，它会以50%的概率将隐含层的神经元输出置为0。以这种方法被置0的神经元不参与网络的前馈和反向传播。因此，每次给网络提供了输入后，神经网络都会采用一个不同的结构，但是这些结构都共享权重。这种技术减少了神经元的复杂适应性，因为神经元无法依赖于其他特定的神经元而存在。因此，它被迫学习更强大更鲁棒的功能，使得这些神经元可以与其他神经元的许多不同的随机子集结合使用。在测试时，我们试着使用了所有的神经元，并将它们的输出乘以0.5。这与采用大量dropout的网络产生的预测结果分布的几何均值近似。我们在图2中的前两个全连接层上使用了dropout。没有dropout，我们的网络会出现严重的过拟合。Dropout大概会使达到收敛的迭代次数翻倍。</p></li><li><p>训练细节。我们使用随机梯度下降法来训练我们的模型，每个batch有128个样本，动量（momentum）为0.9，权重衰减（weight decay）为0.0005。我们发现这种较小的权重衰减对于模型的训练很重要。换句话说，权重衰减在这里不仅仅是一个正则化方法：它减少了模型的训练误差。权重ω的更新法则是：（自己看公式去）<br>我们使用标准差为0.01、均值为0的高斯分布来初始化各层的权重。我们使用常数1来初始化了网络中的第二个、第四个和第五个卷积层以及全连接层中的隐含层中的所有偏置参数。这种初始化权重的方法通过向ReLU提供了正的输入，来加速前期的训练。我们使用常数0来初始化剩余层中的偏置参数。<br>我们对所有层都使用相同的学习率，在训练过程中又手动进行了调整。我们遵循的启发式方法是：以当前的学习速率训练，验证集上的错误率停止降低时，将学习速率除以10.学习率初始时设为0.01，并且在终止前减少3次。我们使用120万张图像的训练集对网络进行了大约90次迭代的训练，这在两块NVIDIA GTX 580 3GB GPU上花费了大约5到6天的时间。（这里说明了优化函数，超参数设置。这里挖个坑，什么是超参数？）</p></li><li><p>结果：我们在ILSVRC-2010上取得的结果如表1所示。我们的网络的top-1和top-5测试集错误率分别为37.5％和17.0％。在ILSVRC-2010比赛期间取得的最佳成绩是47.1％和28.2％，其方法是对六种不同的稀疏编码模型所产生的预测结果求平均[2]。此后公布的最佳结果为45.7％、25.7％，其方法是对两种经过密集采样的特征[24]计算出来的Fisher向量（FV）训练的两个分类器取平均值。我们的网络实现了37.5％和17.0％的前1和前5个测试集错误率5。在ILSVRC-2010比赛期间取得的最佳成绩是47.1％和28.2％，其中一种方法是对六种针对不同特征进行训练的稀疏编码模型所产生的预测进行平均[2]，此后最佳公布结果为45.7％， 25.7％，其中一种方法是：对两个在不同取样密度的Fisher向量上训练的分类器取平均。（纵向对比了，相同数据集，不同模型。）<br>我们还在ILSVRC-2012竞赛中使用了我们的模型，并在表2中给出了我们的结果。由于ILSVRC-2012测试集标签未公开，因此我们无法给出我们测试过的所有模型在测试集上的错误率。在本节的其余部分中，我们将验证集和测试集的错误率互换，因为根据我们的经验，它们之间的差值不超过0.1％（见表2）。本文描述的CNN的top-5错误率达到了18.2％。对五个相似CNN的预测结果计算均值，得到的错误率为16.4％。单独一个CNN，在最后一个池化层之后，额外添加第六个卷积层，对整个ImageNet Fall 2011 release(15M images, 22K categories)进行分类，然后在ILSVRC-2012上“微调”（fine-tuning）网络，得到的错误率为16.6％。对整个ImageNet Fall 2011版本的数据集下预训练的两个CNN，求他们输出的预测值与前面提到的5个不同的CNN输出的预测值的均值，得到的错误率为15.3％。比赛的第二名达到了26.2％的top-5错误率，他们的方法是：对几个在特征取样密度不同的Fisher向量上训练的分类器的预测结果取平均的方法[7]。<br>最后，我们还在ImageNet Fall 2009版本的数据集上提交了错误率，总共有10,184个类别和890万张图像。在这个数据集中，我们遵循文献中的使用一半图像用于训练，一半图像用于测试的惯例。由于没有建立测试集，所以我们的拆分方法有必要与先前作者使用的拆分方法不同，但这并不会对结果产生显著的影响。我们在这个数据集上的top-1和top-5错误率分别是67.4％和40.9％，是通过前面描述的网络获得的，但是在最后的池化层上还有额外的第6个卷积层。该数据集此前公布的最佳结果是78.1％和60.9％[19]。<br>定性评估：图3（自己看论文去）显示了由网络的两个数据连接层学习得到的卷积内核。（网络结构还可以画出来，也是挺有意思的。）该网络已经学习到许多频率和方向提取的内核，以及各种色块。请注意两个GPU所展现的不同特性，这也是3.5节中介绍的限制互连的结果。GPU1上的内核在很大程度上与颜色无关，然而GPU2上的内核在很大程度上都于颜色有关。这种特异性在每次迭代期间都会发生，并且独立于任何特定的随机权重初始化过程（以GPU的重新编号为模）。<br>在图4（自己看论文去，图4展示了一堆实验结果）的左边，我们通过计算8张测试图像的top-5预测来定性评估网络的训练结果。请注意，即使是偏离中心的物体，如左上角的螨虫，也可以被网络识别出来。大多数top-5的标签都显得比较合理。例如，只有其他类型的猫才被认为是豹子的可能标签。在某些情况下（栅栏、樱桃），照片的关注点存在模糊性，不知道到底该关注哪个。另一个研究可视化的网络的方法是，考虑由最后一个4096维隐含层中的图像的特征的激活函数输出值。如果两幅图像产生有的欧氏距离，我们可以认为高层次的神经网络认为它们是相似的。图4显示了测试集中的5个图像和来袭训练集的6个图像，这些图像根据这种度量方法来比较它们中的哪一个与其最相似。请注意，在像素层次上，待检测的训练图像通常不会与第一列中的查询图像有较小的L2距离。例如，检索到的狗和大象有各种不同的姿势。我们在补充材料中提供了更多测试图像的结果。通过使用欧式距离来计算两个4096维实值向量的相似性，效率不高，但是通过训练自编码器可以将这些向量压缩为较短的二进制码，能够使其更高效。与应用自编码器到原始像素[14]相比，这应该是更好的图像检索方法。它不使用图像标签，因此更秦翔宇检索具有相似图案边缘的图像，不管它们的图像语义是否相似。</p></li><li><p>讨论：我们的研究结果表明，一个大的深层卷积神经网络能够在纯粹使用监督学习（这里有个概念，监督学习和无监督学习，半监督学习。挖个坑）的情况下，在极具挑战性的数据集上实现破纪录的结果。值得注意的是，如果移除任何一个卷积层，网络的性能就会下降。例如，删除任何中间层的结果会导致网络性能的top-1错误率下降2%。因此网络的深度对于实现我们的结果真的很重要。（基本上后面的深度学习的思路就是堆网络结构）<br>为了简化我们的实验，我们没有使用任何无监督的预训练方法，尽管这样可能会有所帮助，特别是如果我们获得了足够的计算能力来显著地增加网络的大小而不会相应地增加已标记数据的数量。到目前为止，我们的结果已经获得了足够的进步，因为我们已经使网络更大，并且训练了更长时间。但我们仍然有很大的空间去优化网络，使之能够像人类的视觉系统一样感知。最后，我们希望对视频序列使用非常大的深度卷积神经网路，其中时间结构提供了非常有用的信息，这些信息往往在静态图像中丢失了，或者说不太明显。</p></li></ol><h1 id="个人感觉"><a href="#个人感觉" class="headerlink" title="个人感觉"></a>个人感觉</h1><p>论文很短，内容很多。展现在论文中的，没有展现在论文中的。学习的过程中既有鲜花也有荆棘，这是客观的条件，我承认有人会有论语中的天生的智慧，看待世界的方式就不一样，这是现实，但是那又有什么？不管怎么样先把下面的坑填了。还有就是博客中难免有错别字，记得更改。+</p><h2 id="问题1，卷积是什么？作用什么？"><a href="#问题1，卷积是什么？作用什么？" class="headerlink" title="问题1，卷积是什么？作用什么？"></a>问题1，卷积是什么？作用什么？</h2><h2 id="问题2，池化是什么？作用是什么？"><a href="#问题2，池化是什么？作用是什么？" class="headerlink" title="问题2，池化是什么？作用是什么？"></a>问题2，池化是什么？作用是什么？</h2><h2 id="问题3，全连接是什么？作用是什么？"><a href="#问题3，全连接是什么？作用是什么？" class="headerlink" title="问题3，全连接是什么？作用是什么？"></a>问题3，全连接是什么？作用是什么？</h2><h2 id="问题4，AlexNet论文使用的loss函数是什么？"><a href="#问题4，AlexNet论文使用的loss函数是什么？" class="headerlink" title="问题4，AlexNet论文使用的loss函数是什么？"></a>问题4，AlexNet论文使用的loss函数是什么？</h2><h2 id="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"><a href="#问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？" class="headerlink" title="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"></a>问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？</h2><h2 id="问题6，AlexNet论文中使用的评价指标是什么？"><a href="#问题6，AlexNet论文中使用的评价指标是什么？" class="headerlink" title="问题6，AlexNet论文中使用的评价指标是什么？"></a>问题6，AlexNet论文中使用的评价指标是什么？</h2><h2 id="问题7，AlexNet中的创新点是什么？"><a href="#问题7，AlexNet中的创新点是什么？" class="headerlink" title="问题7，AlexNet中的创新点是什么？"></a>问题7，AlexNet中的创新点是什么？</h2><ol><li>ReLU激活函数的引入，采样非线性单元（ReLU）的深度卷积神经网络训练时间要比tanh单元要快几倍。而时间开销是进行模型训练过程中的很重要的因数。同时ReLU有效的防止了过拟合的现象。</li><li>层叠池化操作，以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize&#x3D;2×2，stride&#x3D;2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize &gt; stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。</li><li>Dropout操作， Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。</li><li>网络层数更深，与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。当然，这种思考的重要节点出现在VGG网络（下一篇博文VGG论文中将会讲到）。</li></ol><h2 id="问题8，优化函数的具体实现是什么？"><a href="#问题8，优化函数的具体实现是什么？" class="headerlink" title="问题8，优化函数的具体实现是什么？"></a>问题8，优化函数的具体实现是什么？</h2><h2 id="问题9，关于卷积后特征图应该怎么计算？"><a href="#问题9，关于卷积后特征图应该怎么计算？" class="headerlink" title="问题9，关于卷积后特征图应该怎么计算？"></a>问题9，关于卷积后特征图应该怎么计算？</h2><h2 id="问题10，什么是过拟合合和欠拟合？"><a href="#问题10，什么是过拟合合和欠拟合？" class="headerlink" title="问题10，什么是过拟合合和欠拟合？"></a>问题10，什么是过拟合合和欠拟合？</h2><h2 id="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"><a href="#问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？" class="headerlink" title="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"></a>问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？</h2><h2 id="问题12，-神经元数量和参数量的计算方法是什么？"><a href="#问题12，-神经元数量和参数量的计算方法是什么？" class="headerlink" title="问题12， 神经元数量和参数量的计算方法是什么？"></a>问题12， 神经元数量和参数量的计算方法是什么？</h2><h2 id="问题13，-softMax的机制是怎么样的？"><a href="#问题13，-softMax的机制是怎么样的？" class="headerlink" title="问题13， softMax的机制是怎么样的？"></a>问题13， softMax的机制是怎么样的？</h2><h2 id="问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？"><a href="#问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？" class="headerlink" title="问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？"></a>问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？</h2><h2 id="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？"><a href="#问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？" class="headerlink" title="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？"></a>问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？</h2><h2 id="问题16，-什么是超参数？"><a href="#问题16，-什么是超参数？" class="headerlink" title="问题16， 什么是超参数？"></a>问题16， 什么是超参数？</h2><h2 id="问题17，-什么是监督学习和无监督学习，半监督学习？"><a href="#问题17，-什么是监督学习和无监督学习，半监督学习？" class="headerlink" title="问题17， 什么是监督学习和无监督学习，半监督学习？"></a>问题17， 什么是监督学习和无监督学习，半监督学习？</h2><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。"><a href="#关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。" class="headerlink" title="关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。"></a>关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch基础——Numpy</title>
    <link href="/2024/04/21/deeplearnbook2/"/>
    <url>/2024/04/21/deeplearnbook2/</url>
    
    <content type="html"><![CDATA[<p>第一部分</p><h1 id="Numpy基础"><a href="#Numpy基础" class="headerlink" title="Numpy基础"></a>Numpy基础</h1><h2 id="什么是Numpy？"><a href="#什么是Numpy？" class="headerlink" title="什么是Numpy？"></a>什么是Numpy？</h2><p>NumPy（Numerical Python的简称）是一个开源的Python库，用于进行科学计算。它提供了一个强大的N维数组对象，以及大量的函数用于处理这些数组。NumPy的主要功能包括：</p><ol><li>多维数组对象：NumPy的核心功能是其多维数组对象（ndarray）。这是一个快速、灵活的容器，可以容纳大量同类型数据，使你能够对这些数据进行数学运算。</li><li>广播功能：NumPy提供了广播功能，这是一种强大的机制，允许NumPy在执行算术运算时处理不同形状的数组。</li><li>数学函数：NumPy提供了大量的数学函数，可以对数组中的元素进行各种数学运算，如加、减、乘、除、平方根等。</li><li>线性代数：NumPy包含了线性代数函数库，可以进行矩阵乘法、求逆、解线性方程，傅里叶变换等操作。</li><li>随机数生成：NumPy提供了生成各种随机数的功能，如均匀分布、正态分布等。</li><li>更方便的读取\写入磁盘上的阵列数据和操作存储映像文件的工具。<br>NumPy是许多科学计算库（如Pandas、Matplotlib、SciPy等）的基础库，也是机器学习和数据科学中常用的库。在深度学习中图像、声音、文本等输入数据最终都要转换为数组或矩阵，NumPy的多维数组可以用来表示向量、矩阵和张量，这些都是深度学习算法的基本构成元素。</li></ol><h2 id="为什么要使用Numpy"><a href="#为什么要使用Numpy" class="headerlink" title="为什么要使用Numpy"></a>为什么要使用Numpy</h2><p>实际上python包含多个数据类型，数值类型（int，float，complex）、布尔类型（bool）、字符串（str）、列表（list）、元组（tuple）、字典（dict，{‘name’:’chenli’,’age’: 114514}）、集合（set{}）。<br>python包含这么多的数据类型，其中的列表（list）和数组（array）为什么不能用？原因是对于大数据来说，这些结构有很多不足。比如由于列表的元素可以是任何对象，因此列表中所保存的是对象的指针。例如为了存储[1,2,3],就需要3个指针和3个整数对象，这样对于数值运算来讲，严重浪费了计算机中的内存和CPU或GPU的算力。对于array，它可以直接保存数值，但是它不支持多维，并且对应操作的函数也不多，因此也不适合。</p><h2 id="怎么使用Numpy？"><a href="#怎么使用Numpy？" class="headerlink" title="怎么使用Numpy？"></a>怎么使用Numpy？</h2><p>第一步使用 <code>pip install numpy </code>来下载numpy库。</p><ol><li><p>生成Numpy数组。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>lst1= <span class="hljs-string">[[3.14, 2.17,0,1,2],[1,2,3,4,5]]</span> # 一个二维列表<br>nd1 = np.array(lst1)<br><span class="hljs-built_in">print</span>(nd1) # 显示的结果为一个二维列表<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(nd1)) # 显示结果为&lt;class <span class="hljs-string">&#x27;numpy.ndarray&#x27;</span>&gt;    <br></code></pre></td></tr></table></figure></li><li><p>numpy中的random模块生成数组</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">nd1</span> = np.random.random([<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]) # 产生一个[<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]ndarray，范围为<span class="hljs-number">0</span>-<span class="hljs-number">1</span>之间的随机数。<br><span class="hljs-attribute">np</span>.random.seed(<span class="hljs-number">123</span>) # 为了每次生成同一份数据，可以指定一个随机种子，生成的随机数据是固定的。<br><span class="hljs-attribute">nd2</span> = np.random.random(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 产生一个[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]的ndarray<br><span class="hljs-attribute">np</span>.random.shuffle(nd2) # 随机打乱nd2中的数据。<br><span class="hljs-attribute">nd3</span> = np.random.uniform(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 生成均匀分布的随机数<br><span class="hljs-attribute">nd4</span> = np.random.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 生成标准正态分布的随机数 <br><span class="hljs-attribute">nd5</span> = np.random.randint(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 生成随机的整数<br></code></pre></td></tr></table></figure></li><li><p>numpy中创建特定形状的多维数组</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">nd1</span> = np.zeros([<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]) # 生成全是<span class="hljs-number">0</span>的<span class="hljs-number">3</span>x3的矩阵，np.zeros()生成全是<span class="hljs-number">0</span>的ndarray<br><span class="hljs-attribute">nd2</span> = np.zeros_like(nd1) # 以ndrr相同维度创建元素为<span class="hljs-number">0</span>的数组<br><span class="hljs-attribute">nd3</span> = np.ones_like(nd1) # 以nd1维度创建元素全是<span class="hljs-number">1</span>的数组<br><span class="hljs-attribute">nd4</span> = np.empty_like(nd1) # 以nd1维度创建一个空数组<br><span class="hljs-attribute">nd5</span> = np.eye(<span class="hljs-number">5</span>) # 该函数用于创建一个<span class="hljs-number">5</span>x5的矩阵，对角线为<span class="hljs-number">1</span>，其余为<span class="hljs-number">0</span><br><span class="hljs-attribute">nd6</span> = np.full((<span class="hljs-number">3</span>,<span class="hljs-number">5</span>),<span class="hljs-number">666</span>) # 创建<span class="hljs-number">3</span>x5的元素全为<span class="hljs-number">666</span>的数组，<span class="hljs-number">666</span>为指定值<br></code></pre></td></tr></table></figure></li><li><p>保存使用numpy生成的数据</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import numpy <span class="hljs-keyword">as</span> np<br>nd = np.<span class="hljs-built_in">random</span>.<span class="hljs-built_in">random</span>([<span class="hljs-number">5</span>,<span class="hljs-number">5</span>])<br>np.savetxt(X=nd,fname=<span class="hljs-string">&#x27;test.txt&#x27;</span>) <span class="hljs-comment"># 保存nd，文件名称为test.txt</span><br>nd2 = np.loadtxt(<span class="hljs-string">&#x27;test.txt&#x27;</span>) <span class="hljs-comment"># 从test.txt中加载数据</span><br></code></pre></td></tr></table></figure></li><li><p>利用arange、linspace函数来生成数组<br>arange是numpy模块中的函数，格式为：<code>arange([start],stop[,step],dtype=None)</code><br>其中start与stop用来限定范围，step是步长默认为1，step可以为小数。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">nd</span> = np.arange(<span class="hljs-number">10</span>) # np中的内容为[<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span> <span class="hljs-number">8</span> <span class="hljs-number">9</span>]<br><span class="hljs-attribute">nd1</span> = np.arange(<span class="hljs-number">1</span>，<span class="hljs-number">4</span>，<span class="hljs-number">0</span>.<span class="hljs-number">5</span>) # np中的内容为[<span class="hljs-number">1</span>.<span class="hljs-number">0</span> <span class="hljs-number">1</span>.<span class="hljs-number">5</span> <span class="hljs-number">2</span>.<span class="hljs-number">0</span> <span class="hljs-number">2</span>.<span class="hljs-number">5</span> <span class="hljs-number">3</span>.<span class="hljs-number">0</span> <span class="hljs-number">3</span>.<span class="hljs-number">5</span> <span class="hljs-number">4</span>.<span class="hljs-number">0</span>]<br><span class="hljs-attribute">nd2</span> = np.aramge(<span class="hljs-number">9</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>) # nd2中的内容为[<span class="hljs-number">9</span> <span class="hljs-number">8</span> <span class="hljs-number">7</span> <span class="hljs-number">6</span> <span class="hljs-number">5</span> <span class="hljs-number">4</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>linspace也是numpy中常用的函数，格式为：<code>np.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None)</code><br>linspace可以根据输入数据的指定范围以及等份数量，自动生成线性分量。endpoint（包含终点）默认为True。等分量num默认为50，如果将retstep设置为True，则会返回一个带步长的ndarray</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">print</span>(np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>)) # 产生<span class="hljs-number">10</span>个数，间隔为<span class="hljs-number">0</span>.<span class="hljs-number">111111</span> <br></code></pre></td></tr></table></figure></li><li><p>获取数据。<br>数据生成后，如何读取数据，常用数据的的方法。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">np</span>.random.seed(<span class="hljs-number">2024</span>)<br><span class="hljs-attribute">nd</span> = np.random.random([<span class="hljs-number">10</span>]) # 产生一维的<span class="hljs-number">10</span>个数据点<br><span class="hljs-attribute">nd</span>[<span class="hljs-number">3</span>]  # 获取指定位置的数据，获取第<span class="hljs-number">4</span>个元素<br><span class="hljs-attribute">nd</span>[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>] # 截取一段数据<br><span class="hljs-attribute">nd</span>[<span class="hljs-number">1</span>:<span class="hljs-number">6</span>:<span class="hljs-number">2</span>] # 获取固定间隔的数据<br><span class="hljs-attribute">nd</span>[::-<span class="hljs-number">2</span>] # 倒序取数<br><span class="hljs-attribute">nd2</span> = np.arange(<span class="hljs-number">25</span>).reshape([<span class="hljs-number">5</span>,<span class="hljs-number">5</span>]) # 产生一个<span class="hljs-number">25</span>个数据点的一维数据，而后通过reshape进行形状重整为[<span class="hljs-number">5</span>,<span class="hljs-number">5</span>]<br><span class="hljs-attribute">nd2</span>[:,<span class="hljs-number">1</span>:<span class="hljs-number">3</span>] # 截取多维数组中，指定的列，读取第<span class="hljs-number">2</span>，<span class="hljs-number">3</span>列。<br></code></pre></td></tr></table></figure></li><li><p>Numpy的算术运算<br>在机器学习和深度学习中，涉及大量的数组或矩阵运算，这里介绍两种常用的运算。一种是对应元素相乘，又称逐元乘法（Element-Wisr Product）运算符为np.multiply()或*。一种是点积或内积元素，运算符为np.dot()</p></li></ol><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="向量和数组之间的关系是什么？向量的定义是什么？"><a href="#向量和数组之间的关系是什么？向量的定义是什么？" class="headerlink" title="向量和数组之间的关系是什么？向量的定义是什么？"></a>向量和数组之间的关系是什么？向量的定义是什么？</h3><p>在数学科物理中，向量被定义为具有大小和方向量。例如速度是一个向量，因为它不仅有大小（数独），还有方向（行进的方向）。<br>数组是编程中的一种基本数据结构，用于存储一组有序的元素。这些元素可以是任何类型，如整形、浮点数、字符串等。<br>标量（scalar）是零维只有大小，没有方向的量，如1，2，3<br>向量（Vector）是一维只有大小和方向的量，如（1，2）。（计算方向的公式为：）<br>矩阵（Matrix）是二维的向量，[[1, 2], [2, 3]]<br>张量（Tensor） 按照任意维排列的一堆数字的推广。矩阵不过是三维张量下的一个二维切面。要在三维张量下找到零维张量需要三个维度的坐标来定位。（注：张量可以是多维的）</p><h3 id="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"><a href="#矩阵是什么，作用是什么？如何实现矩阵的加减乘除" class="headerlink" title="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"></a>矩阵是什么，作用是什么？如何实现矩阵的加减乘除</h3><ol><li>矩阵是一个二维数组，由行和列的元素组成。在数学中，矩阵通常用大写字母表示，如 A，B 等，矩阵中的元素通常用小写字母表示，如aij​，表示矩阵 A 的第 i 行第 j 列的元素。</li><li>矩阵可以用来表示线性变换，解决线性方程组，或者表示图形的变换。在数据科学和机器学习中，矩阵通常用于存储和操作大量的数据。</li></ol><h4 id="实现矩阵的加减乘除。"><a href="#实现矩阵的加减乘除。" class="headerlink" title="实现矩阵的加减乘除。"></a>实现矩阵的加减乘除。</h4><p>加法：两个矩阵相加，只有在它们的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相加的结果。例如，如果A &#x3D; aij 和B &#x3D; bij 是同样大小的矩阵，那么它们的和C &#x3D; [ cij ]是矩阵 ,其中cij &#x3D; aij + bij。对应相加<br>减法：矩阵的减法与加法类似，只有在两个矩阵的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相减的结果。<br>乘法：矩阵的乘法比较复杂。如果A 是一个 m×n 的矩阵，B 是一个n×p 的矩阵，那么它们的乘积 AB 是一个 m×p 的矩阵，其元素由A 的行和 B 的列的对应元素的乘积之和给出。<br>除法：在矩阵中，通常不直接定义除法。但是，我们可以通过乘以逆矩阵来实现类似的效果。如果A是一个可逆的（也就是说，存在一个矩阵 （A-1）使得，A（A-1） &#x3D; （A-1）A &#x3D; I其中 𝐼I 是单位矩阵），那么我们可以定义B&#x2F;A为（BA-1），即是B矩阵除以A矩阵等于B乘以A矩阵的转置。但是，请注意，不是所有的矩阵都是可逆的。 </p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs makefile">import numpy as np<br><br><span class="hljs-comment"># 创建两个矩阵</span><br>A = np.array([[1, 2], [3, 4]])<br>B = np.array([[5, 6], [7, 8]])<br><br><span class="hljs-comment"># 矩阵加法</span><br>C = A + B<br><br><span class="hljs-comment"># 矩阵减法</span><br>D = A - B<br><br><span class="hljs-comment"># 矩阵乘法</span><br>E = np.dot(A, B)<br><br><span class="hljs-comment"># 矩阵除法（通过乘以逆矩阵）</span><br>F = np.dot(A, np.linalg.inv(B)) <br><br></code></pre></td></tr></table></figure><h3 id="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"><a href="#傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）" class="headerlink" title="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"></a>傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）</h3><h4 id="基本介绍。"><a href="#基本介绍。" class="headerlink" title="基本介绍。"></a>基本介绍。</h4><p>傅里叶变换是一种在数学、物理和工程中广泛使用的数学变换，它可以将一个函数或信号从其原始的时间或空间表示转换为频率表示。这对于许多应用都非常有用，因为它可以揭示信号的频率成分，这在原始的时间或空间表示中可能不明显。<br>傅里叶变换的基本思想是，任何函数都可以表示为一系列正弦波和余弦波的叠加。换句话说，我们可以将一个复杂的信号分解为一系列更简单的正弦波和余弦波。</p><h4 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h4><p>傅里叶变换的基本原理是将一个函数或信号从其原始的时间或空间表示转换为频率表示。这是通过将函数表示为一系列正弦波和余弦波的叠加来实现的。<br><img src="/pic/fly1.jpg" alt="傅里叶变换示意图"></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># 创建一个简单的信号</span><br><span class="hljs-attribute">t</span> = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">500</span>)<br><span class="hljs-attribute">f</span> = np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">50</span> * t) + <span class="hljs-number">0</span>.<span class="hljs-number">5</span> * np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">120</span> * t)<br><br><span class="hljs-comment"># 绘制原始信号</span><br><span class="hljs-attribute">plt</span>.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-attribute">plt</span>.plot(t, f)<br><span class="hljs-attribute">plt</span>.title(&#x27;Original Signal&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Time&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Amplitude&#x27;)<br><br><span class="hljs-comment"># 计算傅里叶变换</span><br><span class="hljs-attribute">F</span> = np.fft.fft(f)<br><br><span class="hljs-comment"># 计算频率</span><br><span class="hljs-attribute">freq</span> = np.fft.fftfreq(t.shape[-<span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 绘制频谱</span><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-attribute">plt</span>.plot(freq, np.abs(F))<br><span class="hljs-attribute">plt</span>.title(&#x27;Frequency Spectrum&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Frequency&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Magnitude&#x27;)<br><br><span class="hljs-attribute">plt</span>.tight_layout()<br><span class="hljs-attribute">plt</span>.show()<br><br></code></pre></td></tr></table></figure><h3 id="什么是对象？-封装，继承，多态是什么？"><a href="#什么是对象？-封装，继承，多态是什么？" class="headerlink" title="什么是对象？ 封装，继承，多态是什么？"></a>什么是对象？ 封装，继承，多态是什么？</h3><p>什么是对象？<br>在面向对象编程（Object-Oriented Programming，OOP）中，对象是类的实例。类是一种抽象的概念，用于描述具有相似属性和行为的对象的集合。对象是类的具体实现，它具有类定义的属性和方法。<br>对象可以看作是现实世界中的实体或概念在程序中的表示。每个对象都有自己的状态（属性）和行为（方法），并且可以与其他对象进行交互。</p><p>封装<br>封装是面向对象编程的一种重要概念，它将数据和操作数据的方法捆绑在一起，形成一个称为类的单个实体。封装隐藏了数据的内部实现细节，只暴露对外部可见的接口。这样可以保护数据的完整性，并提供更好的代码组织和维护性。<br>通过封装，对象的内部状态可以被保护起来，只能通过公共接口进行访问和修改。这样可以防止对数据的不合理访问和修改，增加了代码的安全性和可靠性。</p><p>继承<br>继承是面向对象编程中的另一个重要概念，它允许一个类继承另一个类的属性和方法。继承创建了一个类的层次结构，其中一个类（称为子类或派生类）可以从另一个类（称为父类或基类）继承属性和方法。<br>通过继承，子类可以继承父类的特性，并且可以添加自己的特定特性。这样可以实现代码的重用和扩展，减少了重复编写代码的工作量。</p><p>多态<br>多态是面向对象编程中的另一个重要概念，它允许使用统一的接口来处理不同的对象类型。多态性允许同一个方法在不同的对象上产生不同的行为。<br>通过多态，可以编写通用的代码，可以处理多个不同类型的对象，而无需针对每种类型编写特定的代码。这提高了代码的灵活性和可扩展性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 封装示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model</span>):<br>        self.brand = brand<br>        self.model = model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 Car 对象并访问其信息</span><br>my_car = Car(<span class="hljs-string">&quot;Toyota&quot;</span>, <span class="hljs-string">&quot;Corolla&quot;</span>)<br>my_car.display_info()<br><br><span class="hljs-comment"># 继承示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ElectricCar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model, battery_capacity</span>):<br>        <span class="hljs-built_in">super</span>().__init__(brand, model)<br>        self.battery_capacity = battery_capacity<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Electric Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>, Battery Capacity: <span class="hljs-subst">&#123;self.battery_capacity&#125;</span> kWh&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 ElectricCar 对象并访问其信息</span><br>my_electric_car = ElectricCar(<span class="hljs-string">&quot;Tesla&quot;</span>, <span class="hljs-string">&quot;Model S&quot;</span>, <span class="hljs-number">100</span>)<br>my_electric_car.display_info()<br><br><span class="hljs-comment"># 多态示例</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_car_info</span>(<span class="hljs-params">car</span>):<br>    car.display_info()<br><br><span class="hljs-comment"># 使用 show_car_info 函数展示不同类型的车辆信息</span><br>show_car_info(my_car)<br>show_car_info(my_electric_car)<br><br></code></pre></td></tr></table></figure><h3 id="python中的不同代码高亮表示什么？"><a href="#python中的不同代码高亮表示什么？" class="headerlink" title="python中的不同代码高亮表示什么？"></a>python中的不同代码高亮表示什么？</h3><p>在Python的IDLE编程环境中，不同颜色的文本表示不同的含义。以下是IDLE中常见的颜色及其含义：<br>黑色：普通的代码文本。<br>蓝色：关键字，例如if、else、for、while等。<br>绿色：字符串文本。<br>红色：语法错误或代码中的错误。<br>紫色：函数和方法的名称。<br>棕色：数字。<br>橙色：内置函数和模块的名称。<br>灰色：注释。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch介绍</title>
    <link href="/2024/04/19/deeplearnbook1/"/>
    <url>/2024/04/19/deeplearnbook1/</url>
    
    <content type="html"><![CDATA[<p>在深度学习，要永远抱着学徒的心。<br>本人参考书目为《Python深度学习基于PyTorch》 <a href="http://www.feiguyunai.com/">下载链接</a> <a href="https://github.com/Wumg3000/feiguyunai">使用下载链接——github</a></p><h1 id="目前深度学习的框架有什么？"><a href="#目前深度学习的框架有什么？" class="headerlink" title="目前深度学习的框架有什么？"></a>目前深度学习的框架有什么？</h1><ol><li>TensorFlow ：由Google开发的开源深度学习框架，提供了灵活性和高性能计算能力。TensorFlow 2.x版本引入了更加易用的Keras API作为主要接口。<a href="https://github.com/tensorflow/tensorflow">TensorFlow的github链接</a></li><li>PyTorch ：由Facebook开发的开源深度学习框架，以动态计算图的方式进行建模，易于调试和学习。PyTorch在研究领域广泛应用。<a href="https://github.com/pytorch/pytorch">PyTorch的github链接</a></li><li>Keras：最初作为独立的深度学习框架，现在已经成为TensorFlow的高级API。Keras提供了简单易用的接口，适合快速搭建深度学习模型。 <a href="https://github.com/keras-team/keras">Keras的github链接</a></li><li>MXNet：由Apache软件基金会支持的深度学习框架，具有高度可扩展性和灵活性。MXNet支持动态和静态计算图。[MXNet的github链接]（<a href="https://github.com/apache/mxnet%EF%BC%89">https://github.com/apache/mxnet）</a></li><li>CNTK (Microsoft Cognitive Toolkit)：由微软开发的深度学习框架，提供了高效的性能和多GPU支持。 <a href="https://github.com/microsoft/CNTK">CNTK的github链接</a></li><li>PaddlePaddle（百度飞桨）。这是一个由百度开发的开源深度学习平台，它为深度学习研究人员和开发者提供了丰富的API，支持多种模型结构，可以用来创建各种深度学习模型。[百度飞浆的链接]（<a href="https://www.paddlepaddle.org.cn/%EF%BC%89">https://www.paddlepaddle.org.cn/）</a></li></ol><h1 id="为什么要学习PyTorch？"><a href="#为什么要学习PyTorch？" class="headerlink" title="为什么要学习PyTorch？"></a>为什么要学习PyTorch？</h1><ol><li>pytorch是动态计算图，用法更接近python，并且pytoch与python共同使用了numpy的命令，降低了学习的门槛，比TensorFlow更容易上手</li><li>pytorch需要定义网络层、参数更新等关键步骤，有助于学习深度学习的核心（根据梯度更新参数。）</li><li>pytorch的流行仅次于TensorFlow。在github上的stareed为77.7K （此数据截止到2024&#x2F;4&#x2F;19日）</li><li>pytorch的动态图机制在调试方面非常简单，如果计算图运行出错，马上可以跟踪到问题。pytorch的调试和python一样，可以通过断点检查来解决问题。</li></ol><h1 id="解释一下这本书的结构"><a href="#解释一下这本书的结构" class="headerlink" title="解释一下这本书的结构"></a>解释一下这本书的结构</h1><ol><li>第一部分：介绍深度学习的基石Numpy，介绍PyTorch基础于pytorch构建神经网络的工具箱和数据处理工具。</li><li>第二部分：这本书的核心内容，包括机器学习的流程，常用算法和技巧等内容。实现了基于卷积神经网络的多个视觉处理实例，实现了多个自然语言处理、时间序列方面的实例。介绍了编码器——解码器模型、带注意力的编码器——解码器模型、对抗式生成器以及多种衍生生成器。（注：这里阐述一下关于深度学习、机器学习、人工智能之间的关系。人工智能包含机器学习，机器学习包含深度学习）</li><li>第三部分：实战部分，这部分在介绍相关原理、架构的基础上，使用了pytoch实现了多个深度学习典型实例，比如人脸识别、迁移学习、数据增强、中英文互译、生成式网络实例、模型迁移、强化学习、深度强化学习等实例。</li></ol><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="什么是python的断点检查？"><a href="#什么是python的断点检查？" class="headerlink" title="什么是python的断点检查？"></a>什么是python的断点检查？</h3><h3 id="什么是动态计算图？"><a href="#什么是动态计算图？" class="headerlink" title="什么是动态计算图？"></a>什么是动态计算图？</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔7</title>
    <link href="/2024/04/19/ganwu7/"/>
    <url>/2024/04/19/ganwu7/</url>
    
    <content type="html"><![CDATA[<h1 id="what-is-important-in-life？（生命中什么最重要？）"><a href="#what-is-important-in-life？（生命中什么最重要？）" class="headerlink" title="what is important in life？（生命中什么最重要？）"></a>what is important in life？（生命中什么最重要？）</h1><h3 id="Today-I-can’t-help-but-ask-myself-what-is-most-important-in-life-I-don’t-think-I-can-answer-the-question-because-I-don’t-know-how-to-answer-the-question-But-I-think-I-may-have-a-clue-to-this-problem-thinking-from-my-20-years-of-experience-I-often-worry-about-what-I-don’t-have-but-the-result-of-worrying-is-only-worrying-itself-which-does-nothing-to-change-the-status-quo-or-makes-the-result-worse-I-thought-I-needed-to-get-rid-of-this-worry-so-I-saw-death-I-saw-history-the-flow-of-the-past-the-men-I-have-learned-that-nothing-is-permanent-in-the-face-of-death-and-that-time-diminishes-everything-So-I-saw-the-past-the-future-in-the-past-regret-and-anxiety-about-the-future-saw-the-present-the-past-is-gone-the-future-future-instead-of-falling-into-the-past-regret-and-anxiety-about-the-future-why-not-change-a-mentality-to-spend-the-present-Life-has-no-meaning-instead-of-blindly-looking-for-the-meaning-of-the-castle-in-the-air-it-is-better-to-spend-the-present-with-gratitude-for-the-past-and-hope-for-the-future-Look-to-the-future-based-on-the-present-not-to-worry-about-the-future-the-future-is-not-to-worry-but-to-create-For-the-present-attitude-not-happy-with-things-not-sad-Lief-is-short-kiss-slowly-laugh-insanely-love-truly-and-forgive-quickly"><a href="#Today-I-can’t-help-but-ask-myself-what-is-most-important-in-life-I-don’t-think-I-can-answer-the-question-because-I-don’t-know-how-to-answer-the-question-But-I-think-I-may-have-a-clue-to-this-problem-thinking-from-my-20-years-of-experience-I-often-worry-about-what-I-don’t-have-but-the-result-of-worrying-is-only-worrying-itself-which-does-nothing-to-change-the-status-quo-or-makes-the-result-worse-I-thought-I-needed-to-get-rid-of-this-worry-so-I-saw-death-I-saw-history-the-flow-of-the-past-the-men-I-have-learned-that-nothing-is-permanent-in-the-face-of-death-and-that-time-diminishes-everything-So-I-saw-the-past-the-future-in-the-past-regret-and-anxiety-about-the-future-saw-the-present-the-past-is-gone-the-future-future-instead-of-falling-into-the-past-regret-and-anxiety-about-the-future-why-not-change-a-mentality-to-spend-the-present-Life-has-no-meaning-instead-of-blindly-looking-for-the-meaning-of-the-castle-in-the-air-it-is-better-to-spend-the-present-with-gratitude-for-the-past-and-hope-for-the-future-Look-to-the-future-based-on-the-present-not-to-worry-about-the-future-the-future-is-not-to-worry-but-to-create-For-the-present-attitude-not-happy-with-things-not-sad-Lief-is-short-kiss-slowly-laugh-insanely-love-truly-and-forgive-quickly" class="headerlink" title="Today, I can’t help but ask myself what is most important in life. I don’t think I can answer the question because I don’t know how to answer the question. But I think I may have a clue to this problem, thinking from my 20 years of experience, I often worry about what I don’t have, but the result of worrying is only worrying itself, which does nothing to change the status quo, or makes the result worse. I thought I needed to get rid of this worry, so I saw death, I saw history, the flow of the past, the men. I have learned that nothing is permanent in the face of death, and that time diminishes everything. So I saw the past, the future, in the past regret and anxiety about the future saw the present, the past is gone, the future future, instead of falling into the past regret and anxiety about the future, why not change a mentality to spend the present. Life has no meaning, instead of blindly looking for the meaning of the castle in the air, it is better to spend the present with gratitude for the past and hope for the future. Look to the future based on the present, not to worry about the future, the future is not to worry, but to create. For the present attitude, not happy with things, not sad. Lief is short, kiss slowly, laugh insanely, love truly and forgive quickly."></a>Today, I can’t help but ask myself what is most important in life. I don’t think I can answer the question because I don’t know how to answer the question. But I think I may have a clue to this problem, thinking from my 20 years of experience, I often worry about what I don’t have, but the result of worrying is only worrying itself, which does nothing to change the status quo, or makes the result worse. I thought I needed to get rid of this worry, so I saw death, I saw history, the flow of the past, the men. I have learned that nothing is permanent in the face of death, and that time diminishes everything. So I saw the past, the future, in the past regret and anxiety about the future saw the present, the past is gone, the future future, instead of falling into the past regret and anxiety about the future, why not change a mentality to spend the present. Life has no meaning, instead of blindly looking for the meaning of the castle in the air, it is better to spend the present with gratitude for the past and hope for the future. Look to the future based on the present, not to worry about the future, the future is not to worry, but to create. For the present attitude, not happy with things, not sad. Lief is short, kiss slowly, laugh insanely, love truly and forgive quickly.</h3><h3 id="今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！"><a href="#今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！" class="headerlink" title="今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！"></a>今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！</h3>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《诫子书》</title>
    <link href="/2024/04/18/jzs/"/>
    <url>/2024/04/18/jzs/</url>
    
    <content type="html"><![CDATA[<p>分享一下三国时期蜀汉丞相诸葛亮晚年写给他儿子诸葛瞻的一封家书。全文通过智慧理性、简练谨严的文字，将普天下为人父者的爱子之情表达得非常深切，成为历代学子修身立志的名篇。古人的文章在读的过程中结合自身的经历，能感受到很多，虽然和作者想表达的会有出入，但是自己感悟的就是自己感悟的，谁也不能拿走，谁也不能理解。如同此中有真意，欲辩已忘言。</p><h1 id="诫子书"><a href="#诫子书" class="headerlink" title="诫子书"></a>诫子书</h1><h2 id="夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。"><a href="#夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。" class="headerlink" title="夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。"></a>夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。</h2><p>自注：从论语中我们可以得到，圣人的任务是将人不知的世界改造成人不愠的世界。而君子最开始是人不知的，从人不知的群体中诞生的，那么怎么才能从人不知的群体中成长为君子。我想戒子书中给出了一个思路。<br>第一要有志向，但是志向不是说凭空随便想一个就行了，它必须要基于自身条件实现的可能性，不然随便设立的志向即无实现的可能，也把自己的精力给浪费了。那么怎么才能有一个好的志向？在我面前看来，不要期许有什么好的志向，先把眼前要做的做好，基于现在，现实，充分认识到现实存在条件，然后一天天的打算，一天天的实现自己的打算，在实践的过程中不断深化自己做事的观点，想法，我想慢慢的根植于自己内心的想法就能变成志向，而且这个志向会更有实现的可能性。（认知来源于实践，理论指导实践。）<br>第二要对事情的实现减少期望感，为什么要这样说？首先我们讨论事情的实现。怎么才能实现一件事情？构成这件事的基本条件满足时，那么这件事情就已经实现了。比如吃饭，吃饭有人，要有饭，构成吃饭的动作，那么吃饭这件事就可以开始了。那么怎么中断吃饭这件事勒？抽取吃饭的条件就行了，将吃饭的人给抽取，吃饭这件事就不能进行了。所以完成一件事很难，因为要满足各种条件，前提条件满足了，那么这件事就可能完成，为什么是可能完成，因为在事情发展的过程中还会存在各种各样的影响因素，使构成事情的基本条件被抽取，这样事情就不能够发展了。站在这个角度上，不对事情抱有期待感是有道理的。<br>第三要认识到积累的重要性，千里之行，始于足下。可能我看到这个人有很多奖项，很多我没有的条件。但是我想我没有看到的是，人家背后的付出，只看到人家怎么怎么样，没有看到人家付出时的艰辛。经济学上讲，要想收获什么，就必须付出什么。时间，金钱，必须要拿自己拥有的去换取自己想要的。为什么这里要谈积累的重要性？举个例子，拿这个博客来说把，最开始肯定是从第一篇开始的，不可能从第n篇开始吧。而这么多博文的内容，其背后必然是时间，知识的积累才能诞生的。</p><p><img src="/pic/jzs1.jpg" alt="路是靠走出来的，不是靠想出来的"></p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习之开篇</title>
    <link href="/2024/04/16/deeplearn1/"/>
    <url>/2024/04/16/deeplearn1/</url>
    
    <content type="html"><![CDATA[<h1 id="为什么要开这个坑"><a href="#为什么要开这个坑" class="headerlink" title="为什么要开这个坑"></a>为什么要开这个坑</h1><ol><li>因为我是人工智能专业的学生</li><li>在学习的过程中起到记录和反思的作用</li></ol><h1 id="开这个坑，打算怎么填坑"><a href="#开这个坑，打算怎么填坑" class="headerlink" title="开这个坑，打算怎么填坑"></a>开这个坑，打算怎么填坑</h1><p>慢慢填呗，图难于其易，为大于其细。天下难事必作于易，天下大事必作于细。是以圣人终不为大，故能成其大。</p><h1 id="这个坑的流程是什么？"><a href="#这个坑的流程是什么？" class="headerlink" title="这个坑的流程是什么？"></a>这个坑的流程是什么？</h1><ol><li>先讲pytorch</li><li>基于pytorch实现一些经典网络，完成一些案例</li><li>看论文，介绍一些经典的论文，比如AlexNet，VGG，CNN，GoolgeNet，shuffleNet</li><li>开始跑yolo，更改网络模块。</li></ol><p>注： 这里只是一个流程，在开始后会有更多的坑需要去填的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>峨眉山</title>
    <link href="/2024/04/14/tupian2/"/>
    <url>/2024/04/14/tupian2/</url>
    
    <content type="html"><![CDATA[<h1 id="清晨的树"><a href="#清晨的树" class="headerlink" title="清晨的树"></a>清晨的树</h1><p><img src="/pic/e1.jpg"><br><img src="/pic/e2.jpg"><br><img src="/pic/e3.jpg"><br><img src="/pic/e4.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>图片</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论语</title>
    <link href="/2024/04/14/luwnyu/"/>
    <url>/2024/04/14/luwnyu/</url>
    
    <content type="html"><![CDATA[<p>注： 本博客参考的论语注解来源于缠中说禅，仅作为学习使用。<br>论语不仅是一本修身的学说，更是一本治世的学说。<br>修身，齐家，平天下。<br>修身（定、静、安、虑、得）</p><h1 id="第一句"><a href="#第一句" class="headerlink" title="第一句"></a>第一句</h1><ol><li>子曰：学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？</li></ol><p>问：什么是学？<br>答：闻“圣人之道”、见“圣人之道”、“对照”“圣人”、在现实社会中不断地“校对”。<br>问：谁学？<br>答：君子。<br>问：学什么？<br>答：成“圣人”之道。<br>问：学了能成什么？<br>答：“圣人”。</p><p>闻，见，学，行<br>闻圣人之道，见圣人之道，学圣人之道，行圣人之道<br>行“圣人之道”的人就是要使得“不知之人”变得“不愠”，使得“不知之世界”变得“不愠”。<br>人不知，人不相，人不愠。</p><h1 id="第二句"><a href="#第二句" class="headerlink" title="第二句"></a>第二句</h1><ol start="2"><li>子曰：朝闻道夕死，可矣！<br>子曰： 朝闻，夕死。可矣！</li></ol><p>君子慎独，一旦开始走上圣人的道路，就要严格的要求自己。无论什么情况，与大家在一起时与自己一个人在一起时所表现出来的状态是一样的。<br>解释：君子从“闻其道”开始，无论任何地方，无论条件恶劣还是优越，甚至出生入死，都要不断地“固守”，“承担”“圣人之道”之行直到最终成就“不愠的世界”而不退转，只有这样，才可以行“圣人之道”呀。</p><p>自注： 立志走上成为圣人的这条道路上，就要时时刻刻牢记自己内心的要求，以现实为标准。时时刻刻的固守，达到内圣外王的境界。</p><h1 id="第三句"><a href="#第三句" class="headerlink" title="第三句"></a>第三句</h1><ol start="3"><li>子在川上曰：逝者如斯夫，不舍昼夜。</li></ol><p>解释：孔子在河流的源头，抚今追昔、满怀感慨，自告且忠告所有决心开始“见、学、行”“圣人之道”的君子：“立志“见、学、行”“圣人之道”的君子，就要像这江水一样，从“闻其道”的源头开始，后浪推前浪，生生不息、前赴后继，无论任何时候、任何地方，无论条件恶劣还是优越，甚至出生入死，都要不断地“固守”，“承担”“圣人之道”之行直到最终成就“不愠的世界”而不退转。”这里必须明确，这话既是孔子自己的感慨，也是对所有有志于圣人之道的人的忠告和勉励。</p><h1 id="第四句"><a href="#第四句" class="headerlink" title="第四句"></a>第四句</h1><ol start="4"><li>子曰：人能弘道，非道弘人。</li></ol><p>注：孔子说：“人能够把道发扬光大，不是道能把人发扬光大。”</p><h1 id="第五句"><a href="#第五句" class="headerlink" title="第五句"></a>第五句</h1><ol start="5"><li>子曰：攻乎异端，斯害也己。</li></ol><p>对于行“圣人之道”的君子，“异端”只不过是“别为一端行非圣人之道”的“不知”者，如果没有这种人，“圣人之道”之行就成了无源之水。“不知”，如同米；“不愠”，如同饭；<br>在我看来，这句话更像是说要允许矛盾的存在，不能只允许一种情况的存在。毕竟按照矛盾的观点来看，正是矛盾才使事物能够不断地发展。圣人也是从不知者中诞生的，如果不允许不知者的话那么就不可能诞生知者了。<br>那么问题来了，圣人的作用是什么？人不知，人不愠。天下大同。对于“别为一端行非圣人之道”的“不知”者，行“圣人之道”的君子不是要攻打他们、消灭他们，而是要如把“米”煮成“饭”般把他们从“不知”者变成“不愠”者，变成行“圣人之道”的君子，把“不知”的世界变成“不愠”的世界，只有这样，才算是真行“圣人之道”。</p><h1 id="第六句"><a href="#第六句" class="headerlink" title="第六句"></a>第六句</h1><ol start="6"><li>子曰：道，不同、不相为谋。<br>子曰： 道，不同，不相为谋。<br>不相： 不以外在为判断条件，要看其本质。类似于见相非相，即见如来（此中有真意，欲辩以忘言）<br>解释：<br>道，圣人之道，就如同大河，大河是不会去“选择”的、也不会去强迫“一致”，是“不相”、“不同”的。 “圣人之道”之“谋”，就是“不同”、“不相”。最常见的以“相”相之就是所谓的“以貌取人”，延伸下去，根据思想、观点、意识形态、经济水平等等，都是以“相”相之，都不是“不相”，是和“圣人之道”相违的。</li></ol><h1 id="第七句"><a href="#第七句" class="headerlink" title="第七句"></a>第七句</h1><ol start="7"><li>子曰：有教无类。<br>自注：学问就一定有好坏之分吗？什么是检验的标准，实践是检验真理的唯一标准。现实是检验真理的唯一标准。</li></ol><h1 id="第八句"><a href="#第八句" class="headerlink" title="第八句"></a>第八句</h1><ol start="8"><li>子曰：士志於道，而耻恶衣恶食者，未足与议也！<br>注：“耻恶衣恶食者”，就是“相”如果一个人，立志要行“圣人之道”，却把人分为“好衣好食”、“恶衣恶食”两类人，也就是以贫富划分人，而选择以“恶衣恶食”也就是穷人为耻，远离他们，那这种人谈论的“圣人之道”只是羊头狗肉的勾当。为什么？因为他不能“不相”。</li></ol><h1 id="第九句"><a href="#第九句" class="headerlink" title="第九句"></a>第九句</h1><ol start="9"><li>子曰：贤哉，回也！一箪食，一瓢饮，在陋巷，人不堪其忧，回也不改其乐。贤哉，回也！<br>自注：真评和加贫。真有本事和假有本事。<br>颜回这个“安贫乐道”的典型，并不是故意去“贫”，并不是故意要“恶衣恶食”，也不是如某些宗教所教唆的故意去苦行，这些都是严重地“相”了，这些都是和君子谋“圣人之道”所必须坚持的“不相”原则背道而驰的。</li></ol><h1 id="第十句"><a href="#第十句" class="headerlink" title="第十句"></a>第十句</h1><ol start="10"><li>子曰：贫而无怨难；富而无骄易。</li></ol><p>这句有多种解释：第一种，从人不知的社会到人不相的社会，过程中，贫穷而不怨恨是困难的，富贵而不骄横是容易的。<br>问题是贫穷真的会使人怨恨吗？富贵真的会使人不骄横吗？现实却又是穷人经常乐呵呵，富人却骄横无理。（穷乐呵，为富不仁。）<br>第二种解释，在任何“人不知”的社会中，都体现为“贫而怨难；富而骄易。</p><h1 id="第十一句"><a href="#第十一句" class="headerlink" title="第十一句"></a>第十一句</h1><ol start="11"><li>子贡曰：“贫而无谄，富而无骄，何如？”子曰：“可也；未若贫而乐，富而好礼者也。”</li></ol><p>“贫而谄”不得，最终就会“贫而怨难”，因“怨”有“仇”而“敌对”甚至“造反”，但“造反”成功的马上又成为“富而骄”，又有新的“贫而谄”，结果不断循环，都逃不出这个“贫而谄，富而骄”的“人不知”社会。<br>儒家看穿了这个“贫而谄，富而骄”的恶性循环，知道在这里打圈圈是没用的，而要打破这个恶性循环的办法，只有通过“人不相”而达到“人不愠”，最终摆脱“贫而谄，富而骄”的“人不知”的恶性循环。要实现这个打破，首先就要实现“贫而无谄，富而无骄”的“人不相”，为此，就必须要实现对“贫富”之相的“不相”，达到“人不相”。为什么实现对“贫富”之相的“不相”，就能实现“人不相”？是因为只要存在人与人的地方，就必然会出现各种方面的“贫富”之相，消灭这种“贫富”之相、将之抹平是不可能的，唯一办法就是使之“不相”，使得各种“贫富”之相能平等地存在，实现其“不同”，容纳各种“不同”而成其大，最终成就其“大同”。儒家、《论语》认为，这种“大同社会”的实现是当下的，是可以现世实现的，这种看法是由儒家的入世以及现世精神所决定的。<br>就像天幕红尘中说的，你要允许你得允许一部分人先高雅起来，一部分人后高雅起来，一部分人怎么也高雅不起来。” </p><h1 id="第十二句"><a href="#第十二句" class="headerlink" title="第十二句"></a>第十二句</h1><ol start="12"><li>子曰：齐一变，至於鲁；鲁一变，至於道。<br>在孔子时代是打着以“仁”以“德”治国的典型，号称传承着被孔子当成典范的周公之仁德。以“仁”以“德”治国，强调善的力量，对于一个习惯于以恶为前提的“人不知”世界是不可想象的，相比“齐式”国家模式，“鲁式”国家模式的出现是一种进步，所以才有“齐一变，至於鲁”的说法。</li></ol><h1 id="第十三句"><a href="#第十三句" class="headerlink" title="第十三句"></a>第十三句</h1><ol start="13"><li>子曰：放于利而行，多怨。<br>“放于利而行，多怨。”就是无论放弃还是放纵“利”而行，都会产生“多怨”的结果。 其实，现在的人对于这句话，肯定会更容易理解。计划经济年代，都是放弃“利”而行，结果是“多怨”；而市场经济年代，放纵“利”而行，结果还是“多怨”。这“人不知”社会的总规律，绝对不能放弃或放纵“利”而行，要充分把握其“利”，所谓用其刃而不被其刃所伤。行“圣人之道”的君子，首先要是“知人”，如果自己都还“不知”，又如何去让“人不知”之相“不相”？一事不知，儒者之耻，不尽量用这世界上的知识武装自己，是没资格当儒者的。</li></ol><h1 id="第十四句"><a href="#第十四句" class="headerlink" title="第十四句"></a>第十四句</h1><ol start="14"><li>子曰：好勇疾贫，乱也。人而不仁，疾之已甚，乱也。<br>“人不知”社会中同时存在的两种乱相：“贫者”，好勇斗狠；“富者”，为富不仁，被过分享乐之病急速传染，所谓纸醉金迷、醉生梦死。在“人不知”的社会，单纯的道德说教是没意义的，在“利”面前，所有的道德说教都苍白无力。这种“利”的“贫富”之相的严重对立，使得“富者”因得其“利”而放纵无度，而“贫者”因不得其“利”而不平。就算是一个懦夫，当“利”的“贫富”之相严重对立形成的落差储备到了足够大势能后，懦夫也会成为“勇夫”的。这样，自然就有了“好勇疾贫，乱也。人而不仁，疾之已甚，乱也。”。这种图景在“人不知”的社会随处可见、无处不在，《论语》早在两千多年前就已总结出来了。</li></ol><h1 id="第十五句"><a href="#第十五句" class="headerlink" title="第十五句"></a>第十五句</h1><ol start="15"><li>子曰：善人为邦百年，亦可以胜残去杀矣。诚哉是言也！<br>“胜残”、“去杀”，是两个意思相仿的词并列而成，简单说就是“战胜残暴、制止杀戮”；“善人”，就是“使人善”，“善”就是好的意思。“善人”和“胜残去杀”，其并列是一体的，如两腋之于人，双翼之于鸟，钱币的两面之于钱币。“善人、胜残去杀”，才可能“为邦百年”，让国家长治久安。“胜残去杀”，是针对“人而不仁，疾之已甚”，是针对为富不仁的“富者”，包括贼王暴君、贪官污吏、奸商恶霸等等，所谓杀一暴君而救亿万者乃真大仁矣；“善人”，是针对“好勇疾贫”的“贫者”，改善他们的生存条件、扩展他们的生存空间、提高他们的生存能力等等，都可以归之于“善人”之数。但必须强调的是，站在人和社会的整体角度，没有一个人是在任何方面都是“富”者，也没有一个人在任何方面都是“贫”者，但对于现实中的国家来说，经济、社会地位、权力等角度的“贫富”之相才最具有现实力量，这点也是不能忽视的。</li></ol><h1 id="第十六句"><a href="#第十六句" class="headerlink" title="第十六句"></a>第十六句</h1><ol start="16"><li>子曰：如有王者，必世而后仁。<br>一般来解答这一句为大致意思就成了“如果有称王的，一定要经过一世三十年，才能行其仁政。”国家长治久安的六字箴言“善人、胜残去杀”，而现实中，在“人不知”世界里，这六字箴言又有几人能办到？办不到，就必然是“城头变换大王旗”，中国历史上，这种改朝换代的事情，难道还不司空见惯？这种恶性循环中，有一个规律，就是本章的“如有王者，必世而后仁。”“王”，不一定需要有人当皇帝，如资本主义的确立也是一种王，其后到处贩卖的“民主、自由”就是“必世而后仁”了。</li></ol><h1 id="第十七句"><a href="#第十七句" class="headerlink" title="第十七句"></a>第十七句</h1><ol start="17"><li>子适卫，冉有仆。子曰：“庶矣哉！”冉有曰：“既庶矣，又何加焉？”曰：“富之。”曰：“既富矣，又何加焉？”曰：“教之。”<br>不同的社会，有不同的“庶、富、教”发展程度。而“全面发展的自由人的联合体”，就是“庶、富、教”充分发展所呈现的面貌。只有自由人，才会有多样性，才会有“不相”而“不同”，才有真正的“庶”；只有全面发展，才有真正的“富”；由“庶”而“富”，充分发展而形成“全面发展的自由人的联合体”所构成的社会结构，这才是真正的“教”。“庶、富、教”，就是不同成其大而大同。“庶、富”的发展水平，决定了“教”的发展水平，只有“庶、富”充分发展，才有“教”的充分发展，“庶、富”对应的是“人不相”，而“教”的充分发展最终对应的就是“人不愠”，只有“全面发展的自由人的联合体”构成的“教”，才是构成“人不愠”世界的社会结构基础。而只有“人不愠”，才是真正的“善人”。</li></ol><h1 id="第十八句"><a href="#第十八句" class="headerlink" title="第十八句"></a>第十八句</h1><ol start="18"><li>子曰：善人、教民七年，亦可以即戎矣。<br>通常解释为孔子说：“善人教导训练百姓七年时间，就可以叫他们去作战了”。“善人教民七年，亦可以即戎矣。”的通常断句是错的，应该是“善人、教民七年，亦可以即戎矣。”这一章是在彰显“善人”之道的力量，“教”的力量，文明的力量。“善人”之道，就是“圣人之道”一个具体过程中体现的具体形式，“圣人之道”最终要使得“人不知”的世界变成“人不愠”的世界，当然需要融合、同化那些未开化的、文明程度比较低的人、民族和国家，这是“人不知”世界一个很大的组成部分。如果说上一章更侧重于“善人”之道在国家范围的应用，那这一章就指出，“善人”之道在全世界实现的必然性，而只有在全世界的实现，才算真正的“善人”之道。孔子认为，作为“圣人之道”低级阶段的“善人”之道的实现也只能是一个全球性事件，大同，只能是大同天下，而不可能是某一国的大同，必然要“即戎”而达到天下大同。</li></ol><h1 id="第十九句"><a href="#第十九句" class="headerlink" title="第十九句"></a>第十九句</h1><ol start="19"><li>子曰：以不教民战，是谓弃之。<br>通常解释为“以不教民战”解释成“用不经教练的民众去临战阵”孔子说：“用没有经过军事训练的老百姓去打仗，这是有意让他们去送死。”。滑稽，断句应为，以不教，民战，是谓弃之。不行“善人”之道，那只能用“残、杀”，用所谓的白色恐怖来压制，企图让人民战栗、恐惧而治理国家。用“残、杀”企图使民众战栗、恐惧而治理国家的，就是遗弃、背叛民众，而最终也将被民众所遗弃。</li></ol><h1 id="第二十句"><a href="#第二十句" class="headerlink" title="第二十句"></a>第二十句</h1><ol start="20"><li>哀公问社於宰我。宰我对曰：“夏后氏以松，殷人以柏，周人以栗，曰，使民战栗。”子闻之，曰：“成事不说，遂事不谏，既往不咎。”<br>鲁哀公向孔子的弟子宰我问“土地神的祭祀”，宰我自作聪明道：“夏代用松木，殷代用柏木，而周代用栗木是为了借谐音使民战栗。”孔子听到，就告戒：“正成的事不要妄加评议，即成的事就不要徒劳劝告，已成的事就不要再生灾祸。”<br>“成事”，不是指已成的事，而是指正成的事，也就是在萌芽状态的，这时候，还需要观察，不能妄加评议，胡乱定性；“遂事”，马上就要成的事，已经无可挽回的，就不要徒费口舌去劝告了，这样只能产生怨恨；“既往”，已经过去的已成的事，要“不咎”，“咎”的本义是灾祸，已经成的事，如果错了，就不要错上加错，再生灾祸。这句话针对事物发展的三个不同阶段应该采取的态度。</li></ol><h1 id="第二十一句"><a href="#第二十一句" class="headerlink" title="第二十一句"></a>第二十一句</h1><ol start="21"><li>子曰：夷狄之有君、不如，诸夏之亡也。<br>“夷狄之有君、不如，诸夏之亡也。”的意思是：未开化的、文明程度比较低的人、民族和国家，虽然有他们自己的国体、政体，但由于没有遵从、依照文明程度比较高的人、民族和国家的政体、国体，而被后者所轻视。只要有不同的人、民族、国家同时存在，就必然有“诸夏”、“夷狄”之分，对于民族、国家来说，任何不行“圣人之道”的，无论是“齐式”的“王霸之道”还是“鲁式”的“仁德”之道，都必然会有“先进”对“落后”的轻视、压榨。一个国家、民族，如果不行“善人”之道，用“残、杀”企图让别国、别的民族战栗、恐惧而治理世界，就是遗弃、背叛各国、各民族，而最终也将被各国、各民族所遗弃。一个现成的例子，就是美国</li></ol><h1 id="第二十二句"><a href="#第二十二句" class="headerlink" title="第二十二句"></a>第二十二句</h1><ol start="22"><li>子曰：为政以德譬，如北辰居其所而众星共之。<br>这一章，其实就是上一章所说““圣人之道”、“善人之道”是大道，更是现实之道，无位可本，又何来“本位”？正因为无位可本，才可以无所位而生其本、无所本而生其位。这，才是真正的大道、现实之道。”的进一步展开。何谓“为政以德譬，如北辰居其所而众星共之。”？就是“无所位而生其本、无所本而生其位”。 只有明白了这句话，才可能真正明白马克思意义上的“具体问题具体分析”，也才可能真正明白何谓“为政以德譬，如北辰居其所而众星共之”。当人把北极星的位置确定后，执持这位置相应就可以定出其他星星位置；当人从现实出发分析把握了现实关系的逻辑结构后，“孰敢不正？</li></ol><h1 id="第二十三句"><a href="#第二十三句" class="headerlink" title="第二十三句"></a>第二十三句</h1><ol start="23"><li>季康子问政於孔子，孔子对曰：政者，正也，子帅以正，孰敢不正？<br>季康子，鲁国大夫，向孔子问政。“政者，正也”，为政，就是要立行“圣人之道”而成就之这一逻辑支点；“子帅以正，孰敢不正？”为政的人，遵循现实的逻辑，从现实出发，行“圣人之道”而成就之，其它问题就会以此为基础相应地找到解决的办法。这里必须要明确的是，现实，是最底层的支点，行“圣人之道”而成就之这个逻辑支点必须也必然在现实支点之上，离开现实，无所谓“圣人之道”。“圣人之道”，不是离开现实的乌托邦，那种把“圣人之道”装扮成某种口号、旗帜、目标，以此而驱使人，让人为此而折腾，都和“圣人之道”、《论语》、孔子毫无关系。人不是现实的奴隶，现实必须是人参与其中的，没有了人，也无所谓现实，更无所谓现实逻辑。 现实之于人，按其逻辑，有着各种不同的选择，究竟如何去选择，就构成了各色各样的政治。各种政治结构的逻辑支点，都来自现实，这逻辑支点也如同北极星，一旦确立，其它就以此为基础相应地构建。</li></ol><h1 id="第二十四句"><a href="#第二十四句" class="headerlink" title="第二十四句"></a>第二十四句</h1><ol start="24"><li>子曰：不在其位，不谋其政。<br>“不在其位，不谋其政”，就是“不谋不在其位之政”，不谋划与现实变化的位次不符的政事、政治关系、政治制度、上层建筑、生产关系等等。一切都从也只能从现实出发，现实在什么阶段，什么位次，是必须首要分析的问题。</li></ol><h1 id="第二十五句"><a href="#第二十五句" class="headerlink" title="第二十五句"></a>第二十五句</h1><ol start="25"><li>子曰：“不在其位，不谋其政。”曾子曰：“君子思不出其位。”</li></ol><h1 id="第二十六句"><a href="#第二十六句" class="headerlink" title="第二十六句"></a>第二十六句</h1><ol start="26"><li>子曰：“不患，无位；患，所以立。不患，莫己知求，为可知也。”<br>“无所位而生其本、无所本而生其位”，即所“立”、即所“止”、即所“位”。有所“立”，则“立”其“有”，其“有”必有其“位”<br>儒家，内圣、外王，“不在其位，不谋其政”的外王，是和“不患，无位；患，所以立。不患，莫己知求，为可知也。”的内圣相互相成的。这是参悟儒家之说的大关键。</li></ol><p>缠中说禅白话直译<br>子曰：“不患，无位；患，所以立。不患，莫己知求，为可知也。”<br>孔子说：“不患”，无位次；“患”，以“不患”的“无位次”而“位次”。“不患”，不以自己“所知”来选择，就是“能知”。</p><h1 id="第二十七"><a href="#第二十七" class="headerlink" title="第二十七"></a>第二十七</h1><ol start="27"><li>子曰：不患人之不己知；患其不能也。<br>通译：不要担心别人不了解自己，应该担心的是自己不了解别人。<br>正因为“不明了”的现实，所以才有了自己不断“明了”自己的可能，所以才有了“明了”的可能，不明白这一点，是不可能明白何谓“内圣”的。</li></ol><p>缠中说禅白话直译<br>子曰：“不患人之不己知；患其不能也。”<br>孔子说：不患别人或自己不明了自己，患别人或自己不能明了自己啊。</p><h1 id="第二十八句"><a href="#第二十八句" class="headerlink" title="第二十八句"></a>第二十八句</h1><ol start="28"><li>子曰：“不患人之不己知；患不知人也。”<br>通译：“不要担心别人不了解自己，应该担心的是自己不了解别人。”<br>缠中说禅白话直译<br>子曰：“不患人之不己知；患不知人也。”<br>孔子说：不患人不明了自己，患“人不知”的世界啊。</li></ol><h1 id="第二十九句"><a href="#第二十九句" class="headerlink" title="第二十九句"></a>第二十九句</h1><ol start="29"><li>子曰：“性相，近也；习相，远也。”</li></ol><p>缠中说禅白话直译<br>子曰：性相，近也；习相，远也。<br>孔子说：以性性相，缠附呀；以习习相，深奥啊。</p><h1 id="第三十句"><a href="#第三十句" class="headerlink" title="第三十句"></a>第三十句</h1><ol start="30"><li>子曰：人无远虑，必有近忧。<br>“远”，深远、深奥，同于“习相，远也”，和“习相”相关，脱离“习相”无所谓深远、深奥，不过幻想而已。“习相”，先要明其“相”，明其“相”必先明其“相”之位次，明其“相”之位次，必对其“相”的当下逻辑关系有一明确把握。<br>“虑”，审察、思虑、谋划。“虑”，不是哈姆雷特式的，而是审察、思虑、谋划的统一，三者缺一不可，而最终必须落在行动上，没有行动的“虑”也不过是幻想而已。<br>人的行为，必须从其苗头下手，不想吃恶果，最简单的方法就是不要种下其种子，忧患、祸患的种子一旦缠附，一有机会就会萌芽，就要结果。别以为可以用任何方法可以消除这种子，种子一旦种下就是无位次的，准确说，相对于现实系统来说，种子是无位次的，任何现实的把戏都消灭不了种子，种子不一定在眼前发芽，但不发芽只是机会不成熟，一旦成熟，逃都逃不掉，眼前看不到、没迹象的忧患、祸患，往往才是致命的。而这，才是真正的“近忧”。</li></ol><p>缠中说禅白话直译<br>子曰：人无远虑，必有近忧。<br>孔子说：人没有深远的审察、思虑、谋划，必然缠附祸患。</p><h1 id="第三十一句"><a href="#第三十一句" class="headerlink" title="第三十一句"></a>第三十一句</h1><ol start="31"><li>子曰：众，恶之，必察焉；众，好之，必察焉。</li></ol><p>缠中说禅白话直译<br>子曰：众，恶之，必察焉；众，好之，必察焉。<br>孔子说：一切现象，当被认为是恶的就会被厌恶，对此必须摈弃一切厌恶当下直观；一切现象，当被认为是好的就会被喜好，对此必须摈弃一切喜好当下直观。（恶并不是恶，好的并不一定是好的。）</p><h1 id="第三十二句"><a href="#第三十二句" class="headerlink" title="第三十二句"></a>第三十二句</h1><ol start="32"><li>子曰：视，其所以；观，其所由；察，其所安。人焉廋哉？人焉廋哉？<br>视”，人与认识对象之间的看，相当于感性以及康德规定性判断力所连接的知性与理性所构成的高级人类认识能力，也就是人类所有的认识能力；“观”，看法，相当于“反思判断力”所连接的自由意志；“察”，当下的直“观”，是自由意志的当下实践。“视，其所以”，认识能力是人所凭借的；</li></ol><p>缠中说禅白话直译<br>子曰：视，其所以；观，其所由；察，其所安。人焉廋哉？人焉廋哉？<br>孔子说：认识能力，人的凭借；自由意志，人的遵从；当下直 “观”，即自由意志的当下实践，人的归依。人，哪里有隈曲啊？人，哪里有隈曲啊？</p><h1 id="第三十三句"><a href="#第三十三句" class="headerlink" title="第三十三句"></a>第三十三句</h1><ol start="33"><li>子曰：不知，命无以为君子也；不知，礼无以立也；不知，言无以知人也。</li></ol><p>缠中说禅白话直译<br>子曰：不知，命无以为君子也。不知，礼无以立也。不知，言无以知人也。<br>孔子说：没有智慧，不可能承担君子的使命；没有智慧，不可能建立社会正常的秩序；没有智慧，不可能产生使人智慧的言论。</p><h1 id="第三十四句"><a href="#第三十四句" class="headerlink" title="第三十四句"></a>第三十四句</h1><ol start="34"><li>子曰：由知、德者，鲜矣！<br>圣人之道”，就是将“人不知”的世界变为“人不愠”世界的道路，这里没有任何固定的模式和先验的走法，路是人走出来的，是人所“由”而来，是人所“蹈行，践履”而来。没有人的“蹈行，践履”，何来路？除了“知、德”，行“圣人之道”的君子无所“蹈行，践履”也无须“蹈行，践履”。</li></ol><p>缠中说禅白话直译<br>子曰：由知、德者，鲜矣！<br>孔子说：蹈行、践履“闻、见、学、行”“圣人之道”智慧、所得的君子，永远处在创新、创造之中啊。</p><h1 id="第三十五句"><a href="#第三十五句" class="headerlink" title="第三十五句"></a>第三十五句</h1><ol start="35"><li>子曰：民可，使由之；不可，使知之。</li></ol><p>缠中说禅白话直译<br>子曰：民可，使由之；不可，使知之。<br>孔子说：民众当下适合的，放任民众去蹈行、践履；民众当下不适合的，放任民众运用智慧去创造、创新。</p><h1 id="第三十六句"><a href="#第三十六句" class="headerlink" title="第三十六句"></a>第三十六句</h1><ol start="36"><li>子曰：由诲女，知之乎！知之为，知之；不知为，不知；是知也！</li></ol><p>缠中说禅白话直译<br>子曰：由诲女，知之乎！知之为，知之；不知为，不知；是知也。<br>孔子说：实践教导你，以此而有智慧啊。依智慧而进一步实践，以此而有新的智慧；不依以实践而有的智慧进一步实践，就不会有新的智慧。这，就是最根本的智慧。</p><p>自注： 认识来源于实践，从无知到有一定的感性认识，再从感性认识到一定的理性认识。认识是一个过程，不可以说是直接没有感性认识就产生了理性认识。在认识的过程中当然时时刻刻伴随着实践，正式有了实践才让认识不断深化。</p><h1 id="第三十七句"><a href="#第三十七句" class="headerlink" title="第三十七句"></a>第三十七句</h1><ol start="37"><li>子曰：我非生而知之者，好古，敏以求之者也。<br>本章，孔子提出了学习前人知识、智慧的三个步骤：好、敏、求。 首先，对前人知识、智慧所凝结成的遗典、典章等必须尊重、善待进而学习、研究，才谈得上“好”。尊重、善待进而学习、研究，真正把握以后，还需要在实践中继续印证，这才是“敏”。“敏”，有两层的含义：其一，前人知识、智慧都来源于其当下的实践，而时代变化了，条件变化了，其应用可能要失效，可能有所改变，这必须在实践中才能印证、发现；其二，对前人知识、智慧的把握，特别对于那些洞穿时间的智慧的把握，必须在实践中慢慢体会、摸索，才能发现前人的真义，决不能像某些人对待孔子、马克思那样，根本没弄明白就扮代表，这样是谈不上“好”，更谈不上“敏”了。有了印证，自然就有了选择的基础，选择不是机械地挑选，不是用对错等简单标准来划分，而是根据当下的实践有机地发展、延伸，这样才不辜负古人，也不辜负自己，这才算得上是“求”。</li></ol><p>缠中说禅白话直译<br>子曰：我非生而知之者，好古，敏以求之者也。<br>孔子说：我不是天生、先验地依赖天生、先验而有智慧的人，只是爱好学习、研究先哲遗典、古代典章，并在实践中对此印证、选择的人。</p><p>自注： 这句话在讲前人的经验可以学习，但是不能直接使用。为什么这么说，因为条件改变了，当时这个方法能解决这个问题，那是因为由实践这个方法的条件，而现在没有满足这个方法的条件，所以这个问题使用这个方法就解决不了。一句话，实事求是。那么有没有普世的方法？答案是没有的，但是有普世的法则，内心有内心的心法，社会有社会的法则。熟悉和了解这些法则的过程中必然要经历大量的实践，才能认识到。但是即使认识到这些法则，能不能熟练的用于也是一件困难的事情，知之为知之，不知为不知。迎接挑战，这样的人生才有意义。征服一个又一个巅峰，达到内圣外王的境界。</p><h1 id="第三十八句"><a href="#第三十八句" class="headerlink" title="第三十八句"></a>第三十八句</h1><ol start="38"><li>孔子曰∶生而知之者，上也；学而知之者，次也；困而学之，又其次也。困而不学，民斯为下矣！<br>缠中说禅白话直译<br>孔子曰∶生而知之者，上也；学而知之者，次也；困而学之，又其次也。困而不学，民斯为下矣！<br>孔子说：所有人，天生地依赖天生而有智慧，是最好的；所有人，都能自由地学习且通过学习而有智慧，是稍差的；所有人，被分为不同类别而得到不同类别的学习，是更差的。所有人，被分为不同类别而某类人得不到学习的机会，这就是民众被当成卑下的原因啊。</li></ol><h1 id="第三十九句"><a href="#第三十九句" class="headerlink" title="第三十九句"></a>第三十九句</h1><ol start="39"><li>子曰：盖有不知而作之者，我无是也。多闻，择其善者而从之；多见而识之；知之次也。</li></ol><p>缠中说禅白话直译<br>子曰∶盖有不知而作之者，我无是也。多闻，择其善者而从之；多见而识之；知之次也。<br>孔子说：大概存在没有智慧却凭没有智慧而有所作为的人，我不是这样的。在一个能让每个人都能自由见闻的社会里，尽可能地扩展自己的见闻，选择超过自己的见解，依据其见解而不是依据有此见解的人或群体，深入探讨、吸收学习；进而让自己的见识逐步深厚，才能更清楚地去辨别、辩正各种知识的真伪、深浅。但这些都是智慧的临时落脚处，不是智慧的真正所在。</p><h1 id="第四十句"><a href="#第四十句" class="headerlink" title="第四十句"></a>第四十句</h1><ol start="40"><li>子张学干禄。子曰：多闻阙疑，慎言其余，则寡尤。多见阙殆，慎行其余，则寡悔。言寡尤，行寡悔，禄在其中矣。</li></ol><p>缠中说禅白话直译<br>子张学干禄。子曰：多闻阙疑，慎言其余，则寡尤。多见阙殆，慎行其余，则寡悔。言寡尤，行寡悔，禄在其中矣。<br>孔子说：子张求问获取福运的方法。孔子说：见闻广泛而去除疑惑，见识深厚而去除危险，遵循如此“闻见”而如此“言行”，那么言行都会少过失。言行少过失，福运在其中啊。</p><h1 id="第四十一句"><a href="#第四十一句" class="headerlink" title="第四十一句"></a>第四十一句</h1><ol start="41"><li>子曰∶君子谋道不谋食。耕也，馁在其中矣；学也，禄在其中矣。君子忧道不忧贫。</li></ol><p>缠中说禅白话直译<br>子曰∶君子谋道不谋食。耕也，馁在其中矣；学也，禄在其中矣。君子忧道不忧贫。<br>孔子说：“闻、见、学、行”“圣人之道”的君子，按“道之谋”谋划而不按“食之谋”谋划。以人的欲望饥饿为基础的生产，新的欲望饥饿就在其中啊；以人与天地关系中对照、校对确定人之所需，福运、真正的幸福就在其中啊。君子只担忧如何“闻、见、学、行”“圣人之道”的“道之谋”，而不担忧“馁、耕、食” “食之谋”的恶性循环必然导致的人在物质与精神上的贫穷。</p><p>自注： 经常的被不知道来源的焦虑所裹挟，思考一下来源有嘈杂的网络环境，以贩卖焦虑为谋利的文章。君子谋道不谋食，这个时代，饿死还是很难的，解决食之谋的方法还是有很多的。减掉一些愿望，不做一些没有实现可能的幻想，仔细想想我有的，然后再想我这么使用我有的来实现我想要的。</p><h1 id="第四十二句"><a href="#第四十二句" class="headerlink" title="第四十二句"></a>第四十二句</h1><ol start="42"><li>子曰：君子不器。</li></ol><p>缠中说禅白话直译<br>子曰∶君子不器。<br>孔子说：君子不相。</p><h1 id="第四十三句"><a href="#第四十三句" class="headerlink" title="第四十三句"></a>第四十三句</h1><ol start="43"><li>子曰：古之学者为己；今之学者为人。</li></ol><p>缠中说禅白话直译<br>子曰：古之学者为己；今之学者为人。<br>孔子说：无论古今，真正的学问与学人，都不离“内圣外王”、“为己为人”的一体之学。</p><p>自注：“是故内圣外王之道，暗而不明，郁而不发，天下之人，各为其所欲焉，以自为方。”<br>在王阳明看来，“内圣”的基础，是人之为人必要有的独立人格，恰如孔子所说的“古之学者为己”。在王阳明看来，唯有找到“天理”的指引，人才有内在驱动力，才能找到人生方向，而获得“天理”途径，便是通过“格物”进而“正心”，通过“知行合一”。通过书本获得知识算不得“理”，只有在生活中对其进行实践、验证进而获得的个人独特的理解，才算是“真知”。博学、审问、慎思、明辨、笃行者，皆所以为惟精而求惟一也。他如博文者，即约礼之功；格物致知者，即诚意之功；道问学即尊德性之功；明善即诚身之功：无二说也。”观诸圣之一学：基督教曰树一、恒一；伊斯兰曰独一无二；印度教曰不二； 佛教曰三昧(一境)；道教曰贞一；黄帝曰守一； 管子曰专一； 老子曰执一； 孔子曰精一； 山人说的就是一个一，故人戏称山人为一先生、不二先生。人要有所作为就必须独善其一，国家民族之强盛势必用一，有统一的思想，有惟一的民族哲学理念。</p><h1 id="第四十四句"><a href="#第四十四句" class="headerlink" title="第四十四句"></a>第四十四句</h1><ol start="44"><li>子曰：三年学不至，於榖不易，得也。</li></ol><p>缠中说禅白话直译<br>子曰：三年学不至，於榖不易，得也。<br>孔子说：多年闻“圣人之道”、见“圣人之道”、“对照”“圣人”、在现实社会中不断地“校对”，虽然不能达到尽善尽美，但能对“圣人之道”的“学”达到一生不退转的位次，这才算是“学”有所得啊。</p><p>自注： 闻，见，学，行圣人之道，就要坚定的走下去。</p><h1 id="第四十五句"><a href="#第四十五句" class="headerlink" title="第四十五句"></a>第四十五句</h1><ol start="45"><li>子曰：学如不及，犹恐失之。</li></ol><p>缠中说禅白话直译<br>子曰：学如不及，犹恐失之。<br>孔子说：闻“圣人之道”、见“圣人之道”、“对照”“圣人”、在现实社会中不断地“校对”而不能达到尽善尽美，是因为踌躇、恐惧、疑虑使它迷失而不能直下承担。</p><h1 id="第四十六句"><a href="#第四十六句" class="headerlink" title="第四十六句"></a>第四十六句</h1><ol start="46"><li>子曰：学而不思则罔，思而不学则殆。<br>学和思本来就是一体的。</li></ol><p>缠中说禅白话直译<br>子曰：学而不思则罔，思而不学则殆。<br>孔子说：将差异性的“学”与同一性的“思”分开，都只能迷惘、疲怠而无所得。</p><h1 id="第四十七句"><a href="#第四十七句" class="headerlink" title="第四十七句"></a>第四十七句</h1><ol start="47"><li>子曰：唯！女子与小人为难、养也。近之则不孙，远之则怨。</li></ol><p>缠中说禅白话直译<br>子曰：唯！女子与小人为难、养也。近之则不孙，远之则怨。<br>孔子说：是的！你的儿女跟随小人而“闻、见、学、行”，就产生灾难、痒疾。依附小人，就失去子嗣；违背小人，就埋下仇恨。</p><h1 id="第四十八句"><a href="#第四十八句" class="headerlink" title="第四十八句"></a>第四十八句</h1><ol start="48"><li>子曰：唯上知与下愚不移。</li></ol><p>缠中说禅白话直译<br>子曰：唯上知与下愚不移。<br>孔子说：愿真正“见、闻、学、行”“圣人之道”的君子，结交、亲附没有智慧、充满贪婪、恐惧的小人而成就“见、闻、学、行”“圣人之道”的不退转。</p><h1 id="第四十九句"><a href="#第四十九句" class="headerlink" title="第四十九句"></a>第四十九句</h1><ol start="49"><li>子曰：温故而知新，可以为师矣。</li></ol><p>缠中说禅白话直译<br>子曰：温故而知新，可以为师矣。<br>孔子说：应当把“积聚、蕴藏故有的、经过时间沉淀、检验的智慧而保持智慧当下鲜活的创造与呈现”作为君子“见、闻、学、行”“圣人之道”所师法的目标啊。</p><p>自注： 温故而知新”有四解。</p><p>1、温故才知新，温习已学的知识，并且由其中获得新的领悟；</p><p>2、温故及知新，一方面要温习典章故事，另一方面又努力撷取新的知识；</p><p>3、温故，知新。随着自己阅历的丰富和理解能力的提高，回头再看以前看过的知识，总能从中体会到更多的东西；</p><p>4、是指通过回味历史，而可以预见，以及解决未来的问题。这才是一个真正的大师应该具有的能力。</p><h1 id="第五十句"><a href="#第五十句" class="headerlink" title="第五十句"></a>第五十句</h1><ol start="50"><li>子曰：吾十有五而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲不逾矩。</li></ol><p>缠中说禅白话直译<br>子曰：吾十有五而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲不逾矩。<br>孔子说：我十五岁的境界、所为用“从此闻见学行圣人之道”来标记，三十岁的境界、所为用“穷尽闻见学行圣人之道的现实可能位次”来标记，四十岁的境界、所为用“透彻闻见学行圣人之道现实可能位次的不患”来标记，五十岁的境界、所为用“闻见学行圣人之道让智慧依当下生存鲜活地呈现”来标记，六十岁的境界、所为用“遵循当下生存鲜活呈现的智慧而闻见学行圣人之道以成就内圣”来标记，七十岁的境界、所为用“依从民心期望但不超越闻见学行圣人之道在当下现实中可能实现位次而成就外王”来标记。</p><h1 id="第五十一句"><a href="#第五十一句" class="headerlink" title="第五十一句"></a>第五十一句</h1><ol start="51"><li>子曰：君子，食无求饱，居无求安；敏於事而慎於言；就有，道而正焉；可谓好学也已。</li></ol><p>缠中说禅白话直译<br>子曰：君子，食无求饱，居无求安；敏於事而慎於言；就有，道而正焉；可谓好学也已。<br>孔子说：“闻见学行”“圣人之道”的人，对欲望不贪求从而满足，对生存的环境不贪求从而安身；通过当下的事情去印证，使得理论、言论顺应当下的实际；对现实究底穷源，使现实行“圣人之道”而在现实中成就之，称之为“好学”，是适当的啊。</p><p>自注： 走上圣人之道的人，把对欲望的不贪求看作满足。</p><h1 id="第五十二句"><a href="#第五十二句" class="headerlink" title="第五十二句"></a>第五十二句</h1><ol start="52"><li>子曰：十室之邑，必有忠信如丘者焉，不如丘之好学也。</li></ol><p>缠中说禅白话直译<br>子曰：十室之邑，必有忠信如丘者焉，不如丘之好学也。<br>孔子说：所有国家，倘若有遵从我的“忠信”标准的在其中，不若有遵从我的“好学”标准的在其中。</p><h1 id="第五十三句"><a href="#第五十三句" class="headerlink" title="第五十三句"></a>第五十三句</h1><ol start="53"><li>子曰：三人行，必有我师焉：择其善者而从之，其不善者而改之。</li></ol><p>缠中说禅白话直译<br>子曰：三人行，必有我师焉：择其善者而从之，其不善者而改之。<br>孔子说：与“君、父、师”同行，倘若有让我师法的在此：选取他们完善的并在当下现实更广泛的范围应用、检验，选取他们不完善的并在当下现实中不断修改、完善。</p><h1 id="第五十四句"><a href="#第五十四句" class="headerlink" title="第五十四句"></a>第五十四句</h1><ol start="54"><li>子夏曰：日知其所亡，月无忘其所能，可谓好学也已矣！<br>钱穆：子夏说：“每天能知道所不知道的，每月能不忘了所已能的，可说是好学了。”<br>知识是知识，技术高于知识，心法高于技术，法则高于心法。</li></ol><h1 id="第五十五句"><a href="#第五十五句" class="headerlink" title="第五十五句"></a>第五十五句</h1><ol start="55"><li>子夏曰：仕而优则学；学而优则仕。<br>钱穆：子夏说：“仕者有余力宜从学。学者有余力宜从仕。”</li></ol><h1 id="第五十六句"><a href="#第五十六句" class="headerlink" title="第五十六句"></a>第五十六句</h1><ol start="56"><li>子夏曰：百工居肆以成其事；君子学以致其道。<br>钱穆：子夏说：“百工长日居住肆中以成其器物，君子终身在学之中以求致此道。”<br>直译大致就是：就像各种工匠在手工业作坊里为完成他们的制作，君子在学中为完成他们的事业。</li></ol><h1 id="第五十七句"><a href="#第五十七句" class="headerlink" title="第五十七句"></a>第五十七句</h1><ol start="57"><li>子谓子夏曰：女为君子儒！无为小人儒！<br>钱穆：先生对子夏道：“你该为一君子儒，莫为一小人儒。”</li></ol><p>自注： 这句话更像老人对孩子的劝诫，重点就是看怎么解释这君子儒和小人儒了。</p><h1 id="第五十八句"><a href="#第五十八句" class="headerlink" title="第五十八句"></a>第五十八句</h1><ol start="58"><li>哀公问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不迁怒，不贰过。不幸短命死矣，今也则亡，未闻好学者也。”<br>钱穆：鲁哀公问孔子道：“你的学生们，哪个是好学的呀？”孔子对道：“有颜回是好学的，他有怒能不迁向别处，有过失能不再犯。可惜短寿死了，目下则没有听到好学的了。”</li></ol><h1 id="第五十九句"><a href="#第五十九句" class="headerlink" title="第五十九句"></a>第五十九句</h1><ol start="59"><li>季康子问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不幸短命死矣！今也则亡。”<br>钱穆：季康子问孔子：“你的弟子哪个是好学的呀？”孔子对道：“有颜回是好学的，不幸短命死了，现在是没有了。”</li></ol><h1 id="第六十句"><a href="#第六十句" class="headerlink" title="第六十句"></a>第六十句</h1><ol start="60"><li>子曰：语之而不惰者，其回也与？</li></ol><p>缠中说禅白话直译<br>子曰：语之而不惰者，其回也与？<br>孔子说：任何人与他辩论而他都能语不衰败的所谓能辩之士，难道只有颜回吗？</p><h1 id="第六十一句"><a href="#第六十一句" class="headerlink" title="第六十一句"></a>第六十一句</h1><ol start="61"><li>子贡问君子。子曰：先行其言而后从之。</li></ol><p>缠中说禅白话直译<br>子贡问君子。子曰：先行其言而后从之。<br>子贡问君子，孔子说：“先使自己的言论、思想以及相应的行为一以贯之，然后再使之广泛。”</p><h1 id="第六十二句"><a href="#第六十二句" class="headerlink" title="第六十二句"></a>第六十二句</h1><ol start="62"><li>子贡问曰：“赐也何如？”子曰：“女，器也。”曰：“何器也？”曰：“瑚琏也。”</li></ol><p>缠中说禅白话直译<br>子贡问曰：“赐也何如？”子曰：“女，器也。”曰：“何器也？”曰：“瑚琏也。”<br>子贡问：“我，怎么样？”孔子说：“你，“器”呀。”问：“什么器皿？”答：“宗庙里盛黍稷的瑚琏那样的名贵器皿”</p><h1 id="第六十三句"><a href="#第六十三句" class="headerlink" title="第六十三句"></a>第六十三句</h1><ol start="63"><li>子贡问曰：“有一言而可以终身行之者乎？”子曰：“其恕乎？己所不欲，勿施於人。”</li></ol><p>缠中说禅白话直译<br>子贡问曰：“有一言而可以终身行之者乎？”子曰：“其恕乎？己所不欲，勿施於人。”<br>子贡问：“有可以终身一而贯之的言论吗？”孔子说：“自己不想要的就不施加给别人，难道就是“恕”吗？”</p><h1 id="第六十四句"><a href="#第六十四句" class="headerlink" title="第六十四句"></a>第六十四句</h1><ol start="64"><li>子贡曰：“我不欲人之加诸我也，吾亦欲无加诸人。”子曰：“赐也，非尔所及也。”</li></ol><p>缠中说禅白话直译<br>子贡曰：“我不欲人之加诸我也，吾亦欲无加诸人。”子曰：“赐也，非尔所及也。”<br>子贡问：“我不想别人诬枉我，我也不想诬枉别人。”孔子说：“子贡啊，这不是你所能达到的。”</p><h1 id="第六十五句"><a href="#第六十五句" class="headerlink" title="第六十五句"></a>第六十五句</h1><ol start="65"><li>子曰：“赐也，女以予为多学而识之者与？”对曰：“然，非与？”曰：“非也！予一以贯之。”</li></ol><p>缠中说禅白话直译<br>子曰：“赐也，女以予为多学而识之者与？”对曰：“然，非与？”曰：“非也！予一以贯之。”<br>孔子问：“子贡啊，你把我当成不断学习从而了解现实当下的人吗？”子贡回答：“对，不是这样吗？”孔子说：“不是啊，我只是直下承担当下现实而贯通它。”</p><h1 id="第六十六句"><a href="#第六十六句" class="headerlink" title="第六十六句"></a>第六十六句</h1><ol start="66"><li>子曰：“参乎！吾道一以贯之。”曾子曰：“唯。”子出。门人问曰：“何谓也？”曾子曰：“夫子之道，忠恕而已矣。”</li></ol><p>缠中说禅白话直译<br>子曰：“参乎！吾道一以贯之。”曾子曰：“唯。”子出。门人问曰：“何谓也？”曾子曰：“夫子之道，忠恕而已矣。”<br>孔子说：“曾参啊！我“闻见学行”圣人之道一以贯之。”曾参说：“是。”孔子出去。孔子的其他弟子问：““一以贯之”是什么意思？”曾参回答：“老师的道理，只是“尽已之心以待人，推己之心以及人”罢了。”</p><h1 id="第六十七句"><a href="#第六十七句" class="headerlink" title="第六十七句"></a>第六十七句</h1><ol start="67"><li>有子曰：其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道生。孝弟也者，其为仁之本与！<br>钱穆：有子说：“若其人是一个孝弟之人，而会存心喜好犯上的，那必很少了。若其人不喜好犯上，而好作乱的，就更不会有了。君子专力在事情的根本处，根本建立起，道就由此而生了。孝弟该是仁道的根本吧？”</li></ol><h1 id="六十八句"><a href="#六十八句" class="headerlink" title="六十八句"></a>六十八句</h1><ol start="68"><li>孟懿子问孝。子曰：“无违”。樊迟御，子告之曰：“孟孙问孝於我，我对曰，”无违。””樊迟曰：“何谓也？”子曰：“生，事之以礼；死，葬之以礼，祭之以礼。”</li></ol><p>缠中说禅白话直译<br>孟懿子问孝。子曰：“无违”。樊迟御，子告之曰：“孟孙问孝於我，我对曰，”无违。””樊迟曰：“何谓也？”子曰：“生，事之以礼；死，葬之以礼，祭之以礼。”<br>孟懿子问孝。孔子说：“不要离开。”樊迟替孔子赶车，孔子对他说：“孟孙向我问孝，我回答说：“不要离开”。”樊迟说：“什么意思？”孔子道：“父母在世，用社会当下约定俗成的规范去侍奉他们；父母去世，用社会当下约定俗成的规范去安葬、祭祀他们。”</p><h1 id="第六十九句"><a href="#第六十九句" class="headerlink" title="第六十九句"></a>第六十九句</h1><ol start="69"><li>子游问孝。子曰：“今之孝者，是谓能养。至於犬马，皆能有养；不敬，何以别乎。”</li></ol><p>缠中说禅白话直译<br>子游问孝。子曰：“今之孝者，是谓能养。至於犬马，皆能有养；不敬，何以别乎。”<br>子游问孝。孔子说：“能养父母就被认为是现在的孝了。甚至狗和马，都会有人养；如果内心不敬，又用什么来区别这两者？”</p><h1 id="第七十句"><a href="#第七十句" class="headerlink" title="第七十句"></a>第七十句</h1><ol start="70"><li>孟武伯问孝。子曰：“父母唯其疾之忧。”</li></ol><p>缠中说禅白话直译<br>孟武伯问孝。子曰：“父母唯其疾之忧。”<br>孟武伯问孝，孔子说：“（孝就是）纵使自己生病也担忧父母的那种当下产生的感情。”</p><h1 id="第七十一句"><a href="#第七十一句" class="headerlink" title="第七十一句"></a>第七十一句</h1><ol start="71"><li>子夏问孝。子曰：色难。有事，弟子服其劳；有酒食，先生馔，曾是以为孝乎？<br>缠中说禅白话直译<br>子夏问孝。子曰：色难。有事，弟子服其劳；有酒食，先生馔，曾是以为孝乎？<br>子夏问孝，孔子说：“有事故，让年轻人负担其中的烦劳；有酒食，让年长者吃喝；但如果这些行为不是发自当下的情感，只是由于一种道德规范的力量，内心不情愿甚至在外显露出脸色为难，那么，难道就能把这种行为当成孝吗？</li></ol><h1 id="第七十二句"><a href="#第七十二句" class="headerlink" title="第七十二句"></a>第七十二句</h1><ol start="72"><li>子曰：父母在，不远游，游必有方。</li></ol><p>缠中说禅白话直译<br>子曰：父母在，不远游，游必有方。<br>孔子说：“当父母健在时，即使是游学也不能到偏远险恶之地，否则一定招致旁人或命运的诅咒。”</p><h1 id="第七十三句"><a href="#第七十三句" class="headerlink" title="第七十三句"></a>第七十三句</h1><ol start="73"><li>子曰：父母之年，不可不知也。一则以喜，一则以惧。</li></ol><p>缠中说禅白话直译<br>子曰：父母之年，不可不知也。一则以喜，一则以惧。<br>孔子说：“父母的年龄、生日等，不能不常常挂念以至能脱口而出。这种当下的情感，一方面带着欢喜，一方面带着忧惧，悲欣交集。</p><h1 id="第七十四句"><a href="#第七十四句" class="headerlink" title="第七十四句"></a>第七十四句</h1><ol start="74"><li>子曰：君子喻於义，小人喻於利。</li></ol><p>缠中说禅白话直译<br>子曰：君子喻於义，小人喻於利。<br>孔子说：“君子被各种现实社会结构以及对应的各种道德、法度等规范的关系之网中蕴藏的力量所开导，小人被利益、利害关系所组成的现实社会结构以及其对应的一套现实运行机制的关系之网中蕴藏的力量所开导。”</p><h1 id="第七十五句"><a href="#第七十五句" class="headerlink" title="第七十五句"></a>第七十五句</h1><ol start="75"><li>子曰：君子周而不比，小人比而不周。</li></ol><p>缠中说禅白话直译<br>子曰：君子周而不比，小人比而不周。<br>孔子说：君子，见闻学行周遍而没有疏漏，却不会让别人和自己步调一致、比肩而行；小人，让别人和自己步调一致、比肩而行，见闻学行却不能周遍而没有疏漏。</p><h1 id="第七十六句"><a href="#第七十六句" class="headerlink" title="第七十六句"></a>第七十六句</h1><ol start="76"><li>子曰：君子和而不同；小人同而不和。</li></ol><p>缠中说禅白话直译<br>子曰：君子和而不同；小人同而不和。<br>孔子说：君子相应而不聚集，小人聚集而不相应。</p><h1 id="第七十七句"><a href="#第七十七句" class="headerlink" title="第七十七句"></a>第七十七句</h1><ol start="77"><li>子曰：君子成，人之美；不成，人之恶。小人反是。</li></ol><p>缠中说禅白话直译<br>子曰：君子成，人之美；不成，人之恶。小人反是。<br>孔子说：人不断滋生美德，君子成就；人不断滋生恶行，君子不成。小人的成就与此相反。</p><h1 id="第七十八句"><a href="#第七十八句" class="headerlink" title="第七十八句"></a>第七十八句</h1><ol start="78"><li>子曰：君子之於天下也，无适也，无莫也，义之於比。</li></ol><p>缠中说禅白话直译<br>子曰：君子之於天下也，无适也，无莫也，义之於比。<br>孔子说：君子对于天下的一切，没有行为的归向，也没有思想的向往，甚至可以让自己的容貌呈现出小人的“比“相。</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔6</title>
    <link href="/2024/04/10/ganwu6/"/>
    <url>/2024/04/10/ganwu6/</url>
    
    <content type="html"><![CDATA[<h1 id="自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。"><a href="#自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。" class="headerlink" title="自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。"></a>自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。</h1><h1 id="“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。"><a href="#“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。" class="headerlink" title="“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。"></a>“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。</h1><p>君子慎独，自己一个人独处时也要保持着德行。控制自己的贪，嗔、痴、慢、疑。活着挺难的，做人是难的。</p><h1 id="情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。"><a href="#情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。" class="headerlink" title="情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。"></a>情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。</h1>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔5</title>
    <link href="/2024/04/08/ganwu5/"/>
    <url>/2024/04/08/ganwu5/</url>
    
    <content type="html"><![CDATA[<h1 id="活着需要的能力"><a href="#活着需要的能力" class="headerlink" title="活着需要的能力"></a>活着需要的能力</h1><h2 id="基本生存能力"><a href="#基本生存能力" class="headerlink" title="基本生存能力"></a>基本生存能力</h2><p>1.自我安全保障能力<br>2.基础急救<br>3.基本生存<br>4.方位感</p><h2 id="基础工作能力"><a href="#基础工作能力" class="headerlink" title="基础工作能力"></a>基础工作能力</h2><p>1.制作个人简历<br>2.时间管理能力-如何使用日历和计划清单<br>3.基础写作<br>4.公众讲话<br>5.有效沟通<br>6.基础电脑操作技术<br>7.基础的媒体和文档管理能力<br>8.OFFICE的应用能力<br>9.研究和探索能力</p><h2 id="家务处理能力"><a href="#家务处理能力" class="headerlink" title="家务处理能力"></a>家务处理能力</h2><p>1.如何打扫卫生<br>2.基础烹饪能力<br>3.基础家装修理能力</p><h2 id="财务管理能力"><a href="#财务管理能力" class="headerlink" title="财务管理能力"></a>财务管理能力</h2><p>1.制作家庭预算<br>2.制作家庭账本<br>3.基本投资能力<br>4.基本谈判能力</p><h2 id="自我认知能力"><a href="#自我认知能力" class="headerlink" title="自我认知能力"></a>自我认知能力</h2><p>1.搞清楚自己的使命、方向和人生目标的能力<br>2.平衡生活的能力<br>3.探索并清晰自己的价值观系统<br>4.管理自己情绪的能力</p><h2 id="人际沟通能力"><a href="#人际沟通能力" class="headerlink" title="人际沟通能力"></a>人际沟通能力</h2><p>1.基本礼节常识<br>2.幽默感<br>3.亲密关系的沟通和爱的能力<br>4.表达和赞赏<br>5.接受批评的能力</p><h2 id="思维认知能力"><a href="#思维认知能力" class="headerlink" title="思维认知能力"></a>思维认知能力</h2><p>1.批判性思维（本质思考）<br>2.整合思维能力（迁移思考）<br>3.解决问题能力<br>4.自我控制能力（安排规律的生活作息）<br>5.养生的能力</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔4</title>
    <link href="/2024/04/07/ganwu3/"/>
    <url>/2024/04/07/ganwu3/</url>
    
    <content type="html"><![CDATA[<h1 id="王国维先生曾在《人间词话》中写到对人生三重境界的感悟："><a href="#王国维先生曾在《人间词话》中写到对人生三重境界的感悟：" class="headerlink" title="王国维先生曾在《人间词话》中写到对人生三重境界的感悟："></a>王国维先生曾在《人间词话》中写到对人生三重境界的感悟：</h1><p>“古今成大事业、大学问者，必经过三种境界。‘昨夜西风凋碧树，独上高楼，望尽天涯路’，此第一境界；‘衣带渐宽终不悔，为伊消得人憔悴’，此第二境界；‘众里寻他千百度，蓦然回首，那人却在，灯火阑珊处’，此第三境界。”</p><h2 id="第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》"><a href="#第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》" class="headerlink" title="第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》"></a>第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》</h2><p>在西风的狂吹下，枝繁叶茂的绿树也开始凋谢了，表示形式非常危急，环境十分恶劣，在这种状态下作者夜不成眠，辗转反侧，为自己前途命运无比担忧，但他并没有丧失信心，因此颓废，而是想要努力克服困难，力求上进，争取找到自己的前进方向。于是，作者愤然起身，独上高楼，高瞻远瞩，想要望尽天涯海角，找到前进的路。在这一境界中，可以看做人涉世不久，对人生的无比迷茫，正如现在刚毕业的大学生。但是在迷茫中有多少人因此而坠入歧途，自暴自弃，人生路漫漫，我们也应该上下而求索。正如鲁迅先生所说的一句话：“世界上本没有路，走的人多了，也便成了路”</p><h2 id="第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》："><a href="#第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》：" class="headerlink" title="第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》："></a>第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》：</h2><p>柳永如此艳丽之词，也被王国维拿来说明学问之事。诗人所忧之事，是“相思”，但相思到如此地步，我只有柳永能做到了，可见柳永真是一个重情之人。联系到人生，做一件事能专一到这种地步，不成功都难。继第一阶段的迷茫之后，在这一阶段中便有了目标了，在追逐目标的过程中，必然会遇到许多的困难，而我们要做的就是像柳永想念女子一样，即使被折磨得瘦骨伶仃，形容憔悴，我始终不放弃自己的目标，一往直前。</p><h2 id="第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。”-出自南宋辛弃疾《青玉案·元夕》"><a href="#第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。”-出自南宋辛弃疾《青玉案·元夕》" class="headerlink" title="第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。” 出自南宋辛弃疾《青玉案·元夕》"></a>第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。” 出自南宋辛弃疾《青玉案·元夕》</h2><p>寻觅了千百次，却在无意间看到那人在灯火阑珊处。在苦苦追寻，历经磨难之后，总算看到惊喜了，之前的付出都有了回报。在人生的旅途中，在追求成功的路上，我们时常会陷入迷茫、困惑、苦恼中，甚至怀疑自己有没有选错目标，还该不该坚持下去，这些都很正常的，毕竟成功哪有那么容易呢？在这场旅途中，必定有很多人中途放弃，马云有句话说得好：“今天很残酷，明天更残酷，后天很美好，但大多数人都死在明天晚上，真正的英雄才能见到后天的太阳。”成功总是不经意间到来，这是一个人历尽千帆之后上天赐予的惊喜。这个惊喜有时来的很晚，但只要我们一直坚持，他迟早是要来的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔3</title>
    <link href="/2024/04/06/ganwu2/"/>
    <url>/2024/04/06/ganwu2/</url>
    
    <content type="html"><![CDATA[<h1 id="学习最重要的是什么？"><a href="#学习最重要的是什么？" class="headerlink" title="学习最重要的是什么？"></a>学习最重要的是什么？</h1><p>清醒，知道自己在做什么？ 即明白此时此刻到底在做什么。这一刻我在干什么？正在解决什么问题？这一天我学到了什么东西？取得了什么进步？不断回答这些问题的过程就是不断给予大脑正反馈的过程。时时刻刻用收获刺激大脑是学习上瘾和长时间专注学习的秘诀。</p><h1 id="什么在阻挡我进步？"><a href="#什么在阻挡我进步？" class="headerlink" title="什么在阻挡我进步？"></a>什么在阻挡我进步？</h1><p>我自己，我自己的思维在阻挡我的进步。为什么我在刷手机，看视频时不会觉得无聊，在放下手机后，反而觉得我聊了。这样当再次拿起手机时才会解决。我不喜欢这样没有掌控感的感觉，这样的感觉是矛盾的。这样的矛盾怎么来的勒？大脑中的思维和自我的觉知的意识之间的矛盾，怎么化解这种矛盾，我认为的解决方法是清楚自己在干嘛，清楚自己此时此刻在干嘛。建立一个观察者来观察自己。</p><h1 id="我为什么会刷手机？"><a href="#我为什么会刷手机？" class="headerlink" title="我为什么会刷手机？"></a>我为什么会刷手机？</h1><p>我为什么会刷手机，是因为我觉得无聊。我为什么觉得无聊？是因为没有事情可以做，或者是有些事情做了看不到效果，没有刷手机那样即时的奖励感。但是刷手机的过程中的信息是我不喜欢的，这些信息太单一，太主观（酒色财气，好色、贪财、逞气，为人生四戒），很不幸这些都有，当然也有好的，但是需要我去花时间去寻找。这也是一个麻烦事。回到怎么解决刷手机这件事，拿回对做事情的掌控感，明白自己现在在干嘛。就这样简单。</p><h1 id="阐述一下上瘾机制"><a href="#阐述一下上瘾机制" class="headerlink" title="阐述一下上瘾机制"></a>阐述一下上瘾机制</h1><p>上瘾的机理与多巴胺有关，并且随着上瘾行为的次数增多，大脑中的对与多巴胺的神经受体的敏感程度会下降，进而导致要想获取与之前相同的快感，就需要付出更多或更具有刺激性的行为来刺激多巴胺的生成。理论依据，行为和感觉是一体的，感觉会影响行为，行为会影响感觉，但是使用行为去影响感觉是更有理智的。第二点，失乐园理论，按照上述的描述，随着大脑中多巴胺快感产生的受体的敏感性下降，要想获得与之前相同的快感，需要付出的行为会更多，但是这样总会有个上限，导致人在做不出要产生多巴胺的行为时，就会陷入失乐园状态。第三点，痛苦和快感需要平衡，当我感觉痛苦时我需要快乐（多巴胺）来平衡，当我感到快乐时，快乐完成后会产生空虚（痛苦）来平衡快乐。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔2</title>
    <link href="/2024/04/05/ganwu0/"/>
    <url>/2024/04/05/ganwu0/</url>
    
    <content type="html"><![CDATA[<h1 id="怎么能做好一件事？"><a href="#怎么能做好一件事？" class="headerlink" title="怎么能做好一件事？"></a>怎么能做好一件事？</h1><p>做事，我将其解释为使用自己的方式去实现自己的想法。一个足以支撑去做这件事的动机，加上没有复杂的情绪影响，持之以恒的勤奋，那我觉得这件事的成功概率会大大的加强。一点点自己给自己的意义加上不要脑子的勤奋就可以成事了。为什么说不要脑子，因为带上脑子就会产生情绪，有了情绪就会产生内耗，当然对待情绪要辩证的看待，但是在做事初期一定不要带有情绪，在后面做这件事有一定积累后，就可以带上脑子了。<br>这里需要说明一点的是，成功是一个随机事件，不是必然事件。任何人做任何事情，都不能百分之百的把握成功。但是所有人都可以做一件事情，那就是持之以恒的勤奋，去提升成功的概率。知易行难，从小事做起，不断改变，积少成多。这是一个很重要的思维方法。</p><h1 id="心法、法则、技术之间的关系。"><a href="#心法、法则、技术之间的关系。" class="headerlink" title="心法、法则、技术之间的关系。"></a>心法、法则、技术之间的关系。</h1><p>心法胜于法则，法则胜于技术（方法）。<br>所谓技术，比如说使用进步本的技术来操作管理各个学科的知识，知识会遗忘，技术则能熟能生巧，历久弥新。<br>所谓法则，例如学习过程中唯一不变的目的就是进步，认识到这个法则之后所开发的技术都是为了这个法则服务的。世界上没有普世的方法，却有普世的法则。比如要把一件事做好，就要不断学习，不断学习的目的是不断进步，进而能把事情做的更好。<br>所谓心法，是指人对生命、对世界的基本态度和根本认知。例如，“自胜者强”，给强者下了定义，不胜别人，只胜自己的人是强者，战胜自己内心的情绪，内心的恐惧，控制自己不再内耗的人是强者。以自胜者强的定义去看待别人和自己，自然形成心态，进而形成心法。自胜者强包含了学习进步的法则，但是学习进步的法则取不涵盖自胜者强。</p><h1 id="我该怎么才能做好一件事。"><a href="#我该怎么才能做好一件事。" class="headerlink" title="我该怎么才能做好一件事。"></a>我该怎么才能做好一件事。</h1><p>注意这里的主语是我，我该怎么才能做好一件事？ 那就是不要带着情绪去做一件事，带着脑子去做这件事就行了，凡事想多了，就做不成了。<br>不要急躁，慢慢来才最快。不要傲慢，不要觉得这个太简单就不去做，认识是要不断重复的。<br>要早睡早起，11点之前睡觉，在早上7点起床是一件很容易的一件事。但是要是在11点之后睡觉，早上7点起床就不是那么一件容易的事情了。<br>不要给自己增加烦劳，世上本无事，庸人自扰之。<br>要给自己制定一个计划，在本子上写上自己要做的事情。<br>不要再做的过程中去看距离结果还有多远，不要和别人分享自己的喜悦，万一没有成功岂不是很尴尬。生命是一修行。<br>勤奋，不要让事情来裹挟着你，而是让我去驱赶着事情。生命是场旅行，我可以当过过客，也可以成为风景中的一部分。<br>体验是最重要的，体验自己的喜悦，体验自己的悲伤，体验自己的失落，体验自己的愉悦。活在当下，不要考虑过去和未来。<br>不要在乎结果，做好当下的事情，现在的事情做好了，结果不会太差。听天命，尽人事。<br>内心的法则和社会的法则不同，甚至相反。<br>决心来自一个明确的、具体的理由。记住是一个理由，唯一的理由，不是许多理由。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一点小确幸</title>
    <link href="/2024/04/04/tupian/"/>
    <url>/2024/04/04/tupian/</url>
    
    <content type="html"><![CDATA[<h1 id="这里是图片"><a href="#这里是图片" class="headerlink" title="这里是图片"></a>这里是图片</h1><h2 id="清晨的树"><a href="#清晨的树" class="headerlink" title="清晨的树"></a>清晨的树</h2><p><img src="/pic/79f388cc7d53668d55773baf30bb10c.jpg"></p><h2 id="早晨的红日"><a href="#早晨的红日" class="headerlink" title="早晨的红日"></a>早晨的红日</h2><p><img src="/pic/2.jpg"></p><h2 id="一个不知名的地方"><a href="#一个不知名的地方" class="headerlink" title="一个不知名的地方"></a>一个不知名的地方</h2><p><img src="/pic/l2.jpg"></p><h2 id="图书馆"><a href="#图书馆" class="headerlink" title="图书馆"></a>图书馆</h2><p><img src="/pic/jc1.jpg"></p><h2 id="好看的壁纸"><a href="#好看的壁纸" class="headerlink" title="好看的壁纸"></a>好看的壁纸</h2><p><img src="/pic/3.jpg"><br><img src="/pic/3.png"><br><img src="/pic/v2-3a1fb23e19b2448033a9b7333941f465_r.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>图片</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔</title>
    <link href="/2024/04/04/ganwu1/"/>
    <url>/2024/04/04/ganwu1/</url>
    
    <content type="html"><![CDATA[<p>为什么要写下这篇随笔，最近经常上网，突然觉得什么都不真实，所以我决定写下一些我觉得真实的想法。并且决定这段时间先不上网看他人的对于这个世界的解释，太嘈杂了。先看看书，集百家之言，成一家之说。先把自己的世界观，人生观，价值观建立完成，才能更好的和人交流，辩论。</p><p>  见相非相，即见如来</p><p>很多时候，我们这个世界的认识是荒谬到极点而不自知的。人是立场的，好恶的，我们总是只相信自己愿意相信的东西。我们习惯于先形成观点，然后再寻找既有立场的正面证据，在不知不觉中偏离真实。改变总是很难的，一点点的进步总是让我感到欢喜的。每个人的生命中都隐含着一颗觉知的种子，一旦唤醒，便势不可挡，在追求真知、真实的路上一去不反。</p><p>世界是真实的，世界就是那样，但是世界的解释权却不再世界本身，而在于我们。解释权归我们所有，对世界的解释的方法造就了不同的人，并且我们对世界的解释的方式会受到各种各样的影响，正确的，错误的，客观的，主观的。不同的解释方式产生不同的行为，不同的行为会造成不同的结果，造成的结果无论好坏都要受着，这是因果，不可能逃脱。</p><p>最近觉得网络的东西太过于无聊，各种各样的信息，不同立场的人，不同角度的人。热爱生活的，厌世嫉俗的。不同境遇的，充满选择的人和没有选择的人。世界就是这么的参差不齐。回到自我本身，我该怎么去解释这个世界。最近读马克思，实事求是是必要的，实践是检验真理的唯一标准是必要的，与其去网络上看别人任何解释这个世界，不如自己拿起笔写下自己对世界的解释。别人的经验是只能参照的，</p><p>要有自我主动权，解放思想，我们要有敢于摸着石头过河的勇气。成功了，增长能力也有了经验；失败了，就有了一次知道错误出在哪儿的认识。不要高估自己的能力，不要傲慢。人都有一个通病：说到别人的时候，就是这也不行，那也不行；而说到自己的时候，就有一种世外高人的感觉。平庸来自傲慢，如果人人生而伟大，如果每个人本来都是生命的奇迹。那么，傲慢毁掉了多少人？<br>成功的经验可以借鉴但是不可以复制，“见路不走”是不唯经验教条。成功的经验我们一般不能复制，因为那个经验发生的条件已成过去式，现在的条件只符合现在时。众生总是看到事物的各种表相而偏离本质，要见，见自本性，向内求解，以自己的觉性来看待事物。<br>对人必须有理有节，对己则可自由豁达。小到个人，家庭，大到国家，社会，自由都是有前提的；不同的人有不同的自由。</p><p>认识是慢慢的认识的，不可能突然从小学生到大学生的，这需要个过程。如果想要从小学生到大学生，这是不可能的。想可想之想，能可能之能。<br>关于认识我觉得实践论中的阐述是最好的，这里直接粘贴了，实践过程中，开始只是看到过程中各个事物的现象方面，看到各个事物的片面，看到各个事物之间的外部联系。例如有些外面的人们到延安来考察，头一二天，他们看到了延安的地形、街道、屋宇，接触了许多的人，参加了宴会、晚会和群众大会，听到了各种说话，看到了各种文件，这些就是事物的现象，事物的各个片面以及这些事物的外部联系。这叫做认识的感性阶段，就是感觉和印象的阶段。（认识的第一阶段）社会实践的继续，使人们在实践中引起感觉和印象的东西反复了多次，于是在人们的脑子里生起了一个认识过程中的突变（即飞跃），产生了概念。概念这种东西已经不是事物的现象，不是事物的各个片面，不是它们的外部联系，而是抓着了事物的本质，事物的全体，事物的内部联系了。《三国演义》上所谓“眉头一皱计上心来”，我们普通说话所谓“让我想一想”，就是人在脑子中运用概念以作判断和推理的工夫。这是认识的第二个阶段。<br>外来的考察团先生们在他们集合了各种材料，加上他们“想了一想”之后，他们就能够作出“共产党的抗日民族统一战线的政策是彻底的、诚恳的和真实的”这样一个判断了。在他们作出这个判断之后，如果他们对于团结救国也是真实的的话，那末他们就能够进一步作出这样的结论：“抗日民族统一战线是能够成功的。”这个概念、判断和推理的阶段，在人们对于一个事物的整个认识过程中是更重要的阶段，也就是理性认识的阶段。  认识的真正任务在于经过感觉而到达于思维，到达于逐步了解客观事物的内部矛盾，了解它的规律性，了解这一过程和那一过程间的内部联系，即到达于论理的认识。<br>在低级阶段，认识表现为感性的，在高级阶段，认识表现为论理的，但任何阶段，都是统一的认识过程中的阶段。感性和理性二者的性质不同，但又不是互相分离的，它们在实践的基础上统一起来了。</p><p>关于做事（做事也是认识的范畴），也是这样，慢慢来才最快。这里同样copy实践论中的内容。<br>常常听到一些同志在不能勇敢接受工作任务时说出来的一句话：没有把握。为什么没有把握呢？<br>因为他对于这项工作的内容和环境没有规律性的了解，或者他从来就没有接触过这类工作，或者接触得不多，因而无从谈到这类工作的规律性。及至把工作的情况和环境给以详细分析之后，他就觉得比较地有了把握，愿意去做这项工作。如果这个人在这项工作中经过了一个时期，他有了这项工作的经验了，而他又是一个肯虚心体察情况的人，不是一个主观地、片面地、表面地看问题的人，他就能够自己做出应该怎样进行工作的结论，他的工作勇气也就可以大大地提高了。只有那些主观地、片面地和表面地看问题的人，跑到一个地方，不问环境的情况，不看事情的全体（事情的历史和全部现状），也不触到事情的本质（事情的性质及此一事情和其他事情的内部联系），就自以为是地发号施令起来，这样的人是没有不跌交子的。<br>由此看来，认识的过程，第一步，是开始接触外界事情，属于感觉的阶段。第二步，是综合感觉的材料加以整理和改造，属于概念、判断和推理的阶段。只有感觉的材料十分丰富（不是零碎不全）和合于实际（不是错觉），才能根据这样的材料造出正确的概念和论理来。</p><p>实践是认识的来源，认识又可以变革实践。实践和认识的关系如同美和丑的关系，如同硬币的一体两面，是辩证的。<br>通过实践而发现真理，又通过实践而证实真理和发展真理。从感性认识而能动地发展到理性认识，又从理性认识而能动地指导革命实践，改造主观世界和客观世界。实践、认识、再实践、再认识，这种形式，循环往复以至无穷，而实践和认识之每一循环的内容，都比较地进到了高一级的程度。这就是辩证唯物论的全部认识论，这就是辩证唯物论的知行统一观。</p><p>所以我的想法是，自己去实践，去认识，而不是去把他人带有主观的解释当作自己的解释，我不愿意吃别人的口水，关于这个世界我要亲自去尝一尝，去变革一下。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>句子</title>
    <link href="/2024/04/02/text1/"/>
    <url>/2024/04/02/text1/</url>
    
    <content type="html"><![CDATA[<BR><h2 id="1"><a href="#1" class="headerlink" title="1"></a>1</h2><p>图难于其易，为大于其细。天下难事必作于易，天下大事必作于细。是以圣人终不为大，故能成其大。</p><h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><p>天有道，则无常道。事于道，则天有道看与事则无常，无常则明，明则通，则世事洞明。世事洞明则世事可治愈渐达佳境。</p><p>解释： 天如果真的有直指究竟的道，也肯定不是一直不变的道。做事依照着前人经验，则“道”看似有规律可循，其实世事无常没有相同一件事，懂得无常的道理就能把事情看清楚，把事情看清楚了就能做到通达。对于人世间的各种事情，都看得透彻明白，就能世事可治愈渐达佳境。</p><h2 id="3"><a href="#3" class="headerlink" title="3"></a>3</h2><p>夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。</p><h2 id="4"><a href="#4" class="headerlink" title="4"></a>4</h2><p>为天地立心，为生民立命，为往圣继绝学，为万世开太平</p><h2 id="5"><a href="#5" class="headerlink" title="5"></a>5</h2><p>真正的理性从来都是当下的，从来都是实践的，而实践，从来都是当下的理性。</p><h2 id="6"><a href="#6" class="headerlink" title="6"></a>6</h2><p>这是一个纷纷扰扰的滚滚红尘，众生沉迷而不求解脱，驱使着尘世中的我们有时也不得不加入其中滚一下；这是一个没有方向的名利场，大家每个人都有自己的执念，且价值观单一，在单一的价值观之下，为了同一个东西，大家便如丛林法则中的生物一样，让名利场成为一个绞肉机。</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/04/02/hello-world/"/>
    <url>/2024/04/02/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span> <br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
