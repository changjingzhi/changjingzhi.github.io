<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>深度学习环境配置-pytroch</title>
    <link href="/2024/05/20/deeplearnbook0/"/>
    <url>/2024/05/20/deeplearnbook0/</url>
    
    <content type="html"><![CDATA[<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>查看显卡能支持最高的CUDA版本：<code>nvidia-smi</code><br>查看安装的CUDA <code>nvcc -V</code></p><ol><li>安装NVIDIA驱动,<a href="https://www.nvidia.cn/content/DriverDownloads/confirmation.php?url=/Windows/552.44/552.44-desktop-win10-win11-64bit-international-dch-whql.exe&lang=cn&type=GeForce">3080</a></li><li>安装CUDA Toolkit <a href="https://developer.nvidia.com/cuda-toolkit-archive"></a></li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">absl</span>-py==<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">addict</span>==<span class="hljs-number">2</span>.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">asttokens</span>==<span class="hljs-number">2</span>.<span class="hljs-number">4</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">backcall</span>==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">cachetools</span>==<span class="hljs-number">5</span>.<span class="hljs-number">3</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">certifi</span>==<span class="hljs-number">2023</span>.<span class="hljs-number">7</span>.<span class="hljs-number">22</span><br><span class="hljs-attribute">chardet</span>==<span class="hljs-number">5</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">charset</span>-normalizer==<span class="hljs-number">3</span>.<span class="hljs-number">3</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">click</span>==<span class="hljs-number">8</span>.<span class="hljs-number">1</span>.<span class="hljs-number">7</span><br><span class="hljs-attribute">colorama</span>==<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.<span class="hljs-number">6</span><br><span class="hljs-attribute">comm</span>==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">contourpy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">cycler</span>==<span class="hljs-number">0</span>.<span class="hljs-number">12</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">debugpy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">8</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">decorator</span>==<span class="hljs-number">5</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">defusedxml</span>==<span class="hljs-number">0</span>.<span class="hljs-number">7</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">executing</span>==<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">filelock</span>==<span class="hljs-number">3</span>.<span class="hljs-number">13</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">fonttools</span>==<span class="hljs-number">4</span>.<span class="hljs-number">45</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">fsspec</span>==<span class="hljs-number">2023</span>.<span class="hljs-number">10</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">google</span>-auth==<span class="hljs-number">2</span>.<span class="hljs-number">23</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">google</span>-auth-oauthlib==<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">graphviz</span>==<span class="hljs-number">0</span>.<span class="hljs-number">20</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">grpcio</span>==<span class="hljs-number">1</span>.<span class="hljs-number">59</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">huggingface</span>-hub==<span class="hljs-number">0</span>.<span class="hljs-number">19</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">idna</span>==<span class="hljs-number">3</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">imageio</span>==<span class="hljs-number">2</span>.<span class="hljs-number">33</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">importlib</span>-metadata==<span class="hljs-number">6</span>.<span class="hljs-number">8</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">importlib</span>-resources==<span class="hljs-number">6</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">ipykernel</span>==<span class="hljs-number">6</span>.<span class="hljs-number">26</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">ipython</span>==<span class="hljs-number">8</span>.<span class="hljs-number">12</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">jedi</span>==<span class="hljs-number">0</span>.<span class="hljs-number">19</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">Jinja2</span>==<span class="hljs-number">3</span>.<span class="hljs-number">1</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">joblib</span>==<span class="hljs-number">1</span>.<span class="hljs-number">3</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">jupyter_client</span>==<span class="hljs-number">8</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">jupyter_core</span>==<span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">kiwisolver</span>==<span class="hljs-number">1</span>.<span class="hljs-number">4</span>.<span class="hljs-number">5</span><br><span class="hljs-attribute">lazy_loader</span>==<span class="hljs-number">0</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">lxml</span>==<span class="hljs-number">4</span>.<span class="hljs-number">9</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">Markdown</span>==<span class="hljs-number">3</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">MarkupSafe</span>==<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">matplotlib</span>==<span class="hljs-number">3</span>.<span class="hljs-number">7</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">matplotlib</span>-inline==<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.<span class="hljs-number">6</span><br><span class="hljs-attribute">mne</span>==<span class="hljs-number">1</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">nest</span>-asyncio==<span class="hljs-number">1</span>.<span class="hljs-number">5</span>.<span class="hljs-number">8</span><br><span class="hljs-attribute">netron</span>==<span class="hljs-number">7</span>.<span class="hljs-number">2</span>.<span class="hljs-number">9</span><br><span class="hljs-attribute">networkx</span>==<span class="hljs-number">3</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">nltk</span>==<span class="hljs-number">3</span>.<span class="hljs-number">8</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">numpy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">24</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">oauthlib</span>==<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">opencv</span>-python==<span class="hljs-number">4.2.0.32</span><br><span class="hljs-attribute">packaging</span>==<span class="hljs-number">23</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">pandas</span>==<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">parso</span>==<span class="hljs-number">0</span>.<span class="hljs-number">8</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">pickleshare</span>==<span class="hljs-number">0</span>.<span class="hljs-number">7</span>.<span class="hljs-number">5</span><br><span class="hljs-attribute">Pillow</span>==<span class="hljs-number">10</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">platformdirs</span>==<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">pooch</span>==<span class="hljs-number">1</span>.<span class="hljs-number">8</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">prompt</span>-toolkit==<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">41</span><br><span class="hljs-attribute">protobuf</span>==<span class="hljs-number">4</span>.<span class="hljs-number">25</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">psutil</span>==<span class="hljs-number">5</span>.<span class="hljs-number">9</span>.<span class="hljs-number">6</span><br><span class="hljs-attribute">pure</span>-eval==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">pyasn1</span>==<span class="hljs-number">0</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">pyasn1</span>-modules==<span class="hljs-number">0</span>.<span class="hljs-number">3</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">pycocotools</span>==<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">7</span><br><span class="hljs-attribute">pyemd</span>==<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">pygame</span>==<span class="hljs-number">2</span>.<span class="hljs-number">5</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">Pygments</span>==<span class="hljs-number">2</span>.<span class="hljs-number">16</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">pyparsing</span>==<span class="hljs-number">3</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">python</span>-dateutil==<span class="hljs-number">2</span>.<span class="hljs-number">8</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">pytz</span>==<span class="hljs-number">2024</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">PyWavelets</span>==<span class="hljs-number">1</span>.<span class="hljs-number">4</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">pywin32</span>==<span class="hljs-number">306</span><br><span class="hljs-attribute">PyYAML</span>==<span class="hljs-number">6</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">pyzmq</span>==<span class="hljs-number">25</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">regex</span>==<span class="hljs-number">2023</span>.<span class="hljs-number">10</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">requests</span>==<span class="hljs-number">2</span>.<span class="hljs-number">31</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">requests</span>-oauthlib==<span class="hljs-number">1</span>.<span class="hljs-number">3</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">rsa</span>==<span class="hljs-number">4</span>.<span class="hljs-number">9</span><br><span class="hljs-attribute">safetensors</span>==<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">scikit</span>-image==<span class="hljs-number">0</span>.<span class="hljs-number">21</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">scikit</span>-learn==<span class="hljs-number">1</span>.<span class="hljs-number">3</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">scipy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">10</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">seaborn</span>==<span class="hljs-number">0</span>.<span class="hljs-number">13</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">six</span>==<span class="hljs-number">1</span>.<span class="hljs-number">16</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">stack</span>-data==<span class="hljs-number">0</span>.<span class="hljs-number">6</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">summary</span>==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tensorboard</span>==<span class="hljs-number">2</span>.<span class="hljs-number">14</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tensorboard</span>-data-server==<span class="hljs-number">0</span>.<span class="hljs-number">7</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">thop</span>==<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.post2209072238<br><span class="hljs-attribute">threadpoolctl</span>==<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tifffile</span>==<span class="hljs-number">2023</span>.<span class="hljs-number">7</span>.<span class="hljs-number">10</span><br><span class="hljs-attribute">timm</span>==<span class="hljs-number">0</span>.<span class="hljs-number">6</span>.<span class="hljs-number">13</span><br><span class="hljs-attribute">torch</span>==<span class="hljs-number">1</span>.<span class="hljs-number">12</span>.<span class="hljs-number">1</span>+cu116<br><span class="hljs-attribute">torch</span>-tb-profiler==<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">torchaudio</span>==<span class="hljs-number">0</span>.<span class="hljs-number">12</span>.<span class="hljs-number">1</span>+cu116<br><span class="hljs-attribute">torchsummary</span>==<span class="hljs-number">1</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">torchvision</span>==<span class="hljs-number">0</span>.<span class="hljs-number">13</span>.<span class="hljs-number">1</span>+cu116<br><span class="hljs-attribute">torchviz</span>==<span class="hljs-number">0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">tornado</span>==<span class="hljs-number">6</span>.<span class="hljs-number">3</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">tqdm</span>==<span class="hljs-number">4</span>.<span class="hljs-number">66</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">traitlets</span>==<span class="hljs-number">5</span>.<span class="hljs-number">13</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">typing_extensions</span>==<span class="hljs-number">4</span>.<span class="hljs-number">8</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tzdata</span>==<span class="hljs-number">2024</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">urllib3</span>==<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">wcwidth</span>==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">10</span><br><span class="hljs-attribute">Werkzeug</span>==<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">zipp</span>==<span class="hljs-number">3</span>.<span class="hljs-number">17</span>.<span class="hljs-number">0</span><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>听力-2022年12月大学英语六级考试真题（第一套）</title>
    <link href="/2024/05/20/English4/"/>
    <url>/2024/05/20/English4/</url>
    
    <content type="html"><![CDATA[<h2 id="第一段听力"><a href="#第一段听力" class="headerlink" title="第一段听力"></a>第一段听力</h2><p>1-4题： 对话<br>M: How’s your dissertation going? Fm proofreading my first draft and will submit it to my professor tomorrow.<br>M：你的论文写得怎么样了？我在校对我的初稿，明天就会提交给我的教授。</p><p>W: Oh, (1) I haven’t even started writing mine yet, so I’m really worried about finishing by the end of next semester.<br>W：哦，我还没开始写呢，所以我很担心能否在下学期结束前完成。</p><p>M: You mean you haven’t even begun yours yet? The final draft is due in five months.<br>M：你是说你还没开始写？最终稿在五个月后就要交了。</p><p>W: Of course Fve started it, but I can’t get to the writing yet as I haven’t found enough resources to use, so I’m still researching the topic.<br>W：当然开始了，但还没能动笔，因为还没找到足够的资料，所以还在研究主题。</p><p>M: Maybe the problem is the way you’re doing your research. (2) I started by talking to my professor about where to look for information. And based on that, I found books in the library and a lot of reputable journal articles on the Internet.<br>M：也许问题出在你做研究的方法上。我一开始就和教授讨论了该去哪里找信息。然后根据这些信息，我在图书馆找到了书籍，并在网上找到了很多可信的期刊文章。</p><p>W: I’ve tried all that, but don’t have enough to write the dissertation as my department’s minimum length is 70 pages. I think the problem is that my topic isn’t viable. (3) And honestly, my professor did warn me at the beginning that I might not be able to find enough material. But I was so interested in the topic that I didn’t let his advice to turn me.<br>W：我试过这些，但还是不够写论文，因为我们系的最低篇幅要求是70页。我觉得问题在于我的主题不太可行。而且，说实话，我的教授一开始就警告我，可能找不到足够的资料。但我对这个主题太感兴趣了，没有听从他的建议。</p><p>M: Well, I suggest you find a new topic. After all, our professors are here to guide us, so it’s best to listen to them.<br>M：我建议你换个主题。毕竟，教授在这里是为了指导我们，所以最好听从他们的建议。</p><p>W: In retrospect, I wish I had listened to him, but I didn’t. And now I don’t want to give up my topic as I’ve already invested so much time and energy.<br>W：回想起来，我希望当时听了他的建议，但我没有。现在我不想放弃这个主题，因为我已经投入了太多的时间和精力。</p><p>M: If you’re committed to your current topic, maybe you could make some adjustments rather than abandon it completely. What is your topic?<br>M：如果你坚持现有的主题，也许可以做一些调整，而不是完全放弃。你的主题是什么？</p><p>W: It’s “Depictions of Femininity and Folklore from the South of the Country”.<br>W：是“南部地区民俗中的女性形象描绘”。</p><p>M: That’s pretty narrow. You could find more material if you made the topic broader, maybe by including other kinds of depictions.<br>M：这个主题确实很狭窄。如果把主题扩展一点，比如包括其他类型的描绘，可能会找到更多的资料。</p><p>W: (4) Broadening the topic is a great idea. I’ll start by including folklore from other regions of the country.<br>W：扩展主题是个好主意。我会先从包括其他地区的民俗开始。</p><h2 id="听力第二段"><a href="#听力第二段" class="headerlink" title="听力第二段"></a>听力第二段</h2><p>5-8题：对话<br>W: Today, on Book Talk, we are lucky enough to host John Robbins and discuss his new book, Why Americans Are Fat and How We Can Lose Weight. (5-1) John isn’t just a respected writer；he’s also one of the rare celebrity authors writing about science today.</p><p>M: Thanks for having me, Rebecca, but I’m hardly a celebrity.</p><p>W: That’s very modest of you to say, (5-2) considering that your four books have sold a total of seven million copies worldwide, and they’ve been translated into 12 different languages. What makes people so fascinated with your work?</p><p>M: (6) Well, people read my books because more than 60% of Americans are overweight or obese. And other countries are facing similar problems. Basically, we all want to know how to fix things.</p><p>W: We certainly do. I’ve read your new book and it’s fabulous, especially when it comes to the way you make difficult science easy for laymen to understand. That’s no small achievement.</p><p>M: I’m glad to hear you find my work accessible, because I was worried when I wrote it that discussing the science might make the book more suited for a specialist audience. (7) My last book was written primarily for the medical community. But this time, I want to help ordinary people take control of their weight. </p><p>W: And how do you suggest they do that? Can you give us the basics of your advice for people who want to lose weight?</p><p>M: Briefly, (8) I argue that every person needs to sn sid er their metabolism and eat what suits their body’s needs. I don’t advocate one single diet. Some people should eat more carbohydrates than others. And different people need different amounts of protein and fat.</p><p>W: But you do have some recommendations for everyone, including eating ten servings of vegetables and three of fruit a day. We’ll talk about those recommendations next, but now we need to take a short break for a message from our sponsor.<br>W：今天在《书籍访谈》节目中，我们很荣幸邀请到约翰·罗宾斯来讨论他的新书《为什么美国人肥胖以及我们如何减肥》。约翰不仅是一位受人尊敬的作家，他也是当今为数不多的撰写科学类作品的名人作者之一。</p><p>M：谢谢你的邀请，丽贝卡，但我可不算是名人。</p><p>W：你这么说真是太谦虚了，考虑到你的四本书在全球总共卖出了七百万册，并且被翻译成了12种不同的语言。是什么让人们对你的作品如此着迷？</p><p>M：嗯，人们读我的书是因为超过60%的美国人超重或肥胖。而其他国家也面临类似的问题。基本上，我们都想知道如何解决这些问题。</p><p>W：我们确实如此。我读了你的新书，真是太棒了，尤其是你让普通人能够理解复杂的科学。这可不是件容易的事。</p><p>M：很高兴听到你觉得我的作品通俗易懂，因为我写这本书时担心讨论科学内容会让它更适合专业观众。我上一本书主要是为医学界写的。但这次，我希望能帮助普通人控制他们的体重。</p><p>W：那你建议他们怎么做呢？你能给我们讲讲你的减肥建议的基本要点吗？</p><p>M：简单来说，我认为每个人都需要考虑他们的新陈代谢，并吃适合他们身体需求的食物。我不提倡单一的饮食。有些人应该比其他人吃更多的碳水化合物。而不同的人需要不同量的蛋白质和脂肪。</p><p>W：但是你确实有一些对所有人的建议，包括每天吃十份蔬菜和三份水果。我们接下来会讨论这些建议，但现在我们需要休息一下，听一段赞助商的信息。</p><h2 id="听力第三段"><a href="#听力第三段" class="headerlink" title="听力第三段"></a>听力第三段</h2><p>9-11:短文</p><p>Stress is often depicted as negative, but research shows that moderate amounts of it can be beneficial for your brain and your body.</p><p>First, the benefits for the brain. Studies have shown that short periods of stress can actually bolster cognitive functioning. (9) Researchers discovered that placing rats in a stressful situation for just a few hours doubled the growth of new brain cells. The rats also did better on a memory test later on. Scientists think the same thing happens in humans. But how does stress improve memory? It’s simple. When your brain cells multiply, your memory can improve. Viewed from a biological perspective, this makes sense, (10) because animals that are better at remembering dangerous situations can avoid them in the future. If an animal encounters a predator and escapes, for example, it’s important to remember where and when that encounter happened. Experts assert that the same principle applies to humans.</p><p>Now, let’s turn to how stress benefits the body. This may come as a surprise to laymen. But experts say that stress can keep you from getting sick. (11) Scientists concede that chronic stress can make you more prone to illness. But research shows that short periods of stress can actually provide some protection against getting sick, because it increases your immune functioning. One study shows that rats that experienced brief stress had a surge of immune cell response, which makes the immune system better prepared to fight illness. For humans, there’s even evidence that experiencing stress before getting vaccinated could help make vaccines more effective.</p><h3 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h3><p><strong>压力通常被认为是负面的，但研究表明，适量的压力对你的大脑和身体是有益的。</strong></p><p><strong>首先是对大脑的好处。研究表明，短时间的压力实际上可以增强认知功能。</strong> 研究人员发现，让老鼠在一个有压力的环境中呆上几个小时，新的脑细胞的生长量会翻倍。这些老鼠在后来的记忆测试中表现得更好。科学家们认为同样的事情也会发生在人类身上。但是压力是如何改善记忆的呢？这很简单。当你的脑细胞增多时，你的记忆力就能得到提升。从生物学的角度来看，这是有道理的，因为能更好记住危险情况的动物可以在未来避免这些情况。例如，如果一只动物遇到捕食者并成功逃脱，那么记住那次遭遇的时间和地点就非常重要。专家们断言，同样的原理也适用于人类。</p><p><strong>现在，让我们看看压力是如何对身体有益的。这可能会让普通人感到惊讶。</strong> 但专家们表示，压力可以防止你生病。科学家们承认，长期的压力会使你更容易生病。但研究表明，短时间的压力实际上可以提供一些防病的保护，因为它可以增强你的免疫功能。一项研究显示，经历短暂压力的老鼠，其免疫细胞反应会激增，使免疫系统更好地准备好对抗疾病。对于人类来说，还有证据表明，在接种疫苗前经历压力可以帮助提高疫苗的有效性。</p><h2 id="听力第四段"><a href="#听力第四段" class="headerlink" title="听力第四段"></a>听力第四段</h2><p>12-15：短文<br>For many managers and people who work in leadership positions, dealing with emails is a dilemma. It’s likely the unpredictable, uncontrollable and ongoing nature of day-to-day email in terms of volume, importance and urgency contributes to their levels of anxiety and to diminished leadership skills. That’s because it’s not unusual for many leaders to prioritize email management over people management. An obsession with managing their inbox prevents them from dealing with their employees. (13) As a result, they ignore the issues that might only be mild problems at first, until unfortunately, they inevitably transform into a major problem or crisis by virtue of neglect. (14) As leaders, they are expected to motivate and inspire their team in pursuit of longer term strategic goals and also, less ambitiously but more practically, to monitor their daily output, to set clear expectations, and to give regular feedback. When presented with a choice between the appeal of their inbox and other more important activities, many sacrifice the latter. Daily email demands have a negative impact on their goal progress. This is because leaders must divert resources from other tasks to check, filter and respond to emails. (15) The solution is cultivating self-control which is like a muscle- - it can be strengthened or improved over time through exercise. Some suggestions include making space in your diary for the only periods during which you’ll be checking emails, setting a timer for yourself so you don’t become distracted by your inbox for too long, turning off email alerts so you’re not interrupted by them.<br>对于许多管理者和领导岗位上的人来说，处理电子邮件是一种困境。日常电子邮件在数量、重要性和紧迫性方面的不可预测性、不可控性和持续性可能导致他们的焦虑水平上升，并削弱他们的领导技能。这是因为许多领导者往往优先处理电子邮件，而不是管理人员。他们对管理收件箱的痴迷阻碍了他们与员工的互动。</p><p>因此，他们忽视了最初可能只是轻微问题的事项，直到这些问题因为被忽视而不可避免地演变成重大问题或危机。作为领导者，他们被期望能够激励和激发团队，以追求长期的战略目标，同时也需要实际地监控团队的日常工作，设定明确的期望，并给予定期的反馈。然而，在面对收件箱的吸引力和其他更重要的活动时，许多人选择牺牲后者。日常的电子邮件需求对他们的目标进展产生了负面影响。这是因为领导者必须将资源从其他任务上转移出来，以检查、过滤和回复电子邮件。</p><p>解决方案是培养自我控制力，这就像一块肌肉——通过锻炼可以得到加强或改善。一些建议包括在日程表中留出专门检查电子邮件的时间段，为自己设定一个计时器，以免因处理收件箱而分散注意力太久，关闭电子邮件提醒，以免被其打断。</p>]]></content>
    
    
    
    <tags>
      
      <tag>英语</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>英语-听力</title>
    <link href="/2024/05/19/English3/"/>
    <url>/2024/05/19/English3/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cet.itongzhuo.com/fourSix/exam/jumpFourSixExam.do?meType=11">在线练习网站</a><br>2023年英语6级听力原文及翻译</p><h2 id="第一段听力"><a href="#第一段听力" class="headerlink" title="第一段听力"></a>第一段听力</h2><p>对话：1-4题</p><p>M:Thanks for inviting me out tonight. I’ve been wantic. to try this place for weeks. I usually pass it on my way home from work but never seem to have time to stop.<br>男：今晚邀请我出去真是太感谢了。我几个星期前就想来这个地方尝试一下了。我通常下班回家的路上经过这里，但似乎从来没时间停下来。</p><p>W: I’ve been dying to come here as well. I was worried that the menu might not be accommodating.But one of my close friends ate here last week. She’s a vegetarian. She told me that there were a lot of options for her,and they offered alternative dishes.<br>女：我也一直想来这里。我担心菜单可能不太适合我，但我的一个好朋友上周在这里吃过。她是素食主义者。她告诉我这里有很多适合她的选择，而且他们还提供了替代菜肴。</p><p>M: But you’re not vegetarian, are you?W:Well,not entirely.I don’t eat meat at all,But I don’t have a problem eating fish,seafood,or eggs.<br>男：但你不是素食主义者，对吧？</p><p>M isnt that very dificult? imagine giving up a premiumjuicy steak or bacon cheeseburgers. </p><p>W:I wouldn’t know.My parents followed the same diet. Even when I was younger. We never had meat during meals. And I never really had a craving to try it.I bet it would be hard though.Giving up something you’re accustomed to eacogitied cutting back on coffee in the morning and it was awful.Come to think of it.It was probably even worse for my colleagues that had to deal with me at morning meetings.<br>女：嗯，不完全是。我完全不吃肉，但我吃鱼、海鲜或者蛋没问题。我想象不出要放弃一个优质多汁的牛排或培根芝士汉堡有多困难。我不知道。我的父母也是这样饮食的。即使在我年轻的时候，我们吃饭时也从未吃过肉。我从未真正渴望尝试过。我打赌这应该很难。放弃你习惯的东西，比如减少早上的咖啡摄入量，那是很糟糕的。回想起来，对于那些不得不忍受我在早晨会议上的同事们来说，可能更糟糕。</p><p>M: That’s even harder to imago.out without coffee.So you say your parents also bought a partially vegetarian diet. Why did they decide to do that?<br>男：没有咖啡确实很难想象。所以你说你的父母也是部分素食主义者。他们为什么决定这样做呢？</p><p>W: That’s an interesting question. I never thought to ask them though. My best guess is for health reasons. They’re not terribly active or knowledgeable about animal preservation efforts,but they’re serious about their health.Both are in their 70s now eat organic as often as posibte and take part in regular physical exer sep betther of them has any healtn probtems, and they hardly ever get sick.<br>女：这是个有趣的问题。我从未想过问他们。我最好的猜测是出于健康原因。他们对动物保护工作并不是很积极或了解，但他们对健康非常认真。他们现在都七十多岁了，尽可能经常食用有机食品并参加定期的体育锻炼。他们俩都没有健康问题，几乎从不生病。</p><p>M: In that case, I might give it a try someday.<br>男：那我也许会有一天试试看。</p><p>W: Now back to the menu. What are you going to have?<br>女：现在回到菜单上。你打算吃什么？</p><h2 id="第二段听力"><a href="#第二段听力" class="headerlink" title="第二段听力"></a>第二段听力</h2><p>对话：5-8题<br>W: How did your annual performance evaluation meeting with your manager go? Did you get much in the way of praise?<br>女：你和经理的年度绩效评估会谈进行得怎么样？他有没有对你表示赞扬？</p><p>M: Next to zero.When we came to the part about discussing my areas for growth,he bluntly told me that thad an attitude problem(Question 5).But he woutdn’t’really expand on that description.He said it’s the little things I do and say(Question.5).<br>男：几乎没有。当我们讨论我需要改进的方面时，他直言告诉我说我有态度问题。但他没有详细解释。他说是我做和说的一些小事情。</p><p>W:Did you ask him for a specific example?<br>女：你有没有问他要一个具体的例子？</p><p>M: I did ask him a few more questions to try to narrow it down. He said my constant questioning of him and his decisions was the most obvious example he could give.Then our conversation was swiftly brought to a close.<br>男：我确实问了他几个问题，试图缩小范围。他说我经常质疑他和他的决定是最明显的例子。然后我们的对话很快就结束了。</p><p>W:I had a similar experience once,my manager told me I was too honest. In most cases, I thought my honesty was helpful, as did my colleagues, but my manager thought otherwise. Some managers really adopt manipulative language to disguise the unreasonable choices they have made,They don’t use the rational power of arguments to resolve issues of conflict or complaints about unfairness(Question 6).<br>女：我曾经有过类似的经历，我的经理告诉我我太诚实了。在大多数情况下，我认为我的诚实是有帮助的，我的同事也是这么认为的，但我的经理却持相反意见。有些经理确实采用操纵性语言来掩饰他们所做的不合理选择，他们不使用理性的论证力量来解决冲突或投诉不公平的问题。</p><p>M:Right.Some employ their persuasive l and influential communication style to win workplace arguments by compelling people to perform the intended action.<br>男：是的。有些人利用自己有说服力和影响力的沟通方式来赢得工作场所的争论，迫使人们执行既定的行动。</p><p>W:Exactly.Too honest,isn’t that a good thing?I thought that’s surely something we should have covered earlier in the section discussing strengths and talents. But now, honesty was deemed to be more of a sin than a virtue,since it made people feel uncomfortable(Question 7)or at least it made the most powerful folks feel uncomfortable.<br>女：确实。太诚实了，这不是一件好事吗？我原以为这是我们在讨论优点和才能的部分应该提前讨论的内容。但现在，诚实被认为更像是一种罪过而不是美德，因为它让人感到不舒服，或者至少让最有权势的人感到不舒服。</p><p>M:lndeed,rhetoric is viewed by many philpsophers a’the method through which the powerful accumulate more power. By virtue of having the loudest voice,they’re able to command greater attention and to further assert their dominance. Even when what they’re saying doesn’t really mak much sense.<br>男：的确，许多哲学家将修辞视为强大积累更多权力的方法。凭借声音最响亮，他们能够获得更多关注并进一步巩固他们的统治地位。即使他们说的话并没有太多意义。</p><p>W: And retaining their power and orestige is their priority(Question 8)<br>女：保持他们的权力和威望是他们的首要任务。</p><p>M: It’s what makes the most sense to a lot of them Question<br>男：这对许多人来说是最合理的选择。</p><h2 id="第三段听力"><a href="#第三段听力" class="headerlink" title="第三段听力"></a>第三段听力</h2><p>短文：9-11</p><p>Athletes are seen as heroes (9) because they can do things that most of us can’t do. They can hit fast balls coming at them at nearly 100 miles an hour, and leap and hang in the air, seemingly defying gravity. They get paid millions of dollars for their efforts, and their names and faces appear on everything from running shoes to advertising boards. Athletes who are champions also show qualities such as perseverance, dedication, and the ability to keep their cool under pressure. Many show those same qualities off the playing field,too. Stories about superstar athletes teach us about working hard and believing in ourselves and in being passionate about what we do.<br>运动员被视为英雄，因为他们能够做大多数人无法做到的事情。他们能击中时速接近100英里的快速球，并且在空中腾跃，似乎违背了重力。他们因努力而获得数百万美元的报酬，他们的名字和面孔出现在从跑鞋到广告牌的各种物品上。成为冠军的运动员还表现出坚持不懈、奉献精神以及在压力下保持冷静的能力。许多人在场外也展现出这些品质。关于超级运动员的故事教会我们努力工作、相信自己并热爱我们所做的事情。</p><p>Although it’s usually bad behavior that gets an athlete a spot on the 6 o’clock news,(10)many high -profile players work hard to be positive role models to children They raise money for charities and act as mentor,talling to student groups and volunteering their time to programs that help children keep off drugs and stay in school. Still, even the greatest champions have flaws. Just because an athlete has the perfect golf swing doesn’t mean he is the perfect parent,jfriend,or spokesperson.They also make mistakes.(11)Separating an athlete’s professional and personal lives can be tough.When a sports star gets in trouble with the law or does something wrong in their private life, fans are often left disappointed. Before he died,baseball star,Mickey Mantle,who was plagued with alcohol problems,told young ball players and the fans who admired him “To play like me; don’t be like me”.<br>虽然通常是不良行为让运动员登上晚间新闻，但许多知名球员努力成为孩子们的正面榜样。他们为慈善机构筹集资金，作为导师与学生团体交流，并志愿参加帮助孩子远离毒品和继续上学的项目。然而，即使是最伟大的冠军也有缺陷。仅仅因为一名运动员有完美的高尔夫挥杆动作，并不意味着他是完美的父母、朋友或代言人。他们也会犯错。将运动员的职业生活和个人生活分开可能很难。当一名体育明星陷入法律纠纷或在私生活中做错事时，粉丝们常常感到失望。棒球明星米奇·曼托在去世前曾饱受酒精问题困扰，他对年轻的棒球选手和崇拜他的粉丝说：“要像我一样打球，但不要像我一样生活。”</p><h2 id="第四段听力"><a href="#第四段听力" class="headerlink" title="第四段听力"></a>第四段听力</h2><p>短文：12-15<br>Q12.We don’t need to tell you that weddings can get expensive.Even with the most meticulous budgeting, a few unexpected costs are bound to occur.While most brides tend to:ascept this as fact, one Canadian woman who is only known as Susan, attempted to avoid all wedding costs. She did this by asking her friends and family to pay up to attend her wedding.It went about as well as you’d expect.<br>我们无需告诉你，婚礼可能会变得很昂贵。即使进行了最细致的预算，也可能会发生一些意外费用。虽然大多数新娘倾向于接受这一事实，但一位只被称为苏珊的加拿大女士试图避免所有婚礼费用。她这样做是通过要求她的朋友和家人支付参加她婚礼的费用。结果可想而知。</p><p>Susan is causing quite the debate online after posting a bizarre Facebook complaint about her now canceled wedding. Yes, the couple called off the wedding just days before their wedding vows,Q13.since the guest refused to-pay the $1500 attendance fee,the $60000dream wedding was put on permanent hold. In her long explanation filled with cursing and swearing,the bride accused her friends and family of ruining her marriage and her life. She stated thatjeach guest would only need to pay S1500, while she sacrificed everything for the day. Q14. Her maid of honour told her to stick to the budget as she was asking too much from her guest,but Susan ignored her.Not surprisingly, only 8 people responded positively to the wedding invitations and money requests.<br>苏珊在发布了一条关于她现在已经取消的婚礼的奇怪Facebook投诉后，在网上引起了很大争议。是的，这对夫妇在举行婚礼誓言的几天前取消了婚礼，因为客人拒绝支付1500美元的参加费用，这场价值60000美元的梦幻婚礼被永久搁置了。在她充满诅咒和咒骂的长篇解释中，新娘指责她的朋友和家人毁了她的婚姻和她的生活。她说每位客人只需要支付1500美元，而她为这一天牺牲了一切。她的伴娘告诉她要坚持预算，因为她向客人要求的太多了，但苏珊没有听从她的建议。不足为奇的是，只有8个人对婚礼邀请和资金要求做出了积极回应。</p><p>Realizing they would not be able:te afford their dream wedding,Susan’s future husband suggested getting married in Las Vegas:Q15,jThe bride quickly shut down the idea,saying she did not want a wedding of gambling and heavy drinking. It seems her dream wedding has now become a nightmare.<br>意识到他们无法承担自己的梦想婚礼，苏珊的未来丈夫建议在拉斯维加斯结婚。新娘迅速拒绝了这个想法，称她不想要一个以赌博和酗酒为主题的婚礼。看来她的梦幻婚礼现在变成了一场噩梦。</p><h2 id="第5段听力"><a href="#第5段听力" class="headerlink" title="第5段听力"></a>第5段听力</h2><p>演讲： 16-18<br>It has long been scientifically established that weather changes can affect people’s moods. Q16 Now a new study has provided evidence that temperature can influence people’s personalities. This study of over 1.6 million people revealed that 22-degrees Celsius is the perfect air temperature to live in. A city with an average annual temperature closer to 22 tends to have a population who are more agreeable,conscientious,emotionatlyistable,and outgoing. It is the least taxing temperature for the body to regulate its own temperature. The study was observational and didn’t show cause and effect,Q17 but the scientists behind it theorized that better weather leads people to leave their home more often. This in turn leads to more social interaction,which encourages them to develop a friendlier and socially more acceptable personality.<br>新的研究已经长期确立了天气变化会影响人们的情绪。现在一项新研究提供了证据表明温度可以影响人们的个性。这项对160多万人进行的研究发现，22摄氏度是理想的生活气温。一个年平均气温接近22摄氏度的城市往往有更多性格友善、有责任心、情绪稳定和外向的人口。这是身体调节自身体温最不费力的温度。这项研究是观察性的，没有显示因果关系，但背后的科学家们推测，更好的天气会促使人们更频繁地离开家。这反过来会导致更多的社交互动，鼓励他们发展出更友好和社交上更可接受的个性。更温暖的气候也会让人们总体感觉更积极。他们往往更为友善和有责任心。这些发现可能有助于解释为什么寒冷和温暖的国家往往会产生具有不同个性的人。</p><p>Warmer climates also make people feel more positive in general. They tend to be more agreeable and conscientious. The findings might help explain why colder and warmer countries tend to produce people with different personalities. Roughbspeaking, about 40% of a person’s personality is determined bletheir genes, the other 60% by their environment. It was already well-known that personality traits vary across geographic regions. Scientists also knew that these geographic personality traits are associated with a broad range of consequential outcomes. These outcomes include economic activity,for”example entrepreneurial startup rates, and also crime rates, health behaviors, and health outcomes.<br>大致而言，一个人大约40%的个性是由他们的基因决定的，另外60%是由他们的环境决定的。已经众所周知，个性特征在地理区域之间有所不同。科学家们也知道，这些地理上的个性特征与一系列重要的结果相关。这些结果包括经济活动，例如创业创业率，以及犯罪率，健康行为和健康结果。并且已经确定，个性特征在不同国家之间是不同的。</p><p>And it is well established that personality traits differ between countriesThe research team speculated the two might be linked. To test this, they gave online personality tests to 5587 Chinese students and 1.66 million Americans. They then compared the results with the average annual temperature where they grew up. The tests measured personality along 5 well-studied characteristics. The 5 were agreeability, conscientiousness, emotional stability, outgoingness, and openness to new experiences. In both groups, the researchers found the closer a town’s average-annual temperature was to 22 degrees, the more its population exhibited those personality characteristics. However, the findings were much stronger for the Chinese group than the Americans studied,Q18which suggests that though temperature plays a role,it does not play a dominant role. The effects are fairly weak. It’s unlikely to lead to many arguments over the temperature setting of the office air conditioner.<br>研究团队推测这两者可能有关联。为了测试这一点，他们给5587名中国学生和160多万名美国人进行了在线个性测试。然后，他们将结果与他们成长的地方的年平均气温进行了比较。这些测试测量了个性的5个经过深入研究的特征。这5个特征是宜人性、责任心、情绪稳定性、外向性和对新经验的开放性。在两组人中，研究人员发现一个城镇的年平均气温越接近22摄氏度，其人口就越表现出这些个性特征。然而，对于中国人群来说，这一发现要比研究的美国人群更为明显，这表明尽管温度起一定作用，但并不起主导作用。效果相当弱。这不太可能会导致人们就办公室空调的温度设置发生许多争论。</p><h2 id="第六段听力"><a href="#第六段听力" class="headerlink" title="第六段听力"></a>第六段听力</h2><p>演讲： 19-21</p><p>Today we’re talking abooly Loneliness and social isolation are growing public health concerns for people of all ages in the United States, from adolescence to the elderly.Public health experts are worried because loneliness seems to be on the rise. And studies have long found correlations between loneliness and an assortment of medical conditions that threaten health and longevity.<br>孤独和社交孤立正成为美国各个年龄段人群的日益严重的公共健康问题，从青少年到老年人。公共卫生专家感到担忧，因为孤独现象似乎在加剧。长期以来，研究已经发现孤独与一系列威胁健康和寿命的医疗条件之间存在相关性。孤独问题可能甚至比我们想象的更为严重。</p><p>The problem of loneliness maybe even greater than we thought.A new national poll found that about a third of older Americans are to nely; and almost as many seniors feet isolated. This is a serious problem, as research shows that chronic lone particula That’s because it can impair olders adults’ memory and damage their physical and mental health. Chronic loneliness even impacts the life expectancy of seniors,inciosingitheir risk of early mortality.(Question 19)<br>一项新的全国民意调查发现，大约三分之一的美国老年人感到孤独，几乎有同样多的老年人感到孤立。这是一个严重的问题，因为研究显示，慢性孤独可能会损害老年人的记忆力，损害他们的身体和心理健康。慢性孤独甚至会影响老年人的预期寿命，增加他们早逝的风险。</p><p>Let’s take a closer look at that poll now. More than a third of seniors in the poll said they felt lonely at least some of the time,and 27% said they somet exor often felt isolated. This reflects how much time the sehiors spent with others.Almost 30% said they socialized with friends, family, or neighbors once a week or less. Women were more likely than men to report loneliness. But there is good news.It looks like loneliness can be reversed.(Question 20) But researchers are still trying to determine the best way to do so.Why is that? Resolving the problem of loneliness among seniors often isn’t as simple as getting them together with others or moving them in with their children. In fact, the poll found that seniors who lived with their children were more likely to report feeling lonely than those who didn’t. This may be because loneliness refers to the discrepancy between aw ual and desired relationships. So it’s possible that someone who lives alone doesn’t meet that definition, while someone in a house full of busy people does. How can we solve the problem? Well, the researchers assert that it’s important to address each person’s underlying cause of loneliness,(Question 21)whether it’s the death of a spouse, mediral problems, or social exptations that haven’t been fulfilled. It’s noteworthy that there is one general recommendation. While finding solutions for loneliness is highly personal, research suggests the best interventions are those that involve meaningful social contact at least once a week.(Question 21)Depending on the person, that could mean volunteering, ein old friend, or something else.<br>让我们现在更仔细地看看这项调查。调查中超过三分之一的老年人表示他们至少有时感到孤独，27%的人表示他们有时或经常感到孤立。这反映了老年人与他人相处的时间。几乎30%的人表示他们每周与朋友、家人或邻居社交一次或更少。女性比男性更有可能报告孤独感。但也有好消息，看起来孤独是可以逆转的。但研究人员仍在努力确定最佳解决方法。为什么呢？解决老年人孤独问题通常并不像让他们与他人在一起或搬到子女家中那样简单。事实上，调查发现与子女同住的老年人比没有与子女同住的老年人更有可能报告感到孤独。这可能是因为孤独是指实际关系与期望关系之间的差距。因此，一个独居的人可能不符合这一定义，而一个住满繁忙人群的房子的人可能符合。我们如何解决这个问题呢？研究人员强调，重要的是要解决每个人孤独的根本原因，无论是配偶去世、医疗问题还是未实现的社交期望。值得注意的是，有一个普遍的建议。虽然找到孤独问题的解决方案是非常个人化的，但研究表明，最好的干预措施是每周至少进行一次有意义的社交接触。根据个人的情况，这可能意味着做志愿工作，见老朋友，或其他事情。</p><h2 id="第七段听力"><a href="#第七段听力" class="headerlink" title="第七段听力"></a>第七段听力</h2><p>演讲：22-25</p><p>Hello, I am co-founder of the popular female travel community, We Are Trawnl:Girls. We collect and publish stories from women traveling all over the world.We promote comeh’s blogs. We host meetings and events and are getting ready to launch our Travel with Us trips in Bali, Japan and Malawi,<br>您好，我是女性旅行社区“We Are Travel Girls”的联合创始人。我们收集并发布来自世界各地旅行的女性的故事。我们推广女性的博客。我们举办会议和活动，并准备在巴厘岛、日本和马拉维推出我们的“与我们一起旅行”之旅。</p><p>Before I started We Are Travel Girls. I had a successful 10-year career in finance in Jondon where I advised private clients on their investments. Having always had a huge love for travel, I finally took the leap and left finance to pursue my drearc fjstarting a travel company. For as long as I can remember, I wanted to be a travel writer. Before blogs existed, and everything we read was online, I would go to travel writing seminars by writers who were published in travel magarinesel was desperate to write for one of those magazines but didn’t know how to break into-that industry.After university, I ended up working in finance, but always had a desire to travel and write about it.<br>在我开始We Are Travel Girls之前，我在伦敦有着成功的10年金融职业生涯，在那里我为私人客户提供投资建议。我一直热爱旅行，最终我决定离开金融业，追求我的梦想，开办一家旅行公司。我记得很久以前我就想成为一名旅行作家。在博客存在之前，我们阅读的一切都在网上，我曾经参加过旅行写作研讨会，听那些在旅行杂志上发表作品的作家讲课。我曾经非常渴望为那些杂志之一写作，但不知道如何打入这个行业。大学毕业后，我最终进入金融业工作，但一直渴望旅行并写下旅行见闻。</p><p>In 2015, I was looking at ways to leave finance, and my b Vanessa,who grew up on a’ranch on the central coast of California, suggested starting a blog. But when we started creating it,we realized that was the same thing every girl was doing. So we turned our attention to creating a community among these women. This led us to start We Are Travel Girls, which has now grown into a community of over 200,000 followers.<br>2015年，我在寻找离开金融业的方法时，我的朋友Vanessa建议我开始写博客。但当我们开始创建博客时，我们意识到每个女孩都在做同样的事情。因此，我们将注意力转向在这些女性之间创建一个社区。这促使我们创建了We Are Travel Girls，现在已经发展成为拥有超过20万名关注者的社区。</p><p>To anyone thinking about becoming a travel writer, I would suggest they first try and look for a unique way to enter the industry. There are a lot of travel writers now and it can be hard to stand out from the crowd, which is really why we started We Are Travel Girls. Be prepared to work hard if you want to turn it into a full-time business. Q25 And try not to rush to selling advertising spaces before you have created a dedicated audience. The size of your audience doesn’t necessarily need to be huge, but you want them to be engaged. If you post too many promotions early on, you will turn many people off.<br>对于任何想成为旅行作家的人，我建议他们首先尝试寻找进入该行业的独特方式。现在有很多旅行作家，要脱颖而出可能很困难，这也是我们创办We Are Travel Girls的初衷。如果您想把它变成全职业务，就要做好充分准备。并且在您拥有忠实读者之前，不要急于销售广告空间。您的受众规模不一定需要很大，但您希望他们参与进来。如果您在早期发布过多的推广信息，将会失去很多人的兴趣。</p>]]></content>
    
    
    
    <tags>
      
      <tag>英语</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高等数学——常微分方程</title>
    <link href="/2024/05/18/gaodengshuxue5/"/>
    <url>/2024/05/18/gaodengshuxue5/</url>
    
    <content type="html"><![CDATA[<p>常微分方程。<br>常微分方程的基本概念，变量可分离的微分方程，齐次微分方程，一阶线性微分方程 ，可降阶的高阶微分方程，线性微分方程解的性质及解的结构定理，二阶常系数齐次线性微分方程， 高于二阶的某些常系数齐次线性微分方程，简单的二阶常系数非齐次线性微分方程 ，微分方程的简单应用 </p>]]></content>
    
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑——强化学习——使用智能体来预测股票</title>
    <link href="/2024/05/17/tiankeng10/"/>
    <url>/2024/05/17/tiankeng10/</url>
    
    <content type="html"><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p><a href="https://gym-trading-env.readthedocs.io/en/latest/">Gym Trading Env</a>Gym Trading Env is a Gymnasium environment for simulating stocks and training Reinforcement Learning (RL) trading agents. It was designed to be fast and customizable for easy RL trading algorithms implementation.<br><a href="https://github.com/wangshub/RL-Stock">如何用深度强化学习自动炒股</a><br><a href="https://github.com/Jack-Cherish/quantitative?tab=readme-ov-file">量化交易</a><br><a href="https://github.com/waditu/czsc">czsc</a></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>获取股票数据<br>‘’’<br>import baostock as bs<br>import pandas as pd<br>import os</p><p>def mkdir(directory):<br>    if not os.path.exists(directory):<br>        os.makedirs(directory)</p><p>class Downloader(object):<br>    def <strong>init</strong>(self,<br>                 output_dir,<br>                 date_start&#x3D;’1990-01-01’,<br>                 date_end&#x3D;’2024-03-16’):<br>        self._bs &#x3D; bs<br>        bs.login()<br>        self.date_start &#x3D; date_start<br>        # self.date_end &#x3D; datetime.datetime.now().strftime(“%Y-%m-%d”)<br>        self.date_end &#x3D; date_end<br>        self.output_dir &#x3D; output_dir<br>        self.fields &#x3D; “date,open,high,low,close,amount,volume”</p><pre><code class="hljs">def exit(self):    bs.logout()def get_codes_by_date(self, date):    print(date)    stock_rs = bs.query_all_stock(date)    stock_df = stock_rs.get_data()    print(stock_df)    return stock_dfdef run(self):    stock_df = self.get_codes_by_date(self.date_end)    for index, row in stock_df.iterrows():        print(f&#39;processing &#123;row[&quot;code&quot;]&#125; &#123;row[&quot;code_name&quot;]&#125;&#39;)        df_code = bs.query_history_k_data_plus(row[&quot;code&quot;], self.fields,                                           start_date=self.date_start,                                           end_date=self.date_end).get_data()    # 替换文件名中的*字符为_        code_name = row[&quot;code_name&quot;].replace(&#39;*&#39;, &#39;_&#39;)        df_code.to_csv(f&#39;&#123;self.output_dir&#125;/&#123;row[&quot;code&quot;]&#125;.&#123;code_name&#125;.csv&#39;, index=False)    self.exit()</code></pre><p>if <strong>name</strong> &#x3D;&#x3D; ‘<strong>main</strong>‘:<br>    # 获取全部股票的日K线数据<br>    mkdir(‘.&#x2F;data&#x2F;train’)<br>    downloader &#x3D; Downloader(‘.&#x2F;data&#x2F;train’, date_start&#x3D;’2000-01-01’, date_end&#x3D;’2024-2-29’)<br>    downloader.run()</p><pre><code class="hljs">mkdir(&#39;./data/test&#39;)downloader = Downloader(&#39;./data/test&#39;, date_start=&#39;2024-3-1&#39;, date_end=&#39;2024-3-15&#39;)downloader.run()</code></pre><p>‘’’</p><h2 id="选股器"><a href="#选股器" class="headerlink" title="选股器"></a>选股器</h2><p>思路，股票是波动的，但是波动是现象，可能今天是下跌，但是明天就上升了。</p><h2 id="推荐书籍"><a href="#推荐书籍" class="headerlink" title="推荐书籍"></a>推荐书籍</h2><p><a href="https://weread.qq.com/web/reader/1b5325907159cacc1b5e0e1">股票大作手回忆录</a></p><h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><p>炒股就是和自己修炼的过程，在整个过程中需要对抗自己的贪，疑，痴，慢。是使用一个自己来战胜另一个自己的过程。时机，多层次的关系。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑-AIGC和数字化新时代</title>
    <link href="/2024/05/16/tiankeng9/"/>
    <url>/2024/05/16/tiankeng9/</url>
    
    <content type="html"><![CDATA[<h2 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h2><p>2022年在集群式和聚变式的科技革命中，人工智能生成内容（AIGC， AI Generated Content）后来居上，以OpenAI的chatgpt（Chat Generative Pre-trained Transformer）为例，实现了自然语言与人工智能的融合，更进一步的通过大规模训练模型预训练模型，形成人工智能技术理解自然语言和文本生成能力，可以生成文字，语音、代码、图像、视频、而且能完成脚本编写、文案撰写、翻译等任务。GAN（生成对抗网络）的出现加快了AIGC的实现，Transformer在2021年8月被斯坦福大学的众多学者撰写论文，将Transformer架构的模型称为’基础模型’(Foundation model) 又译为大模型。<a href="https://arxiv.org/abs/2108.07258">论文链接</a>,人类大脑皮层有140亿个神经元，触突总数有100万亿个，GPT-3 最大模型有1750个参数，最近chat-gpt-4o出现，融合了多模态的模型。</p><p>1913年埃米尔.博雷尔发表《静力学与不可逆性》论文中提出： 假设猴子学会了任意按下打字机，当无限只猴子在无限台打字机上乱敲，并持续无限长的时间，在某个片刻，将会有猴子能打出莎士比亚的全部著作。其中的条件是无穷的时间量度和随机性。人工智能或许能够缩短这整个的时间量度。<br>万物的智能成本无限降低，人类生产力与创造力得到解放。 AIGC的生成能力极大的解放了人类的内容生产力，从PGC（专业生产，专家创作时代，专业人士来生产高质量内容，在wed1.0时代互联网内容大多是专家生产的。）、UGC（用户生产，用户创作，用户不仅是内容的创作消费者，也是内容的创作者。比如贴吧，豆瓣，微博，微信，抖音，快手）到AIGC（AI生产，人工智能创建文本，音频，图像，视频等各种模态的信息。最初的AIGC通常基于小模型展开，这类模型的一般需要特殊的标注数据，解决特定的问题，通用性较差。后来，大数据量，大参数量，强算法的大模型（Foundation Model）取代，这种形式的AIGC只需要经过微调或经过少量微调（Fine-tuning）就可以迁移到多种生成任务）。生产力是推进社会变革的根本动力，而生产工具则是衡量生产力发展水平的客观尺度，也是划分经济时代的物质标志。<br>2022年美国科罗拉多州博览会上数字艺术类冠军<a href="pic/space_opera_house.jpg">Spaec Opera House</a></p><p>人工智能赋能内容的创作的四大模态——文本、音频、图像、视频四大模态</p><ol><li>文本（Natural Language Processing ， 简称NLP）例子： AI结构化写作 GPT2——NewsTitle， 2017年，微软小冰 人类史上第一部AI编写的诗集《阳光失了玻璃窗》。  故事、剧本和小说， 2016年 人类史上第一部由AI撰写的剧本的电影《阳春》（sunspring）<a href="https://www.bilibili.com/video/BV1dx411879x/?spm_id_from=333.337.search-card.all.click">链接</a>.（看了之后觉得有点抽象。）2021年Netfix 发布了AI创作的电影《谜题先生希望你少活一点》（Mr Puzzles Wants You to Be Less Alive）<a href="https://www.bilibili.com/video/BV1s44y1x742/?vd_source=9814cf6702c46a0b906cb31de22baa58">链接</a>还是抽象，金句 他醉了，但被清醒所困扰。 文字类游戏，《AI 地下城2》 chatgpt </li><li>音频（Audio1） 文本转语音（Test to Speech， 简称：TTS） 短视频配音， AI歌曲生成， 比如腾讯在2020年推出的AI歌姬‘艾灵’ </li><li>图像生成，两种方向 图像编辑工具与图像自主生成。 图像编辑工具的功能包括去除水印、提高分辨率、特定滤镜等。 图像自主生成即 AI绘画，包括创意图像生成（时机或按照特定属性生成画作）与功能性图像生成（生成logo、模特图、营销海报） AI绘画大致可以分为三类： 借助文字描述（Prompt）生成图像、借助已有图像生成新图像，以及两者结合版。 AI绘画工具 Stable DIffusion , Midjourney，文心一格、意间AI绘画、AICreator等  </li><li>AI视频生成， AI技术不仅可以生成图片，也可以生成序列帧，组成一个完整的视频。 2022年10月 AI重置版《幻觉东京》发布， 2022年9月 Meta推出Make-A-video,根据文本描述生成相应短视频的能力。 AI换脸</li></ol><p>AIGC+ 元宇宙 2022年谷歌发布了文本生成3D文本模型DreamFUsion，可以通过生成3D物品模型，AIGC生成制作NFT，绘画风格确权，NFT+AIGC。HiiImeta就是这样一个集艺术风格的确权、授权和使用为一体的AI艺术生态。 </p><p>人们总喜欢活在舒适区内，用出粗暴的断言来安慰自己，例如机器永远无法模仿人类的某些特征。但我给不了这样的安慰，因为我认为并不存在无法模仿的人类特性。——艾伦.图灵 人工智能技术经历了漫长的演进过程，见证了基于规则、机器学习、深度学习、强化学习等领域的兴起。1950年，图灵在《计算机器与智能》提出了图灵测试，为人工智能的实现提供。人工智能于1956年达特茅斯学院举行的人工智能夏季研讨会，人工智能的名字和任务被真正界定下来。<br>符号主义、联结主义和行为主义。机器学习，感知器与神经网络，强化学习<br><a href="https://arxiv.org/abs/1701.07274">Deep Reinforcement Learning: An Overview</a></p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p><a href="https://www.bilibili.com/video/BV1eU411o71r/?spm_id_from=333.337.search-card.all.click">Stable Diffusion</a><br>动漫 [瑞克和莫迪] [爱， 死亡机器人]</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高等数学——多元函数微分学。</title>
    <link href="/2024/05/16/gaodengshuxue4/"/>
    <url>/2024/05/16/gaodengshuxue4/</url>
    
    <content type="html"><![CDATA[<p>多元函数微分学。<br>多元函数的概念，二元函数的几何意义，二元函数的极限与连续的概念，有界闭区间上二元连续函数的性质，多元函数的偏导数和全微分，多元复合函数、隐函数的求导法，二阶偏导数，多元函数的极值和条件极值、最大值和最小值，二重积分的概念、基本性子和计算。</p>]]></content>
    
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高等数学——一元函数积分学</title>
    <link href="/2024/05/15/gaodengshuxue3/"/>
    <url>/2024/05/15/gaodengshuxue3/</url>
    
    <content type="html"><![CDATA[<p> 一元函数积分学。<br>原函数和不定积分的概念，不定积分的基本性质，基本积分公式，定积分的概念和基本性质，定积分中值定理，积分上限的函数及其导数，牛顿-莱布尼茨（Newton-Leibniz）公式，不定积分和定积分的换元积分法与分部积分法。<br>有理函数、三角函数的有理式和简单无理函数的积分，反常（广义）积分，定积分的应用（平面图形的面积，平面曲线的弧长，旋转体的体积及侧面积，平行截面积为已知的立体体积、功、引力、压力、质心、形心等）及函数平均值。</p>]]></content>
    
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔17</title>
    <link href="/2024/05/15/ganwu17/"/>
    <url>/2024/05/15/ganwu17/</url>
    
    <content type="html"><![CDATA[<p>实践的过程中取决于动机，今天的招聘会给我带来的感受是，虽然企业众多但是对于我的就业面却很窄，感受到现实和想法之间的差距。不过也挺好的，把差距看为进步的方向，自强的心态就是这样使用的。对于每个人都是立场的，都有自己的利益观点，做人不要尽可能的不要去侵犯他人的利益，和人交好要先给后取，切记只取不予。面试企业也是一样的，企业看重的是你能为企业解决什么问题，在我的立场中我就是为了找份事情干，能养活自己，先走一步看一步。<br>对应的书籍为:《原则：应对变化中的世界秩序》<a href="https://weread.qq.com/web/reader/19332dd0728b621d193d571k02e32f0021b02e74f10ece8">链接</a><br>世上聪明之人不少，看得透彻之人也不少，但聪明人往往狷介有余，宽厚不足。要知道，厚德载物比自强不息难上千万倍。越年长之人，当越对这种美好又脆弱的东西怀有柔情。唯有中二少年，才自以为看透了社会黑暗，嘲笑一切美好事物，事事都要发一番政治狂热。所谓心结，就是像这样执着于一种观念，乃至为之仇视鲜活的真情与生命。</p><p>之前有一种态度，觉得什么都很水，我自己去做一定能做的更好，这不是傲慢是什么？ </p><h2 id="挖坑：重看原则这本书。"><a href="#挖坑：重看原则这本书。" class="headerlink" title="挖坑：重看原则这本书。"></a>挖坑：重看原则这本书。</h2><p><a href="https://weread.qq.com/web/bookDetail/694329f0813ab7c0dg0179a5">原则（实践版）</a><br><a href="https://weread.qq.com/web/reader/19332dd0728b621d193d571k02e32f0021b02e74f10ece8">原则：应对变化中的世界秩序</a></p><p>我打电话找遍附近五个州，我说只有这样能让我老婆嫁给我。-你甚至还不认识我。-我有一辈子可以认识你。-I called everywhere in five states. I told them it was the only way to get my wife to marry me. -You don’t even know me. -I have the rest of my life to find out.</p><p>生命就是这样。说真的，走远路比较简单，但那比较长。Life will do that to you. And truthfully, the long way is easier. But it’s longer.</p><p>人们说当你遇上你的挚爱，时间会暂停，那是真的，但人们没有告诉你，当时间再度恢复转动，它会无比飞快，让人无法赶上。They say when you meet the love of your life, time stops. And that’s true. What they don’t tell you is that once time starts again……it moves extra fast to catch up.</p><p>就在当晚，我发现，你觉得最邪恶或最坏的事物，其实只是孤独，缺乏融洽的个性。That night, I discovered that most things you consider evil or wicked……are simply lonely and lacking in social niceties.</p><p>懂道理的人，终会有按下自尊，坦承他犯下严重错误的时刻。事实是，我一直都不是讲道理的人，我记得主日学都这么讲，事情愈艰难，最后愈能得到丰厚的果实。Now, there comes a point when a reasonable man……will swallow his pride and admit that he’s made a terrible mistake. The truth is, I was never a reasonable man. And what I recall of Sunday school was that……the more difficult something is, the more rewarding it is in the end.</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>目标检测——Fast R-CNN</title>
    <link href="/2024/05/15/deeplearnpaper8/"/>
    <url>/2024/05/15/deeplearnpaper8/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>目标检测——RCNN</title>
    <link href="/2024/05/15/deeplearnpaper7/"/>
    <url>/2024/05/15/deeplearnpaper7/</url>
    
    <content type="html"><![CDATA[<p>目标检测，最为经典的项目实例就是人脸检测，在paper with code<a href="https://paperswithcode.com/sota">链接</a> 网站中在Object Decection中包含大量案例，但是最为经典的还是RCNN，开山之作。</p><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>论文链接<a href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a><br><a href="https://zhuanlan.zhihu.com/p/23006190">参考链接</a><br><a href="https://github.com/yangxue0827/RCNN">代码实现</a></p><h2 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h2><p>CNN算法分为4个步骤</p><p>候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）<br>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）<br>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类<br>位置精修： 使用回归器精细修正候选框位置</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>英语-作文</title>
    <link href="/2024/05/14/English/"/>
    <url>/2024/05/14/English/</url>
    
    <content type="html"><![CDATA[<p>一般来讲：英语作文分成三段就可以了。首段： 第一句话题引入，第二句，主题阐述。 中间段：过渡句。一方面，背景句+论据+论点。另一方面， not only, but also + 举例。 尾段： 总结， 建议措施（2个）</p><p>六级作文一般可以分为议论文，议论文——意义措施类， 议论文——观点选择类， 议论文——现象解释类， 议论文——谚语解释类， 议论文——图画类</p><h2 id="议论文——意义措施类"><a href="#议论文——意义措施类" class="headerlink" title="议论文——意义措施类"></a>议论文——意义措施类</h2><p>介绍，Your essay should include the importance of innovation and measures to be taken to encourage innovation&#x2F;creation&#x2F;invention. （关键词： innovation 创新。）</p><p>In a rapidly developing society,It is generally accepted that innovation plays a fundamental role in personal progress and national development.（话题引入句子，首段第一句， + 介词短语引入）Innovation,which is held by many to be core competitiveness,has captured a great deal of public attention.（首段第二句，话题阐述。 结构：主题词+ which（非限定性定语从句）+ 模板句）</p><p>在一个快速发展的社会中，人们普遍认为创新对个人进步和国家发展起着基础性作用发展、创新被许多人认为是核心竞争力的，引起了公众的极大关注。</p><p>As regards to the social implication of innovation,my discussion on the issue is mainly twofold.（过渡句）On the one hand,in a society where competition in job markets becomes increasingly fierce(stiff),an increasing number of people find it helpful to enhance our competitiveness.Thus,continuous self innovation is not a choice, but a must to meet the ever-growing demands of our society.（中间段第一方面:背景句+ 论据+ 论点）On the other hand,People with the spirit of innovation can not only accumulate various knowledge,but also can promote social science and technology innovation. For example,Einstein,honored for his success in the theory of relativism,is always remembered for the spirit of innovation,which makes him one of the greatest scientists.（另一方面，not noly，but also一定要分两个方面（一小一大）,小就是对于咱们个人，或者对于我们自己有什么影响；大就是对社会，对国家，乃至对于全人类有什么影响。）<br>关于创新的社会内涵，我的讨论有如下两个方面。第一一方面，在一个就业市场竞争日益激烈的社会中，越来越多的人发现这有助于提高我们的竞争力竞争力。因此，不断的自主创新不是一种选择，但必须满足我们国家日益增长的需求社会。另一方面，具有创新精神的人不仅可以积累各种知识，而且可以推动社会科技进步创新。为了例如，爱因斯坦因其在相对主义理论上的成功而受到尊敬，他以创新精神而被人们铭记，这使他成为最伟大的科学家之一。</p><p>To sum up, it is of great benefit for us to cultivate innovative spirit during college and learn how to create some new inventions.（总结： 直接套用）Specifically,The fundamental way in which the government advocates this spirit is to enhance people’s awareness.Meanwhile,the school should also join in the efforts in promoting quality-oriented education。（建议措施2个）<br>综上所述，在大学期间培养创新精神，学会创造新发明，对我们有很大好处。具体而言，政府倡导这种精神的根本途径是提高人民的意识。同时，学校也应该参与到推进素质教育的工作中来。</p><ol><li>首段的结构中，分为两个部分，话题引入+ 主题阐述。<br>首先，话题引入，主从句开头,从下面选取一个可以用来作为开头<br>There is little doubt that… 毫无疑问…<br>It cannot be denied that… 不可否认…<br>It is beyond doubt that… 毫无疑问…<br>It is generally accepted that… 人们普遍认为…<br>There is a growing recognition that… 人们越来越认识到…<br>It has been widely noted that… 人们普遍注意到…<br>It goes without saying that… 不用说的是…</li></ol><p>主题阐述：结构，主题词 + ,which(非限定性定语从句) + 模板句。（注： 主题词即议论文中的主题，比如上面的模板文章中的主题是innovation，）<br>which(非限定定语从句)<br>选择1：人们认为该现象… ,which is held by many to be…（be动词后面要么名词，要么加形容词）<br>选择2：带来…影响 ,which exerts a great impact&#x2F;a positive influence&#x2F;a negative impact on…</p><p>模板句<br>选择1：sth has&#x2F;have captured&#x2F;drawn one’s attention.某事吸引了某人的注意力<br>选择2：sth has&#x2F;have emerged into our vision.某事出现在我们的视野中。</p><ol start="2"><li>中间段：，包括，过渡句， 一方面（背景句+论据+论点），另一方面（not only， but aslo）</li></ol><p>过渡句<br>选择1：There are two fundamental factors contributing to this phenomenon.造成这种现象有两个基本因素。<br>选择2：As regards to the social implication of innovation,my discussion on the issue is mainly twofold.关于创新的社会影响，我的讨论主要有两个方面。</p><p>一方面（背景句+论据+论点）<br>背景句<br>社会竞争:In a society where competition in job markets becomes increasingly fierce(stiff),在一个就业市场竞争日益激烈的社会里，<br>发展(科技):In an age where globalization and information technology revolution are developing(advancing) rapidly,在全球化和信息技术革命迅猛发展的时代，<br>发展(经济):In an age where people’s standard of living has been raised significantly,在人民生活水平大幅度提高的时代，<br>教育:In an age where parents and educators always focus on sth，在一个父母和教育者总是关注某件事的时代，<br>媒体:In an age where sth have（not）been highly advocated in mass media,在一个大众传媒一直大力提倡某事的时代，</p><p>论据<br>an increasing number of people find it helpful&#x2F;difficult&#x2F;harmful to enhance our competitiveness.（竞争力）越来越多的人发现提高我们的竞争力是有益的。<br>这个句型的基本结构是sb find it +形容词+ to do sth。<br>“紧跟不断变化的社会（to to keep up with the change of society）”</p><p>论点<br>Thus continuous self innovation is not a choice, but a must to meet the ever-growing demands of our society.因此，不断的自主创新不是一种选择，而是满足社会日益增长的需求的必然选择。（语气较强烈）<br>另一方面（（not only， but aslo）一大一小）<br>小<br>not only，but also你可能用到的语句我给你总结出来了：（写作素材）<br>overcome difficulties and challenges 克服困难和挑战<br>achieve success 取得成功<br>accumulate knowledge 增长知识 broaden our horizons 开拓视野 expand social circle 扩大社交圈<br>enrich our minds(lives) 丰富我们的思想（生活）<br>protect our living environment and conserve energy 保护环境，节约能源<br>promote social harmony 增进社会和谐<br>preserve and carry forward traditional Chinese culture 保护和发扬中国传统文化</p><p>大<br>大家举例时要注意，举具体的人或事，比如姚明，屠呦呦，华为，苹果等这些具体的事物，然后套用举例模板：<br>1.创新话题：For example,Einstein,honored for his success in the theory of relativism,is always remembered for the spirit of innovation,which makes him one of the greatest scientists.（划线的地方是你要结合话题写的，其他的地方直接套用即可。）例如，爱因斯坦因其在相对主义理论上的成功而受到表彰，他因其创新精神而被人们铭记，这使他成为最伟大的科学家之一。<br>我作为阅卷老师，只需要看到你与话题结合即可，至于你具体写的啥内容我不是很在意。再举一个例子：<br>2.合作话题：For instance,Apple,honored for its success in mobile phone and personal computer,is always remembered for its collaboration with Google in software,which makes them the most competitive technology company in the world.例如，苹果公司因其在手机和个人电脑领域的成功而备受赞誉，却因其与谷歌在软件领域的合作而被人们铭记，这使他们成为世界上最具竞争力的科技公司。（只需要在划线部分与话题结合即可）<br>3.教育：For instance,Confucius,honored for his success in education and philosophy in ancient times,is always remembered for showing a great respect to others,which makes him the most famous educator and philosopher.例如，孔子在古代因其在教育和哲学方面的成就而受到尊崇，他因对他人的尊重而被人们铭记，这使他成为最著名的教育家和哲学家。</p><ol start="3"><li>尾段（总结，建议措施）<br>总结<br>To sum up&#x2F;Based on the reasons above, it is of great benefit for us to cultivate innovative spirit during college and learn how to create some new inventions. 综上所述，在大学期间培养创新精神，学会创造新发明，对我们有很大好处。</li></ol><p>建议措施<br>措施1：Specifically,The fundamental way in which the government advocates this spirit is to enhance people’s awareness.具体来说，政府倡导这一精神的根本途径就是增强人们的觉悟。（模板句直接套用，也可以在划线部分体现出主题词，任何话题都可以从政府的角度去写）<br>措施2：Meanwhile,the school should also join in the efforts in promoting quality-oriented education.同时，学校也要参与到素质教育的推进中来。（学校或媒体，直接套用即可）</p><h2 id="议论文——观点选择类"><a href="#议论文——观点选择类" class="headerlink" title="议论文——观点选择类"></a>议论文——观点选择类</h2><p>观点选择类，顾名思义，就是两个观点二选一。例如：Directions: Suppose you are asked to give advice on whether to major in humanities or science, write an essay to state your opinion. You are required to write at least 150 words but no more than 200 words.选文科还是选理科？</p><p>观点选择类完整模板演示：<br>In modern society,it is generally accepted that higher education plays a fundamental role in personal progress and national development.（话题引入（依然是介词短语+主语从句引导））Opinions vary greatly when it comes to whether one should choose science or humanities at college.If I were you,I would like to take science as my major.（话题阐述（观点选择类特有阐述框架））<br>在现代社会，人们普遍认为高等教育对个人进步和国家发展起着基础性作用.意见在选择科学还是人文科学的问题上，差别很大。如果我是你，我想主修理科。</p><p>There are two fundamental factors contributing to my preference.（中间段第一句（过渡句，直接套用之前的模板即可）） In a society where competition in job markets becomes increasingly fierce,an increasing number of teenagers find it helpful that learning science can obtain a decent job.Thus,majoring scientic knowledge is a wise choice to meet the ever-growing demands of our society.（中间段第一方面：背景句+论据+论点（3个部分）） science not only can improve ourselves to meet the ever-growing demands of our society,but also can push the whole human race forward. If it were not for the science and technology,our daily life today would not be so convenient and colorful.（中间段第二方面：not only,but also+if虚拟语气）<br>有两个基本因素解释了我的偏好。一方面，在就业市场竞争日益激烈的社会中，越来越多的青少年发现学习科学能获得体面的工作是有帮助的工作。因此,主修理科是满足我们社会日益增长的需求的明智选择。另一方面，科学不仅可以提高我们自己，以满足我们社会日益增长的需求，而且可以推动整个人类向前发展。如果没有理科和科学技术，我们今天的日常生活就不会如此方便多彩。</p><p>To sum up, it is of great benefit for our college students to choose siience as our major in university and learn actively more innovative knowledge in technology and science.（尾段第一句：总结）Specifically,The fundamental way in which the government and school advocate students from different backgrounds is to study science.（题目中并没有明确要求我们写措施，所以我们写“一个措施”即可，如果字数不够，可以写两个措施）<br>综上所述，选择理科作为大学的专业，积极学习更多的科技创新知识，对我国大学生有很大的帮助。具体而言，根本途径是政府和学校倡导不同背景的学生学习理科。</p><h2 id="议论文——现象解释类"><a href="#议论文——现象解释类" class="headerlink" title="议论文——现象解释类"></a>议论文——现象解释类</h2><p>Directions：For this part, you are allowed 30 minutes to write a short essay on the use of robots. Try to imagine what will happen when more and more robots take the place of human beings in industry as well as people’s daily lives. You are required to write at least 150 words but no more than 200 words.想象越来越多的机器人替代人类的工作和生活会发生什么。</p><p>In a rapidly developing society, There is little doubt that the development of technology plays an increasingly important role in our life. （话题引入（依然是介词短语+主语从句引导）Robots,which is held by many to be controversial,has captured a great deal of public attention if robots replace human beings in the future.（话题阐述，结构：主题词+,which(非限制性定语从句)+模板句）<br>在一个快速发展的社会中，毫无疑问，科技的发展在我们的生活中扮演着越来越重要的角色。机器人被许多人认为是有争议的，如果机器人在未来取代人类的话，会引起公众的极大关注。（注意“话题阐述”部分的区别）</p><p>As regards to the social implication of robots,my discussion on the issue is mainly twofold. （中间段第一句（过渡句，直接套用之前的模板即可））In an age where globalization and information technology revolution are developing(advancing) rapidly,an increasing number of people find it increasingly difficult to find a proper job for them. Thus,Changing traditional way of thinking in work is not a choice,but a must to meet the ever-growing demands of our society.（中间段第一方面：背景句+论据+论点（3个部分）） High-skilled working abilities not only can give people an opportunity to increase your income,but also will not be replaced by robots. If people always work with traditional thinking, they will be replaced by robots.（中间段第二方面：not only,but also+if条件状语（依然是用”if”替代“举例”的方案，但是大家一定要搞清楚“条件状语从句”和“虚拟语气”的区别，）<br>关于机器人的社会影响，我对这个问题的讨论主要有两个方面。在全球化和信息技术革命迅猛发展的时代，越来越多的人发现找一份适合自己的工作越来越困难。因此，改变传统的工作方式不是一种选择，而是必须，从而满足我们社会日益增长的需求。高技能的工作能力不仅能给人们增加收入的机会，而且不会被机器人取代。如果人们总是用传统的思维方式工作，他们将被机器人所取代。</p><p>To sum up, it is of great benefit for us to cultivate innovative thinking when we work and learn how to learn knowledge in high-skilled jobs.（尾段第一句：总结）Specifically,The fundamental way is that the government advocates sensible use of robots balance the relationship between human beings and robots.Meanwhile,the school should also join in the efforts in studying innovative knowledge.<br>总之，在工作中培养创新思维，在高技能工作中学习知识，对我们非常有好处。综上所述，选择理科作为大学的专业，积极学习更多的科技创新知识，对我国大学生有很大的帮助。具体而言，根本途径是政府和学校倡导不同背景的学生学习理科。</p><h2 id="议论文——谚语警句类"><a href="#议论文——谚语警句类" class="headerlink" title="议论文——谚语警句类"></a>议论文——谚语警句类</h2><p>Directions: For this part, you are allowed 30 minutes to write an essay commenting on the saying “Respect others, and you will be respected. “ You can cite examples to illustrate your views. You should write at least 150 words but no more than 200 words.</p><p>In modern society, There is little doubt that the respect plays an increasingly important role in interpersonal relationship. Just as the saying goes, “Respect others, and you will be respected”,which exerts a positive influence on daily communication.<br>人际关系中扮演着越来越重要的角色。正如俗话所说，“尊重别人，你就会被尊重”，这对日常交流产生了积极的影响。</p><p>There are two fundamental factors cited to account for this saying.On the one hand,In an age where people’s standard of living has been raised significantly,an increasing number of people find it helpful to bring people closer. Thus respecting others is not a choice,but a must to satisfy the need of people‘s self-esteem.On the other hand,Being respected not only plays an indispensable role in gaining respect from others but also contributes to the construction of a harmonious society.For instance, Confucius, a great philosopher of ancient times in China, was very polite and showed a great respect to Laozi,and Laozi treated Confucius in a respective manner in return.<br>有两个基本因素可以用来解释这句话。在人民生活水平大幅度提高的时代，越来越多的人发现尊重帮助拉近人们的距离。因此，尊重他人不是一种选择，而是满足人们自尊需要的必要条件。尊重他人不仅对赢得他人的尊重起着不可或缺的作用，而且有助于构建和谐社会。例如，孔子在古代因其在教育和哲学方面的成就而受到尊崇，他因对他人的尊重而被人们铭记，这使他成为最著名的教育家和哲学家。</p><p>To sum up,it is of great benefit for us to cultivate equal thinking and learn how to repsect others.Specifically,The fundamental way is that the government advocates a wholesome atmosphere to publicize the importance of mutual respect.Meanwhile,the school should also join in the efforts in promoting quality-oriented education.<br>综上所述，培养平等思维，学会代表他人，对我们非常有好处。具体而言，根本途径是政府倡导一种健康的氛围，以强调相互尊重的重要性。同时，学校也要参与到素质教育的推进中来。</p>]]></content>
    
    
    
    <tags>
      
      <tag>英语</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>英语-单词</title>
    <link href="/2024/05/14/English2/"/>
    <url>/2024/05/14/English2/</url>
    
    <content type="html"><![CDATA[<p>Babies suck their thumbs for comfort. (Suck: 吮吸)<br>  婴儿吮吸他们的拇指以获得安慰。</p><ul><li><p>She wrote a biography of the famous artist. (Biography: 传记)<br>她写了一部著名艺术家的传记。</p></li><li><p>They went canoeing on the river. (Canoe: 独木舟)<br>他们在河上划独木舟。</p></li><li><p>He gazed out of the window at the beautiful view. (Gaze: 凝视)<br>他凝视着窗外美丽的景色。</p></li><li><p>She sipped her drink through a straw. (Straw: 吸管)<br>她用吸管慢慢地喝着饮料。</p></li><li><p>The total cost of the project was much higher than expected. (Total: 总的)<br>项目的总成本远远超出了预期。</p></li><li><p>I feel happy when I’m with my friends. (Feel: 感觉)<br>和朋友在一起时，我感觉很开心。</p></li><li><p>Hold the rope tightly so it doesn’t slip. (Hold: 抓住)<br>抓紧绳子，不要让它滑掉。</p></li><li><p>Iron deficiency can lead to fatigue and weakness. (Deficiency: 缺乏)<br>缺铁会导致疲劳和虚弱。</p></li><li><p>According to the weather forecast, it will rain tomorrow. (According to: 根据)<br>根据天气预报，明天会下雨。</p></li><li><p>The cut on his hand started to bleed. (Bleed: 流血)<br>他手上的伤口开始流血。</p></li><li><p>The fields were full of golden wheat. (Wheat: 小麦)<br>田野里满是金黄色的小麦。</p></li><li><p>He was seen as a traitor by his former colleagues. (Traitor: 叛徒)<br>他被他以前的同事视为叛徒。</p></li></ul><ol start="2"><li></li></ol><ul><li><p>She has a deep understanding of the subject. (Understanding: 理解)<br>她对这个课题有着深刻的理解。</p></li><li><p>The children dressed up as fairies for the party. (Fairy: 仙女)<br>孩子们为派对装扮成了仙女。</p></li><li><p>The walls were painted a bright yellow. (Yellow: 黄色)<br>墙壁被涂成了明亮的黄色。</p></li><li><p>They sailed to a remote isle for their vacation. (Isle: 小岛)<br>他们航行到了一个偏远的小岛度假。</p></li><li><p>The project is in the final phase now. (Phase: 阶段)<br>该项目现在进入了最后阶段。</p></li><li><p>The bed was comfortable and soft. (Comfortable: 舒适的)<br>床又舒适又软。</p></li><li><p>The statue was made of metal. (Metal: 金属)<br>雕像是用金属制成的。</p></li><li><p>There has been an increase in the incidence of flu this year. (Incidence: 发生率)<br>今年流感发病率有所增加。</p></li><li><p>The technician repaired the computer quickly. (Technician: 技术员)<br>技术员很快修好了电脑。</p></li><li><p>They went to the temple to worship. (Worship: 崇拜)<br>他们去寺庙做礼拜。</p></li><li><p>We need to decide on a date for the meeting. (Decide: 决定)<br>我们需要决定一个会议日期。</p></li><li><p>She was an eloquent speaker, captivating the audience with her words. (Eloquent: 雄辩的)<br>她是一个雄辩的演讲者，她的话语让听众着迷。</p></li><li><p>They took shelter from the rain under a tree. (Shelter: 庇护所)<br>他们在树下避雨。</p></li><li><p>The historian wrote a detailed account of the war. (Historian: 历史学家)<br>历史学家详细记录了这场战争。</p></li><li><p>The concert was held in a large auditorium. (Auditorium: 礼堂)<br>音乐会在一个大礼堂举行。</p></li></ul><ol start="3"><li></li></ol><ul><li><p>The proposal was approved by a majority of the members. (Majority: 多数)<br>提案获得了大多数成员的批准。</p></li><li><p>The painting turned out to be a fake. (Fake: 伪造的)<br>这幅画原来是假的。</p></li><li><p>The exam was difficult, but I managed to pass. (Difficult: 困难的)<br>考试很难，但我设法通过了。</p></li><li><p>They walked along the shore, enjoying the sound of the waves. (Shore: 海岸)<br>他们沿着海岸走，享受着海浪的声音。</p></li><li><p>His opinion on the matter is purely subjective. (Subjective: 主观的)<br>他对这个问题的看法纯粹是主观的。</p></li><li><p>She used a sponge to clean the kitchen counter. (Sponge: 海绵)<br>她用海绵擦洗厨房台面。</p></li><li><p>The queen visited the school to meet the students. (Queen: 女王)<br>女王访问了学校，见到了学生们。</p></li><li><p>The country celebrated the anniversary of its conquest. (Conquest: 征服)<br>这个国家庆祝征服周年纪念。</p></li><li><p>He was praised for his saving of the child from the fire. (Saving: 挽救)<br>他因为从火灾中挽救孩子而受到表扬。</p></li><li><p>There was a minor incident at the factory, but no one was hurt. (Minor: 微小的)<br>工厂发生了一起小事故，但没有人受伤。</p></li><li><p>The ancient artifacts were found intact in the tomb. (Intact: 完整的)<br>古代文物在墓穴中被发现完好无损。</p></li><li><p>The citizens showed their patriotic spirit during the national anthem. (Patriotic: 爱国的)<br>公民们在国歌响起时展现了他们的爱国精神。</p></li><li><p>The explosive device was safely removed by the bomb squad. (Explosive: 爆炸性的)<br>爆炸装置被爆炸物处理小组安全地移除了。</p></li><li><p>The scientist used a complex apparatus to conduct the experiment. (Apparatus: 设备)<br>科学家使用复杂的设备进行实验。</p></li><li><p>His account of the events corresponded with the witness’s testimony. (Correspond: 符合)<br>他对事件的描述与证人的证词相符。</p></li></ul><ol start="4"><li></li></ol><ul><li><p>He spoke loudly, emphasizing each word. (Adverb: 副词)<br>他大声说话，强调每个词。</p></li><li><p>Please provide a sample of your work for our review. (Sample: 样本)<br>请提供您的一份工作样本供我们审查。</p></li><li><p>He was idle for most of the day, just lounging around. (Idle: 懒散的)<br>他大部分时间都很懒散，整天无所事事。</p></li><li><p>The pump was used to remove water from the basement. (Pump: 泵)<br>泵被用来将地下室的水抽出。</p></li><li><p>The lateral movement of the car caused it to skid off the road. (Lateral: 侧面的)<br>汽车的横向移动导致它滑出了道路。</p></li><li><p>The chamber was filled with smoke after the explosion. (Chamber: 室，腔)<br>爆炸后，房间里充满了烟雾。</p></li><li><p>The circular shape of the table made it easy to fit in the corner. (Circular: 圆形的)<br>桌子的圆形使其容易放在角落里。</p></li><li><p>I bumped into an old classmate at the grocery store. (Classmate: 同学)<br>我在杂货店里碰到了一个老同学。</p></li><li><p>The price of the product is ten cents. (Cent: 分)<br>这个产品的价格是十分之一美元。</p></li><li><p>The general consensus was that the plan needed to be revised. (General: 一般的)<br>普遍的共识是需要修改计划。</p></li><li><p>There was a small slit in the curtain, allowing a beam of light to enter the room. (Slit: 狭缝)<br>窗帘上有一个小狭缝，允许一束光进入房间。</p></li><li><p>He injured his shoulder while playing football. (Shoulder: 肩膀)<br>他在踢足球时伤到了肩膀。</p></li><li><p>She always carries a notebook with her to jot down ideas. (Notebook: 笔记本)<br>她总是随身携带一个笔记本来记录想法。</p></li><li><p>The statute of limitations for the crime had expired. (Statute: 法令)<br>该犯罪的诉讼时效已过。</p></li><li><p>The kid was excited to go to the zoo for the first time. (Kid: 孩子)<br>孩子第一次去动物园很兴奋。</p></li></ul><ol start="5"><li></li></ol><ul><li><p>She is a famous actress known for her award-winning performances. (Famous: 著名的)<br>她是一位著名的女演员，以她屡获殊荣的表演而闻名。</p></li><li><p>The recipient of the scholarship was announced at the ceremony. (Recipient: 接受者)<br>奖学金的获得者在仪式上宣布。</p></li><li><p>I owe you an apology for my behavior yesterday. (Owe: 欠)<br>我应该为昨天的行为向你道歉。</p></li><li><p>The external appearance of the building was impressive. (External: 外部的)<br>建筑物的外部外观令人印象深刻。</p></li><li><p>He had a nightmare about being chased by a monster. (Nightmare: 噩梦)<br>他做了一个被怪物追赶的噩梦。</p></li><li><p>She uses her laptop to work remotely from home. (Laptop: 笔记本电脑)<br>她用笔记本电脑在家远程工作。</p></li><li><p>The longitude and latitude coordinates were used to pinpoint the location. (Longitude: 经度)<br>经度和纬度坐标被用来确定位置。</p></li><li><p>The computer chip is responsible for processing information. (Chip: 芯片)<br>计算机芯片负责处理信息。</p></li><li><p>The theory behind the experiment was complex. (Theory: 理论)<br>实验背后的理论很复杂。</p></li><li><p>How do you relate to the characters in this novel? (Relate: 关联)<br>你如何与这本小说中的角色产生共鸣？</p></li><li><p>There is a wide variety of fruits available at the market. (Variety: 种类)<br>市场上有各种各样的水果。</p></li><li><p>His speech left a lasting impression on the audience. (Impression: 印象)<br>他的演讲给观众留下了深刻的印象。</p></li><li><p>He decided to run for mayor in the upcoming election. (Run: 竞选)<br>他决定在即将到来的选举中竞选市长。</p></li><li><p>All that remains of the ancient city are its ruins. (Remains: 遗迹)<br>古城的遗迹是它的一切。</p></li></ul><ol start="6"><li></li></ol><ul><li><p>We boarded the ship and set sail for our destination. (Aboard: 在船上)<br>我们登上船，启航前往目的地。</p></li><li><p>She decided to skim through the report to get a general idea. (Skim: 略读)<br>她决定略读报告，获取一个大致的了解。</p></li><li><p>The vertical lines on the paper formed a grid. (Vertical: 垂直的)<br>纸张上的垂直线形成了一个网格。</p></li><li><p>The company plans to promote several employees to management positions. (Promote: 提升)<br>公司计划将几名员工提升为管理职位。</p></li><li><p>Please boil the water before drinking it. (Boil: 煮沸)<br>请把水煮沸后再喝。</p></li><li><p>The weather forecast is gloomy, predicting rain for the next few days. (Gloomy: 阴沉的)<br>天气预报很阴沉，预测接下来几天会下雨。</p></li><li><p>The term “Negro” is now considered outdated and offensive. (Negro: 黑人)<br>“Negro”这个词现在被认为是过时和冒犯性的。</p></li><li><p>Add salt and pepper to taste. (Add: 添加)<br>按口味加盐和胡椒。</p></li><li><p>He suffered from acute pain in his back. (Acute: 急性的)<br>他背部剧烈疼痛。</p></li><li><p>Please make a note of the meeting time. (Note: 记录)<br>请记下会议时间。</p></li><li><p>She mashed the potatoes and served them with butter. (Potato: 马铃薯)<br>她把马铃薯捣碎，加上黄油。</p></li><li><p>The doctor will induce labor to help with the delivery. (Induce: 诱导)<br>医生将诱导分娩来帮助分娩。</p></li><li><p>The two countries agreed to increase trade between them. (Trade: 贸易)<br>这两个国家同意增加彼此之间的贸易。</p></li></ul><ol start="7"><li></li></ol><ul><li><p>My surname is Smith. (Surname: 姓氏)<br>我的姓氏是史密斯。</p></li><li><p>The old building is being used as a community center. (Used: 被使用)<br>这座旧建筑被用作社区中心。</p></li><li><p>She gave the bucket a whirl and watched it spin. (Whirl: 旋转)<br>她使桶快速旋转，并观察它旋转。</p></li><li><p>The company’s security breach compromised thousands of customer accounts. (Breach: 违反，裂缝)<br>公司的安全漏洞影响了数千个客户账户。</p></li><li><p>The house is well insulated, so it stays warm in the winter. (Insulate: 隔离，使绝缘)<br>这座房子隔热效果很好，所以冬天保持温暖。</p></li><li><p>Eggs are a common ingredient in many recipes. (Ingredient: 成分，原料)<br>鸡蛋是许多食谱中常见的成分。</p></li><li><p>It’s normal to feel nervous before a big exam. (Normal: 正常的)<br>在一次重要考试之前感到紧张是正常的。</p></li><li><p>The path through the forest was overgrown and hard to follow. (Path: 路径)<br>穿过森林的小路长满了草，很难跟随。</p></li><li><p>The word “home” has different meanings for different people. (Meaning: 意义)<br>“家”这个词对不同的人有不同的意义。</p></li><li><p>Marine life refers to organisms that live in the sea. (Marine: 海洋的)<br>海洋生物指的是生活在海洋中的生物。</p></li><li><p>She received a letter from her friend who was studying abroad. (Letter: 信件)<br>她收到了一封来自正在国外留学的朋友的信。</p></li><li><p>The controversial decision sparked heated debate among the members. (Controversy: 争议)<br>这个有争议的决定引发了成员之间的激烈争论。</p></li><li><p>The hawk soared high in the sky, searching for its next meal. (Hawk: 鹰，掠夺者)<br>鹰在天空中高飞，寻找下一顿食物。</p></li></ul><ol start="8"><li></li></ol><ul><li><p>Many people emigrate from their home countries in search of better opportunities. (Emigrate: 移民)<br>许多人为了寻找更好的机会而移民到其他国家。</p></li><li><p>Nuts are a good source of protein and healthy fats. (Nut: 坚果)<br>坚果是蛋白质和健康脂肪的良好来源。</p></li><li><p>The voyage across the ocean took several weeks. (Voyage: 航行)<br>穿越海洋的航行持续了几个星期。</p></li><li><p>It’s important to use proper grammar when writing formal documents. (Proper: 适当的)<br>在撰写正式文件时使用正确的语法很重要。</p></li><li><p>She was exceedingly happy to hear the good news. (Exceedingly: 极其)<br>她听到好消息后非常高兴。</p></li><li><p>I’m going to bake a pie for dessert. (Pie: 派)<br>我打算做一个派作为甜点。</p></li><li><p>The study focused on quantitative analysis of the data. (Quantitative: 定量的)<br>这项研究着重于对数据的定量分析。</p></li><li><p>The company’s stock prices have been steadily increasing. (Stock: 股票)<br>公司的股价一直在稳步上升。</p></li><li><p>The opposite of “hot” is “cold”. (Opposite: 相反的)<br>“热”的反义词是“冷”。</p></li><li><p>I intend to finish reading this book by the end of the week. (Intend: 打算)<br>我打算在本周结束前读完这本书。</p></li><li><p>“Break a leg” is an idiom that means good luck. (Idiom: 成语)<br>“祝你好运”是一个意为好运的成语。</p></li><li><p>She received her diploma after completing her studies. (Diploma: 文凭)<br>她在完成学业后获得了文凭。</p></li></ul><ol start="9"><li></li></ol><ul><li><p>The region has its own unique dialect that differs from standard language. (Dialect: 方言)<br>这个地区有着与标准语言不同的独特方言。</p></li><li><p>They arrived in the afternoon, just in time for lunch. (Afternoon: 下午)<br>他们在下午抵达，正好赶上午饭时间。</p></li><li><p>The helicopter landed on the rooftop to evacuate the injured. (Helicopter: 直升机)<br>直升机降落在屋顶上，撤离受伤者。</p></li><li><p>Can you answer the phone while I’m away? (Answer: 回答)<br>我不在的时候，你能接电话吗？</p></li><li><p>She tied a knot in the rope to secure it. (Knot: 结)<br>她在绳子上打了个结来固定它。</p></li><li><p>The pie crust was perfectly golden and crispy. (Crust: 面包皮)<br>派饼皮呈现出完美的金黄色，脆脆的。</p></li><li><p>He was known for his evil deeds and cruel intentions. (Evil: 邪恶)<br>他以邪恶的行为和残忍的意图而闻名。</p></li><li><p>The train was delayed due to track maintenance. (Delay: 延迟)<br>火车因轨道维护而延误了。</p></li><li><p>The new product received a lot of publicity before its launch. (Publicity: 宣传)<br>这个新产品在推出前受到了很多宣传。</p></li><li><p>Please keep this information confidential. (Keep: 保持)<br>请保密这些信息。</p></li><li><p>The professor’s counterpart in the other department is equally respected. (Counterpart: 对应物)<br>另一个部门的教授同样受到尊敬。</p></li><li><p>The party had a medieval theme with costumes and decorations. (Theme: 主题)<br>派对以中世纪为主题，有着服装和装饰。</p></li><li><p>The old mill by the river has been converted into a museum. (Mill: 磨坊)<br>河边的老磨坊已经改建成了博物馆。</p></li><li><p>He hurled the ball across the field in a powerful throw. (Hurl: 猛投)<br>他用力将球投向球场的另一边。</p></li><li><p>Her answer was accurate and well-researched. (Accurate: 准确的)<br>她的回答准确无误，经过深入研究。</p></li><li><p>The bathroom is located at the end of the hallway. (Bathroom: 浴室)<br>浴室位于走廊的尽头。</p></li></ul><ol start="10"><li></li></ol><ul><li><p>The video player allows you to embed videos directly into your website. (Embed: 嵌入)<br>视频播放器允许您将视频直接嵌入到您的网站中。</p></li><li><p>The community came together to sustain each other during the difficult times. (Sustain: 支撑)<br>社区在艰难时期团结一致，互相支持。</p></li><li><p>They decided to alternate the leadership role every month. (Alternate: 交替)<br>他们决定每个月轮流担任领导角色。</p></li><li><p>The new technology is designed to propel the spacecraft forward. (Propel: 推进)<br>新技术旨在推进航天器前进。</p></li><li><p>The literacy rate in the region has seen a significant improvement. (Literacy: 识字能力)<br>该地区的识字率有了显著提高。</p></li><li><p>She used art to express her emotions and thoughts. (Express: 表达)<br>她用艺术来表达自己的情感和思想。</p></li><li><p>The assembly of the furniture took several hours. (Assembly: 装配)<br>家具的组装花了几个小时的时间。</p></li><li><p>The ox pulled the heavy cart with ease. (Ox: 牛)<br>牛轻松地拉着沉重的车。</p></li><li><p>The fare for the bus ride was quite reasonable. (Fare: 车费)<br>公交车费相当合理。</p></li><li><p>After careful consideration, she decided to accept the job offer. (Consideration: 考虑)<br>经过仔细考虑，她决定接受这份工作。</p></li><li><p>She stood out from the crowd with her vibrant presence. (Presence: 出现)<br>她以她充满活力的存在感脱颖而出。</p></li><li><p>The clerk at the store was busy attending to customers. (Clerk: 店员)<br>商店的店员忙着接待顾客。</p></li><li><p>He had to yell to be heard over the noise of the crowd. (Yell: 大声喊叫)<br>他不得不大声喊叫才能在人群的嘈杂声中被听到。</p></li></ul><ol start="11"><li></li></ol><ul><li><p>The town is located in the middle of the valley. (Middle: 中间)<br>小镇位于山谷中间。</p></li><li><p>Aluminum is a lightweight metal commonly used in aerospace applications. (Aluminum: 铝)<br>铝是一种轻质金属，常用于航空航天应用。</p></li><li><p>I reckon it will take us about an hour to reach the destination. (Reckon: 估计)<br>我估计我们到达目的地需要大约一个小时。</p></li><li><p>The device is designed to monitor heart rate during exercise. (Device: 设备)<br>这个设备旨在监测运动期间的心率。</p></li><li><p>She was able to recognize her old friend despite not having seen him in years. (Recognize: 认出)<br>尽管多年未见，她仍能认出她的老朋友。</p></li><li><p>He couldn’t stop the sudden sneeze. (Sneeze: 打喷嚏)<br>他无法阻止突然的打喷嚏。</p></li><li><p>It’s polite to greet someone when you meet them. (Greet: 问候)<br>见到别人时打招呼是礼貌的。</p></li><li><p>He was labeled a coward for refusing to confront his fears. (Coward: 胆小鬼)<br>他因拒绝面对自己的恐惧而被贴上懦夫的标签。</p></li><li><p>The magazine publishes quarterly, with a new issue every three months. (Quarterly: 季度)<br>该杂志每季度出版一次，每三个月发布一期。</p></li><li><p>The police made an arrest in connection with the robbery. (Arrest: 逮捕)<br>警方因抢劫案逮捕了一人。</p></li><li><p>The temperature dropped significantly overnight. (Temperature: 温度)<br>温度在一夜之间大幅下降。</p></li><li><p>The museum welcomes thousands of visitors each year. (Visitor: 访客)<br>博物馆每年迎来成千上万的访客。</p></li><li><p>The antenna on the roof receives television signals. (Antenna: 天线)<br>屋顶上的天线接收电视信号。</p></li><li><p>The circumference of a circle is calculated using the formula 2πr. (Circumference: 圆周)<br>圆的周长用公式2πr来计算。</p></li><li><p>She is a well-known actress with a long list of successful films. (Well-known: 知名的)<br>她是一位知名的女演员，出演了一系列成功的电影。</p></li></ul><ol start="12"><li></li></ol><ul><li><p>She enjoys painting landscapes in her free time. (Painting: 绘画)<br>她喜欢在空闲时间里画风景。</p></li><li><p>The old house looked shabby and in need of repair. (Shabby: 破旧的)<br>这座老房子看起来破烂不堪，需要修理。</p></li><li><p>He plays golf every Saturday morning. (Golf: 高尔夫球)<br>他每个星期六早上打高尔夫球。</p></li><li><p>They decided to ride their bikes to the park. (Ride: 骑)<br>他们决定骑自行车去公园。</p></li><li><p>He’s an amateur photographer who enjoys taking pictures as a hobby. (Amateur: 业余的)<br>他是一位业余摄影师，喜欢摄影作为业余爱好。</p></li><li><p>She is the reigning champion of the tennis tournament. (Champion: 冠军)<br>她是这次网球比赛的卫冕冠军。</p></li><li><p>Don’t forget to feed the cat while we’re away. (Feed: 喂养)<br>我们离开时别忘了喂猫。</p></li><li><p>It’s cruel to treat animals poorly. (Cruel: 残忍的)<br>对待动物不好是残忍的。</p></li><li><p>The knight raised his shield to block the attack. (Shield: 盾牌)<br>骑士举起盾牌挡住了攻击。</p></li><li><p>They went on a cruise around the Caribbean. (Cruise: 巡航)<br>他们环加勒比海进行了一次巡航。</p></li><li><p>Kindness is a virtue that should be practiced every day. (Kindness: 仁慈)<br>仁慈是一种应该每天都要实践的美德。</p></li><li><p>The waitress brought us our drinks with a smile. (Waitress: 女服务员)<br>女服务员面带微笑地给我们端来了饮料。</p></li><li><p>The car ran out of gas on the way to the gas station. (Gas: 汽油)<br>车在去加油站的路上没油了。</p></li><li><p>Climbing Mount Everest was a huge challenge, but he did it. (Challenge: 挑战)<br>爬珠穆朗玛峰是一个巨大的挑战，但他做到了。</p></li></ul><ol start="13"><li></li></ol><ul><li><p>Each of the students received a prize for their hard work. (Each: 每个)<br>每个学生因为他们的努力工作都获得了奖品。</p></li><li><p>The mortal remains of the king were laid to rest in a grand ceremony. (Mortal: 凡人的)<br>国王的遗体在一场盛大的仪式上被安葬。</p></li><li><p>The country declared itself a republic after years of monarchy. (Republic: 共和国)<br>经过多年的君主制统治，这个国家宣布成为共和国。</p></li><li><p>The stadium was filled with cheering fans during the final match. (Stadium: 体育场)<br>决赛比赛期间，体育场挤满了欢呼的球迷。</p></li><li><p>The warranty on the product will expire next month. (Expire: 到期)<br>该产品的保修期将在下个月到期。</p></li><li><p>There was a murmur of excitement in the crowd as the performer took the stage. (Murmur: 低语)<br>当表演者登台时，人群中传来一阵兴奋的低语声。</p></li><li><p>I will follow you wherever you go. (Wherever: 无论哪里)<br>无论你去哪里，我都会跟着你。</p></li><li><p>The fallen tree obstructed the path, so we had to find another way. (Obstruct: 阻碍)<br>倒下的树木挡住了路，所以我们不得不另找道路。</p></li><li><p>The farmer loaded the wagon with hay to feed the animals. (Wagon: 四轮马车)<br>农民把马车装满了干草喂动物。</p></li><li><p>The descent down the mountain was more challenging than the climb up. (Descent: 下降)<br>下山比上山更具挑战性。</p></li><li><p>He accidentally touched the hot stove and got a burn on his hand. (Burn: 烧伤)<br>他不小心碰到了热炉子，手上烧伤了。</p></li><li><p>Gold has been a valuable commodity for centuries. (Commodity: 商品)<br>金子几个世纪以来一直是一种有价值的商品。</p></li><li><p>She can smell the delicious aroma with her nose. (Nose: 鼻子)<br>她用鼻子闻到了美味的香气。</p></li><li><p>The old bridge was able to withstand the weight of the heavy truck. (Withstand: 承受)<br>老桥能够承受重型卡车的重量。</p></li><li><p>He grabbed a handful of nuts and started snacking. (Handful: 一把)<br>他抓了一把坚果开始吃零食。</p></li></ul><ol start="14"><li></li></ol><ul><li><p>They met and fell in love, beginning a beautiful romance. (Romance: 浪漫)<br>他们相遇并坠入爱河，开始了一段美丽的浪漫。</p></li><li><p>She slammed the door in anger. (Slam: 砰然关闭)<br>她生气地砰地关上了门。</p></li><li><p>The government imposed a new tariff on imported goods. (Tariff: 关税)<br>政府对进口商品征收了新的关税。</p></li><li><p>The event happened long ago, but its effects are still felt today. (Ago: 以前)<br>这件事情发生在很久以前，但其影响仍然在今天被感受到。</p></li><li><p>The article exposed the corruption within the company. (Expose: 揭露)<br>这篇文章揭露了公司内部的腐败现象。</p></li><li><p>The coach helped the team develop their skills and strategy. (Coach: 教练)<br>教练帮助团队发展他们的技能和战略。</p></li><li><p>Optical illusions can trick the brain into seeing things that aren’t there. (Optical: 光学的)<br>光学幻觉可以欺骗大脑，使其看到不存在的事物。</p></li><li><p>She tried to convince him to change his mind, but he wouldn’t budge. (Convince: 说服)<br>她试图说服他改变主意，但他不肯让步。</p></li><li><p>The transaction was completed smoothly and both parties were satisfied. (Transaction: 交易)<br>交易顺利完成，双方都很满意。</p></li><li><p>The Christmas tree was decorated with beautiful ornaments. (Ornament: 装饰物)<br>圣诞树上装饰着漂亮的装饰品。</p></li><li><p>English is a widely spoken language around the world. (Language: 语言)<br>英语是世界上广泛使用的语言之一。</p></li><li><p>She couldn’t think of the right phrase to describe her feelings. (Phrase: 短语)<br>她想不出合适的短语来描述自己的感受。</p></li><li><p>The organ in the body performs a vital function. (Organ: 器官)<br>身体中的器官执行着重要的功能。</p></li><li><p>The elder of the two sisters was always looking out for her younger sibling. (Elder: 年长的)<br>这两个姐妹中年长的总是照顾着她更年幼的妹妹。</p></li><li><p>He pulled the rope and the curtain opened to reveal the stage. (Pull: 拉)<br>他拉着绳子，帷幕拉开，露出了舞台。</p></li></ul><ol start="15"><li></li></ol><ul><li><p>He tied the rope tightly around the package. (Rope: 绳子)<br>他把绳子牢牢地绑在包裹上。</p></li><li><p>The party was a lot of fun. (Fun: 有趣)<br>派对很有趣。</p></li><li><p>Let’s meet on Wednesday to discuss the project. (Wednesday: 星期三)<br>我们星期三见面讨论项目。</p></li><li><p>The book is protected by copyright law. (Copyright: 版权)<br>这本书受版权法保护。</p></li><li><p>The two teams will compete against each other in the final match. (Compete: 竞争)<br>两支队伍将在决赛中互相竞争。</p></li><li><p>His behavior was bizarre and unpredictable. (Bizarre: 奇异的)<br>他的行为古怪而不可预测。</p></li><li><p>The atmosphere in the room was tense before the exam. (Tense: 紧张的)<br>考试前房间里的气氛很紧张。</p></li><li><p>She has a mild headache. (Mild: 轻微的)<br>她头痛得很轻。</p></li><li><p>The cake is baking in the oven. (Oven: 烤箱)<br>蛋糕正在烤箱里烤。</p></li><li><p>The wire was used to connect the two devices. (Wire: 电线)<br>这根电线用于连接两台设备。</p></li><li><p>The surgeon made an incision in the patient’s abdomen. (Abdomen: 腹部)<br>外科医生在患者的腹部做了一个切口。</p></li><li><p>The wolf howled at the moon. (Wolf: 狼)<br>狼对着月亮嚎叫。</p></li><li><p>The job will require a lot of hard work. (Require: 需要)<br>这份工作需要很多努力。</p></li><li><p>She looked at him with reproach in her eyes. (Reproach: 责备)<br>她眼中带着责备地看着他。</p></li></ul><ol start="16"><li></li></ol><ul><li><p>His smile was indicative of his happiness. (Indicative: 表示的)<br>他的微笑表明他很开心。</p></li><li><p>The bush was full of colorful flowers. (Bush: 灌木丛)<br>灌木丛上开满了五彩缤纷的花朵。</p></li><li><p>The submarine began to submerge beneath the surface of the water. (Submerge: 潜水)<br>潜艇开始在水面下潜水。</p></li><li><p>The war lasted for several years. (War: 战争)<br>战争持续了几年。</p></li><li><p>The car broke down, so we had to tow it to the mechanic. (Tow: 拖)<br>汽车抛锚了，所以我们不得不把它拖到修理厂。</p></li><li><p>The detective was able to detect the hidden clues. (Detect: 发现)<br>侦探能够发现隐藏的线索。</p></li><li><p>She decided to stay home instead of going out. (Instead: 代替)<br>她决定呆在家里，而不是出去。</p></li><li><p>The juvenile delinquent was sent to a correctional facility. (Juvenile: 少年的)<br>青少年罪犯被送到了矫正设施。</p></li><li><p>The old man’s voice was feeble and weak. (Feeble: 虚弱的)<br>老人的声音虚弱而无力。</p></li><li><p>The astronaut floated in space. (Astronaut: 宇航员)<br>宇航员在太空中漂浮。</p></li><li><p>The heavy rain caused a flood in the city. (Flood: 洪水)<br>暴雨导致城市发生了洪水。</p></li><li><p>She has a great skill in painting. (Skill: 技能)<br>她在绘画方面有着很高的技能。</p></li><li><p>The doctor explained how to prevent the spread of germs. (Germ: 细菌)<br>医生解释了如何防止细菌传播。</p></li><li><p>There are millions of stars in the sky. (Million: 百万)<br>天空中有数以百万计的星星。</p></li><li><p>She has always been keen on learning new things. (Keen: 热衷的)<br>她一直热衷于学习新事物。</p></li></ul><ol start="17"><li></li></ol><ul><li><p>The TV show has become a popular serial. (Serial: 连续剧)<br>这个电视节目已经成为一部受欢迎的连续剧。</p></li><li><p>It’s not polite to constantly criticize others. (Criticize: 批评)<br>经常批评别人是不礼貌的。</p></li><li><p>The sky was a beautiful shade of blue. (Blue: 蓝色)<br>天空是一种美丽的蓝色。</p></li><li><p>Good leadership is essential for any successful organization. (Leadership: 领导)<br>良好的领导对于任何成功的组织都是至关重要的。</p></li><li><p>The sheer size of the building was impressive. (Sheer: 纯粹的)<br>建筑物的规模之大令人印象深刻。</p></li><li><p>His expectations for the project were high. (Expectation: 期望)<br>他对这个项目的期望很高。</p></li><li><p>Buying in bulk can be more economical in the long run. (Economical: 节约的)<br>长期来看，大宗购买可能更经济。</p></li><li><p>I’m just going to take a quick nap. (Nap: 小睡)<br>我只是打算小睡一会儿。</p></li><li><p>The dean of the university announced new policies. (Dean: 院长)<br>大学院长宣布了新政策。</p></li><li><p>Can you pass me a napkin, please? (Napkin: 餐巾纸)<br>请递给我一张餐巾纸，好吗？</p></li><li><p>They decided to dump the old furniture. (Dump: 倾倒)<br>他们决定倾倒旧家具。</p></li><li><p>The country adopted communism as its political system. (Communism: 共产主义)<br>这个国家采纳了共产主义作为其政治制度。</p></li><li><p>The police used force to control the situation. (Force: 力量)<br>警察使用武力控制局势。</p></li><li><p>The company was involved in a massive fraud scheme. (Fraud: 欺诈)<br>公司卷入了一场大规模的欺诈计划。</p></li></ul><ol start="18"><li></li></ol><ul><li><p>He used a wrench to tighten the bolt. (Wrench: 扳手)<br>他用扳手拧紧螺栓。</p></li><li><p>The instructions were ambiguous and hard to understand. (Ambiguous: 模糊的)<br>这些说明含糊不清，难以理解。</p></li><li><p>The cake was delicious, everyone loved it. (Delicious: 美味的)<br>这个蛋糕很美味，大家都喜欢。</p></li><li><p>She took a capsule to relieve her headache. (Capsule: 胶囊)<br>她服用了一粒胶囊缓解头痛。</p></li><li><p>The country has made progressive strides in healthcare. (Progressive: 先进的)<br>这个国家在医疗保健方面取得了不断进步。</p></li><li><p>The band recorded their new album in the studio. (Record: 录制)<br>乐队在录音室录制了他们的新专辑。</p></li><li><p>The liquid spilled all over the floor. (Liquid: 液体)<br>液体洒在了地板上。</p></li><li><p>The government condemned the violent protests. (Condemn: 谴责)<br>政府谴责了这场暴力抗议活动。</p></li><li><p>She grew up in the countryside, surrounded by nature. (Countryside: 乡村)<br>她在乡村长大，周围是大自然。</p></li><li><p>We need to coordinate our efforts to achieve success. (Coordinate: 协调)<br>我们需要协调我们的努力以取得成功。</p></li><li><p>The flowers began to bloom in the spring. (Bloom: 开花)<br>花朵在春天开始绽放。</p></li><li><p>He studied geology to learn about the Earth’s structure. (Geology: 地质学)<br>他学习地质学来了解地球的结构。</p></li><li><p>Hurry, we’re going to be late! (Hurry: 匆忙)<br>快点，我们要迟到了！</p></li><li><p>The farmer used a plough to prepare the soil for planting. (Plough: 犁)<br>农民用犁耕地准备种植。</p></li></ul><ol start="19"><li></li></ol><ul><li><p>Regular exercise can help prevent many health problems. (Prevent: 预防)<br>经常锻炼可以帮助预防许多健康问题。</p></li><li><p>The helicopter hovered above the building. (Hover: 盘旋)<br>直升机在建筑物上空盘旋。</p></li><li><p>I’ve almost finished the book. (Almost: 几乎)<br>我几乎看完这本书了。</p></li><li><p>She used an iron to press her clothes. (Iron: 熨斗)<br>她用熨斗熨衣服。</p></li><li><p>Bacterium can be both helpful and harmful. (Bacterium: 细菌)<br>细菌既可以有益也可以有害。</p></li><li><p>He told us a funny anecdote about his trip. (Anecdote: 轶事)<br>他告诉我们关于他旅行的一个有趣的轶事。</p></li><li><p>The bomb exploded with a loud bang. (Bomb: 炸弹)<br>炸弹爆炸发出巨大的声响。</p></li><li><p>Let’s meet at the cafe for lunch. (Cafe: 咖啡馆)<br>我们一起在咖啡馆吃午饭吧。</p></li><li><p>The cost of living has increased significantly. (Cost: 成本)<br>生活成本已经大幅上升。</p></li><li><p>The accountant is responsible for managing the company’s finances. (Accountant: 会计)<br>会计负责管理公司的财务。</p></li><li><p>I hope you have a great day! (Hope: 希望)<br>希望你有个愉快的一天！</p></li><li><p>The temperature dropped to zero degrees Celsius. (Zero: 零)<br>温度降到了零度。</p></li><li><p>The table is sturdy and can hold a lot of weight. (Sturdy: 结实的)<br>这张桌子很结实，能承受很重的重量。</p></li><li><p>The introduction of the new product was well-received. (Introduction: 介绍)<br>新产品的推介受到了良好的反响。</p></li><li><p>Today is a beautiful day to go for a walk. (Today: 今天)<br>今天是个出去散步的好日子。</p></li></ul><ol start="20"><li></li></ol><ul><li><p>Many people hold a strong belief in the power of positive thinking. (Belief: 信念)<br>许多人坚信积极思考的力量。</p></li><li><p>He walked in a straight line to the door. (Straight: 直的)<br>他径直走向门口。</p></li><li><p>Let’s go see a movie this weekend. (Movie: 电影)<br>这个周末我们去看电影吧。</p></li><li><p>I miss my family when I’m away from home. (Miss: 想念)<br>离家时我想念我的家人。</p></li><li><p>She has very liberal views on social issues. (Liberal: 自由开放的)<br>她在社会问题上持有非常开放的观点。</p></li><li><p>The owner of the company is retiring next year. (Owner: 所有者)<br>公司的所有者明年将退休。</p></li><li><p>In this painting, the dominant color is blue. (Dominant: 主导的)<br>在这幅画中，主导色是蓝色。</p></li><li><p>I need to go to the department store to buy some clothes. (Department: 部门)<br>我需要去百货商店买些衣服。</p></li><li><p>The queen wore a beautiful crown on her head. (Crown: 王冠)<br>女王头上戴着一顶美丽的王冠。</p></li><li><p>A corporation is considered a separate legal entity. (Entity: 实体)<br>公司被视为独立的法律实体。</p></li><li><p>She twisted her ankle and hurt her heel. (Heel: 脚后跟)<br>她扭伤了脚踝，伤到了脚后跟。</p></li><li><p>The magician created the illusion of a disappearing rabbit. (Illusion: 幻觉)<br>魔术师制造了兔子消失的幻觉。</p></li><li><p>My nephew is learning to play the piano. (Nephew: 侄子)<br>我的侄子正在学钢琴。</p></li><li><p>The doctor will inject the vaccine into your arm. (Inject: 注射)<br>医生将在你的手臂上注射疫苗。</p></li><li><p>I’m waiting for a reply to my email. (Reply: 回复)<br>我在等待对我的电子邮件的回复。</p></li><li><p>Shareholders receive a dividend from the company’s profits. (Dividend: 红利)<br>股东从公司利润中获得红利。</p></li><li><p>We need to constantly improve our processes to stay competitive. (Improve: 改善)<br>我们需要不断改善我们的流程以保持竞争力。</p></li><li><p>The twins look almost identical. (Identical: 相同的)<br>这对双胞胎看起来几乎一模一样。</p></li><li><p>Hydrogen is the lightest and most abundant element in the universe. (Hydrogen: 氢)<br>氢是宇宙中最轻、最丰富的元素。</p></li></ul><ol start="21"><li></li></ol><ul><li><p>Let’s conclude the meeting with a summary of our discussion. (Conclude: 结束)<br>让我们通过总结讨论内容来结束会议。</p></li><li><p>The thief was sent to gaol for his crimes. (Gaol: 监狱)<br>这名小偷因犯罪被送进了监狱。</p></li><li><p>Each snowflake is unique, just like a fingerprint. (Unique: 独特的)<br>每片雪花都是独一无二的，就像指纹一样。</p></li><li><p>I’ll make dinner tonight; what would you like to eat? (Make: 做)<br>今晚我来做晚饭，你想吃什么？</p></li><li><p>She always puts in a great endeavor in everything she does. (Endeavor: 努力)<br>她做的每件事都付出了极大的努力。</p></li><li><p>The earth is a sphere. (Sphere: 球体)<br>地球是一个球体。</p></li><li><p>The declaration of independence was a significant moment in history. (Declaration: 宣言)<br>独立宣言是历史上一个重要的时刻。</p></li><li><p>The pope is the leader of the Roman Catholic Church. (Pope: 教皇)<br>教皇是罗马天主教会的领袖。</p></li><li><p>These rules are only applicable to students living on campus. (Applicable: 适用的)<br>这些规定只适用于住校的学生。</p></li><li><p>I declare this meeting adjourned. (Declare: 宣布)<br>我宣布会议结束。</p></li><li><p>The possession of illegal drugs is a serious crime. (Possession: 拥有)<br>拥有非法药物是一种严重的犯罪。</p></li><li><p>Make sure you equip yourself with the necessary tools before starting the project. (Equip: 装备)<br>在开始项目之前，确保自己装备好必要的工具。</p></li></ul><ol start="22"><li></li></ol><ul><li><p>He used a hook to hang the picture on the wall. (Hook: 钩子)<br>他用钩子把图片挂在墙上。</p></li><li><p>This jacket is waterproof, so you can wear it in the rain. (Waterproof: 防水的)<br>这件夹克衫是防水的，所以你可以在雨中穿它。</p></li><li><p>The view from the top of the mountain was bleak and desolate. (Bleak: 荒凉的)<br>山顶的景色荒凉而凄凉。</p></li><li><p>The essence of the story lies in its moral message. (Essence: 精华)<br>故事的精髓在于它的道德寓意。</p></li><li><p>She tried to hide her disappointment with a smile. (Hide: 隐藏)<br>她试图用微笑掩饰自己的失望。</p></li><li><p>The audience applauded enthusiastically at the end of the performance. (Audience: 观众)<br>演出结束时，观众们热烈鼓掌。</p></li><li><p>According to legend, dragons are powerful mythical creatures. (Dragon: 龙)<br>根据传说，龙是强大的神秘生物。</p></li><li><p>He gave the ball a hard kick and it flew into the goal. (Kick: 踢)<br>他重重地踢了一脚球，它飞入了球门。</p></li><li><p>The athlete made a high jump over the bar. (Jump: 跳跃)<br>运动员跳过了栏杆。</p></li><li><p>Additional information can be found in the appendix. (Additional: 附加的)<br>附录中可以找到额外的信息。</p></li><li><p>The procession of cars stretched for miles along the highway. (Procession: 行列)<br>汽车的队列沿着公路延伸了几英里。</p></li><li><p>The colors of the painting began to fade over time. (Fade: 褪色)<br>随着时间的推移，这幅画的颜色开始褪色。</p></li><li><p>She took a long bath to relax after a busy day. (Bath: 洗澡)<br>在繁忙的一天过后，她洗了个长长的澡。</p></li><li><p>Plants produce oxygen as a byproduct of photosynthesis. (Oxygen: 氧气)<br>植物在光合作用中产生氧气作为副产品。</p></li></ul><ol start="23"><li></li></ol><ul><li><p>The match will be a showdown between Team A versus Team B. (Versus: 对抗)<br>这场比赛将是A队对阵B队的对决。</p></li><li><p>The train travels along the rail at high speed. (Rail: 铁轨)<br>火车以高速沿着铁轨行驶。</p></li><li><p>The spider built its nest in the corner near the ceiling. (Ceiling: 天花板)<br>蜘蛛在靠近天花板的角落筑巢。</p></li><li><p>The teacher gave a quiz to test the students’ understanding of the material. (Quiz: 测验)<br>老师出了一份测验，以测试学生对知识的理解。</p></li><li><p>It can be difficult to distinguish between the two species of birds. (Distinguish: 区分)<br>很难区分这两种鸟类。</p></li><li><p>The baby fell asleep in her mother’s lap. (Lap: 膝盖)<br>宝宝在妈妈的膝盖上睡着了。</p></li><li><p>There’s nothing in the fridge. (Nothing: 没有东西)<br>冰箱里什么都没有。</p></li><li><p>His comments were not relevant to the discussion. (Relevant: 相关的)<br>他的评论与讨论无关。</p></li><li><p>The company announced new employment opportunities. (Employment: 就业)<br>公司宣布了新的就业机会。</p></li><li><p>The birds built a nest in the tree. (Nest: 巢)<br>鸟儿在树上筑巢。</p></li><li><p>She tried to recall where she had left her keys. (Recall: 回忆起)<br>她试图回忆她把钥匙放在哪里了。</p></li><li><p>Remember to lock the door when you leave. (Remember: 记得)<br>离开时记得锁门。</p></li><li><p>The police enforce the law to maintain order. (Enforce: 执行)<br>警察执行法律以维护秩序。</p></li><li><p>The economic downturn led to a rise in unemployment. (Unemployment: 失业)<br>经济衰退导致失业率上升。</p></li><li><p>She had a happy childhood filled with laughter and play. (Childhood: 童年)<br>她有一个幸福的童年，充满了笑声和游戏。</p></li></ul><p>2024&#x2F;4&#x2F;26</p><p>1. </p><ul><li><p>The singer gained fame after winning the talent show. (Fame: 名声)<br>这位歌手在赢得才艺表演比赛后赢得了名声。</p></li><li><p>She fell and scraped her knee. (Knee: 膝盖)<br>她摔倒了，擦伤了膝盖。</p></li><li><p>The group was diverse, consisting of people from different backgrounds. (Diverse: 多样的)<br>这个团体多样性很大，由来自不同背景的人组成。</p></li><li><p>He always carries an umbrella in case it rains. (Umbrella: 雨伞)<br>他总是带着一把伞，以防下雨。</p></li><li><p>She was unable to utter a single word due to shock. (Utter: 说)<br>她由于震惊而无法说出一句话。</p></li><li><p>I have a cold, so I’m not feeling well. (So: 所以)<br>我感冒了，所以感觉不舒服。</p></li><li><p>The intension of his remarks was unclear. (Intension: 意图)<br>他讲话的意图不明确。</p></li><li><p>The restaurant serves delicious food. (Food: 食物)<br>这家餐馆供应美味的食物。</p></li><li><p>The company released a new software update. (Software: 软件)<br>公司发布了新的软件更新。</p></li><li><p>The project faced a setback when funding was cut. (Setback: 挫折)<br>项目在资金被削减时遇到了挫折。</p></li><li><p>The doctor checked her pulse to measure her heart rate. (Pulse: 脉搏)<br>医生检查了她的脉搏以测量她的心率。</p></li><li><p>The country has a strong naval presence. (Naval: 海军的)<br>这个国家在海军上有很强的存在感。</p></li><li><p>There was no evidence to support his claims. (Evidence: 证据)<br>没有证据支持他的说法。</p></li><li><p>She felt like a stranger in the new city. (Stranger: 陌生人)<br>她在新城市感到像个陌生人。</p></li></ul><ol start="2"><li></li></ol><ul><li><p>The aircraft landed safely at the airport. (Aircraft: 飞机)<br>飞机安全降落在机场。</p></li><li><p>They decided to postpone the meeting until next week. (Postpone: 推迟)<br>他们决定将会议推迟到下周。</p></li><li><p>He didn’t respond to my question. (Respond: 回答)<br>他没有回答我的问题。</p></li><li><p>Perhaps we should consider another approach. (Perhaps: 或许)<br>或许我们应该考虑另一种方法。</p></li><li><p>The new policy will be effective henceforth. (Henceforth: 从此以后)<br>新政策从现在起将生效。</p></li><li><p>What is your opinon on this matter? (Opinon: 意见)<br>你对这件事情有什么看法？</p></li><li><p>Let’s begin our journey. (Begin: 开始)<br>让我们开始我们的旅程。</p></li><li><p>The sun will rise in the east. (Rise: 升起)<br>太阳将从东方升起。</p></li><li><p>The lion was tame and could be petted. (Tame: 温顺的)<br>那只狮子很温顺，可以摸。</p></li><li><p>Their house is located near the park. (Their: 他们的)<br>他们的房子靠近公园。</p></li><li><p>He tried to blame others for his mistake. (Blame: 责怪)<br>他试图把自己的错误归咎于别人。</p></li><li><p>She is aware of the risks involved. (Aware: 意识到)<br>她意识到涉及的风险。</p></li><li><p>The doctor put plaster on his broken arm. (Plaster: 膏药)<br>医生在他的断臂上敷了膏药。</p></li><li><p>Learning basic skills is important. (Basic: 基础的)<br>学习基本技能是很重要的。</p></li></ul><ol start="3"><li></li></ol><ul><li><p>Her income has increased significantly over the past year. (Income: 收入)<br>她的收入在过去一年里显著增加了。</p></li><li><p>She is a successful female entrepreneur. (Female: 女性)<br>她是一位成功的女性企业家。</p></li><li><p>I believe we can achieve our goals if we work hard. (Believe: 相信)<br>我相信如果我们努力工作，我们可以实现我们的目标。</p></li><li><p>He waved goodbye as the train pulled away. (Goodbye: 再见)<br>火车启动时，他挥手告别。</p></li><li><p>The children flew a kite in the park. (Kite: 风筝)<br>孩子们在公园里放风筝。</p></li><li><p>He wore a suit to the interview. (Suit: 西装)<br>他穿着西装去面试。</p></li><li><p>She gave a nod of approval. (Nod: 点头)<br>她点了点头表示同意。</p></li><li><p>Please correct me if I’m wrong. (Correct: 纠正)<br>如果我错了，请纠正我。</p></li><li><p>He likes to listen to music in his free time. (Listen: 听)<br>他喜欢在空闲时间听音乐。</p></li><li><p>She inherited a large estate from her grandfather. (Estate: 财产)<br>她从祖父那里继承了一大笔财产。</p></li><li><p>He sprained his ankle while playing basketball. (Ankle: 踝关节)<br>他在打篮球时扭伤了脚踝。</p></li><li><p>She put a patch on the tear in her jeans. (Patch: 补丁)<br>她在牛仔裤上的破洞处贴了个补丁。</p></li><li><p>The outer layer of the Earth is called the crust. (Outer: 外部的)<br>地球的外层称为地壳。</p></li><li><p>Latin is no longer spoken as a native language. (Latin: 拉丁语)<br>拉丁语不再作为一种母语而被使用。</p></li></ul><ol start="4"><li></li></ol><ul><li><p>The association aims to promote cultural exchange. (Association: 协会)<br>这个协会旨在促进文化交流。</p></li><li><p>The plane ascended to a high altitude. (Altitude: 海拔高度)<br>飞机上升到高海拔。</p></li><li><p>The land was barren and unfit for farming. (Barren: 贫瘠的)<br>这片土地贫瘠，不适合耕种。</p></li><li><p>She slipped on the wet floor. (Slipper: 滑倒)<br>她在湿地板上滑倒了。</p></li><li><p>It is unlikely that he will arrive on time. (Unlikely: 不太可能)<br>他不太可能准时到达。</p></li><li><p>The sheriff’s deputy arrived at the scene. (Deputy: 副手)<br>警长的副手到达了现场。</p></li><li><p>Take one dose of this medicine before bed. (Dose: 剂量)<br>每晚睡前服用一剂这种药物。</p></li><li><p>The siren wailed loudly in the distance. (Siren: 警笛)<br>警报器在远处响起。</p></li><li><p>Can you provide proof of your identity? (Proof: 证据)<br>你能提供你的身份证明吗？</p></li><li><p>She kept her money in her wallet. (Wallet: 钱包)<br>她把钱放在钱包里。</p></li><li><p>The goat grazed in the field. (Goat: 山羊)<br>山羊在田里吃草。</p></li><li><p>I’ll give them a call later. (Them: 他们&#x2F;她们)<br>我稍后会给他们打电话。</p></li></ul><ol start="5"><li></li></ol><ul><li><p>She likes to decorate her house with flowers. (Decorate: 装饰)<br>她喜欢用花装饰她的房子。</p></li><li><p>He swept the floor with a broom. (Broom: 扫帚)<br>他用扫帚扫地。</p></li><li><p>The spider began to creep along the wall. (Creep: 爬行)<br>蜘蛛开始沿着墙壁爬行。</p></li><li><p>The police arrived at the scene of the crime. (Police: 警察)<br>警察到达了犯罪现场。</p></li><li><p>Be careful not to spill the milk. (Spill: 溢出)<br>小心别把牛奶洒出来。</p></li><li><p>He chopped wood with an axe. (Axe: 斧头)<br>他用斧头劈木头。</p></li><li><p>Please provide your address. (Address: 地址)<br>请提供你的地址。</p></li><li><p>The decision was unanimous. (Unanimous: 一致的)<br>决定是一致的。</p></li><li><p>The neighborhood is quiet and peaceful. (Neighborhood: 社区)<br>这个社区安静而和平。</p></li><li><p>The diversion caused a delay in our journey. (Diversion: 转移)<br>这个转移导致我们的旅程延误。</p></li><li><p>She cooked some sausages for breakfast. (Sausage: 香肠)<br>她煮了些香肠作为早餐。</p></li><li><p>The waiter brought the menu to the table. (Waiter: 服务员)<br>服务员把菜单端到桌子上。</p></li><li><p>What’s your favorite color? (Favorite: 最喜爱的)<br>你最喜欢的颜色是什么？</p></li></ul><ol start="6"><li></li></ol><ul><li><p>The outbreak of the disease was sudden and severe. (Outbreak: 爆发)<br>疾病的爆发突然而严重。</p></li><li><p>The regime implemented new policies to improve the economy. (Regime: 政权)<br>政权实施了新的政策来改善经济。</p></li><li><p>She’s a smart girl who excels in math. (Girl: 女孩)<br>她是一个数学学得很好的聪明女孩。</p></li><li><p>The birds were looking for a mate. (Mate: 伴侣)<br>鸟儿在寻找伴侣。</p></li><li><p>He put the food on the plate. (Plate: 盘子)<br>他把食物放在盘子里。</p></li><li><p>The participant won a prize for his performance. (Participant: 参与者)<br>参与者因为他的表现而获得了奖品。</p></li><li><p>There was a shortage of food during the drought. (Shortage: 短缺)<br>干旱期间食物短缺。</p></li><li><p>She has a powerful voice that can fill a room. (Powerful: 强大的)<br>她有一种强大的声音，可以填满整个房间。</p></li><li><p>His apology seemed genuine. (Genuine: 真诚的)<br>他的道歉似乎是真诚的。</p></li><li><p>The plant began to sprout after the rain. (Sprout: 发芽)<br>这棵植物在雨后开始发芽。</p></li><li><p>She had to persevere through many challenges to reach her goal. (Persevere: 坚持)<br>她不得不经历许多挑战才能达到她的目标。</p></li><li><p>He used an amplifier to make his voice louder. (Amplifier: 放大器)<br>他用放大器把声音放大。</p></li><li><p>He pounded the meat to make it tender. (Pound: 捣碎)<br>他捣碎肉使其变嫩。</p></li><li><p>They agreed to refund the money. (Refund: 退款)<br>他们同意退还钱款。</p></li></ul><ol start="7"><li></li></ol><ul><li><p>The sweater shrank in the wash. (Shrink: 收缩)<br>毛衣在洗衣时缩水了。</p></li><li><p>The vase is very fragile, so be careful with it. (Fragile: 易碎的)<br>这个花瓶非常脆弱，所以要小心。</p></li><li><p>He’s always punctual and arrives on time. (Punctual: 准时的)<br>他总是很准时，按时到达。</p></li><li><p>The encyclopedia contains a wealth of information. (Encyclopedia: 百科全书)<br>这本百科全书包含了丰富的信息。</p></li><li><p>They watched a live performance of the play. (Live: 现场的)<br>他们观看了这部戏剧的现场表演。</p></li><li><p>Be careful with the fragile items. (Careful: 小心的)<br>对易碎物品要小心。</p></li><li><p>The issue is too trivial to be of any real importance. (Trivial: 琐碎的)<br>这个问题太琐碎，没有任何真正的重要性。</p></li><li><p>She painted the room a bright color. (Color: 颜色)<br>她把房间涂成了明亮的颜色。</p></li><li><p>The building’s design includes a pillar in the center. (Pillar: 柱子)<br>这座建筑的设计包括中间的一根柱子。</p></li><li><p>He sawed the wood to make it fit. (Saw: 锯子；锯)<br>他用锯子锯木头使之合适。</p></li><li><p>The offer was tempting, but she decided not to accept it. (Tempt: 引诱)<br>这个报价很诱人，但她决定不接受。</p></li><li><p>He demonstrated how to use the new software. (Demonstrate: 示范)<br>他演示了如何使用新软件。</p></li><li><p>They enjoyed a glass of wine with their meal. (Wine: 葡萄酒)<br>他们在用餐时喝了一杯葡萄酒。</p></li><li><p>She walked up the stairs to the second floor. (Stair: 楼梯)<br>她走上楼梯到二楼。</p></li></ul><ol start="8"><li></li></ol><ul><li><p>She teaches drama at the secondary school. (Secondary: 中等的)<br>她在中学教戏剧课程。</p></li><li><p>He plays the guitar in a band. (Guitar: 吉他)<br>他在一个乐队里弹吉他。</p></li><li><p>They were cordial hosts and made us feel welcome. (Cordial: 热诚的)<br>他们是热情的主人，让我们感到受欢迎。</p></li><li><p>Please pass me the soap. (Soap: 肥皂)<br>请递给我肥皂。</p></li><li><p>This is the best cake I’ve ever tasted. (Best: 最好的)<br>这是我尝过的最好吃的蛋糕。</p></li><li><p>The museum features a collection of modern art. (Modern: 现代的)<br>博物馆展示了一系列现代艺术品。</p></li><li><p>The fireworks display was spectacular. (Spectacular: 壮观的)<br>烟火表演非常壮观。</p></li><li><p>The actors wore elaborate costume for the play. (Costume: 服装)<br>演员们在剧中穿着精致的服装。</p></li><li><p>The store had its grand opening last week. (Opening: 开张)<br>这家商店上周开张。</p></li><li><p>She didn’t hesitate to speak her mind. (Hesitate: 犹豫)<br>她毫不犹豫地说出了自己的想法。</p></li><li><p>His expression of satisfaction told us he was pleased. (Satisfaction: 满意)<br>他满意的表情告诉我们他很高兴。</p></li><li><p>She asked the waiter to serve the meal. (Serve: 服务)<br>她要求服务员端上餐点。</p></li></ul><ol start="9"><li></li></ol><ul><li><p>The leaves fall from the trees in autumn. (Fall: 掉落)<br>秋天树叶从树上落下来。</p></li><li><p>Have you ever been to Paris? (Ever: 曾经)<br>你曾经去过巴黎吗？</p></li><li><p>I need to get some groceries from the store. (Get: 获得)<br>我需要从商店买些杂货。</p></li><li><p>Please keep quiet during the movie. (Quiet: 安静)<br>请在电影放映期间保持安静。</p></li><li><p>The brisk wind made her shiver. (Brisk: 轻快的)<br>清风使她打了个寒颤。</p></li><li><p>She reached out to touch the painting. (Touch: 触摸)<br>她伸手去触摸画作。</p></li><li><p>He filled up the car with gasoline. (Gasoline: 汽油)<br>他给车加满了汽油。</p></li><li><p>The mountain was covered in mist. (Mist: 薄雾)<br>山上笼罩着薄雾。</p></li><li><p>The team showed great unity in their efforts. (Unity: 团结)<br>团队在努力中表现出极大的团结。</p></li><li><p>They talked about events from the past. (Past: 过去)<br>他们谈论过去发生的事情。</p></li><li><p>He loaded the cart with boxes. (Cart: 手推车)<br>他把箱子装满了手推车。</p></li><li><p>One drawback of the plan was its high cost. (Drawback: 缺点)<br>这个计划的一个缺点是成本很高。</p></li></ul><ol start="10"><li></li></ol><ul><li><p>It’s important to classify the documents correctly. (Classify: 分类)<br>很重要将文件正确分类。</p></li><li><p>His behavior at the party was unacceptable. (Behavior: 行为)<br>他在派对上的行为是不可接受的。</p></li><li><p>The fog was so thick that you couldn’t see anything. (Thick: 浓厚的)<br>雾很大，你什么都看不见。</p></li><li><p>She wrote him a cheque for the rent. (Cheque: 支票)<br>她给他写了一张付房租的支票。</p></li><li><p>The manager supervises the team’s work. (Supervise: 监督)<br>经理监督团队的工作。</p></li><li><p>This is a significant moment in history. (Significant: 重要的)<br>这是历史上一个重要的时刻。</p></li><li><p>Mercury is a toxic metal. (Mercury: 汞)<br>汞是一种有毒金属。</p></li><li><p>Are you sure about your decision? (Sure: 确定的)<br>你对你的决定有把握吗？</p></li><li><p>The conference had an international audience. (International: 国际的)<br>会议的观众来自国际。</p></li><li><p>They’re planning an extension to the building. (Extension: 扩建)<br>他们计划对建筑进行扩建。</p></li><li><p>He affirmed his commitment to the project. (Affirm: 肯定)<br>他肯定了他对这个项目的承诺。</p></li><li><p>The architect showed them the blueprint for the new house. (Blueprint: 蓝图)<br>建筑师向他们展示了新房子的蓝图。</p></li><li><p>The rain was intermittent, with short periods of heavy rain. (Intermittent: 间歇的)<br>雨势间歇，时而大时而小。</p></li><li><p>Responsibility comes with leadership. (Responsibility: 责任)<br>领导责任伴随而来。</p></li></ul><p>11. </p><ul><li><p>The coach used a whip to urge the horses to run faster. (Whip: 鞭子)<br>教练用鞭子督促马儿跑得更快。</p></li><li><p>His remarks carried an implicit criticism of the government. (Implicit: 含蓄的)<br>他的言论暗含对政府的批评。</p></li><li><p>The government decided to isolate the infected area. (Isolate: 隔离)<br>政府决定将受感染区域隔离。</p></li><li><p>The Earth rotates on its axis. (Rotate: 旋转)<br>地球围绕着自己的轴旋转。</p></li><li><p>Freedom is a fundamental human right. (Liberty: 自由)<br>自由是一项基本人权。</p></li><li><p>As she read, the story began to unfold. (Unfold: 展开)<br>随着她的阅读，故事开始展开。</p></li><li><p>The branch snapped under the weight of the snow. (Snap: 断裂)<br>分支在雪的重压下折断了。</p></li><li><p>The quality of the product is inferior to what was promised. (Inferior: 劣质的)<br>产品的质量低于承诺的水平。</p></li><li><p>Nowadays, technology is advancing at a rapid pace. (Nowadays: 如今)<br>如今，科技正以迅猛的速度发展。</p></li><li><p>Would you like some tea? (Tea: 茶)<br>你想要喝点茶吗？</p></li><li><p>He lived a solitary life in the mountains. (Solitary: 孤独的)<br>他在山里过着孤独的生活。</p></li><li><p>I heard the tick of the clock in the quiet room. (Tick: 滴答声)<br>我听到了安静房间里时钟的滴答声。</p></li><li><p>It’s important to break the habit of smoking. (Habit: 习惯)<br>戒掉吸烟的习惯很重要。</p></li><li><p>She decided to donate some money to charity. (Donate: 捐赠)<br>她决定向慈善机构捐款。</p></li><li><p>The general’s aim was to humiliate his opponent. (Humiliate: 羞辱)<br>将军的目的是羞辱对手。</p></li><li><p>The floor was covered in dirt and dust. (Dirt: 污垢)<br>地板上布满了污垢和灰尘。</p></li><li><p>They came to visit us last summer. (Visit: 访问)<br>他们去年夏天来访问我们。</p></li><li><p>He shoved the door open and entered the room. (Shove: 推)<br>他用力推开门，走进了房间。</p></li><li><p>She cradled the baby in her arms. (Baby: 婴儿)<br>她抱着婴儿。</p></li><li><p>The doctor diagnosed him with a disorder of the nervous system. (Disorder: 失调)<br>医生诊断出他患有神经系统失调。</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>英语</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>github</title>
    <link href="/2024/05/13/tiankeng8/"/>
    <url>/2024/05/13/tiankeng8/</url>
    
    <content type="html"><![CDATA[<p><a href="https://ndpsoftware.com/git-cheatsheet.html#loc=index;">github工作流程代码解释</a></p><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>GitHub是一个代码托管平台。一个共享和开源软件的流行平台</p><ol><li>软件开源，即编写软件的代码对所有人公开，所有人可以在现有代码的基础上进行二次开发，减少不必要的重复劳动（ IT 行话简称不要重复「造轮子」）</li><li>方便团队协作。这个过程有点像是我们把文档放在石墨或语雀这类支持团队协作的平台上，而 GitHub 上存放的是代码，参与编写软件的人可以通过 Git（版本控制工具）从 GitHub 拉取或往 GitHub 上传代码</li></ol><p>类似的代码托管平台还有gitee，开源中国。</p><p>仓库（Repository） 仓库 是 GitHub 最基本的元素。 它们很容易被想象为项目的文件夹。 仓库包含所有项目文件（包括文档），并存储每个文件的修改历史记录。 仓库可以有多个协作者，仓库可以是公开的，也可以设置为私有的。更详细的请看<a href="https://docs.github.com/en/repositories/creating-and-managing-repositories/about-repositories">参考文档</a><br>分支（Branch） 分支是仓库的并行版本。默认情况下，您的仓库具有一个名为 main 的主分支。我们可以复制主分支创建其他分支，您安全地进行任何更改而不会影响”线上“主分支。 完成所需更改后，可以将分支合并回主分支以发布你的更改。<br>README：GitHub 个人主页资料上 “关于我” 的介绍。 内容一般包含：介绍您的工作和兴趣，您引以为豪的贡献以及这些贡献的背景信息，在您参与的社区获得帮助的指南</p><h2 id="配套软件"><a href="#配套软件" class="headerlink" title="配套软件"></a>配套软件</h2><p><a href="https://git-scm.com/download/win">git for windown </a></p><h2 id="使用命令"><a href="#使用命令" class="headerlink" title="使用命令"></a>使用命令</h2><p>初始命令，建立.git文件</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs csharp">git <span class="hljs-keyword">init</span> <br></code></pre></td></tr></table></figure><p>选取上传文件，如果需要选取所有则使用<code>git add .</code></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">git <span class="hljs-built_in">add</span> README.md<br>git <span class="hljs-built_in">add</span> .<br></code></pre></td></tr></table></figure><p>书写提交信息</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">git</span> commit -m <span class="hljs-string">&quot;first commit              &quot;</span><br></code></pre></td></tr></table></figure><p>选取分支</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">git branch -M <span class="hljs-selector-tag">main</span><br></code></pre></td></tr></table></figure><p>添加目标位置，</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile">git remote <span class="hljs-keyword">add</span><span class="language-bash"> origin git@github.com:changjingzhi/test.git</span><br></code></pre></td></tr></table></figure><p>(origin 表示名字)<br>git push 提交文件到远端仓库</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">git <span class="hljs-built_in">push</span> -u <span class="hljs-built_in">origin</span> main<br></code></pre></td></tr></table></figure><p>git clone 克隆远端仓库。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">git <span class="hljs-keyword">clone</span> <span class="hljs-title">远端仓库地址</span><br></code></pre></td></tr></table></figure><h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><p>使用pip freeze命令生成requirements.txt文件：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">pip <span class="hljs-keyword">freeze</span> &gt; requirements.txt<br><br></code></pre></td></tr></table></figure><p>model环境</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">absl</span>-py==<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">addict</span>==<span class="hljs-number">2</span>.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">asttokens</span>==<span class="hljs-number">2</span>.<span class="hljs-number">4</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">backcall</span>==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">cachetools</span>==<span class="hljs-number">5</span>.<span class="hljs-number">3</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">certifi</span>==<span class="hljs-number">2023</span>.<span class="hljs-number">7</span>.<span class="hljs-number">22</span><br><span class="hljs-attribute">chardet</span>==<span class="hljs-number">5</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">charset</span>-normalizer==<span class="hljs-number">3</span>.<span class="hljs-number">3</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">click</span>==<span class="hljs-number">8</span>.<span class="hljs-number">1</span>.<span class="hljs-number">7</span><br><span class="hljs-attribute">colorama</span>==<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.<span class="hljs-number">6</span><br><span class="hljs-attribute">comm</span>==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">contourpy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">cycler</span>==<span class="hljs-number">0</span>.<span class="hljs-number">12</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">debugpy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">8</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">decorator</span>==<span class="hljs-number">5</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">defusedxml</span>==<span class="hljs-number">0</span>.<span class="hljs-number">7</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">executing</span>==<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">filelock</span>==<span class="hljs-number">3</span>.<span class="hljs-number">13</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">fonttools</span>==<span class="hljs-number">4</span>.<span class="hljs-number">45</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">fsspec</span>==<span class="hljs-number">2023</span>.<span class="hljs-number">10</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">google</span>-auth==<span class="hljs-number">2</span>.<span class="hljs-number">23</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">google</span>-auth-oauthlib==<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">graphviz</span>==<span class="hljs-number">0</span>.<span class="hljs-number">20</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">grpcio</span>==<span class="hljs-number">1</span>.<span class="hljs-number">59</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">huggingface</span>-hub==<span class="hljs-number">0</span>.<span class="hljs-number">19</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">idna</span>==<span class="hljs-number">3</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">imageio</span>==<span class="hljs-number">2</span>.<span class="hljs-number">33</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">importlib</span>-metadata==<span class="hljs-number">6</span>.<span class="hljs-number">8</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">importlib</span>-resources==<span class="hljs-number">6</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">ipykernel</span>==<span class="hljs-number">6</span>.<span class="hljs-number">26</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">ipython</span>==<span class="hljs-number">8</span>.<span class="hljs-number">12</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">jedi</span>==<span class="hljs-number">0</span>.<span class="hljs-number">19</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">Jinja2</span>==<span class="hljs-number">3</span>.<span class="hljs-number">1</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">joblib</span>==<span class="hljs-number">1</span>.<span class="hljs-number">3</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">jupyter_client</span>==<span class="hljs-number">8</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">jupyter_core</span>==<span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">kiwisolver</span>==<span class="hljs-number">1</span>.<span class="hljs-number">4</span>.<span class="hljs-number">5</span><br><span class="hljs-attribute">lazy_loader</span>==<span class="hljs-number">0</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">lxml</span>==<span class="hljs-number">4</span>.<span class="hljs-number">9</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">Markdown</span>==<span class="hljs-number">3</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">MarkupSafe</span>==<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">matplotlib</span>==<span class="hljs-number">3</span>.<span class="hljs-number">7</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">matplotlib</span>-inline==<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.<span class="hljs-number">6</span><br><span class="hljs-attribute">mne</span>==<span class="hljs-number">1</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">nest</span>-asyncio==<span class="hljs-number">1</span>.<span class="hljs-number">5</span>.<span class="hljs-number">8</span><br><span class="hljs-attribute">netron</span>==<span class="hljs-number">7</span>.<span class="hljs-number">2</span>.<span class="hljs-number">9</span><br><span class="hljs-attribute">networkx</span>==<span class="hljs-number">3</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">nltk</span>==<span class="hljs-number">3</span>.<span class="hljs-number">8</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">numpy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">24</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">oauthlib</span>==<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">opencv</span>-python==<span class="hljs-number">4.2.0.32</span><br><span class="hljs-attribute">packaging</span>==<span class="hljs-number">23</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">pandas</span>==<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">parso</span>==<span class="hljs-number">0</span>.<span class="hljs-number">8</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">pickleshare</span>==<span class="hljs-number">0</span>.<span class="hljs-number">7</span>.<span class="hljs-number">5</span><br><span class="hljs-attribute">Pillow</span>==<span class="hljs-number">10</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">platformdirs</span>==<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">pooch</span>==<span class="hljs-number">1</span>.<span class="hljs-number">8</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">prompt</span>-toolkit==<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">41</span><br><span class="hljs-attribute">protobuf</span>==<span class="hljs-number">4</span>.<span class="hljs-number">25</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">psutil</span>==<span class="hljs-number">5</span>.<span class="hljs-number">9</span>.<span class="hljs-number">6</span><br><span class="hljs-attribute">pure</span>-eval==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">pyasn1</span>==<span class="hljs-number">0</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">pyasn1</span>-modules==<span class="hljs-number">0</span>.<span class="hljs-number">3</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">pycocotools</span>==<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">7</span><br><span class="hljs-attribute">pygame</span>==<span class="hljs-number">2</span>.<span class="hljs-number">5</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">Pygments</span>==<span class="hljs-number">2</span>.<span class="hljs-number">16</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">pyparsing</span>==<span class="hljs-number">3</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">python</span>-dateutil==<span class="hljs-number">2</span>.<span class="hljs-number">8</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">pytz</span>==<span class="hljs-number">2024</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">PyWavelets</span>==<span class="hljs-number">1</span>.<span class="hljs-number">4</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">pywin32</span>==<span class="hljs-number">306</span><br><span class="hljs-attribute">PyYAML</span>==<span class="hljs-number">6</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">pyzmq</span>==<span class="hljs-number">25</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">regex</span>==<span class="hljs-number">2023</span>.<span class="hljs-number">10</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">requests</span>==<span class="hljs-number">2</span>.<span class="hljs-number">31</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">requests</span>-oauthlib==<span class="hljs-number">1</span>.<span class="hljs-number">3</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">rsa</span>==<span class="hljs-number">4</span>.<span class="hljs-number">9</span><br><span class="hljs-attribute">safetensors</span>==<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">scikit</span>-image==<span class="hljs-number">0</span>.<span class="hljs-number">21</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">scikit</span>-learn==<span class="hljs-number">1</span>.<span class="hljs-number">3</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">scipy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">10</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">seaborn</span>==<span class="hljs-number">0</span>.<span class="hljs-number">13</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">six</span>==<span class="hljs-number">1</span>.<span class="hljs-number">16</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">stack</span>-data==<span class="hljs-number">0</span>.<span class="hljs-number">6</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">summary</span>==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tensorboard</span>==<span class="hljs-number">2</span>.<span class="hljs-number">14</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tensorboard</span>-data-server==<span class="hljs-number">0</span>.<span class="hljs-number">7</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">thop</span>==<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.post2209072238<br><span class="hljs-attribute">threadpoolctl</span>==<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tifffile</span>==<span class="hljs-number">2023</span>.<span class="hljs-number">7</span>.<span class="hljs-number">10</span><br><span class="hljs-attribute">timm</span>==<span class="hljs-number">0</span>.<span class="hljs-number">6</span>.<span class="hljs-number">13</span><br><span class="hljs-attribute">torch</span>==<span class="hljs-number">1</span>.<span class="hljs-number">12</span>.<span class="hljs-number">1</span>+cu116<br><span class="hljs-attribute">torch</span>-tb-profiler==<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">torchaudio</span>==<span class="hljs-number">0</span>.<span class="hljs-number">12</span>.<span class="hljs-number">1</span>+cu116<br><span class="hljs-attribute">torchsummary</span>==<span class="hljs-number">1</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">torchvision</span>==<span class="hljs-number">0</span>.<span class="hljs-number">13</span>.<span class="hljs-number">1</span>+cu116<br><span class="hljs-attribute">torchviz</span>==<span class="hljs-number">0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">tornado</span>==<span class="hljs-number">6</span>.<span class="hljs-number">3</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">tqdm</span>==<span class="hljs-number">4</span>.<span class="hljs-number">66</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">traitlets</span>==<span class="hljs-number">5</span>.<span class="hljs-number">13</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">typing_extensions</span>==<span class="hljs-number">4</span>.<span class="hljs-number">8</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tzdata</span>==<span class="hljs-number">2024</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">urllib3</span>==<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">wcwidth</span>==<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">10</span><br><span class="hljs-attribute">Werkzeug</span>==<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">zipp</span>==<span class="hljs-number">3</span>.<span class="hljs-number">17</span>.<span class="hljs-number">0</span><br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文思路——论文结构</title>
    <link href="/2024/05/11/paper-idear5/"/>
    <url>/2024/05/11/paper-idear5/</url>
    
    <content type="html"><![CDATA[<p>鉴于论文大修，所以我打算重新构建一下思路。</p><ol><li><p>标题</p></li><li><p>摘要 （介绍基础背景，解决什么问题，做了什么工作。）</p></li><li><p>关键词</p></li><li><p>介绍</p></li><li><p>数据和方法</p></li><li><p>结果</p></li><li><p>讨论</p></li><li><p>参考文献</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔16</title>
    <link href="/2024/05/10/ganwu16/"/>
    <url>/2024/05/10/ganwu16/</url>
    
    <content type="html"><![CDATA[<p>死亡赋予我生活的希望，忧虑是没有用的，发掘自己内在的价值，应该有梦想有内在的动机，想好今天我应该收获什么，收获进步，一点点，坚持自己想法的人和随波逐流的人，生命将有不同。所谓成长，就是实现独立生存、完成独立思考能力的自我奋斗。所谓成熟，就是对内消除傲慢，对外消除偏见的自我修行。你问我有哪些进步，我开始成为自己的朋友开始时。我才20岁，我可以成为任何我想成为的人，生命是用来感受而不是用来忧虑的，我能在浪费时间中获得乐趣，就不是浪费时间。生命并没有什么意义，但是活着的话就可能遇见有意思的事，就像你遇见那朵花，就像我遇见了你。改变总是慢慢发生的，人们总说时间可以改变很多事情，但事实上必须由你自己做出哪些改变。</p><p>对待不同的人，不同的事，不必要过多的去带入情感，人之所以言之凿凿，是因为知道的太少，所有的生活都是合理的，我们没必要相互理解。在世间 本就是各人下雪 各人有各人的隐晦和皎洁。最难沟通的，不是没有文化的人，而是那些满脑子都被灌输了标准答案的人。每个人都不是一座孤岛，一个人必须事这世界最坚固的岛屿，然后才能成为大陆的一部分。  ​​</p><p>迷茫的原因在于读书太少而想的太多。但读那么多书干嘛，读得多想得多，烦恼就多。这不能说没道理，但我总觉得哪不对劲，直到后来读到法国诗人勒内.夏尔一席话方才释然，他说:“理解得越多就越痛苦。知道得越多就越撕裂。但他有着同痛苦相对称的清澈，与绝望相均衡的坚韧。</p><h2 id="内心的秩序"><a href="#内心的秩序" class="headerlink" title="内心的秩序"></a>内心的秩序</h2><p>内心也是需要秩序的，儒家讲内圣外王，要想外王，首先得内圣，怎么内圣是个问题。世界纷纷扰扰，认识论指出对于世界的认知是从没有到感性的认知的，然后经过不断的实践，不断的反复论证深化成理性的认知。问题在于在认识的过程中，由无到有，由感性到理性的过程中，在实践的过程中，内心会经历巨大的变化，这个变化是客观的，必然的，不是特例的。</p><p>那么为什么在实践的过程中，不同的人会产生不同的认知，产生不同的结果？第一，对于不同的人来讲，每个人所处的环境，条件是不同的，这是客观的事实（实事求是）。第二，由于不同的人有不同的条件，对于环境的认知是不同的，举个例子，何不食肉糜，为什么不吃肉勒，这就是不同的人有不同的条件，条件的不同产生了不同的认识，这样就产生的各种各样的人。人类物种的多样性就是这样的，这是规律，对于不同的人，对于不同的立场，我的态度是包容，但是这个观点践行起来却是有点困难的。人是立场的，好恶的，我们总是只相信自己愿意相信的东西。我们习惯于先形成观点，然后再寻找既有立场的正面证据，在不知不觉中偏离真实对于与我不同的观点，我的第一想法是对立，辩驳对方的观点，来维护自己的观点，这是正常的，如果我有一点能力，我就能与对方辩驳一下。在与人辩驳的过程中，会有很多州情况，其中一种是友好交流，互相进步，一种是懒得和对方争辩，处于自己的世界中（固步自封），一种是锋芒相对，互相博弈，带动情绪，劳损身体（怒发冲冠）。大部分情况中我是第二种情况固步自封，争辩的勇气都是没有的，稍微好一点的是怒发冲冠，勇气可嘉，捍卫立场，这里需要指出的是立场是没有对错的，只不过是矛盾而已，矛盾就是此消彼长，至于对错（小孩子才在乎）。</p><p>怎么来提升自己的认识？实践，简单来说就是做事，不断反思，但是在实践的过程中，内心是矛盾的，一种情况是觉得这件事情是困难的，我能不能做好，失败了怎么办。一种情况是觉得这件事情做了我也没有什么收获，不如不做。一种情况是在做的过程中不断的内耗。对于第一种情况，需要说明的是，在以前的实践过程中，我形成许多的观点是错误的，比如认为做事只做大事，做人只做大人物。我想这种观点的形成与小时候看的电影，电视有关吧，背景都是什么宫廷争斗剧场，三国演义（刘关张），楚汉争霸（项羽与刘邦），当英雄，做刘备，项羽之类的人物，这样的想法形成后就没有了下文，其实有了想法后应该想怎么成为这样的人，但是大部分想法实际上是胎死腹中了。现在觉得，做事就做事，管他难不难，先做了再说，失败了有经验，成功了最好。在做事的过程中应该有一种心态是，天下难事必作于易，天下大事必作于细。对于第一种心态，能不能做好，其实很多时候接触没有接触过的事物，能做好的概率微乎其微，没有认识，没有经历怎么能够做好，这种情况应该不要畏难，从我不会变成我可以学。</p><p>貌似有点跑题了，标题是内心的秩序，为啥取这个标题，因为互联网的一大好处是看到许多不同的人发生的不同的事，许多不同的价值观，世界观，人生观在交融，在对立。这样是很好的，但是对立不免就有纷争，有暴力（这里的暴力是指语言暴力）。不同的视频反应的是现象，背后更反映着互联网的多元化的价值观。举个例子，小红书，微博，抖音，b站，推特，知乎，默默，快手，百度贴吧，透过这些窗口我能看到世界是分层的，热爱生活的，消极生活的，有钱的，没钱的，美丽的，丑陋的。不同的人展现出不同的现象，而我不经自问，我以什么样的态度来面对这样多元的世界，遇到好的，坏的（注：没有好坏之分，只有矛盾对立）。我想是需要建立内心的秩序去面对这多元的世界了。<br>怎么建立内心的秩序？ 熵增，熵是衡量一个系统的混乱程度，心流这本书中提出了精神熵，用来衡量内心的秩序。我想我的精神熵不是很高吧，在随笔2中我指出，心法胜于法则，法则胜于技术（方法）。法则中基于法则可以诞生出方法（技术）来应对不同的事情，技术则可以统领知识。问题便被缩小为我的心法是什么，怎么使用心法来诞生一个法则来应对多元化的世界。（注意这里是专门建立一个法则来应对多元化的世界不是通用的），我的心法统御着我认知到的法则和方法。（法则有内心的法则和世界的法则，内心的法则和世界的法则相关，但是不对等，比如说做事要慢慢来，一步一步来是做事的法则，但是自己内心的法则不是这样的，和世界的法则不是对等的，因此要改变自己内心的法则），那么选取什么来作为我的心法（有点像武林秘籍了）是一个问题，目前我探寻到一个心法，在随笔14中我写道，死亡指引我方向，赋予我战胜一切的勇气，未知死，焉知生。从死亡中我领悟到自强，天行建，君子以自强不息。地势坤，君子以厚德载物。遥远的救世主，不要去追寻救世主了，自己就是自己的救世主，自强，君子处事，应像天一样，自我力求进步，刚毅坚卓，发奋图强，永不停息；大地的气势厚实和顺，君子应增厚美德，容载万物。选自强做心法的好处是有目标，目标就是变强，进步，获取解决问题的能力，这是很好的。而其他的一切问题都可以根据自强来统御法则，法则来统御方法。而我自己就是方法的践行者，一个人人的组成是多元的，物质的，精神的。<br>需要指出的是可以通过方法看到法则，可以通过法则看到心法。只要不断实践不断反思就能行。<br>还有就是可以参考别人的法则和心法，但不一定使用，因为见路不走，实事求是（这个感觉比心法低一层，比能基本法则高一层）</p><p>参考法则和方法</p><ol><li>天下难事必作于易，天下大事必作于细。 诱导方法， 微行为，微习惯。 </li><li>做事时清楚的明白自己在干嘛，诱导方法，坚持每天写手帐记录的目的就在于使得我们在任何一个给定时间，能够做出最好的选择：现在要采取哪个行动。这样我们做任何事情的时候都明白这个是现在最应该做的事，而不必担心有什么重要的事情没做。我们的心态就永远平静。诱导方法2，坚持写作，你会越来越注重内在，当某一天，你回头看着每一篇文章，全部都是收获和回忆，内心充实而感动。文章写的也许不够好，但都是你亲手写的。一篇篇文章见证了你的成长和进步，也成为了你生活的一部分。这种充实，是无法用言语来描述的。写作，可以深入内心，直接和自己对话。生活本身，其实不是最好的生活方式，我们可以用写作，来质疑和反抗。 诱导法则3， 随时记录下脑海中的灵感， 只要能够令人产生情绪波动的，都是值得捕捉的故事”，“三分眼睛，七分内心”，平时读书，阅读文章，总有一些感动得故事和金句，心灵触动，都要记录下来，慢慢刻意去练习，自己慢慢也能写出想写的内容了。</li><li>法则和方法是可以参考和获取的，诱导方法，坚持阅读，会越来越通透。通过阅读能将自己的内心简化，将自己对待世界的方法简化。不要以为学生生涯结束了，就可以不用再读书（傻孩子，好日子才刚刚开始勒！！！）阅读好书，就是和世界上最优秀的人物进行对话，向他们学习，成为掌握新知识，新信息，新兴趣的起点，可以从他们身上吸收正能量，还能像吸水纸一样，吸走我们身上的负能量，整个人的思想和灵魂都会得到净化，变得通透，丰富，有内涵。整个人生都是我们的学习之路，这条路不会停下来，若是强行停止，那么在停止的那一瞬间我们就已经开启了死亡倒计时。</li><li>身体是最重要的，马克思都说，物质是第一性的，精神是第二性的。 诱导方法，坚持锻炼，越来越年轻，骑行，跑步，行走。坚持锻炼，不仅是为了更健康地生活，更是为了愉快地与自己相处，享受“一个人的喜悦”。感受生命的惬意。 我自己的方法，一个俯卧撑，每天做一个俯卧撑，慢慢来（此方法为做事法则和身体法则推导出的方法的融合版本，一天做一个，不容易失败，即使做不到也没有负罪感。）</li><li>内心不需要减熵，一个系统到达一定程度时会出现自毁的倾向，减少熵是必要的。 诱导方法，坚持断舍离，明确自己的需要，想要和必要，不放纵欲望，把钱花在刀刃。 诱导法则2，冥想，每天关掉手机，闭上眼睛，留给自己一个和“自己独处的时间”，是一件特别重要且必要的事情，让我们平静内心，自在呼吸。不在意别人走多快，专注于自己走多远。专注你想做的事，心无旁骛，你会发现自己控制情绪的能力，专注力，对未来的规划，都会越来越成效，还会乐在其中。</li><li>培养自己的能力是必要的。（法则也分等级的）诱导法则，建立财商观， 你不理财，财不理你， 诱导方法， 坚持记账，做好定投， 手上的纸币某种程度上是负债，流动的纸币才是资产，虽然可能变成负资产。做一名终生定投者，掌握方法，增加本金，提高收益率，让钱生钱的威力争取发挥最大化。</li><li>实践是认识的来源，理论指导实践。诱导方法，每天，每周，每月，每年都要复盘，不抽象，就无法深入思考，不还原，就看不到本来面目。进行复盘，总有一天你就是那个可以看到事情真相的人。 诱导方法， 做一个行动者，你和梦想之间就差一个行动，为了靠近或者实现梦想，除了行动就是行动。唯有行动才能解除所有的不安，焦虑，怀疑，懒惰，拖延。</li><li>不要自己禁锢自己。 推导法则： 自己以为不行是不行，那就是真的不行，但是还是要试一下才能知道行不行。 推导法则2： 不要让别人来禁锢你自己，别人告诉你要干嘛，我就干嘛，那是傻逼，要有自己的想法，自己的思想，被别人买了还帮人数钱。别人凭什么来为你服务，别人没有义务，也没有责任，不要把人想的太好了，也不要把人想的太差。天下没有白来的好处，靠人人倒。靠山山倒。 推导公式3，问题在于不要说自己学了这个专业就要干这个专业的事情，要跨界而行。</li><li>给自己穿件衣服：下意识的看轻你，人的第一印象是最难改变的，预设一旦完成，轻则被怠慢，重则被人故意为难。而假如你衣着体面，从某种程度上其实是在保护自己、方便自己。而懂得游戏规则本身，就是在给自己穿上一件“体面的衣服”，它的目的不是为了“虚荣”，而是为了保护自己、避免一些不必要的麻烦。控制自己需要阐述的欲望，“学会保护好自己，不管在什么环境，首先都要确保自己能应对周围不同的人与事，比如你从来没有把别人当对手，但别人会把你当做对手，对此，你要有所准备，以及要有对应方法。”<br><a href="https://www.zhihu.com/question/31756387/answer/2146825423">参考博客-永生</a><br><a href="https://www.zhihu.com/question/301793024/answer/786846336">参考博客-有哪些可以坚持下去的好习惯</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文思路——代码</title>
    <link href="/2024/05/09/paper-idear4/"/>
    <url>/2024/05/09/paper-idear4/</url>
    
    <content type="html"><![CDATA[<h2 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a>数据划分</h2><ol><li>数据集分类</li></ol><p>将set的文件转换为npy格式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> mne<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">find_set_files</span>(<span class="hljs-params">root_folder</span>):<br>    set_files = []<br>    <span class="hljs-keyword">for</span> root, dirs, files <span class="hljs-keyword">in</span> os.walk(root_folder):<br>        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>            <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&#x27;.set&#x27;</span>):<br>                set_files.append(os.path.join(root, file))<br>    <span class="hljs-keyword">return</span> set_files<br><br><span class="hljs-comment"># 指定您的 AD 文件夹路径</span><br>ad_folder = <span class="hljs-string">r&#x27;FDT&#x27;</span><br><br><span class="hljs-comment"># 指定保存 .npy 文件的新文件夹路径</span><br>output_folder = <span class="hljs-string">r&#x27;data_onremove_npy/FDT&#x27;</span><br>os.makedirs(output_folder, exist_ok=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 找到所有的 .set 文件</span><br>set_files = find_set_files(ad_folder)<br><br><span class="hljs-comment"># 读取 .set 文件并保存为 .npy 文件</span><br><span class="hljs-keyword">for</span> file_path <span class="hljs-keyword">in</span> set_files:<br>    <span class="hljs-comment"># 读取 .set 文件</span><br>    raw = mne.io.read_raw_eeglab(file_path, preload=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 获取原始数据</span><br>    data = raw.get_data()<br><br>    <span class="hljs-comment"># 构造新的文件路径</span><br>    npy_file_name = os.path.basename(file_path).replace(<span class="hljs-string">&#x27;.set&#x27;</span>, <span class="hljs-string">&#x27;.npy&#x27;</span>)<br>    npy_file_path = os.path.join(output_folder, npy_file_name)<br><br>    <span class="hljs-comment"># 保存为 .npy 文件</span><br>    np.save(npy_file_path, data)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Saved <span class="hljs-subst">&#123;npy_file_path&#125;</span>&#x27;</span>)<br><br></code></pre></td></tr></table></figure><p>将npy数据剪切到想要的长度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> mne<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">find_set_files</span>(<span class="hljs-params">root_folder</span>):<br>    set_files = []<br>    <span class="hljs-keyword">for</span> root, dirs, files <span class="hljs-keyword">in</span> os.walk(root_folder):<br>        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>            <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&#x27;.set&#x27;</span>):<br>                set_files.append(os.path.join(root, file))<br>    <span class="hljs-keyword">return</span> set_files<br><br><span class="hljs-comment"># 指定您的 AD 文件夹路径</span><br>ad_folder = <span class="hljs-string">r&#x27;FDT&#x27;</span><br><br><span class="hljs-comment"># 指定保存 .npy 文件的新文件夹路径</span><br>output_folder = <span class="hljs-string">r&#x27;data_onremove_npy/FDT&#x27;</span><br>os.makedirs(output_folder, exist_ok=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 找到所有的 .set 文件</span><br>set_files = find_set_files(ad_folder)<br><br><span class="hljs-comment"># 读取 .set 文件并保存为 .npy 文件</span><br><span class="hljs-keyword">for</span> file_path <span class="hljs-keyword">in</span> set_files:<br>    <span class="hljs-comment"># 读取 .set 文件</span><br>    raw = mne.io.read_raw_eeglab(file_path, preload=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 获取原始数据</span><br>    data = raw.get_data()<br><br>    <span class="hljs-comment"># 构造新的文件路径</span><br>    npy_file_name = os.path.basename(file_path).replace(<span class="hljs-string">&#x27;.set&#x27;</span>, <span class="hljs-string">&#x27;.npy&#x27;</span>)<br>    npy_file_path = os.path.join(output_folder, npy_file_name)<br><br>    <span class="hljs-comment"># 保存为 .npy 文件</span><br>    np.save(npy_file_path, data)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Saved <span class="hljs-subst">&#123;npy_file_path&#125;</span>&#x27;</span>)<br><br><br></code></pre></td></tr></table></figure><p>对数据进行fft变换</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import os<br>import numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-built_in">from</span> tqdm import tqdm<br><br><span class="hljs-comment"># 定义输入和输出文件夹路径</span><br>input_folder = <span class="hljs-string">&#x27;data_cut_npy/AD&#x27;</span>  <span class="hljs-comment"># 输入文件夹路径，包含要进行 FFT 变换的 .npy 文件</span><br>output_folder = <span class="hljs-string">&#x27;data_FFT_npy/AD&#x27;</span>  <span class="hljs-comment"># 输出文件夹路径，用于保存变换后的数据</span><br><br><span class="hljs-comment"># 确保输出文件夹存在</span><br>os.makedirs(output_folder, exist_ok=True)<br><br><span class="hljs-comment"># 获取输入文件夹中的所有 .npy 文件</span><br>file_list = os.listdir(input_folder)<br>npy_files = [<span class="hljs-built_in">file</span> <span class="hljs-keyword">for</span> <span class="hljs-built_in">file</span> <span class="hljs-keyword">in</span> file_list <span class="hljs-keyword">if</span> <span class="hljs-built_in">file</span>.endswith(<span class="hljs-string">&#x27;.npy&#x27;</span>)]<br><br><span class="hljs-comment"># 遍历每个 .npy 文件进行 FFT 变换并保存</span><br><span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> tqdm(npy_files, desc=<span class="hljs-string">&#x27;Processing&#x27;</span>, unit=<span class="hljs-string">&#x27;file&#x27;</span>):<br>    <span class="hljs-comment"># 读取 .npy 文件</span><br>    file_path = os.path.join(input_folder, file_name)<br>    data = np.<span class="hljs-built_in">load</span>(file_path)<br><br>    <span class="hljs-comment"># 对数据中的每一行进行 FFT 变换</span><br>    fft_data = np.apply_along_axis(np.fft.fft, axis=<span class="hljs-number">0</span>, arr=data)<br><br>    <span class="hljs-comment"># 获取 FFT 结果的幅值</span><br>    fft_magnitude = np.<span class="hljs-built_in">abs</span>(fft_data)<br><br>    <span class="hljs-comment"># 构造输出文件路径</span><br>    output_file_name = file_name.<span class="hljs-built_in">replace</span>(<span class="hljs-string">&#x27;.npy&#x27;</span>, <span class="hljs-string">&#x27;_fft.npy&#x27;</span>)<br>    output_file_path = os.path.join(output_folder, output_file_name)<br><br>    <span class="hljs-comment"># 保存 FFT 结果</span><br>    np.save(output_file_path, fft_magnitude)<br><br></code></pre></td></tr></table></figure><p>显示fft数据</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> os<br><br># 替换为包含.npy文件的文件夹路径列表<br>folder_paths = [<span class="hljs-string">&quot;data_FFT_npy/AD&quot;</span>, <span class="hljs-string">&quot;data_FFT_npy/CN&quot;</span>,<span class="hljs-string">&quot;data_FFT_npy/FDT&quot;</span>]  # 替换为实际的文件夹路径列表<br># [<span class="hljs-string">&quot;normal_save&quot;</span>, <span class="hljs-string">&quot;open_normal_save&quot;</span>, <span class="hljs-string">&quot;open_patient_save&quot;</span>,<span class="hljs-string">&#x27;patient_save&#x27;</span>]<br># 循环遍历每个文件夹<br>for folder_path <span class="hljs-keyword">in</span> folder_paths:<br>    # 获取文件夹中的所有.npy文件<br>    npy_files = [file for file <span class="hljs-keyword">in</span> os.listdir(folder_path) <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&#x27;.npy&#x27;</span>)]<br><br>    # 循环加载每个.npy文件并显示其形状<br>    for npy_file <span class="hljs-keyword">in</span> npy_files:<br>        npy_file_path = os.path.join(folder_path, npy_file)<br>        data = np.load(npy_file_path)<br>        print(f<span class="hljs-string">&quot;Folder: &#123;folder_path&#125;, File: &#123;npy_file&#125;, Shape: &#123;data.shape&#125;&quot;</span>)<br><br></code></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>训练代码</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><code class="hljs css">import os<br>import numpy as np<br>import torch<br>import torch<span class="hljs-selector-class">.nn</span> as nn<br>import torch<span class="hljs-selector-class">.optim</span> as optim<br><span class="hljs-selector-tag">from</span> torch<span class="hljs-selector-class">.utils</span><span class="hljs-selector-class">.data</span> import DataLoader<br><span class="hljs-selector-tag">from</span> torchvision import transforms<br><span class="hljs-selector-tag">from</span> tqdm import tqdm<br><span class="hljs-selector-tag">from</span> dataset import EEGDataset,EEGDataset_Batch_normal<br><span class="hljs-selector-tag">from</span> net import IntegratedNet<br><span class="hljs-selector-tag">from</span> sklearn<span class="hljs-selector-class">.metrics</span> import classification_report<br><span class="hljs-selector-tag">from</span> matplotlib import pyplot as plt<br><br><br># 定义归一化操作<br>def normalize(data):<br>    mean = np.<span class="hljs-built_in">mean</span>(data)<br>    std = np.<span class="hljs-built_in">std</span>(data)<br>    return (data - mean) / std<br><br>transform = transforms.<span class="hljs-built_in">Compose</span>([<br>        transforms.<span class="hljs-built_in">Lambda</span>(normalize),  # 使用Lambda函数应用自定义归一化操作<br>        transforms.<span class="hljs-built_in">ToTensor</span>()<br>    ])<br><br>def <span class="hljs-built_in">train_identityformer_model</span>(model, model_name, num_epochs=<span class="hljs-number">100</span>, num_classes=<span class="hljs-number">3</span>, batch_size=<span class="hljs-number">16</span>, learning_rate=<span class="hljs-number">0.0001</span>, w_wight=<span class="hljs-number">1025</span>, chennal=<span class="hljs-number">33</span>):<br>    if torch.cuda.<span class="hljs-built_in">is_available</span>():<br>        device = torch.<span class="hljs-built_in">device</span>(<span class="hljs-string">&quot;cuda&quot;</span>)<br>    else:<br>        device = torch.<span class="hljs-built_in">device</span>(<span class="hljs-string">&quot;cpu&quot;</span>)<br>    m = nn.<span class="hljs-built_in">Softmax</span>(dim=<span class="hljs-number">1</span>)<br><br>    train_dataset = <span class="hljs-built_in">EEGDataset</span>(csv_file=<span class="hljs-string">&#x27;train_data.csv&#x27;</span>, transform=transform)<br>    test_dataset = <span class="hljs-built_in">EEGDataset</span>(csv_file=<span class="hljs-string">&#x27;test_data.csv&#x27;</span>, transform=transform)<br><br>    train_loader = <span class="hljs-built_in">DataLoader</span>(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)<br>    test_loader = <span class="hljs-built_in">DataLoader</span>(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)<br><br>    model.<span class="hljs-built_in">to</span>(device)<br>    loss_fn = nn.<span class="hljs-built_in">CrossEntropyLoss</span>()<br>    optimizer = optim.<span class="hljs-built_in">RMSprop</span>(model.<span class="hljs-built_in">parameters</span>(), lr=learning_rate)<br><br>    save_dir = <span class="hljs-string">&#x27;loss&#x27;</span><br>    os.<span class="hljs-built_in">makedirs</span>(save_dir, exist_ok=True)<br><br>    train_loss_arr = []<br>    train_acc_arr = []<br>    val_loss_arr = []<br>    val_acc_arr = []<br><br>    best_val_acc = <span class="hljs-number">0.0</span><br>    best_epoch = <span class="hljs-number">0</span><br><br>    for epoch in <span class="hljs-built_in">range</span>(num_epochs):<br>        train_loss_total = <span class="hljs-number">0</span><br>        train_acc_total = <span class="hljs-number">0</span><br>        val_loss_total = <span class="hljs-number">0</span><br>        val_acc_total = <span class="hljs-number">0</span><br><br>        model.<span class="hljs-built_in">train</span>()<br>        progress_bar = <span class="hljs-built_in">tqdm</span>(<span class="hljs-built_in">enumerate</span>(train_loader), total=<span class="hljs-built_in">len</span>(train_loader))<br>        for i, (train_x, train_y) in progress_bar:<br>            train_x = train_x.<span class="hljs-built_in">to</span>(device)<br>            train_y = train_y.<span class="hljs-built_in">to</span>(device)<br>            train_x = train_x.<span class="hljs-built_in">unsqueeze</span>(<span class="hljs-number">1</span>)<br>            train_x = train_x.<span class="hljs-built_in">view</span>(batch_size, <span class="hljs-number">1</span>, chennal, w_wight)<br><br>            train_y_pred = <span class="hljs-built_in">model</span>(train_x)<br>            train_loss = <span class="hljs-built_in">loss_fn</span>(train_y_pred, train_y)<br><br>            train_acc = (<span class="hljs-built_in">m</span>(train_y_pred).<span class="hljs-built_in">max</span>(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] == train_y).<span class="hljs-built_in">sum</span>() / train_y.shape[<span class="hljs-number">0</span>]<br>            train_loss_total += train_loss.data.<span class="hljs-built_in">item</span>()<br>            train_acc_total += train_acc.data.<span class="hljs-built_in">item</span>()<br><br>            train_loss.<span class="hljs-built_in">backward</span>()<br>            optimizer.<span class="hljs-built_in">step</span>()<br>            optimizer.<span class="hljs-built_in">zero_grad</span>()<br><br>            progress_bar.<span class="hljs-built_in">set_description</span>(f<span class="hljs-string">&quot;Epoch &#123;epoch+1&#125;/&#123;num_epochs&#125;, Batch &#123;i+1&#125;/&#123;len(train_loader)&#125;, Train Loss: &#123;train_loss.data.item():.4f&#125;, Train Acc: &#123;train_acc.data.item():.4f&#125;&quot;</span>)<br><br>        train_loss_arr.<span class="hljs-built_in">append</span>(train_loss_total / <span class="hljs-built_in">len</span>(train_loader))<br>        train_acc_arr.<span class="hljs-built_in">append</span>(train_acc_total / <span class="hljs-built_in">len</span>(train_loader))<br><br>        model.<span class="hljs-built_in">eval</span>()<br>        for j, (val_x, val_y) in <span class="hljs-built_in">enumerate</span>(test_loader):<br>            val_x = val_x.<span class="hljs-built_in">to</span>(device)<br>            val_y = val_y.<span class="hljs-built_in">to</span>(device)<br>            val_x = val_x.<span class="hljs-built_in">unsqueeze</span>(<span class="hljs-number">1</span>)<br>            val_x = val_x.<span class="hljs-built_in">view</span>(batch_size, <span class="hljs-number">1</span>, chennal, w_wight)<br><br>            val_y_pred = <span class="hljs-built_in">model</span>(val_x)<br>            val_loss = <span class="hljs-built_in">loss_fn</span>(val_y_pred, val_y)<br>            val_acc = (<span class="hljs-built_in">m</span>(val_y_pred).<span class="hljs-built_in">max</span>(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] == val_y).<span class="hljs-built_in">sum</span>() / val_y.shape[<span class="hljs-number">0</span>]<br>            val_loss_total += val_loss.data.<span class="hljs-built_in">item</span>()<br>            val_acc_total += val_acc.data.<span class="hljs-built_in">item</span>()<br><br>        val_loss_arr.<span class="hljs-built_in">append</span>(val_loss_total / <span class="hljs-built_in">len</span>(test_loader))<br>        val_acc_arr.<span class="hljs-built_in">append</span>(val_acc_total / <span class="hljs-built_in">len</span>(test_loader))<br><br>        if val_acc_arr[-<span class="hljs-number">1</span>] &gt; best_val_acc:<br>            best_val_acc = val_acc_arr[-<span class="hljs-number">1</span>]<br>            best_epoch = epoch<br>            torch.<span class="hljs-built_in">save</span>(model.<span class="hljs-built_in">state_dict</span>(), f<span class="hljs-string">&quot;&#123;model_name&#125;_best.pth&quot;</span>)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch:&#123;&#125; val_loss:&#123;&#125; val_acc:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, val_loss_arr[-<span class="hljs-number">1</span>], val_acc_arr[-<span class="hljs-number">1</span>]))<br><br>    np.<span class="hljs-built_in">save</span>(os.path.<span class="hljs-built_in">join</span>(save_dir, <span class="hljs-string">&#x27;train_loss_arr.npy&#x27;</span>), np.<span class="hljs-built_in">array</span>(train_loss_arr))<br>    np.<span class="hljs-built_in">save</span>(os.path.<span class="hljs-built_in">join</span>(save_dir, <span class="hljs-string">&#x27;train_acc_arr.npy&#x27;</span>), np.<span class="hljs-built_in">array</span>(train_acc_arr))<br>    np.<span class="hljs-built_in">save</span>(os.path.<span class="hljs-built_in">join</span>(save_dir, <span class="hljs-string">&#x27;val_loss_arr.npy&#x27;</span>), np.<span class="hljs-built_in">array</span>(val_loss_arr))<br>    np.<span class="hljs-built_in">save</span>(os.path.<span class="hljs-built_in">join</span>(save_dir, <span class="hljs-string">&#x27;val_acc_arr.npy&#x27;</span>), np.<span class="hljs-built_in">array</span>(val_acc_arr))<br><br>    plt.<span class="hljs-built_in">subplot</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    plt.<span class="hljs-built_in">title</span>(<span class="hljs-string">&quot;loss&quot;</span>)<br>    plt.<span class="hljs-built_in">plot</span>(train_loss_arr, <span class="hljs-string">&quot;r&quot;</span>, label=<span class="hljs-string">&quot;train&quot;</span>)<br>    plt.<span class="hljs-built_in">plot</span>(val_loss_arr, <span class="hljs-string">&quot;b&quot;</span>, label=<span class="hljs-string">&quot;val&quot;</span>)<br>    plt.<span class="hljs-built_in">legend</span>()<br><br>    plt.<span class="hljs-built_in">subplot</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    plt.<span class="hljs-built_in">title</span>(<span class="hljs-string">&quot;acc&quot;</span>)<br>    plt.<span class="hljs-built_in">plot</span>(train_acc_arr, <span class="hljs-string">&quot;r&quot;</span>, label=<span class="hljs-string">&quot;train&quot;</span>)<br>    plt.<span class="hljs-built_in">plot</span>(val_acc_arr, <span class="hljs-string">&quot;b&quot;</span>, label=<span class="hljs-string">&quot;val&quot;</span>)<br>    plt.<span class="hljs-built_in">legend</span>()<br><br>    plt.<span class="hljs-built_in">savefig</span>(<span class="hljs-string">&quot;loss_acc.png&quot;</span>)<br>    plt.<span class="hljs-built_in">show</span>()<br><br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Best model at epoch &#123;best_epoch+1&#125;, val_acc=&#123;best_val_acc:.4f&#125;&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Training completed!&#x27;</span>)<br><br><br># 创建模型并训练<br>model = <span class="hljs-built_in">IntegratedNet</span>(input_size=<span class="hljs-number">1</span>,in_feature=<span class="hljs-number">157</span>,num_classes=<span class="hljs-number">2</span>)  # 确保模型的输出层适用于三分类问题<br><span class="hljs-built_in">train_identityformer_model</span>(model, model_name=<span class="hljs-string">&#x27;MLPFormer_betch_16_fft_opendata&#x27;</span>,chennal=<span class="hljs-number">19</span>,w_wight=<span class="hljs-number">2500</span>)<br><br><br></code></pre></td></tr></table></figure><p>测试代码</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch.cuda<br><span class="hljs-keyword">from</span> torchvision import models<br><span class="hljs-keyword">from</span> dataset import *<br><span class="hljs-keyword">from</span> torch.utils.data import DataLoader<br><span class="hljs-keyword">from</span> torch import optim, nn<br><span class="hljs-keyword">from</span> dataset import *<br><span class="hljs-keyword">from</span> sklearn.metrics import recall_score, f1_score, precision_score, confusion_matrix, accuracy_score<br><span class="hljs-keyword">from</span> matplotlib import rcParams<br>import os<br><span class="hljs-keyword">from</span> net import *<br><br>labels = os.listdir(<span class="hljs-string">&quot;data_FFT_npy&quot;</span>)<br>m = nn.Softmax(<span class="hljs-attribute">dim</span>=1)<br>rcParams[<span class="hljs-string">&#x27;font.family&#x27;</span>] = <span class="hljs-string">&#x27;SimHei&#x27;</span><br>def val(<span class="hljs-attribute">batch_size</span>=16,w_wight=2500):<br>    # 数据集和数据加载器<br>    val_dataset = EEGDataset(<span class="hljs-attribute">csv_file</span>=<span class="hljs-string">&#x27;test_data.csv&#x27;</span>,transform=transform)<br><br>    val_data_loader = DataLoader(val_dataset, batch_size, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">drop_last</span>=<span class="hljs-literal">True</span>)<br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    model = IntegratedNet(<span class="hljs-attribute">input_size</span>=1,in_feature=157,num_classes=2).to(device)<br>    model.load_state_dict(torch.load(<span class="hljs-string">&quot;MLPFormer_betch_16_fft_opendata.pth&quot;</span>))<br><br>    arr_y = []<br>    arr_y_pred = []<br>    <span class="hljs-keyword">for</span> val_x, val_y <span class="hljs-keyword">in</span> val_data_loader:<br>        val_x = val_x.<span class="hljs-keyword">to</span>(device)<br>        val_y = val_y.<span class="hljs-keyword">to</span>(device)<br>        val_x = val_x.unsqueeze(1)<br>        val_x = val_x.view(batch_size, 1, 19, w_wight)<br>        val_y_pred = model(val_x)<br>        arr_y.extend(val_y.cpu().numpy())<br>        pred_result = m(val_y_pred).max(<span class="hljs-attribute">dim</span>=1)[1]<br>        arr_y_pred.extend(pred_result.cpu().numpy())<br><br>    p = precision_score(arr_y, arr_y_pred, <span class="hljs-attribute">average</span>=<span class="hljs-string">&quot;macro&quot;</span>)<br>    recall = recall_score(arr_y, arr_y_pred, <span class="hljs-attribute">average</span>=<span class="hljs-string">&quot;macro&quot;</span>)<br>    f1 = f1_score(arr_y, arr_y_pred, <span class="hljs-attribute">average</span>=<span class="hljs-string">&quot;macro&quot;</span>)<br><br>    cm = confusion_matrix(arr_y, arr_y_pred)<br>    tn, fp, fn, tp = cm.ravel()<br>    specificity = tn / (tn + fp)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Precision: &#123;:.5f&#125;, Recall: &#123;:.5f&#125;, F1-score: &#123;:.5f&#125;, Specificity: &#123;:.5f&#125;&quot;</span>.format(p, recall, f1, specificity))<br><br>    plt.imshow(cm, <span class="hljs-attribute">cmap</span>=<span class="hljs-string">&quot;Blues&quot;</span>)<br>    plt.xticks(range(2), <span class="hljs-attribute">labels</span>=labels)<br>    plt.yticks(range(2), <span class="hljs-attribute">labels</span>=labels)<br><br>    plt.colorbar()<br>    plt.xlabel(<span class="hljs-string">&quot;Predicted&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;Actual&quot;</span>)<br>    thresh = cm.mean()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(2):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(2):<br>            <span class="hljs-built_in">info</span> = cm[j, i]<br>            plt.text(i, j, info, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;white&quot;</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">info</span> &gt; thresh <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;black&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;confusion_matrix.jpg&quot;</span>)<br>    plt.show()<br><br><br><br><br><br>import matplotlib.pyplot as plt<br><br>def eval_single_sample(<span class="hljs-attribute">csv_file</span>=<span class="hljs-string">&#x27;test_data.csv&#x27;</span>, <span class="hljs-attribute">model_path</span>=<span class="hljs-string">&#x27;model/MLPFormer_betch_2_normal.pth&#x27;</span>):<br>    # 数据集和数据加载器<br>    val_dataset = EEGDataset_eval(<span class="hljs-attribute">csv_file</span>=csv_file, <span class="hljs-attribute">transform</span>=transform)<br><br>    val_data_loader = DataLoader(val_dataset, <span class="hljs-attribute">batch_size</span>=1, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">False</span>)<br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    model = IntegratedNet().<span class="hljs-keyword">to</span>(device)<br>    model.load_state_dict(torch.load(model_path))<br><br>    labels = [<span class="hljs-string">&#x27;认知功能障碍&#x27;</span>,<span class="hljs-string">&#x27;轻度认知功能障碍&#x27;</span>,<span class="hljs-string">&#x27;认知功能正常&#x27;</span>]<br>    m = nn.Softmax(<span class="hljs-attribute">dim</span>=1)<br>    rcParams[<span class="hljs-string">&#x27;font.family&#x27;</span>] = <span class="hljs-string">&#x27;SimHei&#x27;</span><br><br>    # 初始化一个列表来保存每个预测的概率<br>    all_probs = []<br><br>    <span class="hljs-keyword">for</span> val_x <span class="hljs-keyword">in</span> val_data_loader:<br>        val_x = val_x.<span class="hljs-keyword">to</span>(device)<br>        val_x = val_x.unsqueeze(1)<br>        val_x = val_x.view(1, 1, 33, 1025)<br>        val_y_pred = model(val_x)<br>        pred_probs = m(val_y_pred).squeeze().detach().cpu().numpy()<br><br>        # 保存预测的概率<br>        all_probs.append(pred_probs)<br><br>    # 计算平均概率<br>    avg_probs = np.mean(all_probs, <span class="hljs-attribute">axis</span>=0)<br><br>    # 绘制平均预测概率的柱状图<br>    plt.figure(figsize=(10, 6))<br>    plt.bar(labels, avg_probs)<br>    plt.xlabel(<span class="hljs-string">&#x27;Classes&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;Average Predicted Probability&#x27;</span>)<br>    plt.title(<span class="hljs-string">&#x27;Average Predicted Probabilities for Each Class&#x27;</span>)<br>    plt.show()<br><br>    pred_label = labels[np.argmax(avg_probs)]<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Most probable class: &#123;pred_label&#125; with average probability &#123;np.max(avg_probs)*100:.2f&#125;%&quot;</span>)<br><br><br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    val()<br>    # eval_single_sample()<br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>选导师</title>
    <link href="/2024/05/09/tiankeng7/"/>
    <url>/2024/05/09/tiankeng7/</url>
    
    <content type="html"><![CDATA[<ol><li>四川大学，电子信息（085400） 考察科目：22，874（计算机科学专业基础综合组成是，计算机网络、数据结构、操作系统。）</li><li>西安交通</li><li>西南交大</li><li>重庆邮电</li><li></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>安排1</title>
    <link href="/2024/05/07/anpai/"/>
    <url>/2024/05/07/anpai/</url>
    
    <content type="html"><![CDATA[<h2 id="安排"><a href="#安排" class="headerlink" title="安排"></a>安排</h2><ol><li>数学（进步本本分析 + 刷题 ）（一般在19:00 - 22：00时间段内） （4小件 3本书 + 进步本复习），第一本一天5道题，复习5道题、第二本一天3页码，复习2页码。1</li><li>英语（进步本复习 + 单词记忆 + 刷题 ）（一般在19:00 - 22：00时间段内）（ 4小件 进步本2本 + 作文+翻译+记忆单词 ） 1.单词记忆， 2.作文 3.做题</li><li>论文（目前在阅读论文阶段 + 同步在博客上）（有缘就看，有缘就写字，最多一天100字）</li><li>学校课程（有课就去上，搞完课程 + 同步在博客上）（课程做业吗，同步博客。（全是大坑） ）</li></ol><p>注：复习了之后记得发散思维，专注和发散模式相互切换。</p>]]></content>
    
    
    
    <tags>
      
      <tag>计划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>校园</title>
    <link href="/2024/05/05/tupian3/"/>
    <url>/2024/05/05/tupian3/</url>
    
    <content type="html"><![CDATA[<p><img src="/pic/2024_5_5_1.jpg" alt="玫瑰花"><br><img src="/pic/2024_5_5_2.jpg" alt="图书馆"><br><img src="/pic/2024_5_5_3.jpg" alt="四维大楼"><br><img src="/pic/2024_5_5_4.jpg" alt="图书馆2"><br><img src="/pic/2024_5_5_5.jpg" alt="四维大楼2"><br><img src="/pic/2024_5_5_6.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>图片</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔15</title>
    <link href="/2024/05/04/ganwu15/"/>
    <url>/2024/05/04/ganwu15/</url>
    
    <content type="html"><![CDATA[<h2 id="我的爱情观"><a href="#我的爱情观" class="headerlink" title="我的爱情观"></a>我的爱情观</h2><p>先保护自己不受伤害，才能更好的去爱，如果自己缺爱，那么自己去自己，否然就会让别人来毁灭我吧。保护自己最佳的方式，就是从不高估自己在别人心中的分量，及时止损，别希望、别期望、别盼望、别指望、怎么会有失望？<br>喜欢归喜欢，太卑微就没有必要了，自己的旅程。<br>生活，活着，如果另一半不能共同成长、共同升值，共同承受，而是让自己慢慢长成被生活欺负，那应该好好思考现在的自己了。思考这一段经历，毕竟我的心力有限，与其让你来毁灭我，不如自己毁灭自己。<br>知识和信息密度远大于你的人，愿意俯下身来主动和你交流，尊重你、鼓励你、引导你、这便是温柔。</p><p>如果是为了性来交往，把性当作目的，那么不能称之为人了，而是兽。每当有人问起，为什么我不恋爱的时候，我都以麻烦为理由搪塞过去。又有人开始问我到底喜欢过谁没有，我也轻描淡写地表示否定。无数的朋友告诉我，或许是你还没碰到过真正喜欢的人吧，我却没法开口告诉他们，其实我曾经碰到过，碰得太早，以至于我没来得及分清，也没来得及弄明白。</p><p>你们也许还没法体会到，但人生在世，婚姻其实非常非常重要。我希望大家将来都认真对待婚姻，不求达到钱杨的境界，但最起码也要学习沈从文，把道德和感情这两条底线牢牢地把握住。人一辈子七八十年。其实长的超出你们的预料，想在这么长的时间里不遇到任何变故是不可能的。一个讲道德、有感情的婚姻，就像一把下了的锚，有了它，多大的浪头来了你都不会倾覆。因为你心里知道自己还有想见到的人，有想尽的责任，这种想法的力量是和信仰相当的。”</p><h2 id="人与人"><a href="#人与人" class="headerlink" title="人与人"></a>人与人</h2><p>世界上充满着不同的人，悲伤的，自立的，自私的，慷慨的，无畏的，善念的，恶意的，勤奋的，懒散的，自律的，乐观的，坚持的，忍耐的，乏味、反覆、放任、风趣、风趣幽默、浮躁 、富创造力、富有朝气、富于冒险、才思敏捷、猜疑、沉静、沉著、诚实坦白、成熟、成熟稳重、迟钝、迟缓、冲动、处事洒脱、聪明伶俐、粗心、脆弱、懒惰、老练、老实、老实巴交、唠叨、乐观、乐善好施、雷历风行、冷淡 、冷漠 、礼貌、吝啬、鲁莽 、罗嗦、脾气暴躁、贫乏、平和、婆婆妈妈、普普通通、勤劳、轻浮、轻率、轻率不踏实、轻松、情绪多变、缺乏耐力、缺乏自信、散漫、善变、善交际、善解人意、善良、善于分析、善于体察别人、善组织、少言寡语、深沉、神经质、实事求是、适应能力差、适应能力强、率直、水性扬花、思想开放、随和、踏实、坦率、贪婪、贪小便宜、体贴、挑衅、挑剔、统治欲、妥协、拖延、严肃、言行不一、阳光、一本正经、 依赖、毅力、抑郁、易激动、易见异思迁、易怒、易轻率作决定、易随波逐流、易兴奋、疑神疑鬼、意志坚定、阴险狡诈、勇敢、勇敢正义、友爱、友善、犹豫不决、忧心忡忡、有趣、有韧性、有条理、优柔寡断、幽默、幼稚 、幼稚调皮、愉快、郁郁寡欢、圆滑老练、怨恨。<br>众生百相，经历的不代表世界，只能代表经历的，遇到各种各样的人，不能使用相同的方法来交流，要随机应变，不同的人遇到不同的事，处理方法都不一样但是解决问题就好了，</p><h2 id="安静的做事"><a href="#安静的做事" class="headerlink" title="安静的做事"></a>安静的做事</h2><p>这件事，那件事，许多件事困扰着我，做真正能让我感到进步的事情，安排事件什么时候去做也是一个需要锻炼的能力。做事是为了进步的，每天抽出专门的时间，把本子上面的题拿出来重新做。仅仅是睡觉前看一遍是不够的，因为看一遍以为会了，以为掌握了，实际上不一定。很可能是自欺欺人。事实上，我常常发现,重做的时候，还是有些会做错。所以，在重做的过程中，会对相关的知识点，解题方法进行重新的标记。也会对原有的知识型笔记，进行更多的标注。所以，这绝不是机械式的学习，而是理解式的学习。不断迭代，不断深入的学习。<br>对于自己不想做的事情，先为自己想做的事情和必做的事情让步。减少不必要的物质的欲望（减）。（乘）把已经被验证正确的事，乘以100次。除把目标除分成无数小段。（等）等待满足感。</p><p>利用时间的正确方式是，每一天都有具体目标，有一个Deadline，有个目标截止时间。我该如何来制定每天的具体目标。<br>清醒自己在干嘛是我的追求。每天都在忙，但是不知道在忙什么，还容易心累，所以我决定对这件事进行一定程度的思考。基于清醒的角度来讲这个问题，我是不清醒的，或许有个计划可以在一定程度上解决这个问题。没有目标的计划的人生就像是一艘航行在大海里的船，不知道自己要去哪因而也没用方向。</p><h2 id="你想活出怎样的人生。"><a href="#你想活出怎样的人生。" class="headerlink" title="你想活出怎样的人生。"></a>你想活出怎样的人生。</h2><p>步骤一： 基于现实，确定自己的目标，把目标分类。 分类示例： 个人管理，个人健康，个人技能，兴趣爱好，<br>步骤二： 回顾自己过去的表现，设定长期目标。 三~五年计划 → 年计划 → 月计划 → 周计划 → 日计划。（执行过程中是反过来的，日计划）<br>步骤三： 在执行日计划的过程中进行回顾，回顾，回顾。</p><p>第一步：基于现状分析要完成的事件，分析现状，</p><ol><li>考研，细分为，数学，英语，思政，专业课。</li><li>论文，细分为，看参考文献，跑实验，写论文</li><li>学校上课，细分为，深度学习，数字图像处理，强化学习，专业数学，专业综合实践1，2。占用大部分时间，在上课过程中经常摸鱼。</li></ol><p>分析： 对于考研第一要务，包含数学，英语，思政，专业课四项。其中数学的内容和专业数学内容重合。可以花费大部分时间来对其进行攻打，使用进步本策略（先复习进步本，后进行刷题）。英语还有6级的考试要通过，预先级提升到2（先看单词和做题技巧，后刷题）。<br>论文： 第一阶段是看材料，分析材料讲的什么，构建自己的对比实验，第二阶段是跑实验，分析实验结果。第三阶段是根据看的论文和实验结果来写论文。<br>学校课程： 学校课程的最终目标是通过考试，针对性的学习，深度学习，数字图像处理可以归在一类，强化学习一类。<br>任务记录</p><ol><li>数学（进步本本分析 + 刷题 ）（一般在19:00 - 22：00时间段内）</li><li>英语（进步本复习 + 单词记忆 + 刷题 ）（一般在19:00 - 22：00时间段内）</li><li>论文（阅读论文阶段 + 同步在博客上）（有缘就看，有缘就写字）</li><li>学校课程（有课就去上，搞完课程 + 同步在博客上）（课程做业同步博客。 ）</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高等数学——一元函数的微分</title>
    <link href="/2024/05/04/gaodengshuxue2/"/>
    <url>/2024/05/04/gaodengshuxue2/</url>
    
    <content type="html"><![CDATA[<p>导数和微分的概念，导数的几何意义和物理意义，函数的可导性与连续性之间的关系。<br>平面曲线的切线和法线，导数和微分的四则运算，基本初等函数的导数，复合函数、反函数、隐函数以及参数方程所确定的函数的微分法，高阶段导数，一阶段微分形式的不变性，微分中值定理（罗尔中值定理，拉格朗日中值定理，泰勒定理，柯西中值定理），洛必达法则，函数单调性的判别，函数的极值，函数的凹凸性，拐点以及渐近线，函数图形的描绘，函数的最大小值。<br>弧微分，曲率的概念，曲率圆与曲率半径。</p><h2 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a>概念介绍</h2><ol><li>导数（Derivative）：导数描述了函数在某一点的变化率。对于函数 f(x)，其在点 x 处的导数可以用极限的概念来定义：<img src="/pic/gaodengshuxue_2_1.png"><br><img src="/pic/gaodengshuxue_2_2.png"><br><img src="/pic/gaodengshuxue_2_3.png" alt="基本导数公式"><br><img src="/pic/gaodengshuxue_2_4.png" alt="函数的求导运算法则"><br><a href=""></a></li><li>微分（Differential）：微分是导数的一个应用，它描述了函数在某一点附近的局部线性近似。如果 f(x) 在某点 x 处可导，则在该点附近，函数的微分可以表示为：df&#x3D; f&#96;(x)dx</li></ol><p><img src="/pic/gaodengshuxue_2_5.png" alt="古典数学算法"></p><p>导数的几何意义和物理意义<br>几何意义：在几何学中，导数表示函数图像在某一点的切线斜率。切线的斜率描述了曲线在该点附近的变化率，即在该点处函数的瞬时变化率。通过导数，我们可以了解函数在不同点处的曲率、凹凸性等几何特征。<br>物理意义：在物理学中，导数描述了物理量随时间的变化率。例如，如果一个物体的位置随时间变化，其速度可以表示为位置关于时间的导数，即速度是位置的一阶导数。同样，加速度则是速度关于时间的导数，即加速度描述了速度的变化率。因此，导数在描述运动、变化和变化率方面在物理学中起着关键作用。<br>函数的可导性与连续性之间的关系：一个函数在某一点可导，意味着它在该点附近有良好的局部线性近似，即它在该点连续。因此，如果一个函数在某点可导，则它在该点必然连续。但是，连续并不意味着可导。例如，绝对值函数，在x&#x3D;0处连续，但是在该点不可导。</p><p>导数与微分之间的关系：<br><a href="https://www.zhihu.com/question/53159621">参考博客</a><br>微分本质是一个微小的线性变化量，是用一个线性函数作为原函数变化的逼近（或者叫近似）。<br>导数:是指函数在某一点处变化的快慢,是一种变化率。<br>微分:是指函数在某一点处（趋近于无穷小）的变化量，是一种变化的量。<br>请区别下面不同的算法：古典数学的计算方法<br><img src="/pic/gaodengshuxue_2_6.png" alt="古典数学算法"><br><img src="/pic/gaodengshuxue_2_7.png" alt="极限算法"></p><ol start="3"><li><p>平面曲线的切线和法线。 平面的切线的斜率即f(x)在该点的导数，法线也就是f(x)导数的负分之一。法线和导数之间的关系互为相反数。</p></li><li><p>导数与微分的四则运算<br>导数的运算法则在前面，下面阐述了微分四则运算。<br><img src="/pic/gaodengshuxue_2_8.png" alt="微分运算法则—有理数运算法则"><br><img src="/pic/gaodengshuxue_2_9.png" alt="微分运算法则—复合函数运算法则"></p></li><li><p>参数方程所确定的函数的微分法：<a href="https://zhuanlan.zhihu.com/p/298373433">隐函数和参数方程所确定函数的导数</a></p></li><li><p>高阶导数：导数的导数称为高阶导数。求解答高阶导数的方法，1. 归纳法：常见函数的n阶导，找规律。 2. 莱布尼茨公式 3. 泰勒公式</p></li><li><p>微分形式的不变性：如果两个函数在某一点的微分形式相同，则它们在该点的函数值和导数值也相同。<a href="https://zhuanlan.zhihu.com/p/350936061">对一阶微分形式不变性的理解</a></p></li><li><p>微分中值定理：1.罗尔中值定理 2.拉格朗日中值定理 3.泰勒定理 4. 柯西中值定理。<br><img src="/pic/gaodengshuxue_2_10.png" alt="罗尔中值定理"> <img src="/pic/gaodengshuxue_2_11.png" alt="拉格朗日中值定理"> <img src="/pic/gaodengshuxue_2_12.png" alt="柯西中值定理"><br><a href="https://zhuanlan.zhihu.com/p/377120363">泰勒定理</a><br><img src="/pic/tailezhankai.png" alt="等价无穷小泰勒展开"><br><img src="/pic/tailezhankai2.png" alt="泰勒展开"><br><img src="/pic/tailezhankai3.png" alt="泰勒展开"></p></li><li><p>洛必达法则：用于解决不定型的极限 <a href="https://zhuanlan.zhihu.com/p/553569134">参考链接</a></p></li><li><p>函数的性质<br>函数单调性的判别：通过导数的正负性可以判断函数在区间上的单调性。<br>函数的极值：函数在局部最大值或最小值处称为极值，通过导数的零点或变号来判断。<br>函数的凹凸性：通过二阶导数的正负性来判断函数在某区间上的凹凸性。<br>拐点：函数图像在拐点处由凹转凸或由凸转凹。<br>渐近线：用来描述函数在无穷远处的趋势。<br>函数图形的描绘：通过函数的导数、极值、拐点、渐近线等特性来描绘函数图形。<br>函数的最大小值：通过导数或二阶导数的性质来判断函数的最大值和最小值。</p></li><li><p>弧微分和曲率<br>在一段弧上，一段弧长和它两端点的横坐标有关。当Δx极小时，弧可以看作是一条直线，此时Δx^2+Δy^2&#x3D;Δs^2，又Δy&#x3D;y‘*Δx，所以弧微分公式为<br><img src="/pic/gaodengshuxue_2_13.png" alt="弧微分公式"><br>因为y‘&#x3D;dy&#x2F;dx。所以对于参数方程，将dx移到根号内，再乘以dt&#x2F;dt，分母上的dt移到根号内可得到<br><img src="/pic/gaodengshuxue_2_14.png" alt="参数方程弧微分公式"><br>曲率描述弧的弯曲程度，如果弧角度变化地越快，则曲率越大<br>设一段弧，起点为M终点为M‘，M到M’的弧长为|ds|，两点的切线倾角变化为|dα|，则这一段弧的曲率K为<br><img src="/pic/gaodengshuxue_2_15.png" alt="曲率公式"><br>般求曲率都要通过参数方程，设<br><img src="/pic/gaodengshuxue_2_16.png" alt="参数方程曲率公式"></p></li></ol><p>曲率圆 ： 把一小段弧长当成一个圆的一部分，则可通过曲率求处圆的半径为r&#x3D;1&#x2F;K<br><a href="https://blog.csdn.net/qq_42578970/article/details/106967819">参考链接</a><br><a href="https://zhuanlan.zhihu.com/p/479710119">参考链接2</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分类模型-MetaFormer Baselines for Vision</title>
    <link href="/2024/05/04/deeplearnpaper6/"/>
    <url>/2024/05/04/deeplearnpaper6/</url>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/577709208">参考博客-知乎</a><br><a href="https://arxiv.org/pdf/2210.13452">论文原文链接</a><br><a href="https://zhuanlan.zhihu.com/p/579175302">读论文博客</a><br><a href="https://github.com/sail-sg/metaformer">原文代码</a></p><h2 id="原文参考"><a href="#原文参考" class="headerlink" title="原文参考"></a>原文参考</h2><p>Nevertheless, some work <a href="https://arxiv.org/pdf/2105.01601">17</a>, <a href="https://arxiv.org/pdf/2105.03824">18</a>, <a href="https://arxiv.org/pdf/2108.13002">19</a>,<a href="https://arxiv.org/pdf/2106.04263">20</a>, <a href="https://arxiv.org/pdf/2107.00645">21</a> found that, by replacing the attention module in Transformers with simple operators like spatial MLP <a href="https://arxiv.org/pdf/2105.01601">17</a>,<a href="https://arxiv.org/pdf/2105.03404">22</a>, <a href="https://arxiv.org/pdf/2005.00743">23</a> or Fourier transform [18], the resultant models still produce encouraging performance.<br>最近，一些工作，17，18，19，20 发现，通过在Transformers中的自注意力为简单的operators 为MLP或者其他的 Fourier transform，模型仍然保持着良好的表现。</p><p>基于此这篇论文团队之前的工作在<a href="https://arxiv.org/pdf/2111.11418">24</a>这篇论文中他们提出了MetaFormer，为了更进一步的验证MetaFormer，Our goal is to push the limits of MetaFomer, based on which we may have a comprehensive picture of its capacity. 团队选取了最basic or common token mixers ，such as ，identity mapping or global random mixing， swparable convolution [6],[7] ，[8]  and vanilla self-attention [9]</p><p><img src="/pic/MetaFormer1.png" alt="模型结构图"><br><img src="/pic/MateFormer2.png" alt="模型实验图"><br><img src="/pic/MateFormer3.png" alt="模型实验图"></p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><p>MACs（Multiply-Accumulate Operations，乘加运算）是衡量神经网络计算复杂性的一个重要指标。它表示神经网络在进行一次前向传播（即推理）时所需的乘加运算次数。乘加运算是神经网络中最基本的操作，尤其在卷积层和全连接层中广泛使用。</p><h3 id="代码解析工程"><a href="#代码解析工程" class="headerlink" title="代码解析工程"></a>代码解析工程</h3><p>metaformer_baselines.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br><span class="line">1147</span><br><span class="line">1148</span><br><span class="line">1149</span><br><span class="line">1150</span><br><span class="line">1151</span><br><span class="line">1152</span><br><span class="line">1153</span><br><span class="line">1154</span><br><span class="line">1155</span><br><span class="line">1156</span><br><span class="line">1157</span><br><span class="line">1158</span><br><span class="line">1159</span><br><span class="line">1160</span><br><span class="line">1161</span><br><span class="line">1162</span><br><span class="line">1163</span><br><span class="line">1164</span><br><span class="line">1165</span><br><span class="line">1166</span><br><span class="line">1167</span><br><span class="line">1168</span><br><span class="line">1169</span><br><span class="line">1170</span><br><span class="line">1171</span><br><span class="line">1172</span><br><span class="line">1173</span><br><span class="line">1174</span><br><span class="line">1175</span><br><span class="line">1176</span><br><span class="line">1177</span><br><span class="line">1178</span><br><span class="line">1179</span><br><span class="line">1180</span><br><span class="line">1181</span><br><span class="line">1182</span><br><span class="line">1183</span><br><span class="line">1184</span><br><span class="line">1185</span><br><span class="line">1186</span><br><span class="line">1187</span><br><span class="line">1188</span><br><span class="line">1189</span><br><span class="line">1190</span><br><span class="line">1191</span><br><span class="line">1192</span><br><span class="line">1193</span><br><span class="line">1194</span><br><span class="line">1195</span><br><span class="line">1196</span><br><span class="line">1197</span><br><span class="line">1198</span><br><span class="line">1199</span><br><span class="line">1200</span><br><span class="line">1201</span><br><span class="line">1202</span><br><span class="line">1203</span><br><span class="line">1204</span><br><span class="line">1205</span><br><span class="line">1206</span><br><span class="line">1207</span><br><span class="line">1208</span><br><span class="line">1209</span><br><span class="line">1210</span><br><span class="line">1211</span><br><span class="line">1212</span><br><span class="line">1213</span><br><span class="line">1214</span><br><span class="line">1215</span><br><span class="line">1216</span><br><span class="line">1217</span><br><span class="line">1218</span><br><span class="line">1219</span><br><span class="line">1220</span><br><span class="line">1221</span><br><span class="line">1222</span><br><span class="line">1223</span><br><span class="line">1224</span><br><span class="line">1225</span><br><span class="line">1226</span><br><span class="line">1227</span><br><span class="line">1228</span><br><span class="line">1229</span><br><span class="line">1230</span><br><span class="line">1231</span><br><span class="line">1232</span><br><span class="line">1233</span><br><span class="line">1234</span><br><span class="line">1235</span><br><span class="line">1236</span><br><span class="line">1237</span><br><span class="line">1238</span><br><span class="line">1239</span><br><span class="line">1240</span><br><span class="line">1241</span><br><span class="line">1242</span><br><span class="line">1243</span><br><span class="line">1244</span><br><span class="line">1245</span><br><span class="line">1246</span><br><span class="line">1247</span><br><span class="line">1248</span><br><span class="line">1249</span><br><span class="line">1250</span><br><span class="line">1251</span><br><span class="line">1252</span><br><span class="line">1253</span><br><span class="line">1254</span><br><span class="line">1255</span><br><span class="line">1256</span><br><span class="line">1257</span><br><span class="line">1258</span><br><span class="line">1259</span><br><span class="line">1260</span><br><span class="line">1261</span><br><span class="line">1262</span><br><span class="line">1263</span><br><span class="line">1264</span><br><span class="line">1265</span><br><span class="line">1266</span><br><span class="line">1267</span><br><span class="line">1268</span><br><span class="line">1269</span><br><span class="line">1270</span><br><span class="line">1271</span><br><span class="line">1272</span><br><span class="line">1273</span><br><span class="line">1274</span><br><span class="line">1275</span><br><span class="line">1276</span><br><span class="line">1277</span><br><span class="line">1278</span><br><span class="line">1279</span><br><span class="line">1280</span><br><span class="line">1281</span><br><span class="line">1282</span><br><span class="line">1283</span><br><span class="line">1284</span><br><span class="line">1285</span><br><span class="line">1286</span><br><span class="line">1287</span><br><span class="line">1288</span><br><span class="line">1289</span><br><span class="line">1290</span><br><span class="line">1291</span><br><span class="line">1292</span><br><span class="line">1293</span><br><span class="line">1294</span><br><span class="line">1295</span><br><span class="line">1296</span><br><span class="line">1297</span><br><span class="line">1298</span><br><span class="line">1299</span><br><span class="line">1300</span><br><span class="line">1301</span><br><span class="line">1302</span><br><span class="line">1303</span><br><span class="line">1304</span><br><span class="line">1305</span><br><span class="line">1306</span><br><span class="line">1307</span><br><span class="line">1308</span><br><span class="line">1309</span><br><span class="line">1310</span><br><span class="line">1311</span><br><span class="line">1312</span><br><span class="line">1313</span><br><span class="line">1314</span><br><span class="line">1315</span><br><span class="line">1316</span><br><span class="line">1317</span><br><span class="line">1318</span><br><span class="line">1319</span><br><span class="line">1320</span><br><span class="line">1321</span><br><span class="line">1322</span><br><span class="line">1323</span><br><span class="line">1324</span><br><span class="line">1325</span><br><span class="line">1326</span><br><span class="line">1327</span><br><span class="line">1328</span><br><span class="line">1329</span><br><span class="line">1330</span><br><span class="line">1331</span><br><span class="line">1332</span><br><span class="line">1333</span><br><span class="line">1334</span><br><span class="line">1335</span><br><span class="line">1336</span><br><span class="line">1337</span><br><span class="line">1338</span><br><span class="line">1339</span><br><span class="line">1340</span><br><span class="line">1341</span><br><span class="line">1342</span><br><span class="line">1343</span><br><span class="line">1344</span><br><span class="line">1345</span><br><span class="line">1346</span><br><span class="line">1347</span><br><span class="line">1348</span><br><span class="line">1349</span><br><span class="line">1350</span><br><span class="line">1351</span><br><span class="line">1352</span><br><span class="line">1353</span><br><span class="line">1354</span><br><span class="line">1355</span><br><span class="line">1356</span><br><span class="line">1357</span><br><span class="line">1358</span><br><span class="line">1359</span><br><span class="line">1360</span><br><span class="line">1361</span><br><span class="line">1362</span><br><span class="line">1363</span><br><span class="line">1364</span><br><span class="line">1365</span><br><span class="line">1366</span><br><span class="line">1367</span><br><span class="line">1368</span><br><span class="line">1369</span><br><span class="line">1370</span><br><span class="line">1371</span><br><span class="line">1372</span><br><span class="line">1373</span><br><span class="line">1374</span><br><span class="line">1375</span><br><span class="line">1376</span><br><span class="line">1377</span><br><span class="line">1378</span><br><span class="line">1379</span><br><span class="line">1380</span><br><span class="line">1381</span><br><span class="line">1382</span><br><span class="line">1383</span><br><span class="line">1384</span><br><span class="line">1385</span><br><span class="line">1386</span><br><span class="line">1387</span><br><span class="line">1388</span><br><span class="line">1389</span><br><span class="line">1390</span><br><span class="line">1391</span><br><span class="line">1392</span><br><span class="line">1393</span><br><span class="line">1394</span><br><span class="line">1395</span><br><span class="line">1396</span><br><span class="line">1397</span><br><span class="line">1398</span><br><span class="line">1399</span><br><span class="line">1400</span><br><span class="line">1401</span><br><span class="line">1402</span><br><span class="line">1403</span><br><span class="line">1404</span><br><span class="line">1405</span><br><span class="line">1406</span><br><span class="line">1407</span><br><span class="line">1408</span><br><span class="line">1409</span><br><span class="line">1410</span><br><span class="line">1411</span><br><span class="line">1412</span><br><span class="line">1413</span><br><span class="line">1414</span><br><span class="line">1415</span><br><span class="line">1416</span><br><span class="line">1417</span><br><span class="line">1418</span><br><span class="line">1419</span><br><span class="line">1420</span><br><span class="line">1421</span><br><span class="line">1422</span><br><span class="line">1423</span><br><span class="line">1424</span><br><span class="line">1425</span><br><span class="line">1426</span><br><span class="line">1427</span><br><span class="line">1428</span><br><span class="line">1429</span><br><span class="line">1430</span><br><span class="line">1431</span><br><span class="line">1432</span><br><span class="line">1433</span><br><span class="line">1434</span><br><span class="line">1435</span><br><span class="line">1436</span><br><span class="line">1437</span><br><span class="line">1438</span><br><span class="line">1439</span><br><span class="line">1440</span><br><span class="line">1441</span><br><span class="line">1442</span><br><span class="line">1443</span><br><span class="line">1444</span><br><span class="line">1445</span><br><span class="line">1446</span><br><span class="line">1447</span><br><span class="line">1448</span><br><span class="line">1449</span><br><span class="line">1450</span><br><span class="line">1451</span><br><span class="line">1452</span><br><span class="line">1453</span><br><span class="line">1454</span><br><span class="line">1455</span><br><span class="line">1456</span><br><span class="line">1457</span><br><span class="line">1458</span><br><span class="line">1459</span><br><span class="line">1460</span><br><span class="line">1461</span><br><span class="line">1462</span><br><span class="line">1463</span><br><span class="line">1464</span><br><span class="line">1465</span><br><span class="line">1466</span><br><span class="line">1467</span><br><span class="line">1468</span><br><span class="line">1469</span><br><span class="line">1470</span><br><span class="line">1471</span><br><span class="line">1472</span><br><span class="line">1473</span><br><span class="line">1474</span><br><span class="line">1475</span><br><span class="line">1476</span><br><span class="line">1477</span><br><span class="line">1478</span><br><span class="line">1479</span><br><span class="line">1480</span><br><span class="line">1481</span><br><span class="line">1482</span><br><span class="line">1483</span><br><span class="line">1484</span><br><span class="line">1485</span><br><span class="line">1486</span><br><span class="line">1487</span><br><span class="line">1488</span><br><span class="line">1489</span><br><span class="line">1490</span><br><span class="line">1491</span><br><span class="line">1492</span><br><span class="line">1493</span><br><span class="line">1494</span><br><span class="line">1495</span><br><span class="line">1496</span><br><span class="line">1497</span><br><span class="line">1498</span><br><span class="line">1499</span><br><span class="line">1500</span><br><span class="line">1501</span><br><span class="line">1502</span><br><span class="line">1503</span><br><span class="line">1504</span><br><span class="line">1505</span><br><span class="line">1506</span><br><span class="line">1507</span><br><span class="line">1508</span><br><span class="line">1509</span><br><span class="line">1510</span><br><span class="line">1511</span><br><span class="line">1512</span><br><span class="line">1513</span><br><span class="line">1514</span><br><span class="line">1515</span><br><span class="line">1516</span><br><span class="line">1517</span><br><span class="line">1518</span><br><span class="line">1519</span><br><span class="line">1520</span><br><span class="line">1521</span><br><span class="line">1522</span><br><span class="line">1523</span><br><span class="line">1524</span><br><span class="line">1525</span><br><span class="line">1526</span><br><span class="line">1527</span><br><span class="line">1528</span><br><span class="line">1529</span><br><span class="line">1530</span><br><span class="line">1531</span><br><span class="line">1532</span><br><span class="line">1533</span><br><span class="line">1534</span><br><span class="line">1535</span><br><span class="line">1536</span><br><span class="line">1537</span><br><span class="line">1538</span><br><span class="line">1539</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Copyright 2022 Garena Online Private Limited</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="hljs-comment"># you may not use this file except in compliance with the License.</span><br><span class="hljs-comment"># You may obtain a copy of the License at</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Unless required by applicable law or agreed to in writing, software</span><br><span class="hljs-comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="hljs-comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="hljs-comment"># See the License for the specific language governing permissions and</span><br><span class="hljs-comment"># limitations under the License.</span><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">MetaFormer baselines including IdentityFormer, RandFormer, PoolFormerV2,</span><br><span class="hljs-string">ConvFormer and CAFormer.</span><br><span class="hljs-string">Some implementations are modified from timm (https://github.com/rwightman/pytorch-image-models).</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">from</span> timm.models.layers <span class="hljs-keyword">import</span> trunc_normal_, DropPath<br><span class="hljs-keyword">from</span> timm.models.registry <span class="hljs-keyword">import</span> register_model<br><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<br><span class="hljs-keyword">from</span> timm.models.layers.helpers <span class="hljs-keyword">import</span> to_2tuple<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_cfg</span>(<span class="hljs-params">url=<span class="hljs-string">&#x27;&#x27;</span>, **kwargs</span>):<br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&#x27;url&#x27;</span>: url,<br>        <span class="hljs-string">&#x27;num_classes&#x27;</span>: <span class="hljs-number">1000</span>, <span class="hljs-string">&#x27;input_size&#x27;</span>: (<span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>), <span class="hljs-string">&#x27;pool_size&#x27;</span>: <span class="hljs-literal">None</span>,<br>        <span class="hljs-string">&#x27;crop_pct&#x27;</span>: <span class="hljs-number">1.0</span>, <span class="hljs-string">&#x27;interpolation&#x27;</span>: <span class="hljs-string">&#x27;bicubic&#x27;</span>,<br>        <span class="hljs-string">&#x27;mean&#x27;</span>: IMAGENET_DEFAULT_MEAN, <span class="hljs-string">&#x27;std&#x27;</span>: IMAGENET_DEFAULT_STD, <span class="hljs-string">&#x27;classifier&#x27;</span>: <span class="hljs-string">&#x27;head&#x27;</span>,<br>        **kwargs<br>    &#125;<br><br><br>default_cfgs = &#123;<br>    <span class="hljs-string">&#x27;identityformer_s12&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/identityformer/identityformer_s12.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;identityformer_s24&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/identityformer/identityformer_s24.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;identityformer_s36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/identityformer/identityformer_s36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;identityformer_m36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/identityformer/identityformer_m36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;identityformer_m48&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/identityformer/identityformer_m48.pth&#x27;</span>),<br><br><br>    <span class="hljs-string">&#x27;randformer_s12&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/randformer/randformer_s12.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;randformer_s24&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/randformer/randformer_s24.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;randformer_s36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/randformer/randformer_s36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;randformer_m36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/randformer/randformer_m36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;randformer_m48&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/randformer/randformer_m48.pth&#x27;</span>),<br><br>    <span class="hljs-string">&#x27;poolformerv2_s12&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/poolformerv2/poolformerv2_s12.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;poolformerv2_s24&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/poolformerv2/poolformerv2_s24.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;poolformerv2_s36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/poolformerv2/poolformerv2_s36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;poolformerv2_m36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/poolformerv2/poolformerv2_m36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;poolformerv2_m48&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/poolformerv2/poolformerv2_m48.pth&#x27;</span>),<br><br><br><br>    <span class="hljs-string">&#x27;convformer_s18&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s18.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;convformer_s18_384&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s18_384.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;convformer_s18_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s18_in21ft1k.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;convformer_s18_384_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s18_384_in21ft1k.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;convformer_s18_in21k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s18_in21k.pth&#x27;</span>,<br>        num_classes=<span class="hljs-number">21841</span>),<br><br>    <span class="hljs-string">&#x27;convformer_s36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;convformer_s36_384&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s36_384.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;convformer_s36_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s36_in21ft1k.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;convformer_s36_384_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s36_384_in21ft1k.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;convformer_s36_in21k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_s36_in21k.pth&#x27;</span>,<br>        num_classes=<span class="hljs-number">21841</span>),<br><br>    <span class="hljs-string">&#x27;convformer_m36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_m36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;convformer_m36_384&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_m36_384.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;convformer_m36_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_m36_in21ft1k.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;convformer_m36_384_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_m36_384_in21ft1k.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;convformer_m36_in21k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_m36_in21k.pth&#x27;</span>,<br>        num_classes=<span class="hljs-number">21841</span>),<br><br>    <span class="hljs-string">&#x27;convformer_b36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_b36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;convformer_b36_384&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_b36_384.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;convformer_b36_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_b36_in21ft1k.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;convformer_b36_384_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_b36_384_in21ft1k.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;convformer_b36_in21k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/convformer/convformer_b36_in21k.pth&#x27;</span>,<br>        num_classes=<span class="hljs-number">21841</span>),<br><br><br>    <span class="hljs-string">&#x27;caformer_s18&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s18.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;caformer_s18_384&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s18_384.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;caformer_s18_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s18_in21ft1k.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;caformer_s18_384_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s18_384_in21ft1k.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;caformer_s18_in21k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s18_in21k.pth&#x27;</span>,<br>        num_classes=<span class="hljs-number">21841</span>),<br><br>    <span class="hljs-string">&#x27;caformer_s36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;caformer_s36_384&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s36_384.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;caformer_s36_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s36_in21ft1k.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;caformer_s36_384_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s36_384_in21ft1k.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;caformer_s36_in21k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_s36_in21k.pth&#x27;</span>,<br>        num_classes=<span class="hljs-number">21841</span>),<br><br>    <span class="hljs-string">&#x27;caformer_m36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_m36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;caformer_m36_384&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_m36_384.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;caformer_m36_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_m36_in21ft1k.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;caformer_m36_384_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_m36_384_in21ft1k.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;caformer_m36_in21k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_m36_in21k.pth&#x27;</span>,<br>        num_classes=<span class="hljs-number">21841</span>),<br><br>    <span class="hljs-string">&#x27;caformer_b36&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_b36.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;caformer_b36_384&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_b36_384.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;caformer_b36_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_b36_in21ft1k.pth&#x27;</span>),<br>    <span class="hljs-string">&#x27;caformer_b36_384_in21ft1k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_b36_384_in21ft1k.pth&#x27;</span>,<br>        input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>)),<br>    <span class="hljs-string">&#x27;caformer_b36_in21k&#x27;</span>: _cfg(<br>        url=<span class="hljs-string">&#x27;https://huggingface.co/sail/dl/resolve/main/caformer/caformer_b36_in21k.pth&#x27;</span>,<br>        num_classes=<span class="hljs-number">21841</span>),<br>&#125;<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Downsampling</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Downsampling implemented by a layer of convolution.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, </span><br><span class="hljs-params">        kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, </span><br><span class="hljs-params">        pre_norm=<span class="hljs-literal">None</span>, post_norm=<span class="hljs-literal">None</span>, pre_permute=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.pre_norm = pre_norm(in_channels) <span class="hljs-keyword">if</span> pre_norm <span class="hljs-keyword">else</span> nn.Identity()<br>        self.pre_permute = pre_permute<br>        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, <br>                              stride=stride, padding=padding)<br>        self.post_norm = post_norm(out_channels) <span class="hljs-keyword">if</span> post_norm <span class="hljs-keyword">else</span> nn.Identity()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.pre_norm(x)<br>        <span class="hljs-keyword">if</span> self.pre_permute:<br>            <span class="hljs-comment"># if take [B, H, W, C] as input, permute it to [B, C, H, W]</span><br>            x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        x = self.conv(x)<br>        x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># [B, C, H, W] -&gt; [B, H, W, C]</span><br>        x = self.post_norm(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Scale</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Scale vector by element multiplications.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, init_value=<span class="hljs-number">1.0</span>, trainable=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.scale = nn.Parameter(init_value * torch.ones(dim), requires_grad=trainable)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> x * self.scale<br>        <br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SquaredReLU</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Squared ReLU: https://arxiv.org/abs/2109.08668</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inplace=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.relu = nn.ReLU(inplace=inplace)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> torch.square(self.relu(x))<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">StarReLU</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    StarReLU: s * relu(x) ** 2 + b</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, scale_value=<span class="hljs-number">1.0</span>, bias_value=<span class="hljs-number">0.0</span>,</span><br><span class="hljs-params">        scale_learnable=<span class="hljs-literal">True</span>, bias_learnable=<span class="hljs-literal">True</span>, </span><br><span class="hljs-params">        mode=<span class="hljs-literal">None</span>, inplace=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.inplace = inplace<br>        self.relu = nn.ReLU(inplace=inplace)<br>        self.scale = nn.Parameter(scale_value * torch.ones(<span class="hljs-number">1</span>),<br>            requires_grad=scale_learnable)<br>        self.bias = nn.Parameter(bias_value * torch.ones(<span class="hljs-number">1</span>),<br>            requires_grad=bias_learnable)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> self.scale * self.relu(x)**<span class="hljs-number">2</span> + self.bias<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Attention</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Vanilla self-attention from Transformer: https://arxiv.org/abs/1706.03762.</span><br><span class="hljs-string">    Modified from timm.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, head_dim=<span class="hljs-number">32</span>, num_heads=<span class="hljs-literal">None</span>, qkv_bias=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        attn_drop=<span class="hljs-number">0.</span>, proj_drop=<span class="hljs-number">0.</span>, proj_bias=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        self.head_dim = head_dim<br>        self.scale = head_dim ** -<span class="hljs-number">0.5</span><br><br>        self.num_heads = num_heads <span class="hljs-keyword">if</span> num_heads <span class="hljs-keyword">else</span> dim // head_dim<br>        <span class="hljs-keyword">if</span> self.num_heads == <span class="hljs-number">0</span>:<br>            self.num_heads = <span class="hljs-number">1</span><br>        <br>        self.attention_dim = self.num_heads * self.head_dim<br><br>        self.qkv = nn.Linear(dim, self.attention_dim * <span class="hljs-number">3</span>, bias=qkv_bias)<br>        self.attn_drop = nn.Dropout(attn_drop)<br>        self.proj = nn.Linear(self.attention_dim, dim, bias=proj_bias)<br>        self.proj_drop = nn.Dropout(proj_drop)<br><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        B, H, W, C = x.shape<br>        N = H * W<br>        qkv = self.qkv(x).reshape(B, N, <span class="hljs-number">3</span>, self.num_heads, self.head_dim).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>)<br>        q, k, v = qkv.unbind(<span class="hljs-number">0</span>)   <span class="hljs-comment"># make torchscript happy (cannot use tensor as tuple)</span><br><br>        attn = (q @ k.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) * self.scale<br>        attn = attn.softmax(dim=-<span class="hljs-number">1</span>)<br>        attn = self.attn_drop(attn)<br><br>        x = (attn @ v).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).reshape(B, H, W, self.attention_dim)<br>        x = self.proj(x)<br>        x = self.proj_drop(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RandomMixing</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_tokens=<span class="hljs-number">196</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.random_matrix = nn.parameter.Parameter(<br>            data=torch.softmax(torch.rand(num_tokens, num_tokens), dim=-<span class="hljs-number">1</span>), <br>            requires_grad=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        B, H, W, C = x.shape<br>        x = x.reshape(B, H*W, C)<br>        x = torch.einsum(<span class="hljs-string">&#x27;mn, bnc -&gt; bmc&#x27;</span>, self.random_matrix, x)<br>        x = x.reshape(B, H, W, C)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNormGeneral</span>(nn.Module):<br>    <span class="hljs-string">r&quot;&quot;&quot; General LayerNorm for different situations.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        affine_shape (int, list or tuple): The shape of affine weight and bias.</span><br><span class="hljs-string">            Usually the affine_shape=C, but in some implementation, like torch.nn.LayerNorm,</span><br><span class="hljs-string">            the affine_shape is the same as normalized_dim by default. </span><br><span class="hljs-string">            To adapt to different situations, we offer this argument here.</span><br><span class="hljs-string">        normalized_dim (tuple or list): Which dims to compute mean and variance. </span><br><span class="hljs-string">        scale (bool): Flag indicates whether to use scale or not.</span><br><span class="hljs-string">        bias (bool): Flag indicates whether to use scale or not.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        We give several examples to show how to specify the arguments.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        LayerNorm (https://arxiv.org/abs/1607.06450):</span><br><span class="hljs-string">            For input shape of (B, *, C) like (B, N, C) or (B, H, W, C),</span><br><span class="hljs-string">                affine_shape=C, normalized_dim=(-1, ), scale=True, bias=True;</span><br><span class="hljs-string">            For input shape of (B, C, H, W),</span><br><span class="hljs-string">                affine_shape=(C, 1, 1), normalized_dim=(1, ), scale=True, bias=True.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Modified LayerNorm (https://arxiv.org/abs/2111.11418)</span><br><span class="hljs-string">            that is idental to partial(torch.nn.GroupNorm, num_groups=1):</span><br><span class="hljs-string">            For input shape of (B, N, C),</span><br><span class="hljs-string">                affine_shape=C, normalized_dim=(1, 2), scale=True, bias=True;</span><br><span class="hljs-string">            For input shape of (B, H, W, C),</span><br><span class="hljs-string">                affine_shape=C, normalized_dim=(1, 2, 3), scale=True, bias=True;</span><br><span class="hljs-string">            For input shape of (B, C, H, W),</span><br><span class="hljs-string">                affine_shape=(C, 1, 1), normalized_dim=(1, 2, 3), scale=True, bias=True.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        For the several metaformer baslines,</span><br><span class="hljs-string">            IdentityFormer, RandFormer and PoolFormerV2 utilize Modified LayerNorm without bias (bias=False);</span><br><span class="hljs-string">            ConvFormer and CAFormer utilizes LayerNorm without bias (bias=False).</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, affine_shape=<span class="hljs-literal">None</span>, normalized_dim=(<span class="hljs-params">-<span class="hljs-number">1</span>, </span>), scale=<span class="hljs-literal">True</span>, </span><br><span class="hljs-params">        bias=<span class="hljs-literal">True</span>, eps=<span class="hljs-number">1e-5</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.normalized_dim = normalized_dim<br>        self.use_scale = scale<br>        self.use_bias = bias<br>        self.weight = nn.Parameter(torch.ones(affine_shape)) <span class="hljs-keyword">if</span> scale <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        self.bias = nn.Parameter(torch.zeros(affine_shape)) <span class="hljs-keyword">if</span> bias <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        self.eps = eps<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        c = x - x.mean(self.normalized_dim, keepdim=<span class="hljs-literal">True</span>)<br>        s = c.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean(self.normalized_dim, keepdim=<span class="hljs-literal">True</span>)<br>        x = c / torch.sqrt(s + self.eps)<br>        <span class="hljs-keyword">if</span> self.use_scale:<br>            x = x * self.weight<br>        <span class="hljs-keyword">if</span> self.use_bias:<br>            x = x + self.bias<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNormWithoutBias</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Equal to partial(LayerNormGeneral, bias=False) but faster, </span><br><span class="hljs-string">    because it directly utilizes otpimized F.layer_norm</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, normalized_shape, eps=<span class="hljs-number">1e-5</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.eps = eps<br>        self.bias = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(normalized_shape, <span class="hljs-built_in">int</span>):<br>            normalized_shape = (normalized_shape,)<br>        self.weight = nn.Parameter(torch.ones(normalized_shape))<br>        self.normalized_shape = normalized_shape<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> F.layer_norm(x, self.normalized_shape, weight=self.weight, bias=self.bias, eps=self.eps)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SepConv</span>(nn.Module):<br>    <span class="hljs-string">r&quot;&quot;&quot;</span><br><span class="hljs-string">    Inverted separable convolution from MobileNetV2: https://arxiv.org/abs/1801.04381.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, expansion_ratio=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">        act1_layer=StarReLU, act2_layer=nn.Identity, </span><br><span class="hljs-params">        bias=<span class="hljs-literal">False</span>, kernel_size=<span class="hljs-number">7</span>, padding=<span class="hljs-number">3</span>,</span><br><span class="hljs-params">        **kwargs, </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        med_channels = <span class="hljs-built_in">int</span>(expansion_ratio * dim)<br>        self.pwconv1 = nn.Linear(dim, med_channels, bias=bias)<br>        self.act1 = act1_layer()<br>        self.dwconv = nn.Conv2d(<br>            med_channels, med_channels, kernel_size=kernel_size,<br>            padding=padding, groups=med_channels, bias=bias) <span class="hljs-comment"># depthwise conv</span><br>        self.act2 = act2_layer()<br>        self.pwconv2 = nn.Linear(med_channels, dim, bias=bias)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.pwconv1(x)<br>        x = self.act1(x)<br>        x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        x = self.dwconv(x)<br>        x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        x = self.act2(x)<br>        x = self.pwconv2(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Pooling</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Implementation of pooling for PoolFormer: https://arxiv.org/abs/2111.11418</span><br><span class="hljs-string">    Modfiled for [B, H, W, C] input</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, pool_size=<span class="hljs-number">3</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.pool = nn.AvgPool2d(<br>            pool_size, stride=<span class="hljs-number">1</span>, padding=pool_size//<span class="hljs-number">2</span>, count_include_pad=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        y = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        y = self.pool(y)<br>        y = y.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> y - x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Mlp</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot; MLP as used in MetaFormer models, eg Transformer, MLP-Mixer, PoolFormer, MetaFormer baslines and related networks.</span><br><span class="hljs-string">    Mostly copied from timm.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, mlp_ratio=<span class="hljs-number">4</span>, out_features=<span class="hljs-literal">None</span>, act_layer=StarReLU, drop=<span class="hljs-number">0.</span>, bias=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        in_features = dim<br>        out_features = out_features <span class="hljs-keyword">or</span> in_features<br>        hidden_features = <span class="hljs-built_in">int</span>(mlp_ratio * in_features)<br>        drop_probs = to_2tuple(drop)<br><br>        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)<br>        self.act = act_layer()<br>        self.drop1 = nn.Dropout(drop_probs[<span class="hljs-number">0</span>])<br>        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)<br>        self.drop2 = nn.Dropout(drop_probs[<span class="hljs-number">1</span>])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.fc1(x)<br>        x = self.act(x)<br>        x = self.drop1(x)<br>        x = self.fc2(x)<br>        x = self.drop2(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MlpHead</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot; MLP classification head</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, num_classes=<span class="hljs-number">1000</span>, mlp_ratio=<span class="hljs-number">4</span>, act_layer=SquaredReLU,</span><br><span class="hljs-params">        norm_layer=nn.LayerNorm, head_dropout=<span class="hljs-number">0.</span>, bias=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        hidden_features = <span class="hljs-built_in">int</span>(mlp_ratio * dim)<br>        self.fc1 = nn.Linear(dim, hidden_features, bias=bias)<br>        self.act = act_layer()<br>        self.norm = norm_layer(hidden_features)<br>        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias)<br>        self.head_dropout = nn.Dropout(head_dropout)<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.fc1(x)<br>        x = self.act(x)<br>        x = self.norm(x)<br>        x = self.head_dropout(x)<br>        x = self.fc2(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MetaFormerBlock</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Implementation of one MetaFormer block.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim,</span><br><span class="hljs-params">                 token_mixer=nn.Identity, mlp=Mlp,</span><br><span class="hljs-params">                 norm_layer=nn.LayerNorm,</span><br><span class="hljs-params">                 drop=<span class="hljs-number">0.</span>, drop_path=<span class="hljs-number">0.</span>,</span><br><span class="hljs-params">                 layer_scale_init_value=<span class="hljs-literal">None</span>, res_scale_init_value=<span class="hljs-literal">None</span></span><br><span class="hljs-params">                 </span>):<br><br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        self.norm1 = norm_layer(dim)<br>        self.token_mixer = token_mixer(dim=dim, drop=drop)<br>        self.drop_path1 = DropPath(drop_path) <span class="hljs-keyword">if</span> drop_path &gt; <span class="hljs-number">0.</span> <span class="hljs-keyword">else</span> nn.Identity()<br>        self.layer_scale1 = Scale(dim=dim, init_value=layer_scale_init_value) \<br>            <span class="hljs-keyword">if</span> layer_scale_init_value <span class="hljs-keyword">else</span> nn.Identity()<br>        self.res_scale1 = Scale(dim=dim, init_value=res_scale_init_value) \<br>            <span class="hljs-keyword">if</span> res_scale_init_value <span class="hljs-keyword">else</span> nn.Identity()<br><br>        self.norm2 = norm_layer(dim)<br>        self.mlp = mlp(dim=dim, drop=drop)<br>        self.drop_path2 = DropPath(drop_path) <span class="hljs-keyword">if</span> drop_path &gt; <span class="hljs-number">0.</span> <span class="hljs-keyword">else</span> nn.Identity()<br>        self.layer_scale2 = Scale(dim=dim, init_value=layer_scale_init_value) \<br>            <span class="hljs-keyword">if</span> layer_scale_init_value <span class="hljs-keyword">else</span> nn.Identity()<br>        self.res_scale2 = Scale(dim=dim, init_value=res_scale_init_value) \<br>            <span class="hljs-keyword">if</span> res_scale_init_value <span class="hljs-keyword">else</span> nn.Identity()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.res_scale1(x) + \<br>            self.layer_scale1(<br>                self.drop_path1(<br>                    self.token_mixer(self.norm1(x))<br>                )<br>            )<br>        x = self.res_scale2(x) + \<br>            self.layer_scale2(<br>                self.drop_path2(<br>                    self.mlp(self.norm2(x))<br>                )<br>            )<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-string">r&quot;&quot;&quot;</span><br><span class="hljs-string">downsampling (stem) for the first stage is a layer of conv with k7, s4 and p2</span><br><span class="hljs-string">downsamplings for the last 3 stages is a layer of conv with k3, s2 and p1</span><br><span class="hljs-string">DOWNSAMPLE_LAYERS_FOUR_STAGES format: [Downsampling, Downsampling, Downsampling, Downsampling]</span><br><span class="hljs-string">use `partial` to specify some arguments</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>DOWNSAMPLE_LAYERS_FOUR_STAGES = [partial(Downsampling,<br>            kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">2</span>,<br>            post_norm=partial(LayerNormGeneral, bias=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>            )] + \<br>            [partial(Downsampling,<br>                kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, <br>                pre_norm=partial(LayerNormGeneral, bias=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>), pre_permute=<span class="hljs-literal">True</span><br>            )]*<span class="hljs-number">3</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MetaFormer</span>(nn.Module):<br>    <span class="hljs-string">r&quot;&quot;&quot; MetaFormer</span><br><span class="hljs-string">        A PyTorch impl of : `MetaFormer Baselines for Vision`  -</span><br><span class="hljs-string">          https://arxiv.org/abs/2210.13452</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        in_chans (int): Number of input image channels. Default: 3.</span><br><span class="hljs-string">        num_classes (int): Number of classes for classification head. Default: 1000.</span><br><span class="hljs-string">        depths (list or tuple): Number of blocks at each stage. Default: [2, 2, 6, 2].</span><br><span class="hljs-string">        dims (int): Feature dimension at each stage. Default: [64, 128, 320, 512].</span><br><span class="hljs-string">        downsample_layers: (list or tuple): Downsampling layers before each stage.</span><br><span class="hljs-string">        token_mixers (list, tuple or token_fcn): Token mixer for each stage. Default: nn.Identity.</span><br><span class="hljs-string">        mlps (list, tuple or mlp_fcn): Mlp for each stage. Default: Mlp.</span><br><span class="hljs-string">        norm_layers (list, tuple or norm_fcn): Norm layers for each stage. Default: partial(LayerNormGeneral, eps=1e-6, bias=False).</span><br><span class="hljs-string">        drop_path_rate (float): Stochastic depth rate. Default: 0.</span><br><span class="hljs-string">        head_dropout (float): dropout for MLP classifier. Default: 0.</span><br><span class="hljs-string">        layer_scale_init_values (list, tuple, float or None): Init value for Layer Scale. Default: None.</span><br><span class="hljs-string">            None means not use the layer scale. Form: https://arxiv.org/abs/2103.17239.</span><br><span class="hljs-string">        res_scale_init_values (list, tuple, float or None): Init value for Layer Scale. Default: [None, None, 1.0, 1.0].</span><br><span class="hljs-string">            None means not use the layer scale. From: https://arxiv.org/abs/2110.09456.</span><br><span class="hljs-string">        output_norm: norm before classifier head. Default: partial(nn.LayerNorm, eps=1e-6).</span><br><span class="hljs-string">        head_fn: classification head. Default: nn.Linear.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_chans=<span class="hljs-number">3</span>, num_classes=<span class="hljs-number">1000</span>, </span><br><span class="hljs-params">                 depths=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">2</span>],</span><br><span class="hljs-params">                 dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],</span><br><span class="hljs-params">                 downsample_layers=DOWNSAMPLE_LAYERS_FOUR_STAGES,</span><br><span class="hljs-params">                 token_mixers=nn.Identity,</span><br><span class="hljs-params">                 mlps=Mlp,</span><br><span class="hljs-params">                 norm_layers=partial(<span class="hljs-params">LayerNormWithoutBias, eps=<span class="hljs-number">1e-6</span></span>), <span class="hljs-comment"># partial(LayerNormGeneral, eps=1e-6, bias=False),</span></span><br><span class="hljs-params">                 drop_path_rate=<span class="hljs-number">0.</span>,</span><br><span class="hljs-params">                 head_dropout=<span class="hljs-number">0.0</span>, </span><br><span class="hljs-params">                 layer_scale_init_values=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 res_scale_init_values=[<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>],</span><br><span class="hljs-params">                 output_norm=partial(<span class="hljs-params">nn.LayerNorm, eps=<span class="hljs-number">1e-6</span></span>), </span><br><span class="hljs-params">                 head_fn=nn.Linear,</span><br><span class="hljs-params">                 **kwargs,</span><br><span class="hljs-params">                 </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.num_classes = num_classes<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(depths, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            depths = [depths] <span class="hljs-comment"># it means the model has only one stage</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(dims, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            dims = [dims]<br><br>        num_stage = <span class="hljs-built_in">len</span>(depths)<br>        self.num_stage = num_stage<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(downsample_layers, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            downsample_layers = [downsample_layers] * num_stage<br>        down_dims = [in_chans] + dims<br>        self.downsample_layers = nn.ModuleList(<br>            [downsample_layers[i](down_dims[i], down_dims[i+<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_stage)]<br>        )<br>        <br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(token_mixers, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            token_mixers = [token_mixers] * num_stage<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(mlps, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            mlps = [mlps] * num_stage<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(norm_layers, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            norm_layers = [norm_layers] * num_stage<br>        <br>        dp_rates=[x.item() <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> torch.linspace(<span class="hljs-number">0</span>, drop_path_rate, <span class="hljs-built_in">sum</span>(depths))]<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(layer_scale_init_values, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            layer_scale_init_values = [layer_scale_init_values] * num_stage<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(res_scale_init_values, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            res_scale_init_values = [res_scale_init_values] * num_stage<br><br>        self.stages = nn.ModuleList() <span class="hljs-comment"># each stage consists of multiple metaformer blocks</span><br>        cur = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_stage):<br>            stage = nn.Sequential(<br>                *[MetaFormerBlock(dim=dims[i],<br>                token_mixer=token_mixers[i],<br>                mlp=mlps[i],<br>                norm_layer=norm_layers[i],<br>                drop_path=dp_rates[cur + j],<br>                layer_scale_init_value=layer_scale_init_values[i],<br>                res_scale_init_value=res_scale_init_values[i],<br>                ) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(depths[i])]<br>            )<br>            self.stages.append(stage)<br>            cur += depths[i]<br><br>        self.norm = output_norm(dims[-<span class="hljs-number">1</span>])<br><br>        <span class="hljs-keyword">if</span> head_dropout &gt; <span class="hljs-number">0.0</span>:<br>            self.head = head_fn(dims[-<span class="hljs-number">1</span>], num_classes, head_dropout=head_dropout)<br>        <span class="hljs-keyword">else</span>:<br>            self.head = head_fn(dims[-<span class="hljs-number">1</span>], num_classes)<br><br>        self.apply(self._init_weights)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_weights</span>(<span class="hljs-params">self, m</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, (nn.Conv2d, nn.Linear)):<br>            trunc_normal_(m.weight, std=<span class="hljs-number">.02</span>)<br>            <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br><br><span class="hljs-meta">    @torch.jit.ignore</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">no_weight_decay</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;norm&#x27;</span>&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_features</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_stage):<br>            x = self.downsample_layers[i](x)<br>            x = self.stages[i](x)<br>        <span class="hljs-keyword">return</span> self.norm(x.mean([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])) <span class="hljs-comment"># (B, H, W, C) -&gt; (B, C)</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.forward_features(x)<br>        x = self.head(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">identityformer_s12</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">2</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=nn.Identity,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;identityformer_s12&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">identityformer_s24</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">12</span>, <span class="hljs-number">4</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=nn.Identity,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;identityformer_s24&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">identityformer_s36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">18</span>, <span class="hljs-number">6</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=nn.Identity,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;identityformer_s36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">identityformer_m36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">18</span>, <span class="hljs-number">6</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">768</span>],<br>        token_mixers=nn.Identity,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;identityformer_m36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">identityformer_m48</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">24</span>, <span class="hljs-number">8</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">768</span>],<br>        token_mixers=nn.Identity,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;identityformer_m48&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">randformer_s12</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">2</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[nn.Identity, nn.Identity, RandomMixing, partial(RandomMixing, num_tokens=<span class="hljs-number">49</span>)],<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;randformer_s12&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">randformer_s24</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">12</span>, <span class="hljs-number">4</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[nn.Identity, nn.Identity, RandomMixing, partial(RandomMixing, num_tokens=<span class="hljs-number">49</span>)],<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;randformer_s24&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">randformer_s36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">18</span>, <span class="hljs-number">6</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[nn.Identity, nn.Identity, RandomMixing, partial(RandomMixing, num_tokens=<span class="hljs-number">49</span>)],<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;randformer_s36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">randformer_m36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">18</span>, <span class="hljs-number">6</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">768</span>],<br>        token_mixers=[nn.Identity, nn.Identity, RandomMixing, partial(RandomMixing, num_tokens=<span class="hljs-number">49</span>)],<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;randformer_m36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">randformer_m48</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">24</span>, <span class="hljs-number">8</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">768</span>],<br>        token_mixers=[nn.Identity, nn.Identity, RandomMixing, partial(RandomMixing, num_tokens=<span class="hljs-number">49</span>)],<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;randformer_m48&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">poolformerv2_s12</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">2</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=Pooling,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;poolformerv2_s12&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">poolformerv2_s24</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">12</span>, <span class="hljs-number">4</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=Pooling,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;poolformerv2_s24&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">poolformerv2_s36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">18</span>, <span class="hljs-number">6</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=Pooling,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;poolformerv2_s36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">poolformerv2_m36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">18</span>, <span class="hljs-number">6</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">768</span>],<br>        token_mixers=Pooling,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;poolformerv2_m36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">poolformerv2_m48</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">24</span>, <span class="hljs-number">8</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">768</span>],<br>        token_mixers=Pooling,<br>        norm_layers=partial(LayerNormGeneral, normalized_dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), eps=<span class="hljs-number">1e-6</span>, bias=<span class="hljs-literal">False</span>),<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;poolformerv2_m48&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s18</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s18&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s18_384</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s18_384&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s18_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s18_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s18_384_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s18_384_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s18_in21k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s18_in21k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s36_384</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s36_384&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s36_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s36_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s36_384_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s36_384_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_s36_in21k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_s36_in21k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_m36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_m36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_m36_384</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_m36_384&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_m36_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_m36_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_m36_384_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_m36_384_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_m36_in21k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_m36_in21k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_b36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_b36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_b36_384</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_b36_384&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_b36_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_b36_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_b36_384_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_b36_384_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convformer_b36_in21k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=SepConv,<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;convformer_b36_in21k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s18</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s18&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s18_384</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s18_384&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s18_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s18_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s18_384_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s18_384_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s18_in21k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s18_in21k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s36_384</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s36_384&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s36_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s36_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s36_384_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s36_384_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_s36_in21k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_s36_in21k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_m36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_m36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_m36_384</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_m36_384&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_m36_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_m36_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_m36_384_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_m36_384_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_m364_in21k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">576</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_m364_in21k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_b36</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_b36&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_b36_384</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_b36_384&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_b36_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_b36_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_b36_384_in21ft1k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_b36_384_in21ft1k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-meta">@register_model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">caformer_b36_in21k</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    model = MetaFormer(<br>        depths=[<span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">3</span>],<br>        dims=[<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">768</span>],<br>        token_mixers=[SepConv, SepConv, Attention, Attention],<br>        head_fn=MlpHead,<br>        **kwargs)<br>    model.default_cfg = default_cfgs[<span class="hljs-string">&#x27;caformer_b36_in21k&#x27;</span>]<br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = torch.hub.load_state_dict_from_url(<br>            url= model.default_cfg[<span class="hljs-string">&#x27;url&#x27;</span>], map_location=<span class="hljs-string">&quot;cpu&quot;</span>, check_hash=<span class="hljs-literal">True</span>)<br>        model.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> model<br><br></code></pre></td></tr></table></figure><p>训练代码</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs css">import os<br>import numpy as np<br>import torch<br>import torch<span class="hljs-selector-class">.nn</span> as nn<br>import torch<span class="hljs-selector-class">.optim</span> as optim<br><span class="hljs-selector-tag">from</span> torch<span class="hljs-selector-class">.utils</span><span class="hljs-selector-class">.data</span> import DataLoader<br><span class="hljs-selector-tag">from</span> torchvision import transforms<br><span class="hljs-selector-tag">from</span> tqdm import tqdm<br><span class="hljs-selector-tag">from</span> dataset import EEGDataset, EEGDataset_Batch_normal<br><span class="hljs-selector-tag">from</span> net import IntegratedNet<br><span class="hljs-selector-tag">from</span> sklearn<span class="hljs-selector-class">.metrics</span> import classification_report<br><span class="hljs-selector-tag">from</span> matplotlib import pyplot as plt<br><br># 定义归一化操作<br>def normalize(data):<br>    mean = np.<span class="hljs-built_in">mean</span>(data)<br>    std = np.<span class="hljs-built_in">std</span>(data)<br>    return (data - mean) / std<br><br>transform = transforms.<span class="hljs-built_in">Compose</span>([<br>    transforms.<span class="hljs-built_in">Lambda</span>(normalize),  # 使用Lambda函数应用自定义归一化操作<br>    transforms.<span class="hljs-built_in">ToTensor</span>()<br>])<br><br>def <span class="hljs-built_in">train_identityformer_model</span>(model, model_name, num_epochs=<span class="hljs-number">100</span>, num_classes=<span class="hljs-number">3</span>, batch_size=<span class="hljs-number">16</span>, learning_rate=<span class="hljs-number">0.0001</span>, w_wight=<span class="hljs-number">2560</span>, chennal=<span class="hljs-number">32</span>):<br>    # Checking CUDA availability<br>    if torch.cuda.<span class="hljs-built_in">is_available</span>():<br>        device = torch.<span class="hljs-built_in">device</span>(<span class="hljs-string">&quot;cuda&quot;</span>)<br>    else:<br>        device = torch.<span class="hljs-built_in">device</span>(<span class="hljs-string">&quot;cpu&quot;</span>)<br>    m = nn.<span class="hljs-built_in">Softmax</span>(dim=<span class="hljs-number">1</span>)  # 只对样本的维度做softmax<br>    # Creating datasets and data loaders<br>    train_dataset = <span class="hljs-built_in">EEGDataset</span>(csv_file=<span class="hljs-string">&#x27;train_data.csv&#x27;</span>, transform=transform)<br>    test_dataset = <span class="hljs-built_in">EEGDataset</span>(csv_file=<span class="hljs-string">&#x27;test_data.csv&#x27;</span>, transform=transform)<br><br>    train_loader = <span class="hljs-built_in">DataLoader</span>(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)<br>    test_loader = <span class="hljs-built_in">DataLoader</span>(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)<br><br>    model.<span class="hljs-built_in">to</span>(device)<br>    loss_fn = nn.<span class="hljs-built_in">CrossEntropyLoss</span>()<br>    optimizer = optim.<span class="hljs-built_in">RMSprop</span>(model.<span class="hljs-built_in">parameters</span>(), lr=learning_rate)<br><br>    save_dir = os.path.<span class="hljs-built_in">join</span>(<span class="hljs-string">&#x27;model&#x27;</span>, model_name)<br>    os.<span class="hljs-built_in">makedirs</span>(save_dir, exist_ok=True)<br><br>    best_val_acc = <span class="hljs-number">0</span><br>    best_model_path = os.path.<span class="hljs-built_in">join</span>(save_dir, <span class="hljs-string">&quot;&#123;&#125;_best_model.pth&quot;</span>.<span class="hljs-built_in">format</span>(model_name))<br><br>    # 训练<br>    train_loss_arr = []<br>    train_acc_arr = []<br>    val_loss_arr = []<br>    val_acc_arr = []<br><br>    for epoch in <span class="hljs-built_in">range</span>(num_epochs):<br>        train_loss_total = <span class="hljs-number">0</span>  # 所有batch的loss累加值<br>        train_acc_total = <span class="hljs-number">0</span>   # 所有batch的acc累加值`<br>        val_loss_total = <span class="hljs-number">0</span><br>        val_acc_total = <span class="hljs-number">0</span><br><br>        model.<span class="hljs-built_in">train</span>()    # 标志模型的模式是什么，因为dropout只在训练时启用<br>        for i, (train_x, train_y) in <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">tqdm</span>(train_loader, desc=f<span class="hljs-string">&quot;Epoch &#123;epoch+1&#125;/&#123;num_epochs&#125;&quot;</span>)):<br>            train_x = train_x.<span class="hljs-built_in">to</span>(device)<br>            train_y = train_y.<span class="hljs-built_in">to</span>(device)<br>            train_x = train_x.<span class="hljs-built_in">unsqueeze</span>(<span class="hljs-number">1</span>)<br>            train_x = train_x.<span class="hljs-built_in">view</span>(batch_size, <span class="hljs-number">1</span>, chennal, w_wight)<br>            # 前向传播<br>            train_y_pred = <span class="hljs-built_in">model</span>(train_x)<br>            train_loss = <span class="hljs-built_in">loss_fn</span>(train_y_pred, train_y)<br><br>            # 通过模型每个样本得到<span class="hljs-number">3</span>个实数值（train_y_pred）,通过softmax将实数值转换成概率值，通过max取概率最大的下标，最后用下标和标签做比较<br>            train_acc = (<span class="hljs-built_in">m</span>(train_y_pred).<span class="hljs-built_in">max</span>(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] == train_y).<span class="hljs-built_in">sum</span>()/train_y.shape[<span class="hljs-number">0</span>]<br>            train_loss_total += train_loss.data.<span class="hljs-built_in">item</span>()<br>            train_acc_total += train_acc.data.<span class="hljs-built_in">item</span>()<br>            # 反向传播<br>            train_loss.<span class="hljs-built_in">backward</span>()<br>            # 梯度下降<br>            optimizer.<span class="hljs-built_in">step</span>()<br>            optimizer.<span class="hljs-built_in">zero_grad</span>()<br><br>            # <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch:&#123;&#125; train_loss:&#123;&#125; train_acc:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, train_loss.data.<span class="hljs-built_in">item</span>(), train_acc.data.<span class="hljs-built_in">item</span>()))<br><br>        train_loss_arr.<span class="hljs-built_in">append</span>(train_loss_total / <span class="hljs-built_in">len</span>(train_loader))  # 平均值<br>        train_acc_arr.<span class="hljs-built_in">append</span>(train_acc_total / <span class="hljs-built_in">len</span>(train_loader))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch:&#123;&#125; train_loss:&#123;&#125; train_acc:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, train_loss_arr[-<span class="hljs-number">1</span>], train_acc_arr[-<span class="hljs-number">1</span>]))<br>        # 测试集<br>        model.<span class="hljs-built_in">eval</span>()<br>        for j, (val_x, val_y) in <span class="hljs-built_in">enumerate</span>(test_loader):<br>            val_x = val_x.<span class="hljs-built_in">to</span>(device)<br>            val_y = val_y.<span class="hljs-built_in">to</span>(device)<br>            val_x = val_x.<span class="hljs-built_in">unsqueeze</span>(<span class="hljs-number">1</span>)<br>            val_x = val_x.<span class="hljs-built_in">view</span>(batch_size, <span class="hljs-number">1</span>, chennal, w_wight)<br>            # 前向传播<br>            val_y_pred = <span class="hljs-built_in">model</span>(val_x)<br>            val_loss = <span class="hljs-built_in">loss_fn</span>(val_y_pred, val_y)<br>            val_acc = (<span class="hljs-built_in">m</span>(val_y_pred).<span class="hljs-built_in">max</span>(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] == val_y).<span class="hljs-built_in">sum</span>()/val_y.shape[<span class="hljs-number">0</span>]<br>            val_loss_total += val_loss.data.<span class="hljs-built_in">item</span>()<br>            val_acc_total += val_acc.data.<span class="hljs-built_in">item</span>()<br><br>        val_loss_arr.<span class="hljs-built_in">append</span>(val_loss_total / <span class="hljs-built_in">len</span>(test_loader)) # 平均值<br>        val_acc_arr.<span class="hljs-built_in">append</span>(val_acc_total / <span class="hljs-built_in">len</span>(test_loader))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch:&#123;&#125; val_loss:&#123;&#125; val_acc:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, val_loss_arr[-<span class="hljs-number">1</span>], val_acc_arr[-<span class="hljs-number">1</span>]))<br><br>        # 保存最佳模型<br>        if val_acc_arr[-<span class="hljs-number">1</span>] &gt; best_val_acc:<br>            best_val_acc = val_acc_arr[-<span class="hljs-number">1</span>]<br>            torch.<span class="hljs-built_in">save</span>(model.<span class="hljs-built_in">state_dict</span>(), best_model_path)<br><br>        # 保存训练和验证过程中的loss和acc<br>    np.<span class="hljs-built_in">save</span>(os.path.<span class="hljs-built_in">join</span>(save_dir, f<span class="hljs-string">&quot;&#123;model_name&#125;_train_loss.npy&quot;</span>), np.<span class="hljs-built_in">array</span>(train_loss_arr))<br>    np.<span class="hljs-built_in">save</span>(os.path.<span class="hljs-built_in">join</span>(save_dir, f<span class="hljs-string">&quot;&#123;model_name&#125;_train_acc.npy&quot;</span>), np.<span class="hljs-built_in">array</span>(train_acc_arr))<br>    np.<span class="hljs-built_in">save</span>(os.path.<span class="hljs-built_in">join</span>(save_dir, f<span class="hljs-string">&quot;&#123;model_name&#125;_val_loss.npy&quot;</span>), np.<span class="hljs-built_in">array</span>(val_loss_arr))<br>    np.<span class="hljs-built_in">save</span>(os.path.<span class="hljs-built_in">join</span>(save_dir, f<span class="hljs-string">&quot;&#123;model_name&#125;_val_acc.npy&quot;</span>), np.<span class="hljs-built_in">array</span>(val_acc_arr))<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;保存模型成功!&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Training completed!&#x27;</span>)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔14</title>
    <link href="/2024/05/01/ganwu14/"/>
    <url>/2024/05/01/ganwu14/</url>
    
    <content type="html"><![CDATA[<h2 id="内心的想法"><a href="#内心的想法" class="headerlink" title="内心的想法"></a>内心的想法</h2><p>清空掉头脑中的思想，没有欲望，感受着时间的流动，像是生命在流动。没有感情的波动，一切都刚刚好。发自内心中的喜悦，获得了片刻的自由。<br>人啊人啊，什么禁锢了我，心中的牢笼束缚了自己内心的想法。欲望来源于什么，物质禁锢了身体，外在便不再自由。<br>我该怎么突破这内心和外在的牢笼，去感受那片刻、短暂的喜悦。<br>心啊心啊，为何要自我束缚，物啊物啊，将我带入了沉沦。<br>外在的法则和心中的法则并不相通，我连接了内心和外在。<br>时间啊，你在不断的消逝，也在不断的增加。<br>新生和死亡，未来和过去，或许只有现在最重要。<br>牢笼将我们禁锢，死亡为我指引方向，感受提出了方法。<br>去感受悲伤，感受快乐，感受情绪，感受内心。去感受风，感受雨，感受这个世界。<br>累了就休息，困了就睡觉。生活就是如此简单。<br>活出自己想要的人生，只有自己才知道自己的价值。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑——强化学习——使用智能体来玩游戏</title>
    <link href="/2024/04/30/tiankeng6/"/>
    <url>/2024/04/30/tiankeng6/</url>
    
    <content type="html"><![CDATA[<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://github.com/markub3327/flappy-bird-gymnasium">flappy-bird-gymnasium环境</a><br><a href="https://gymnasium.farama.org/">Gymnasim官网</a><br><a href="https://github.com/luozhiyun993/FlappyBird-PPO-pytorch">github上的代码</a><br><a href="https://hrl.boyuai.com/chapter/">动手强化学习</a><br><a href="https://github.com/jalaxy33/learn-lr">动手强化学习代码</a><br><a href="https://github.com/alexzhuustc/gym-flappybird">tensorflow实现的强化学习</a><br><a href="https://github.com/alexzhuustc/gym-flappybird">tensorflow+gym实现的flyappybird</a><br><a href="https://stackoverflow.com/questions/69442971/error-in-importing-environment-openai-gym">问答网站</a></p><h2 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h2><p>（广义的讲）强化学习是机器通过与环境交互来实现目标的一种计算方法。机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，这个环境发生相应的改变并且将相应的奖励反馈和下一轮状态传回机器。这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的累积奖励的期望。强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。<br>在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移。智能体则在下一轮交互中感知到新的环境状态，依次类推。<br>在这个过程中，智能体有3种控制要素、即感知、决策和奖励<br>感知：智能体在某种程度上感知环境的状态，从而知道自己所处的现状。<br>决策： 智能体根据感知的现状计算出达到目标需要采取的动作的过程叫做决策。比如，针对当前棋盘决定下一颗落子的位置。<br>奖励： 环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈。这个标量信号衡量智能体的好坏。（类似于在深度学习中的损失函数，）<br>强化学习中模型和环境交互，对于模型的目的来讲，就是从环境中取得最大的奖励值，主要在于策略的更新方法。<br>参数更新方法：价值更新、梯度更新。输出的值，连续的值，离散的值。</p><p>强化学习的构成元素：</p><ol><li>智能体（Agent）： 人工智能操作的游戏角色，它就是这个游戏的主要玩家。</li><li>环境（Environment） ： 提供游玩的条件，agent做出的任何选择都会得到游戏环境的反馈。</li><li>状态（State）： 游戏汉奸内所有元素所处的状态，基于环境来进行反馈。</li><li>行动（action）: agent做出的行为来随着状态变化而变化。</li><li>奖励（Reward）： agent的目标在于获取更高的奖励，根据环境的反馈，如果反馈是负向的也可以被描述为惩罚。</li><li>目标（Goal）： 在合理设置奖励后，目标应该被表示为最大化奖励之和。<br>整个强化学习的过程，是为了学到好的策略（Policy）,本质上就是学习在某个状态下应该采取什么样的行动。</li></ol><p>目前已经有的算法; </p><h2 id="游戏选择-FlappyBird"><a href="#游戏选择-FlappyBird" class="headerlink" title="游戏选择 FlappyBird"></a>游戏选择 FlappyBird</h2><p>flappy bird》是一款由来自越南的独立游戏开发者Dong Nguyen所开发的作品，游戏于2013年5月24日上线，并在2014年2月突然暴红。2014年2月，《Flappy Bird》被开发者本人从苹果及谷歌应用商店（Google Play）撤下。2014年8月份正式回归App Store，正式加入Flappy迷们期待已久的多人对战模式。游戏中玩家必须控制一只小鸟，跨越由各种不同长度水管所组成的障碍。</p><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><ol><li>使用miniconda 配置环境，命令<code>conda create -n rl_python python=3.10</code>，配置环境，如果配置错误使用<code>conda remove rl_python --all </code>来删除环境，使用<code>conda activate rl_python</code>来启动环境。</li><li>配置游戏环境，使用<code>pip install flappy-bird-gymnasium</code>安装游戏</li><li>安装tersorflow深度学习框架<a href="https://tensorflow.google.cn/install/gpu?hl=zh-cn">官网连接</a></li><li>安装pytorch-GPU环境 <a href="https://pytorch.org/get-started/previous-versions/">官网链接</a> 通过 nvcc –version 来查看CUDA版本。安装10.2版本的pytorch<code>pip install torch==1.12.1+cu102 torchvision==0.13.1+cu102 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu102</code> python 的3.10不兼容，换11.8的pytorch版本<code>pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu118</code></li></ol><h2 id="ppo算法"><a href="#ppo算法" class="headerlink" title="ppo算法"></a>ppo算法</h2><p>PPO（Proximal Policy Optimization）近端策略优化算法，是一种基于策略（policy-based）的强化学习算法，是一种off-policy算法。由OpenAI于2017年提出，主要用于解决强化学习中的策略优化问题。它是Trust Region Policy Optimization（TRPO）的简化版本，旨在保持TRPO的优点，同时降低其计算复杂性。<br>核心原理：PPO的核心在于限制策略更新的步长，确保新策略不会偏离旧策略太远。这通过引入一个剪辑的目标函数来实现，该函数可以最小化策略更新过程中的风险。PPO算法通过这种方式平衡了探索与利用，提高了算法的稳定性和效率。PPO算法的核心在于更新策略梯度，主流方法有两种，分别是KL散度做penalty，另一种是Clip剪裁，它们的主要作用都是限制策略梯度更新的幅度，从而推导出不同的神经网络参数更新方式</p><h2 id="代码解析工程"><a href="#代码解析工程" class="headerlink" title="代码解析工程"></a>代码解析工程</h2><p>下面这份代码实现了使用ppo算法训练模型来玩Flappy Bird游戏。<br>get_args()函数用于解析命令行参数，包括学习率、折扣因子、迭代次数等。<br>PolicyNet类定义了策略网络，用于输出动作的概率分布。<br>ValueNet类定义了值函数网络，用于评估状态的价值。<br>compute_advantage()函数计算优势值，用于策略更新。<br>train()函数是训练的主要逻辑，包括初始化网络和优化器，采样游戏轨迹，计算优势值，更新策略网络和值函数网络等过程。<br>在训练过程中，使用TensorBoard记录训练过程中的奖励和其他指标。<br>在每轮训练结束后，保存表现最好的模型。<br>if <strong>name</strong> &#x3D;&#x3D; “<strong>main</strong>“:部分用于执行整个训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> argparse <span class="hljs-comment"># 用于解析命令行参数，方便地从命令行中读取参数值</span><br><span class="hljs-keyword">import</span> os <br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span>   BatchSampler, SubsetRandomSampler <span class="hljs-comment"># BatchSampler 用于批次采样，subsetRandomSampler用于随机采样子集</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn <br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter <span class="hljs-comment"># 提供了与TensorBoard集成的功能，用于可视化训练过程和结果。</span><br><br><span class="hljs-keyword">from</span> src.flappy_bird <span class="hljs-keyword">import</span> FlappyBird <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_args</span>():<br>    <span class="hljs-comment"># 该函数用于解析命令行参数，包括学习率、折扣因子、迭代次数</span><br>    parser = argparse.ArgumentParser(<br>        <span class="hljs-string">&quot;&quot;&quot;Implementation of PPO to play Flappy Bird&quot;&quot;&quot;</span>)<br>    parser.add_argument(<span class="hljs-string">&quot;--lr&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">1e-5</span>) <span class="hljs-comment"># 学习率</span><br>    parser.add_argument(<span class="hljs-string">&quot;--gamma&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.99</span>) <span class="hljs-comment"># 折扣因子</span><br>    parser.add_argument(<span class="hljs-string">&quot;--num_iters&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">20000</span>) <span class="hljs-comment"># 迭代次数</span><br>    parser.add_argument(<span class="hljs-string">&quot;--log_path&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&quot;tensorboard_ppo&quot;</span>) <span class="hljs-comment"># 保存日志</span><br>    parser.add_argument(<span class="hljs-string">&quot;--saved_path&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&quot;trained_models&quot;</span>) <span class="hljs-comment"># 模型保存路径</span><br>    parser.add_argument(<span class="hljs-string">&quot;--lmbda&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.95</span>) <span class="hljs-comment"># lmbda 参数</span><br>    parser.add_argument(<span class="hljs-string">&quot;--epochs&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">10</span>) <span class="hljs-comment"># 训练轮数</span><br>    parser.add_argument(<span class="hljs-string">&quot;--eps&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.2</span>) <span class="hljs-comment"># eps ppo算法中的参数，默认值为0.2</span><br>    parser.add_argument(<span class="hljs-string">&quot;--batch_size&quot;</span>,<span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">2048</span> ) <span class="hljs-comment"># 批处理大小，默认值为2048</span><br>    parser.add_argument(<span class="hljs-string">&quot;--mini_batch_size&quot;</span>,<span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">64</span> ) <span class="hljs-comment"># 小批量大小</span><br><br>    args = parser.parse_args()<br>    <span class="hljs-keyword">return</span> args<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PolicyNet</span>(nn.Module):<br>    <span class="hljs-comment"># 定义策略网络，用于输出动作的概率分布，表演者。</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(PolicyNet, self).__init__()<br>        self.conv1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">4</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">8</span>, stride=<span class="hljs-number">4</span>), nn.ReLU())<br>        self.conv2 = nn.Sequential(nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>), nn.ReLU())<br>        self.conv3 = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>), nn.ReLU())<br>        self.flat = nn.Flatten()<br>        self.fc1 = nn.Sequential(nn.Linear(<span class="hljs-number">7</span> * <span class="hljs-number">7</span> * <span class="hljs-number">64</span>, <span class="hljs-number">512</span>), nn.Tanh())<br>        self.drop = nn.Dropout(<span class="hljs-number">0.5</span>)<br>        self.fc3 = nn.Sequential(nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">2</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        <span class="hljs-comment"># 前向传播</span><br>        output = self.conv1(<span class="hljs-built_in">input</span>)<br>        output = self.conv2(output)<br>        output = self.conv3(output)<br>        output = self.flat(output)<br>        output = self.drop(output)<br>        output = self.fc1(output)<br>        <span class="hljs-keyword">return</span> nn.functional.softmax(self.fc3(output), dim=<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ValueNet</span>(nn.Module):<br>    <span class="hljs-comment"># 定义值函数网络，用于评估状态的价值，评论家</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(ValueNet, self).__init__()<br>        self.net = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">4</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">8</span>, stride=<span class="hljs-number">4</span>), nn.ReLU(),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>), nn.ReLU(),<br>            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>), nn.ReLU(),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">7</span> * <span class="hljs-number">7</span> * <span class="hljs-number">64</span>, <span class="hljs-number">512</span>), nn.Tanh(),<br>            nn.Dropout(<span class="hljs-number">0.5</span>),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">1</span>),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        <span class="hljs-keyword">return</span> self.net(<span class="hljs-built_in">input</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_advantage</span>(<span class="hljs-params">gamma, lmbda, td_delta</span>):<br>    <span class="hljs-comment"># 函数计算优势值，用于策略更新</span><br>    td_delta = td_delta.detach().numpy()<br>    advantage_list = []<br>    advantage = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> delta <span class="hljs-keyword">in</span> td_delta[::-<span class="hljs-number">1</span>]:<br>        advantage = gamma * lmbda * advantage + delta<br>        advantage_list.append(advantage)<br>    advantage_list.reverse()<br>    <span class="hljs-keyword">return</span> torch.tensor(advantage_list, dtype=torch.<span class="hljs-built_in">float</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">opt</span>):<br>    <span class="hljs-comment"># 训练的主要逻辑，包括初始化网络和优化器，采样游戏轨迹，计算优势值，更新策略网络和值函数网络等过程。</span><br>    <span class="hljs-keyword">if</span> torch.cuda.is_available(): <span class="hljs-comment"># 检查CUDA是否可以使用，</span><br>        torch.cuda.manual_seed(<span class="hljs-number">1993</span>) <span class="hljs-comment"># 设计随机种子，确保实验的可用性。</span><br>    <span class="hljs-keyword">else</span>:<br>        torch.manual_seed(<span class="hljs-number">123</span>)<br><br>    actor = PolicyNet().cuda() <span class="hljs-comment"># 初始化策略函数，演员</span><br>    critic = ValueNet().cuda() <span class="hljs-comment"># 初始化值函数，评论家</span><br><br>    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=opt.lr) <span class="hljs-comment"># 定义策略函数网络的优化器器</span><br>    critic_optimizer = torch.optim.Adam(critic.parameters(), lr=opt.lr) <span class="hljs-comment"># 定义值函数网络的优化器</span><br><br><br>    writer = SummaryWriter(opt.log_path) <span class="hljs-comment"># 创建一个TensorBoard的SummaryWriter对象，用于记录训练过程中的指标</span><br>    game_state = FlappyBird(<span class="hljs-string">&quot;ppo&quot;</span>) <span class="hljs-comment"># </span><br>    state, reward, terminal = game_state.step(<span class="hljs-number">0</span>)<br>    max_reward = <span class="hljs-number">0</span><br>    <span class="hljs-built_in">iter</span> = <span class="hljs-number">0</span><br>    replay_memory = [] <span class="hljs-comment"># 初始化回放内存，用于存储游戏轨迹</span><br>    evaluate_num = <span class="hljs-number">0</span>  <span class="hljs-comment"># Record the number of evaluations</span><br>    evaluate_rewards = [] <span class="hljs-comment"># 初始化评估次数和评估列表，</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-built_in">iter</span> &lt; opt.num_iters:<br>        terminal = <span class="hljs-literal">False</span><br>        episode_return = <span class="hljs-number">0.0</span><br><br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> terminal:<br>            prediction = actor(state)<br>            <span class="hljs-built_in">print</span>(prediction)<br>            action_dist = torch.distributions.Categorical(prediction)<br>            action_sample = action_dist.sample()<br>            action = action_sample.item()<br>            next_state, reward, terminal = game_state.step(action) <span class="hljs-comment"># 执行游戏环境的一步，获取下一个状态、奖励和终止状态。</span><br>            replay_memory.append([state, action, reward, next_state, terminal])<br>            state = next_state<br>            episode_return += reward<br><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(replay_memory) &gt; opt.batch_size:<br>                state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = <span class="hljs-built_in">zip</span>(*replay_memory)<br>                states = torch.cat(state_batch, dim=<span class="hljs-number">0</span>).cuda()<br>                actions = torch.tensor(action_batch).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).cuda()<br>                rewards = torch.tensor(reward_batch).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).cuda()<br>                dones = torch.tensor(terminal_batch).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).<span class="hljs-built_in">int</span>().cuda()<br>                next_states = torch.cat(next_state_batch, dim=<span class="hljs-number">0</span>).cuda()<br><br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    td_target = rewards + opt.gamma * critic(next_states) * (<span class="hljs-number">1</span> - dones)<br>                    td_delta = td_target - critic(states)<br>                    advantage = compute_advantage(opt.gamma, opt.lmbda, td_delta.cpu()).cuda()<br>                    old_log_probs = torch.log(actor(states).gather(<span class="hljs-number">1</span>, actions)).detach()<br><br>                <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(opt.epochs):<br>                    <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> BatchSampler(SubsetRandomSampler(<span class="hljs-built_in">range</span>(opt.batch_size)), opt.mini_batch_size, <span class="hljs-literal">False</span>):<br>                        log_probs = torch.log(actor(states[index]).gather(<span class="hljs-number">1</span>, actions[index]))<br>                        ratio = torch.exp(log_probs - old_log_probs[index])<br>                        surr1 = ratio * advantage[index]<br>                        surr2 = torch.clamp(ratio, <span class="hljs-number">1</span> - opt.eps, <span class="hljs-number">1</span> + opt.eps) * advantage[index]  <span class="hljs-comment"># 截断</span><br>                        actor_loss = torch.mean(-torch.<span class="hljs-built_in">min</span>(surr1, surr2))<br>                        critic_loss = torch.mean(<br>                            nn.functional.mse_loss(critic(states[index]), td_target[index].detach()))<br>                        actor_optimizer.zero_grad()<br>                        critic_optimizer.zero_grad()<br>                        actor_loss.backward()<br>                        critic_loss.backward()<br>                        actor_optimizer.step()<br>                        critic_optimizer.step()<br>                replay_memory = []<br><br>        <span class="hljs-keyword">if</span> episode_return &gt; max_reward:<br>            max_reward = episode_return<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot; max_reward Iteration: &#123;&#125;/&#123;&#125;, Reward: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">iter</span> + <span class="hljs-number">1</span>, opt.num_iters, episode_return))<br><br>        <span class="hljs-built_in">iter</span> += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">iter</span>+<span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            evaluate_num += <span class="hljs-number">1</span><br>            evaluate_rewards.append(episode_return)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;evaluate_num:&#123;&#125; \t episode_return:&#123;&#125; \t&quot;</span>.<span class="hljs-built_in">format</span>(evaluate_num, episode_return))<br>            writer.add_scalar(<span class="hljs-string">&#x27;step_rewards&#x27;</span>, evaluate_rewards[-<span class="hljs-number">1</span>], global_step= <span class="hljs-built_in">iter</span>)<br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">iter</span>+<span class="hljs-number">1</span>) % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>            actor_dict = &#123;<span class="hljs-string">&quot;net&quot;</span>: actor.state_dict(), <span class="hljs-string">&quot;optimizer&quot;</span>: actor_optimizer.state_dict()&#125;<br>            critic_dict = &#123;<span class="hljs-string">&quot;net&quot;</span>: critic.state_dict(), <span class="hljs-string">&quot;optimizer&quot;</span>: critic_optimizer.state_dict()&#125;<br>            torch.save(actor_dict, <span class="hljs-string">&quot;&#123;&#125;/flappy_bird_actor_good&quot;</span>.<span class="hljs-built_in">format</span>(opt.saved_path))<br>            torch.save(critic_dict, <span class="hljs-string">&quot;&#123;&#125;/flappy_bird_critic_good&quot;</span>.<span class="hljs-built_in">format</span>(opt.saved_path))<br><br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    opt = get_args()<br>    train(opt)<br><br><br></code></pre></td></tr></table></figure><p>问题是上面代码使用了自定义的Flappy Bird来实现游戏环境，没有使用flappy-bird-gymnasium环境来创建游戏环境，具有一定的参考意义。注： 代码暂时不能移植到Flappy Bird，上我需要解析上面的代码的逻辑。</p><h3 id="移植工程。"><a href="#移植工程。" class="headerlink" title="移植工程。"></a>移植工程。</h3><h4 id="第一步，明白怎么调用flappy-Bird环境"><a href="#第一步，明白怎么调用flappy-Bird环境" class="headerlink" title="第一步，明白怎么调用flappy_Bird环境"></a>第一步，明白怎么调用flappy_Bird环境</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> flappy_bird_gymnasium<br><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br>env = gym.make(<span class="hljs-string">&quot;FlappyBird-v0&quot;</span>, render_mode=<span class="hljs-string">&quot;human&quot;</span>, use_lidar=<span class="hljs-literal">True</span>)<br><br>obs, _ = env.reset() <span class="hljs-comment">#  reset() 用于重置环境并返回初始观测值</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>    <span class="hljs-comment"># Next action:</span><br>    <span class="hljs-comment"># (feed the observation to your agent here)</span><br>    action = env.action_space.sample() <br>    <span class="hljs-built_in">print</span>(action)<br>    <span class="hljs-comment"># Processing:</span><br>    obs, reward, terminated, _, info = env.step(action)<br>    <span class="hljs-comment"># obs: 这是一个变量，用于存储执行动作后的观测值（observation）。在强化学习中，代理根据观测值来决定下一步的动作。</span><br>    <span class="hljs-comment"># reward: 这是一个变量，用于存储执行动作后获得的奖励值（reward）。奖励值表示环境对代理执行动作的评价，可以是正数、负数或零。</span><br>    <span class="hljs-comment"># terminated: 这是一个布尔变量，用于表示游戏是否结束（terminated）。如果游戏结束，则该变量为 True；否则为 False。</span><br>    <span class="hljs-comment"># _: 下划线是一个通用的占位符，通常用于表示我们不关心的变量。在这里，它被用作占位符，因为 env.step(action) 返回的是一个元组，我们只关心其中的前三个元素（obs、reward 和 terminated），而不关心其他的返回值。</span><br>    <span class="hljs-comment"># info: 这是一个字典，用于存储额外的环境信息（info）。这些信息可能包括调试信息、性能指标等，可以帮助我们更好地理解环境的运行情况。</span><br>    <span class="hljs-built_in">print</span>(env.step(action))<br>    <span class="hljs-comment"># Checking if the player is still alive</span><br>    <span class="hljs-keyword">if</span> terminated:<br>        <span class="hljs-keyword">break</span><br><br>env.close()<br></code></pre></td></tr></table></figure><p>在github上，作者表明 在论文中有更详细的参考：<a href="https://www.mdpi.com/1424-8220/24/6/1905">论文链接</a></p><ol><li>状态空间：<br>在”FlappyBird-v0”环境中，提供了代表游戏屏幕的观测数据，这些数据为游戏状态提供了简单的数值信息。</li></ol><p>FlappyBird-v0<br>存在两种观测选项：</p><p>选项一<br>激光雷达传感器的180个读数（论文：使用变压器模型和激光雷达传感器进行运动识别玩Flappy Bird）<br>选项二<br>最后一个管道的水平位置<br>最后一个顶部管道的垂直位置<br>最后一个底部管道的垂直位置<br>下一个管道的水平位置<br>下一个顶部管道的垂直位置<br>下一个底部管道的垂直位置<br>下下一个管道的水平位置<br>下下一个顶部管道的垂直位置<br>下下一个底部管道的垂直位置<br>玩家的垂直位置<br>玩家的垂直速度<br>玩家的旋转</p><ol start="2"><li><p>动作空间：<br>0 - 什么都不做<br>1 - 拍打翅膀</p></li><li><p>奖励：<br>+0.1 - 每帧保持存活状态<br>+1.0 - 成功通过一根管道<br>-1.0 - 死亡<br>−0.5 - 触摸屏幕顶部</p></li></ol><h2 id="第二步，使用一个简单的游戏来实践dqn"><a href="#第二步，使用一个简单的游戏来实践dqn" class="headerlink" title="第二步，使用一个简单的游戏来实践dqn"></a>第二步，使用一个简单的游戏来实践dqn</h2><p>上代码：实践了打砖块的游戏，游戏输入为（4，80，80）图片架构，</p><p>rl_utils代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> collections<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReplayBuffer</span>: <span class="hljs-comment"># 记忆池子</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, capacity</span>):<br>        self.buffer = collections.deque(maxlen=capacity) <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, state, action, reward, next_state, done</span>): <br>        self.buffer.append((state, action, reward, next_state, done)) <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, batch_size</span>): <br>        transitions = random.sample(self.buffer, batch_size)<br>        state, action, reward, next_state, done = <span class="hljs-built_in">zip</span>(*transitions)<br>        <span class="hljs-keyword">return</span> np.array(state), action, reward, np.array(next_state), done <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">size</span>(<span class="hljs-params">self</span>): <br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.buffer)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">moving_average</span>(<span class="hljs-params">a, window_size</span>):<br>    cumulative_sum = np.cumsum(np.insert(a, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)) <br>    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size<br>    r = np.arange(<span class="hljs-number">1</span>, window_size-<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>    begin = np.cumsum(a[:window_size-<span class="hljs-number">1</span>])[::<span class="hljs-number">2</span>] / r<br>    end = (np.cumsum(a[:-window_size:-<span class="hljs-number">1</span>])[::<span class="hljs-number">2</span>] / r)[::-<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span> np.concatenate((begin, middle, end))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_on_policy_agent</span>(<span class="hljs-params">env, agent, num_episodes</span>):<br>    return_list = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br>            <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>)):<br>                episode_return = <span class="hljs-number">0</span><br>                transition_dict = &#123;<span class="hljs-string">&#x27;states&#x27;</span>: [], <span class="hljs-string">&#x27;actions&#x27;</span>: [], <span class="hljs-string">&#x27;next_states&#x27;</span>: [], <span class="hljs-string">&#x27;rewards&#x27;</span>: [], <span class="hljs-string">&#x27;dones&#x27;</span>: []&#125;<br>                state = env.reset()<br>                done = <span class="hljs-literal">False</span><br>                <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                    action = agent.take_action(state)<br>                    next_state, reward, done, _ = env.step(action)<br>                    transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>].append(state)<br>                    transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>].append(action)<br>                    transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>].append(next_state)<br>                    transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>].append(reward)<br>                    transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>].append(done)<br>                    state = next_state<br>                    episode_return += reward<br>                return_list.append(episode_return)<br>                agent.update(transition_dict)<br>                <span class="hljs-keyword">if</span> (i_episode+<span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>                    pbar.set_postfix(&#123;<span class="hljs-string">&#x27;episode&#x27;</span>: <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes/<span class="hljs-number">10</span> * i + i_episode+<span class="hljs-number">1</span>), <span class="hljs-string">&#x27;return&#x27;</span>: <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])&#125;)<br>                pbar.update(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> return_list<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_off_policy_agent</span>(<span class="hljs-params">env, agent, num_episodes, replay_buffer, minimal_size, batch_size</span>):<br>    return_list = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br>            <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>)):<br>                episode_return = <span class="hljs-number">0</span><br>                state = env.reset()<br>                done = <span class="hljs-literal">False</span><br>                <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                    action = agent.take_action(state)<br>                    next_state, reward, done, _ = env.step(action)<br>                    replay_buffer.add(state, action, reward, next_state, done)<br>                    state = next_state<br>                    episode_return += reward<br>                    <span class="hljs-keyword">if</span> replay_buffer.size() &gt; minimal_size:<br>                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)<br>                        transition_dict = &#123;<span class="hljs-string">&#x27;states&#x27;</span>: b_s, <span class="hljs-string">&#x27;actions&#x27;</span>: b_a, <span class="hljs-string">&#x27;next_states&#x27;</span>: b_ns, <span class="hljs-string">&#x27;rewards&#x27;</span>: b_r, <span class="hljs-string">&#x27;dones&#x27;</span>: b_d&#125;<br>                        agent.update(transition_dict)<br>                return_list.append(episode_return)<br>                <span class="hljs-keyword">if</span> (i_episode+<span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>                    pbar.set_postfix(&#123;<span class="hljs-string">&#x27;episode&#x27;</span>: <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes/<span class="hljs-number">10</span> * i + i_episode+<span class="hljs-number">1</span>), <span class="hljs-string">&#x27;return&#x27;</span>: <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])&#125;)<br>                pbar.update(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> return_list<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_advantage</span>(<span class="hljs-params">gamma, lmbda, td_delta</span>):<br>    td_delta = td_delta.detach().numpy()<br>    advantage_list = []<br>    advantage = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> delta <span class="hljs-keyword">in</span> td_delta[::-<span class="hljs-number">1</span>]:<br>        advantage = gamma * lmbda * advantage + delta<br>        advantage_list.append(advantage)<br>    advantage_list.reverse()<br>    <span class="hljs-keyword">return</span> torch.tensor(advantage_list, dtype=torch.<span class="hljs-built_in">float</span>)<br>                <br><br></code></pre></td></tr></table></figure><p>dqn_breakout 文件包括dqn网络的构建，模型的训练<a href="https://gymnasium.farama.org/environments/atari/breakout/">介绍的链接</a>，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> collections<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> rl_utils<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReplayBuffer</span>:<br>    <span class="hljs-string">&#x27;&#x27;&#x27; 经验回放池 &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, capacity</span>):<br>        self.buffer = collections.deque(maxlen=capacity)  <span class="hljs-comment"># 队列,先进先出</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, state, action, reward, next_state, done</span>):  <span class="hljs-comment"># 将数据加入buffer</span><br>        self.buffer.append((state, action, reward, next_state, done))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, batch_size</span>):  <span class="hljs-comment"># 从buffer中采样数据,数量为batch_size</span><br>        transitions = random.sample(self.buffer, batch_size)<br>        state, action, reward, next_state, done = <span class="hljs-built_in">zip</span>(*transitions)<br>        <span class="hljs-keyword">return</span> np.array(state), action, reward, np.array(next_state), done<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">size</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 目前buffer中数据的数量</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.buffer)<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qnet</span>(torch.nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; 加入卷积层的Q网络 &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, action_dim, in_channels=<span class="hljs-number">4</span></span>):<br>        <span class="hljs-built_in">super</span>(Qnet, self).__init__()<br>        self.conv1 = torch.nn.Conv2d(in_channels, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">8</span>, stride=<span class="hljs-number">4</span>) <span class="hljs-comment"># 卷积层，输入</span><br>        self.conv2 = torch.nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>)<br>        self.conv3 = torch.nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>)<br>        self.fc4 = torch.nn.Linear(<span class="hljs-number">7</span> * <span class="hljs-number">7</span> * <span class="hljs-number">64</span>, <span class="hljs-number">512</span>)<br>        self.head = torch.nn.Linear(<span class="hljs-number">512</span>, action_dim)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = x / <span class="hljs-number">255</span><br>        x = F.relu(self.conv1(x))<br>        x = F.relu(self.conv2(x))<br>        x = F.relu(self.conv3(x))<br>        x = F.relu(self.fc4(x.reshape(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)))<br>        <span class="hljs-keyword">return</span> self.head(x)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DQN</span>:<br>    <span class="hljs-string">&#x27;&#x27;&#x27; DQN算法 &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, action_dim, learning_rate, gamma,</span><br><span class="hljs-params">                 epsilon, target_update, device</span>):<br>        self.action_dim = action_dim<br>        self.q_net = Qnet(in_channels=<span class="hljs-number">4</span>, action_dim=action_dim).to(device)  <span class="hljs-comment"># Q网络</span><br>        <span class="hljs-comment"># 目标网络</span><br>        self.target_q_net = Qnet(self.action_dim).to(device)<br>        <span class="hljs-comment"># 使用Adam优化器</span><br>        self.optimizer = torch.optim.Adam(self.q_net.parameters(),<br>                                          lr=learning_rate)<br>        self.gamma = gamma  <span class="hljs-comment"># 折扣因子</span><br>        self.epsilon = epsilon  <span class="hljs-comment"># epsilon-贪婪策略</span><br>        self.target_update = target_update  <span class="hljs-comment"># 目标网络更新频率</span><br>        self.count = <span class="hljs-number">0</span>  <span class="hljs-comment"># 计数器,记录更新次数</span><br>        self.device = device<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save</span>(<span class="hljs-params">self, model_file=<span class="hljs-string">&quot;mymodel&quot;</span></span>):<br>        torch.save(self.q_net, model_file)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):  <span class="hljs-comment"># epsilon-贪婪策略采取动作</span><br>        <span class="hljs-keyword">if</span> np.random.random() &lt; self.epsilon:<br>            action = np.random.randint(self.action_dim)<br>        <span class="hljs-keyword">else</span>:<br>            state = torch.tensor([state], dtype=torch.<span class="hljs-built_in">float</span>).to(self.device)<br>            action = self.q_net(state).argmax().item()<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, transition_dict</span>):<br>        states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>],<br>                              dtype=torch.<span class="hljs-built_in">float</span>).to(self.device)<br>        actions = torch.tensor(transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>]).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<br>            self.device)<br>        rewards = torch.tensor(transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>],<br>                               dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(self.device)<br>        next_states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>],<br>                                   dtype=torch.<span class="hljs-built_in">float</span>).to(self.device)<br>        dones = torch.tensor(transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>],<br>                             dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(self.device)<br><br>        q_values = self.q_net(states).gather(<span class="hljs-number">1</span>, actions)  <span class="hljs-comment"># Q值</span><br>        <span class="hljs-comment"># 下个状态的最大Q值</span><br>        max_next_q_values = self.target_q_net(next_states).<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].view(<br>            -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        q_targets = rewards + self.gamma * max_next_q_values * (<span class="hljs-number">1</span> - dones<br>                                                                )  <span class="hljs-comment"># TD误差目标</span><br>        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  <span class="hljs-comment"># 均方误差损失函数</span><br>        self.optimizer.zero_grad()  <span class="hljs-comment"># PyTorch中默认梯度会累积,这里需要显式将梯度置为0</span><br>        dqn_loss.backward()  <span class="hljs-comment"># 反向传播更新参数</span><br>        self.optimizer.step()<br><br>        <span class="hljs-keyword">if</span> self.count % self.target_update == <span class="hljs-number">0</span>:<br>            self.target_q_net.load_state_dict(<br>                self.q_net.state_dict())  <span class="hljs-comment"># 更新目标网络</span><br>        self.count += <span class="hljs-number">1</span><br><br>lr = <span class="hljs-number">2e-3</span><br>num_episodes = <span class="hljs-number">500</span><br>hidden_dim = <span class="hljs-number">128</span><br>gamma = <span class="hljs-number">0.98</span><br>epsilon = <span class="hljs-number">0.01</span><br>target_update = <span class="hljs-number">10</span><br>buffer_size = <span class="hljs-number">100000</span><br>minimal_size = <span class="hljs-number">500</span><br>batch_size = <span class="hljs-number">64</span><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br><br>env_name = <span class="hljs-string">&#x27;BreakoutNoFrameskip-v4&#x27;</span> <span class="hljs-comment"># 雅达利的弹球游戏</span><br>env = gym.make(env_name)<br>env = gym.wrappers.AtariPreprocessing(env)<br>env = gym.wrappers.FrameStack(env, num_stack=<span class="hljs-number">4</span>)<br>random.seed(<span class="hljs-number">0</span>)<br>np.random.seed(<span class="hljs-number">0</span>)<br>torch.manual_seed(<span class="hljs-number">0</span>)<br>replay_buffer = ReplayBuffer(buffer_size)<br>state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]<br>action_dim = env.action_space.n<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;state shape:&#x27;</span>, env.observation_space.shape)<br>agent = DQN(action_dim, lr, gamma, epsilon,target_update, device)<br><br>return_list = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br>        <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>)):<br>            episode_return = <span class="hljs-number">0</span><br>            state, _ = env.reset()<br>            done = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                action = agent.take_action(state)<br>                next_state, reward, terminate, truncated, _ = env.step(action)<br>                done = terminate <span class="hljs-keyword">or</span> truncated<br>                replay_buffer.add(state, action, reward, next_state, done)<br>                state = next_state<br>                episode_return += reward<br>                <span class="hljs-comment"># 当buffer数据的数量超过一定值后,才进行Q网络训练</span><br>                <span class="hljs-keyword">if</span> replay_buffer.size() &gt; minimal_size:<br>                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)<br>                    transition_dict = &#123;<br>                        <span class="hljs-string">&#x27;states&#x27;</span>: b_s,<br>                        <span class="hljs-string">&#x27;actions&#x27;</span>: b_a,<br>                        <span class="hljs-string">&#x27;next_states&#x27;</span>: b_ns,<br>                        <span class="hljs-string">&#x27;rewards&#x27;</span>: b_r,<br>                        <span class="hljs-string">&#x27;dones&#x27;</span>: b_d<br>                    &#125;<br>                    agent.update(transition_dict)<br>            return_list.append(episode_return)<br>            <span class="hljs-keyword">if</span> (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>                pbar.set_postfix(&#123;<br>                    <span class="hljs-string">&#x27;episode&#x27;</span>:<br>                    <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),<br>                    <span class="hljs-string">&#x27;return&#x27;</span>:<br>                    <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])<br>                &#125;)<br>            pbar.update(<span class="hljs-number">1</span>)<br><br>agent.save()<br>episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br>plt.plot(episodes_list, return_list)<br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br>plt.show()<br><br>mv_return = rl_utils.moving_average(return_list, <span class="hljs-number">9</span>)<br>plt.plot(episodes_list, mv_return)<br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br>plt.show()<br><br></code></pre></td></tr></table></figure><h2 id="第三步解析上面实现的弹球游戏"><a href="#第三步解析上面实现的弹球游戏" class="headerlink" title="第三步解析上面实现的弹球游戏"></a>第三步解析上面实现的弹球游戏</h2><p>打砖块的游戏的输入为(4, 84, 84)，动作有4个有四个动作( 0 , 1 , 2 , 3 )，奖励为分数。对于Flappy Bird for Gymnasium环境中游戏action 为（180，）The LIDAR sensor 180 readings， action space 为 0 do nothing， 1 为 flap 奖励为0.1 - every frame it stays alive，1.0 - successfully passing a pipe，1.0 - dying， 0.5 - touch the top of the screen 输入有点少。</p><h2 id="第四步，使用DQN网络"><a href="#第四步，使用DQN网络" class="headerlink" title="第四步，使用DQN网络"></a>第四步，使用DQN网络</h2><p>笔记： 函数的方法（DQN）和基于策略的方法（REINFORCE），其中基于值函数的方法只学习一个价值函数，而基于策略的方法只学习一个策略函数。那么，一个很自然的问题是，有没有什么方法既学习价值函数，又学习策略函数呢？答案就是 Actor-Critic。在 REINFORCE 算法中，目标函数的梯度中有一项轨迹回报，用于指导策略的更新。Actor 的更新采用策略梯度的原则，之前路走偏了使用了价值策略来更新参数，不容易收敛。</p><p>model</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ACnet</span>(torch.nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; 加入卷积层的Q网络 &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, action_dim, in_channels=<span class="hljs-number">4</span></span>):<br>        <span class="hljs-built_in">super</span>(ACnet, self).__init__()<br>        self.conv1 = torch.nn.Conv2d(in_channels, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">8</span>, stride=<span class="hljs-number">4</span>)<br>        self.conv2 = torch.nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>)<br>        self.conv3 = torch.nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>)<br>        self.flatten = torch.nn.Flatten()<br>        self.fc4 = torch.nn.Linear(<span class="hljs-number">7</span> * <span class="hljs-number">7</span> * <span class="hljs-number">64</span>, <span class="hljs-number">512</span>)<br>        <span class="hljs-comment">#Actor                </span><br>        self.actor_fc = torch.nn.Linear(<span class="hljs-number">512</span>, action_dim)<br>        <span class="hljs-comment">#Critic</span><br>        self.policy_fc = torch.nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">policy</span>(<span class="hljs-params">self, obs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            obs: A float32 tensor array of shape [B, C, H, W]</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            policy_logits: B * ACT_DIM</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        obs = obs / <span class="hljs-number">255.0</span><br>        conv1 = F.relu(self.conv1(obs))<br>        conv2 = F.relu(self.conv2(conv1))<br>        conv3 = F.relu(self.conv3(conv2))<br>        flatten = self.flatten(conv3)<br>        fc_output = F.relu(self.fc(flatten))<br>        policy_logits = self.policy_fc(fc_output)<br><br>        <span class="hljs-keyword">return</span> policy_logits<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">value</span>(<span class="hljs-params">self, obs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            obs: A float32 tensor of shape [B, C, H, W]</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            values: B</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        obs = obs / <span class="hljs-number">255.0</span><br>        conv1 = F.relu(self.conv1(obs))<br>        conv2 = F.relu(self.conv2(conv1))<br>        conv3 = F.relu(self.conv3(conv2))<br>        flatten = self.flatten(conv3)<br>        fc_output = F.relu(self.fc(flatten))<br>        values = self.value_fc(fc_output)<br>        values = torch.squeeze(values, axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> values<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">policy_and_value</span>(<span class="hljs-params">self, obs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            obs: A tensor array of shape [B, C, H, W]</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            policy_logits: B * ACT_DIM</span><br><span class="hljs-string">            values: B</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        obs = obs / <span class="hljs-number">255.0</span><br>        conv1 = F.relu(self.conv1(obs))<br>        conv2 = F.relu(self.conv2(conv1))<br>        conv3 = F.relu(self.conv3(conv2))<br>        flatten = self.flatten(conv3)<br>        fc_output = F.relu(self.fc(flatten))<br><br>        policy_logits = self.policy_fc(fc_output)<br><br>        values = self.value_fc(fc_output)<br>        values = torch.squeeze(values, axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> policy_logits, values<br>    <br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PolicyNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim, action_dim</span>):<br>        <span class="hljs-built_in">super</span>(PolicyNet, self).__init__()<br>        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)<br>        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)<br>        self.fc3 = torch.nn.Linear(hidden_dim, hidden_dim)<br>        self.fc4 = torch.nn.Linear(hidden_dim, action_dim)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.fc1(x))<br>        x = F.relu(self.fc2(x))<br>        x = F.relu(self.fc3(x))<br>        <span class="hljs-keyword">return</span> F.softmax(self.fc4(x), dim=<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ValueNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim</span>):<br>        <span class="hljs-built_in">super</span>(ValueNet, self).__init__()<br>        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)<br>        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)<br>        self.fc3 = torch.nn.Linear(hidden_dim, hidden_dim)<br>        self.fc4 = torch.nn.Linear(hidden_dim, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.fc1(x))<br>        x = F.relu(self.fc2(x))<br>        x = F.relu(self.fc3(x))<br>        <span class="hljs-keyword">return</span> self.fc4(x)<br><br><br></code></pre></td></tr></table></figure><p>agent</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> PolicyNet, ValueNet<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ActorCritic</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,</span><br><span class="hljs-params">                 gamma, device</span>):<br>        <span class="hljs-comment"># 策略网络</span><br>        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)<br>        self.critic = ValueNet(state_dim, hidden_dim).to(device)  <span class="hljs-comment"># 价值网络</span><br>        <br>                <span class="hljs-comment"># 策略网络优化器</span><br>        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),<br>                                                lr=actor_lr)<br>        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),<br>                                                 lr=critic_lr)  <span class="hljs-comment"># 价值网络优化器</span><br>        <span class="hljs-comment">#学习率衰减</span><br>        <span class="hljs-comment"># self.actor_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.actor_optimizer, gamma=0.99)</span><br>        <span class="hljs-comment"># self.critic_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.critic_optimizer, gamma=0.99)</span><br>        self.actor_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.actor_optimizer, T_max=<span class="hljs-number">50</span>, eta_min=<span class="hljs-number">1e-6</span>)<br>        self.critic_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.critic_optimizer, T_max=<span class="hljs-number">50</span>, eta_min=<span class="hljs-number">1e-6</span>)<br>        self.gamma = gamma<br>        self.device = device<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):<br>        state = torch.tensor([state], dtype=torch.<span class="hljs-built_in">float</span>).to(self.device)<br>        probs = self.actor(state)<br>        action_dist = torch.distributions.Categorical(probs)<br>        action = action_dist.sample()<br>        <span class="hljs-keyword">return</span> action.item()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, transition_dict</span>):<br>        states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>],<br>                              dtype=torch.<span class="hljs-built_in">float</span>).to(self.device)<br>        actions = torch.tensor(transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>]).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<br>            self.device)<br>        rewards = torch.tensor(transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>],<br>                               dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(self.device)<br>        next_states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>],<br>                                   dtype=torch.<span class="hljs-built_in">float</span>).to(self.device)<br>        dones = torch.tensor(transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>],<br>                             dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(self.device)<br><br>        <span class="hljs-comment"># 时序差分目标</span><br>        td_target = rewards + self.gamma * self.critic(next_states) * (<span class="hljs-number">1</span> -<br>                                                                       dones)<br>        td_delta = td_target - self.critic(states)  <span class="hljs-comment"># 时序差分误差</span><br>        log_probs = torch.log(self.actor(states).gather(<span class="hljs-number">1</span>, actions))<br>        actor_loss = torch.mean(-log_probs * td_delta.detach())<br>        <span class="hljs-comment"># 均方误差损失函数</span><br>        critic_loss = torch.mean(<br>            F.mse_loss(self.critic(states), td_target.detach()))<br>        self.actor_optimizer.zero_grad()<br>        self.critic_optimizer.zero_grad()<br>        actor_loss.backward()  <span class="hljs-comment"># 计算策略网络的梯度</span><br>        critic_loss.backward()  <span class="hljs-comment"># 计算价值网络的梯度</span><br>        self.actor_optimizer.step()  <span class="hljs-comment"># 更新策略网络的参数</span><br>        self.critic_optimizer.step()  <span class="hljs-comment"># 更新价值网络的参数</span><br>        self.actor_lr_scheduler.step()<br>        self.critic_lr_scheduler.step()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save</span>(<span class="hljs-params">self, path=<span class="hljs-string">&#x27;mymodel&#x27;</span>, epoch=<span class="hljs-number">0</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;保存模型&#x27;&#x27;&#x27;</span><br>        checkpoint = &#123;<span class="hljs-string">&quot;actor&quot;</span>: self.actor.state_dict(),<br>                      <span class="hljs-string">&quot;critic&quot;</span>: self.critic.state_dict(),<br>                      <span class="hljs-string">&quot;actor_opt&quot;</span>: self.actor_optimizer.state_dict(),<br>                      <span class="hljs-string">&quot;critic_opt&quot;</span>: self.critic_optimizer.state_dict(),<br>                      <span class="hljs-string">&quot;epoch&quot;</span>: epoch&#125;        <br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(path):<br>            os.makedirs(path, <span class="hljs-literal">True</span>)<br>        path = os.path.join(path, <span class="hljs-string">f&#x27;checkpoint_<span class="hljs-subst">&#123;epoch&#125;</span>.ckpt&#x27;</span>)<br>        torch.save(checkpoint, path)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load</span>(<span class="hljs-params">self, path=<span class="hljs-string">&#x27;mymodel&#x27;</span></span>):<br>        checkpoint = torch.load(path)<br>        self.actor.load_state_dict(checkpoint[<span class="hljs-string">&#x27;actor&#x27;</span>])<br>        self.critic.load_state_dict(checkpoint[<span class="hljs-string">&#x27;critic&#x27;</span>])<br>        self.actor_optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;actor_opt&#x27;</span>])<br>        self.critic_optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;critic_opt&#x27;</span>])<br>        <span class="hljs-keyword">return</span> checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>]<br><br></code></pre></td></tr></table></figure><p>rl_utils 文件，用于测试代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> collections<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> tensorboardX <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReplayBuffer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, capacity</span>):<br>        self.buffer = collections.deque(maxlen=capacity) <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, state, action, reward, next_state, done</span>): <br>        self.buffer.append((state, action, reward, next_state, done)) <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, batch_size</span>): <br>        transitions = random.sample(self.buffer, batch_size)<br>        state, action, reward, next_state, done = <span class="hljs-built_in">zip</span>(*transitions)<br>        <span class="hljs-keyword">return</span> np.array(state), action, reward, np.array(next_state), done <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">size</span>(<span class="hljs-params">self</span>): <br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.buffer)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">moving_average</span>(<span class="hljs-params">a, window_size</span>):<br>    cumulative_sum = np.cumsum(np.insert(a, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)) <br>    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size<br>    r = np.arange(<span class="hljs-number">1</span>, window_size-<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>    begin = np.cumsum(a[:window_size-<span class="hljs-number">1</span>])[::<span class="hljs-number">2</span>] / r<br>    end = (np.cumsum(a[:-window_size:-<span class="hljs-number">1</span>])[::<span class="hljs-number">2</span>] / r)[::-<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span> np.concatenate((begin, middle, end))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_on_policy_agent</span>(<span class="hljs-params">env, agent, num_episodes</span>):<br>    current_time = time.strftime(<span class="hljs-string">&quot;logs/%Y-%m-%dT%H_%M&quot;</span>, time.localtime())<br>    writer = SummaryWriter(log_dir=current_time)<br>    return_list = []<br>    global_step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br>            <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>)):<br>                episode_return = <span class="hljs-number">0</span><br>                transition_dict = &#123;<span class="hljs-string">&#x27;states&#x27;</span>: [], <span class="hljs-string">&#x27;actions&#x27;</span>: [], <span class="hljs-string">&#x27;next_states&#x27;</span>: [], <span class="hljs-string">&#x27;rewards&#x27;</span>: [], <span class="hljs-string">&#x27;dones&#x27;</span>: []&#125;<br>                state,_ = env.reset()<br>                <span class="hljs-comment"># print(state)</span><br>                done = <span class="hljs-literal">False</span><br>                <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                    action = agent.take_action(state)<br>                    <span class="hljs-comment"># print(action)</span><br>                    next_state, reward, terminated,truncated, _ = env.step(action)<br>                    done = terminated <span class="hljs-keyword">or</span> truncated<br>                    transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>].append(state)<br>                    transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>].append(action)<br>                    transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>].append(next_state)<br>                    transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>].append(reward)<br>                    transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>].append(done)<br>                    state = next_state<br>                    episode_return += reward<br>                <br>                return_list.append(episode_return)<br>                writer.add_scalar(<span class="hljs-string">&#x27;reward&#x27;</span>, episode_return, global_step)<br>                writer.add_scalar(<span class="hljs-string">&#x27;actor LR&#x27;</span>, agent.actor_lr_scheduler.get_last_lr(),global_step)<br>                writer.add_scalar(<span class="hljs-string">&#x27;critic LR&#x27;</span>, agent.critic_lr_scheduler.get_last_lr(),global_step)<br>                global_step += <span class="hljs-number">1</span><br>                agent.update(transition_dict)<br>                <span class="hljs-keyword">if</span> (i_episode+<span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>                    pbar.set_postfix(&#123;<span class="hljs-string">&#x27;episode&#x27;</span>: <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes/<span class="hljs-number">10</span> * i + i_episode+<span class="hljs-number">1</span>), <span class="hljs-string">&#x27;return&#x27;</span>: <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])&#125;)<br>                pbar.update(<span class="hljs-number">1</span>)<br>        agent.save(path=<span class="hljs-string">&#x27;mymodel&#x27;</span>, epoch=i)<br><br>    <span class="hljs-keyword">return</span> return_list<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_off_policy_agent</span>(<span class="hljs-params">env, agent, num_episodes, replay_buffer, minimal_size, batch_size</span>):<br>    return_list = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br>            <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>)):<br>                episode_return = <span class="hljs-number">0</span><br>                state = env.reset()<br>                done = <span class="hljs-literal">False</span><br>                <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                    action = agent.take_action(state)<br>                    next_state, reward, done, _ = env.step(action)<br>                    replay_buffer.add(state, action, reward, next_state, done)<br>                    state = next_state<br>                    episode_return += reward<br>                    <span class="hljs-keyword">if</span> replay_buffer.size() &gt; minimal_size:<br>                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)<br>                        transition_dict = &#123;<span class="hljs-string">&#x27;states&#x27;</span>: b_s, <span class="hljs-string">&#x27;actions&#x27;</span>: b_a, <span class="hljs-string">&#x27;next_states&#x27;</span>: b_ns, <span class="hljs-string">&#x27;rewards&#x27;</span>: b_r, <span class="hljs-string">&#x27;dones&#x27;</span>: b_d&#125;<br>                        agent.update(transition_dict)<br>                return_list.append(episode_return)<br>                <span class="hljs-keyword">if</span> (i_episode+<span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>                    pbar.set_postfix(&#123;<span class="hljs-string">&#x27;episode&#x27;</span>: <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes/<span class="hljs-number">10</span> * i + i_episode+<span class="hljs-number">1</span>), <span class="hljs-string">&#x27;return&#x27;</span>: <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])&#125;)<br>                pbar.update(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> return_list<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_advantage</span>(<span class="hljs-params">gamma, lmbda, td_delta</span>):<br>    td_delta = td_delta.detach().numpy()<br>    advantage_list = []<br>    advantage = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> delta <span class="hljs-keyword">in</span> td_delta[::-<span class="hljs-number">1</span>]:<br>        advantage = gamma * lmbda * advantage + delta<br>        advantage_list.append(advantage)<br>    advantage_list.reverse()<br>    <span class="hljs-keyword">return</span> torch.tensor(advantage_list, dtype=torch.<span class="hljs-built_in">float</span>)<br>                <br></code></pre></td></tr></table></figure><p>训练代码，使用Ac—Critic算法来进行梯度更新</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> flappy_bird_gymnasium<br><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> rl_utils<br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> ACnet<br><span class="hljs-keyword">from</span> agent <span class="hljs-keyword">import</span> ActorCritic<br><br><br><br>actor_lr = <span class="hljs-number">1e-3</span><br>critic_lr = <span class="hljs-number">1e-2</span><br>num_episodes = <span class="hljs-number">4000</span><br>hidden_dim = <span class="hljs-number">256</span><br>gamma = <span class="hljs-number">0.96</span><br>device = torch.device(&quot;cuda&quot;) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<br>    &quot;cpu&quot;)<br><br>env_name = <span class="hljs-string">&#x27;FlappyBird-v0&#x27;</span><br>env = gym.make(env_name)<br>torch.manual_seed(<span class="hljs-number">0</span>)<br>state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]<br>action_dim = env.action_space.n<br>agent = ActorCritic(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,<br>                    gamma, device)<br><br>#恢复训练<br># agent.<span class="hljs-keyword">load</span>(<span class="hljs-string">&#x27;mymodel/checkpoint_9&#x27;</span>)<br><br>return_list = rl_utils.train_on_policy_agent(env, agent, num_episodes)<br><br>agent.save()<br><br><br>episodes_list = list(range(len(return_list)))<br>plt.plot(episodes_list, return_list)<br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Actor-Critic on &#123;&#125;&#x27;</span>.format(env_name))<br>plt.<span class="hljs-keyword">show</span>()<br><br>mv_return = rl_utils.moving_average(return_list, <span class="hljs-number">9</span>)<br>plt.plot(episodes_list, mv_return)<br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Actor-Critic on &#123;&#125;&#x27;</span>.format(env_name))<br>plt.<span class="hljs-keyword">show</span>()<br><br></code></pre></td></tr></table></figure><p>结果不怎么样，一直维持到0.8 左右。agent一直没有行动。某种程度上是数据输入太少了，论文中的结果怎么好我看不懂，真的服了。更换算法，</p><h2 id="第五步ppo算法"><a href="#第五步ppo算法" class="headerlink" title="第五步ppo算法."></a>第五步ppo算法.</h2><p>代码 详细代码我放在了github上<a href="https://github.com/changjingzhi/FlappyBird-with-ppo">github链接</a><br>配置文件，rl_utils</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> abc <span class="hljs-keyword">import</span> ABC, abstractmethod<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> collections<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br><span class="hljs-keyword">import</span> flappy_bird_gymnasium<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasePolicy</span>(<span class="hljs-title class_ inherited__">ABC</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    策略类的抽象基类</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-literal">True</span><br><br><span class="hljs-meta">    @abstractmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state:np.ndarray</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br><span class="hljs-meta">    @abstractmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, transition_dict:<span class="hljs-built_in">dict</span></span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReplayBuffer</span>:<br>    <span class="hljs-string">&#x27;&#x27;&#x27; 经验回放池 &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, capacity:<span class="hljs-built_in">int</span></span>):<br>        self.buffer = collections.deque(maxlen=capacity)    <span class="hljs-comment"># 队列,先进先出</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, state:np.ndarray, action:<span class="hljs-built_in">int</span>, reward:<span class="hljs-built_in">float</span>, next_state:np.ndarray, done:<span class="hljs-built_in">bool</span></span>):     <span class="hljs-comment"># 将数据加入buffer</span><br>        self.buffer.append((state, action, reward, next_state, done)) <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, batch_size:<span class="hljs-built_in">int</span></span>):   <span class="hljs-comment"># 从buffer中采样数据,数量为batch_size</span><br>        transitions = random.sample(self.buffer, batch_size)<br>        state, action, reward, next_state, done = <span class="hljs-built_in">zip</span>(*transitions)<br>        <span class="hljs-keyword">return</span> np.array(state), action, reward, np.array(next_state), done <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">size</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:  <span class="hljs-comment"># 目前buffer中数据的数量</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.buffer)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">moving_average</span>(<span class="hljs-params">a:<span class="hljs-built_in">list</span>, window_size:<span class="hljs-built_in">int</span></span>) -&gt; np.ndarray:<br>    cumulative_sum = np.cumsum(np.insert(a, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)) <br>    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size<br>    r = np.arange(<span class="hljs-number">1</span>, window_size-<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>    begin = np.cumsum(a[:window_size-<span class="hljs-number">1</span>])[::<span class="hljs-number">2</span>] / r<br>    end = (np.cumsum(a[:-window_size:-<span class="hljs-number">1</span>])[::<span class="hljs-number">2</span>] / r)[::-<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span> np.concatenate((begin, middle, end))<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_on_policy_agent</span>(<span class="hljs-params">env:gym.Env, agent:BasePolicy, num_episodes:<span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">list</span>:<br>    return_list = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br>            <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>)):<br>                episode_return = <span class="hljs-number">0</span><br>                transition_dict = &#123;<span class="hljs-string">&#x27;states&#x27;</span>: [], <span class="hljs-string">&#x27;actions&#x27;</span>: [], <span class="hljs-string">&#x27;next_states&#x27;</span>: [], <span class="hljs-string">&#x27;rewards&#x27;</span>: [], <span class="hljs-string">&#x27;dones&#x27;</span>: []&#125;<br>                state,_ = env.reset()<br>                done = <span class="hljs-literal">False</span><br>                <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                    action = agent.take_action(state)<br>                    next_state, reward, done, _,info = env.step(action)<br>                    transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>].append(state)<br>                    transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>].append(action)<br>                    transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>].append(next_state)<br>                    transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>].append(reward)<br>                    transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>].append(done)<br>                    state = next_state<br>                    episode_return += reward<br>                return_list.append(episode_return)<br>                agent.update(transition_dict)<br>                <span class="hljs-keyword">if</span> (i_episode+<span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>                    pbar.set_postfix(&#123;<br>                            <span class="hljs-string">&#x27;episode&#x27;</span>: <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes/<span class="hljs-number">10</span> * i + i_episode+<span class="hljs-number">1</span>), <br>                            <span class="hljs-string">&#x27;return&#x27;</span>: <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])<br>                        &#125;)<br>                pbar.update(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> return_list<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_off_policy_agent</span>(<span class="hljs-params"></span><br><span class="hljs-params">        env:gym.Env, agent:BasePolicy, num_episodes:<span class="hljs-built_in">int</span>, </span><br><span class="hljs-params">        replay_buffer:ReplayBuffer, minimal_size:<span class="hljs-built_in">int</span>, batch_size:<span class="hljs-built_in">int</span></span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-built_in">list</span>:<br>    return_list = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br>            <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes/<span class="hljs-number">10</span>)):<br>                episode_return = <span class="hljs-number">0</span><br>                state = env.reset()<br>                done = <span class="hljs-literal">False</span><br>                <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                    action = agent.take_action(state)<br>                    next_state, reward, done, _ = env.step(action)<br>                    replay_buffer.add(state, action, reward, next_state, done)<br>                    state = next_state<br>                    episode_return += reward<br>                    <span class="hljs-keyword">if</span> replay_buffer.size() &gt; minimal_size:<br>                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)<br>                        transition_dict = &#123;<span class="hljs-string">&#x27;states&#x27;</span>: b_s, <span class="hljs-string">&#x27;actions&#x27;</span>: b_a, <span class="hljs-string">&#x27;next_states&#x27;</span>: b_ns, <span class="hljs-string">&#x27;rewards&#x27;</span>: b_r, <span class="hljs-string">&#x27;dones&#x27;</span>: b_d&#125;<br>                        agent.update(transition_dict)<br>                return_list.append(episode_return)<br>                <span class="hljs-keyword">if</span> (i_episode+<span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>                    pbar.set_postfix(&#123;<br>                            <span class="hljs-string">&#x27;episode&#x27;</span>: <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes/<span class="hljs-number">10</span> * i + i_episode+<span class="hljs-number">1</span>), <br>                            <span class="hljs-string">&#x27;return&#x27;</span>: <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])<br>                        &#125;)<br>                pbar.update(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> return_list<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_advantage</span>(<span class="hljs-params">gamma:<span class="hljs-built_in">float</span>, lmbda:<span class="hljs-built_in">float</span>, td_delta:torch.Tensor</span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    广义优势估计 (Generalized Advantage Estimation, GAE)，</span><br><span class="hljs-string">    解释：https://hrl.boyuai.com/chapter/2/trpo%E7%AE%97%E6%B3%95#116-%E5%B9%BF%E4%B9%89%E4%BC%98%E5%8A%BF%E4%BC%B0%E8%AE%A1</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    td_delta = td_delta.detach().numpy()<br>    advantage_list = []<br>    advantage = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> delta <span class="hljs-keyword">in</span> td_delta[::-<span class="hljs-number">1</span>]:<br>        advantage = gamma * lmbda * advantage + delta<br>        advantage_list.append(advantage)<br>    advantage_list.reverse()<br>    <span class="hljs-keyword">return</span> torch.tensor(advantage_list, dtype=torch.<span class="hljs-built_in">float</span>)<br>                <br><br></code></pre></td></tr></table></figure><p>训练代码,ppo</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br><span class="hljs-keyword">import</span> flappy_bird_gymnasium<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> rl_utils<br><span class="hljs-keyword">import</span> os<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PolicyNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim, action_dim</span>):<br>        <span class="hljs-built_in">super</span>(PolicyNet, self).__init__()<br>        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)<br>        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.fc1(x))<br>        <span class="hljs-keyword">return</span> F.softmax(self.fc2(x), dim=<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ValueNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim</span>):<br>        <span class="hljs-built_in">super</span>(ValueNet, self).__init__()<br>        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)<br>        self.fc2 = torch.nn.Linear(hidden_dim, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.fc1(x))<br>        <span class="hljs-keyword">return</span> self.fc2(x)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,</span><br><span class="hljs-params">                 lmbda, epochs, eps, gamma, device</span>):<br>        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)<br>        self.critic = ValueNet(state_dim, hidden_dim).to(device)<br>        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),<br>                                                lr=actor_lr)<br>        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),<br>                                                 lr=critic_lr)<br>        self.gamma = gamma<br>        self.lmbda = lmbda<br>        self.epochs = epochs<br>        self.eps = eps<br>        self.device = device<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):<br>        state = torch.tensor([state], dtype=torch.<span class="hljs-built_in">float</span>).to(self.device)<br>        probs = self.actor(state)<br>        action_dist = torch.distributions.Categorical(probs)<br>        action = action_dist.sample()<br>        <span class="hljs-keyword">return</span> action.item()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, transition_dict</span>):<br>        states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>],<br>                              dtype=torch.<span class="hljs-built_in">float</span>).to(self.device)<br>        actions = torch.tensor(transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>]).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<br>            self.device)<br>        rewards = torch.tensor(transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>],<br>                               dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(self.device)<br>        next_states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>],<br>                                   dtype=torch.<span class="hljs-built_in">float</span>).to(self.device)<br>        dones = torch.tensor(transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>],<br>                             dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(self.device)<br>        td_target = rewards + self.gamma * self.critic(next_states) * (<span class="hljs-number">1</span> -<br>                                                                       dones)<br>        td_delta = td_target - self.critic(states)<br>        advantage = rl_utils.compute_advantage(self.gamma, self.lmbda,<br>                                               td_delta.cpu()).to(self.device)<br>        old_log_probs = torch.log(self.actor(states).gather(<span class="hljs-number">1</span>,<br>                                                            actions)).detach()<br><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.epochs):<br>            log_probs = torch.log(self.actor(states).gather(<span class="hljs-number">1</span>, actions))<br>            ratio = torch.exp(log_probs - old_log_probs)<br>            surr1 = ratio * advantage<br>            surr2 = torch.clamp(ratio, <span class="hljs-number">1</span> - self.eps,<br>                                <span class="hljs-number">1</span> + self.eps) * advantage<br>            actor_loss = torch.mean(-torch.<span class="hljs-built_in">min</span>(surr1, surr2))<br>            critic_loss = torch.mean(<br>                F.mse_loss(self.critic(states), td_target.detach()))<br>            self.actor_optimizer.zero_grad()<br>            self.critic_optimizer.zero_grad()<br>            actor_loss.backward()<br>            critic_loss.backward()<br>            self.actor_optimizer.step()<br>            self.critic_optimizer.step()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, model_save_path</span>):<br>        os.makedirs(os.path.dirname(model_save_path), exist_ok=<span class="hljs-literal">True</span>)<br>        torch.save(&#123;<br>            <span class="hljs-string">&#x27;actor&#x27;</span>: self.actor.state_dict(),<br>            <span class="hljs-string">&#x27;critic&#x27;</span>: self.critic.state_dict(),<br>            <span class="hljs-string">&#x27;actor_optimizer&#x27;</span>: self.actor_optimizer.state_dict(),<br>            <span class="hljs-string">&#x27;critic_optimizer&#x27;</span>: self.critic_optimizer.state_dict()<br>        &#125;, model_save_path)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_save_path</span>):<br>        <span class="hljs-keyword">if</span> os.path.exists(model_save_path):<br>            checkpoint = torch.load(model_save_path)<br>            self.actor.load_state_dict(checkpoint[<span class="hljs-string">&#x27;actor&#x27;</span>])<br>            self.critic.load_state_dict(checkpoint[<span class="hljs-string">&#x27;critic&#x27;</span>])<br>            self.actor_optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;actor_optimizer&#x27;</span>])<br>            self.critic_optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;critic_optimizer&#x27;</span>])<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型成功加载！&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型路径不存在，将从头开始训练。&quot;</span>)<br><br><span class="hljs-comment"># 定义超参数和环境</span><br>actor_lr = <span class="hljs-number">1e-6</span><br>critic_lr = <span class="hljs-number">1e-5</span><br>num_episodes = <span class="hljs-number">10000</span><br>hidden_dim = <span class="hljs-number">64</span><br>gamma = <span class="hljs-number">0.99</span><br>lmbda = <span class="hljs-number">0.95</span><br>epochs = <span class="hljs-number">10</span><br>eps = <span class="hljs-number">0.2</span><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>env_name = <span class="hljs-string">&#x27;FlappyBird-v0&#x27;</span><br><br><span class="hljs-comment"># 创建环境和代理</span><br>env = gym.make(env_name)<br>state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]<br>action_dim = env.action_space.n<br><br><span class="hljs-comment"># 创建PPO代理</span><br>agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda,<br>            epochs, eps, gamma, device)<br><br><span class="hljs-comment"># 定义模型保存路径</span><br>model_save_path = <span class="hljs-string">&quot;model/ppo_model.pth&quot;</span><br><br><span class="hljs-comment"># 加载模型或训练新模型</span><br>agent.load_model(model_save_path)<br><br><span class="hljs-comment"># 训练或测试代理</span><br>return_list = rl_utils.train_on_policy_agent(env, agent, num_episodes)<br><br>agent.save_model(model_save_path)<br><br><span class="hljs-comment"># 绘制训练曲线</span><br>episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br>plt.plot(episodes_list, return_list)<br>plt.xlabel(<span class="hljs-string">&#x27;num_episodes&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;reward&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;PPO on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br>plt.show()<br></code></pre></td></tr></table></figure><p>测试代码，test</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br><span class="hljs-keyword">import</span> flappy_bird_gymnasium<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 定义游戏环境和设备</span><br>env = gym.make(<span class="hljs-string">&quot;FlappyBird-v0&quot;</span>, render_mode=<span class="hljs-string">&quot;human&quot;</span>)<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-comment"># 定义模型路径</span><br>model_path = <span class="hljs-string">&quot;ppo_model.pth&quot;</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PolicyNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim, action_dim</span>):<br>        <span class="hljs-built_in">super</span>(PolicyNet, self).__init__()<br>        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)<br>        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.fc1(x))<br>        <span class="hljs-keyword">return</span> F.softmax(self.fc2(x), dim=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 定义 PPO 代理类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPOAgent</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim, action_dim, model_path</span>):<br>        self.model = PolicyNet(state_dim, hidden_dim, action_dim).to(device)<br>        self.model.load_state_dict(torch.load(model_path)[<span class="hljs-string">&#x27;actor&#x27;</span>])  <span class="hljs-comment"># 加载模型的 &#x27;actor&#x27; 部分</span><br>        self.model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 设置模型为评估模式</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):<br>        state_tensor = torch.tensor([state], dtype=torch.<span class="hljs-built_in">float</span>).to(device)<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            action = self.model(state_tensor).argmax().item()<br>        <span class="hljs-keyword">return</span> action<br><br><span class="hljs-comment"># 创建 PPO 代理</span><br>state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]<br>action_dim = env.action_space.n<br>hidden_dim = <span class="hljs-number">64</span><br>agent = PPOAgent(state_dim, hidden_dim, action_dim, model_path)<br><br><span class="hljs-comment"># 运行游戏并观察动画</span><br>state, _ = env.reset()<br>done = <span class="hljs-literal">False</span><br>total_reward = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>        <span class="hljs-comment"># 使用代理选择动作</span><br>        action = agent.take_action(state)<br><br>        <span class="hljs-comment"># 执行动作并观察游戏动画</span><br>        next_state, reward, terminate, truncated, _ = env.step(action)<br>        done = terminate <span class="hljs-keyword">or</span> truncated<br>        total_reward += reward<br>        env.render()<br><br>        state = next_state<br><br><span class="hljs-keyword">finally</span>:<br>    <span class="hljs-comment"># 输出游戏得分</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;游戏结束，得分：<span class="hljs-subst">&#123;total_reward&#125;</span>&quot;</span>)<br>    <span class="hljs-comment"># 手动关闭游戏画面</span><br>    <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;按任意键关闭游戏画面...&quot;</span>)<br>    env.close()  <span class="hljs-comment"># 关闭游戏环境</span><br><br></code></pre></td></tr></table></figure><h2 id="补充知识"><a href="#补充知识" class="headerlink" title="补充知识"></a>补充知识</h2><p>价值方法： Sarsa、Qlearning、DQN<br>策略方法： Reinfore、Actor-Critic、A2C、TRPO、PPO、DDPC</p><p>on-policy（在策略） 的算法： Sarsa、Reinforce、Actor-Critic、A2C、TRPO、PPO<br>非on-policy算法：Qlearning，DQN，Double DQN<br>两者区别在于： on-policy是在线训练的，采样做预测做动作的思路。非on-policy离线训练。对于on-policy算法，采样先预测然后计算概率来采取动作。（on-policy是激进的，非on-policy是保守的。）</p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="怎么使用tensorboar来进行可视化过程。"><a href="#怎么使用tensorboar来进行可视化过程。" class="headerlink" title="怎么使用tensorboar来进行可视化过程。"></a>怎么使用tensorboar来进行可视化过程。</h3><p>TensorBoard是一个可视化工具，它可以用来展示网络图、张量的指标变化、张量的分布情况等。特别是在训练网络的时候，我们可以设置不同的参数（比如：权重W、偏置B、卷积层数、全连接层数等），使用TensorBoader可以很直观的帮我们进行参数的选择。它通过运行一个本地服务器，来监听6006端口。在浏览器发出请求时，分析训练时记录的数据，绘制训练过程中的图像。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构-shuffleNet</title>
    <link href="/2024/04/30/deeplearnpaper5/"/>
    <url>/2024/04/30/deeplearnpaper5/</url>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/32304419">ShuffleNet论文参考</a><br><a href="https://arxiv.org/pdf/1707.01083">ShuffleNet的论文原文</a></p><h2 id="实践使用shuffleNet来实现垃圾的40分类"><a href="#实践使用shuffleNet来实现垃圾的40分类" class="headerlink" title="实践使用shuffleNet来实现垃圾的40分类"></a>实践使用shuffleNet来实现垃圾的40分类</h2><h3 id="划分固定数据集"><a href="#划分固定数据集" class="headerlink" title="划分固定数据集"></a>划分固定数据集</h3><p>在这里划分固定数据集，生成两个csv表，一个是训练集，一个是验证集</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs lua">import <span class="hljs-built_in">os</span><br>import csv<br>import numpy as np<br>train_path = <span class="hljs-string">&quot;train_data.csv&quot;</span><br>val_path = <span class="hljs-string">&quot;val_data.csv&quot;</span><br><br>train_percent = <span class="hljs-number">0.9</span><br><br>def create_data_txt(<span class="hljs-built_in">path</span>):<br>    f_train = <span class="hljs-built_in">open</span>(train_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&quot;&quot;</span>)<br>    f_val = <span class="hljs-built_in">open</span>(val_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&quot;&quot;</span>)<br>    train_writer = csv.writer(f_train)<br>    val_writer = csv.writer(f_val)<br><br>    <span class="hljs-keyword">for</span> cls,dirname <span class="hljs-keyword">in</span> enumerate(<span class="hljs-built_in">os</span>.listdir(<span class="hljs-built_in">path</span>)):<br>        flist = <span class="hljs-built_in">os</span>.listdir(<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>,dirname))<br>        np.<span class="hljs-built_in">random</span>.shuffle(flist)<br>        fnum = <span class="hljs-built_in">len</span>(flist)<br>        <span class="hljs-keyword">for</span> i,filename <span class="hljs-keyword">in</span> enumerate(flist):<br>            <span class="hljs-keyword">if</span> i &lt; fnum*train_percent:<br>                train_writer.writerow([<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>,dirname,filename),str(cls)])<br>            <span class="hljs-keyword">else</span>:<br>                val_writer.writerow([<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>, dirname, filename), str(cls)])<br><br>    f_train.<span class="hljs-built_in">close</span>()<br>    f_val.<span class="hljs-built_in">close</span>()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    create_data_txt(<span class="hljs-string">&quot;data_garbage&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="dataset-设置。"><a href="#dataset-设置。" class="headerlink" title="dataset 设置。"></a>dataset 设置。</h3><p>在这里设置数据预处理的操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms,utils<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset,DataLoader<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>train_tf = transforms.Compose([<br>    <span class="hljs-comment"># transforms.RandomResizedCrop(size=(224,224), scale=(0.9,1.1)),</span><br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    transforms.RandomRotation(<span class="hljs-number">10</span>),<br>    transforms.ColorJitter(brightness=(<span class="hljs-number">0.9</span>,<span class="hljs-number">1.1</span>),contrast=(<span class="hljs-number">0.9</span>,<span class="hljs-number">1.1</span>)),<br>    <span class="hljs-comment"># transforms.Resize((50,50)),</span><br>    transforms.ToTensor(),<br>])<br><br>val_tf = transforms.Compose([<br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>    <span class="hljs-comment"># transforms.Grayscale(1),</span><br>    transforms.ToTensor(),<br>])<br><br><span class="hljs-comment">#自定义数据集</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Animals_dataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,istrain=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-keyword">if</span> istrain:<br>            f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;train_data.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;val_data.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>)<br>        self.dataset = f.readlines()<br>        f.close()<br>        self.istrain = istrain<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.dataset)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        data = self.dataset[index]<br>        img_path = data.split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">0</span>]<br>        cls = <span class="hljs-built_in">int</span>(data.split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">1</span>])<br><br>        img_data = Image.<span class="hljs-built_in">open</span>(img_path).convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>        <span class="hljs-keyword">if</span> self.istrain:<br>            dst = train_tf(img_data)<br>        <span class="hljs-keyword">else</span>:<br>            dst =val_tf(img_data)<br><br>        <span class="hljs-keyword">return</span> dst,torch.tensor(cls)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">visulization</span>():<br>    train_dataset = Animals_dataset(<span class="hljs-literal">True</span>)<br>    train_dataloader = DataLoader(train_dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>    examples = <span class="hljs-built_in">enumerate</span>(train_dataloader)<br>    batch_index,(data, lable) = <span class="hljs-built_in">next</span>(examples)<br>    <span class="hljs-built_in">print</span>(data.shape)<br><br>    grid = utils.make_grid(data)<br>    plt.imshow(grid.numpy().transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>    plt.show()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    visulization()<br></code></pre></td></tr></table></figure><h3 id="训练代码"><a href="#训练代码" class="headerlink" title="训练代码"></a>训练代码</h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim,nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> dataset <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>m = nn.Softmax(dim=<span class="hljs-number">1</span>)<br>def train(<span class="hljs-keyword">method</span>=&quot;normal&quot;,ckpt_path=&quot;&quot;):<br>    # 数据集和数据加载器<br>    train_dataset = Animals_dataset(<span class="hljs-keyword">True</span>)<br>    train_dataloader = DataLoader(train_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-keyword">True</span>)<br>    val_dataset = Animals_dataset(<span class="hljs-keyword">False</span>)<br>    val_dataloader = DataLoader(val_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-keyword">False</span>)<br><br>    #模型<br>    device = torch.device(&quot;cuda&quot; <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> &quot;cpu&quot;)#系统自己决定有啥训练<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">method</span>==&quot;normal&quot;:<br>       # 创建ShuffleNet模型<br>        model = models.shufflenet_v2_x1_0(pretrained=<span class="hljs-keyword">True</span>)  # 使用预训练的ShuffleNetV2模型<br><br>        # 修改最后的全连接层以适应您的数据集<br>        num_ftrs = model.fc.in_features<br>        model.fc = nn.Linear(num_ftrs,<span class="hljs-number">40</span>)  # 将全连接层输出维度修改为您数据集的类别数<br>        model.<span class="hljs-keyword">to</span>(device)<br>    print(&quot;train on &quot;,device)<br>    #损失函数（二分类交叉熵）<br>    loss_fn = nn.CrossEntropyLoss()<br><br>    #优化器<br>    optimizer = optim.RMSprop(model.parameters(),lr=<span class="hljs-number">0.0001</span>)<br><br>    #断点恢复<br>    start_epoch = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">if</span> ckpt_path != &quot;&quot;:<br>        <span class="hljs-keyword">checkpoint</span> = torch.<span class="hljs-keyword">load</span>(ckpt_path)<br>        model.load_state_dict(<span class="hljs-keyword">checkpoint</span>[&quot;net&quot;])<br>        optimizer.load_state_dict(<span class="hljs-keyword">checkpoint</span>[&quot;optimizer&quot;])<br>        start_epoch = <span class="hljs-keyword">checkpoint</span>[&quot;epoch&quot;] + <span class="hljs-number">1</span><br><br>    #训练<br>    train_loss_arr = []<br>    train_acc_arr = []<br>    val_loss_arr = []<br>    val_acc_arr = []<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">30</span>):<br>        train_loss_total = <span class="hljs-number">0</span><br>        train_acc_total = <span class="hljs-number">0</span><br>        val_loss_total = <span class="hljs-number">0</span><br>        val_acc_total = <span class="hljs-number">0</span><br>        model.train()<br>        <span class="hljs-keyword">for</span> i,(train_x,train_y) <span class="hljs-keyword">in</span> enumerate(train_dataloader):<br>            train_x = train_x.<span class="hljs-keyword">to</span>(device)<br>            train_y = train_y.<span class="hljs-keyword">to</span>(device)<br><br>            train_y_pred = model(train_x)<br>            train_loss = loss_fn(train_y_pred.squeeze(),train_y)<br>            train_acc = (m(train_y_pred).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] == train_y).sum()/train_y.shape[<span class="hljs-number">0</span>]<br>            train_loss_total += train_loss.data.item()<br>            train_acc_total += train_acc.data.item()<br><br>            train_loss.backward()<br>            optimizer.step()<br>            optimizer.zero_grad()<br><br>            print(&quot;epoch:&#123;&#125; train_loss: &#123;&#125; train_acc: &#123;&#125;&quot;.format(epoch,train_loss.data.item(),train_acc.data.item()))<br>        <br>        train_loss_arr.append(train_loss_total / len(train_dataloader))<br>        train_acc_arr.append(train_acc_total / len(train_dataloader))<br><br>        model.eval()<br><br>        <span class="hljs-keyword">for</span> j, (val_x,val_y) <span class="hljs-keyword">in</span> enumerate(val_dataloader):<br>            val_x = val_x.<span class="hljs-keyword">to</span>(device)<br>            val_y = val_y.<span class="hljs-keyword">to</span>(device)<br><br>            val_y_pred = model(val_x)<br>            val_loss = loss_fn(val_y_pred.squeeze(),val_y)<br>            val_acc = (m(val_y_pred).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]==val_y).sum()/val_y.shape[<span class="hljs-number">0</span>]<br>            val_loss_total += val_loss.data.item()<br>            val_acc_total += val_acc.data.item()<br><br>        val_loss_arr.append(val_loss_total / len(val_dataloader))  # 平均值<br>        val_acc_arr.append(val_acc_total / len(val_dataloader))<br>        print(&quot;epoch:&#123;&#125; val_loss:&#123;&#125; val_acc:&#123;&#125;&quot;.format(epoch, val_loss_arr[<span class="hljs-number">-1</span>], val_acc_arr[<span class="hljs-number">-1</span>]))<br><br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)   # 画布一分为二,<span class="hljs-number">1</span>行<span class="hljs-number">2</span>列，用第一个<br>    plt.title(&quot;loss&quot;)<br>    plt.plot(train_loss_arr, &quot;r&quot;, label=&quot;train&quot;)<br>    plt.plot(val_loss_arr, &quot;b&quot;, label=&quot;val&quot;)<br>    plt.legend()<br><br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  # 画布一分为二,<span class="hljs-number">1</span>行<span class="hljs-number">2</span>列，用第一个<br>    plt.title(&quot;acc&quot;)<br>    plt.plot(train_acc_arr, &quot;r&quot;, label=&quot;train&quot;)<br>    plt.plot(val_acc_arr, &quot;b&quot;, label=&quot;val&quot;)<br>    plt.legend()<br>    plt.savefig(&quot;loss_acc-1.png&quot;)<br><br>    plt.<span class="hljs-keyword">show</span>()<br><br>    # 保存模型<br>    # <span class="hljs-number">1.</span>torch.save()<br>    # <span class="hljs-number">2.</span>文件的后缀名：.pt、.pth、.pkl<br>    torch.save(model.state_dict(), r&quot;shuffeNet.pth&quot;)<br>    print(&quot;保存模型成功!&quot;)<br><br><br><br><span class="hljs-keyword">if</span> __name__ == &quot;__main__&quot;:<br>    train()<br><br><br>    train()<br><br><br></code></pre></td></tr></table></figure><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="什么是断点训练"><a href="#什么是断点训练" class="headerlink" title="什么是断点训练"></a>什么是断点训练</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文思路——论文阅读文献</title>
    <link href="/2024/04/30/paper-idear3/"/>
    <url>/2024/04/30/paper-idear3/</url>
    
    <content type="html"><![CDATA[<p>阅读工具<a href="https://kimi.moonshot.cn/">Kimi</a></p><h2 id="论文阅读记录"><a href="#论文阅读记录" class="headerlink" title="论文阅读记录"></a>论文阅读记录</h2><ol><li><p><a href="https://ieeexplore.ieee.org/abstract/document/10179900">DICE-Net</a> 发表在IEEE Access 2022-2023年实时影响因子为3.9，中科院分区3区到4区。论文为OA论文，开源。<br>启发是对数据预处理的方法（比如RBP，SCC），一种未验证的双输入数据模型结构（注：我看来就是数据的通道加一）。这篇论文在公开 <a href="https://www.mdpi.com/2306-5729/8/6/95">数据集</a>  做的分类为CN&#x2F;AD,FTD&#x2F;CN,这么划分，是因为他要做对比实验，横向对比。实验结果是CN&#x2F;AD 的ACC为83.28%，SENS 79.81% ，SPEC 为87.94%, PREC为88.94%，F1为84.12% 。在FTD&#x2F;CN上ACC为74.96%，SENS 60.62% ，SPEC 为78.63%, PREC为64.01%，F1为62.27%<br><img src="/pic/lwsl2.png" alt="论文结果"></p></li><li><p><a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1272834/full">静息状态下脑电信号多特征融合学习预测阿尔茨海默病</a> 2023年<br>这篇论文在AD&#x2F;FDT&#x2F;CN 三分类上达到了80.23%的准确率。论文中提到了（DICE-net），还提到了一篇论文使用Transformer-based methodlogy<a href="https://www.sciencedirect.com/science/article/abs/pii/S0378437122004642">链接</a>     在公开数据集SEED上进行情绪检测在三类问题上实现了83.03的准确率。<br>在2020年，一篇论文<a href="https://ieeexplore.ieee.org/document/9162148"></a>中使用了 Fast Fourier Transform （FFT）来进行数据预处理，然后经过处理的数据给入CNN网络，实现了79%的结果。<br><img src="/pic/lwsl3.png" alt="论文结果"><br>看着看着感觉这篇论文有点细节问题没有处理好。（注： 这篇论文有点小毛病,明明做的FDT的分类，为什么有MCI的标记呀）</p></li><li><p><a href="https://www.mdpi.com/2306-5729/8/6/95">A Dataset of Scalp EEG Recordings of Alzheimer’s Disease, Frontotemporal Dementia and Healthy Subjects from Routine EEG</a>提供了19通道的 (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2) 的脑电记录数据，有36个AD患者，23个FDT（前额叶痴呆），29个CN对照。数据包含了未经过伪迹处理的和已经经过伪迹处理的信号,同时论文中也使用了一定的方法来对数据进行处理。<br><img src="/pic/lwsl.png" alt="论文结果"><br>数据集获取链接<a href="https://openneuro.org/datasets/ds004504/versions/1.0.7">链接</a></p></li><li><p><a href="https://iopscience.iop.org/article/10.1088/1741-2552/ac05d8">Deep learning of resting-state electroencephalogram signals for three-class classification of Alzheimer’s disease, mild cognitive impairment and healthy ageing</a>2021年，使用AlexNet来作分类。进一步数据处理的方法为CWT，使用Adam，作为思路。</p></li></ol><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="公开数据集介绍"><a href="#公开数据集介绍" class="headerlink" title="公开数据集介绍"></a>公开数据集介绍</h3><p>This article provides a detailed description of a resting-state EEG dataset of individuals with Alzheimer’s disease and frontotemporal dementia, and healthy controls. The dataset was collected using a clinical EEG system with 19 scalp electrodes while participants were in a resting state with their eyes closed. The data collection process included rigorous quality control measures to ensure data accuracy and consistency. The dataset contains recordings of 36 Alzheimer’s patients, 23 frontotemporal dementia patients, and 29 healthy age-matched subjects.<br>提供了19通道的 (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2) 的脑电记录数据，有36个AD患者，23个FDT（前额叶痴呆），29个CN对照。数据包含了未经过伪迹处理的和已经经过伪迹处理的信号（伪迹处理的信号过程请参考论文）</p><h3 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h3><p>鉴于论文大修，所以我打算重新构建一下思路。</p><ol><li><p>标题</p></li><li><p>摘要</p></li><li><p>关键词</p></li><li><p>介绍</p></li><li><p>数据和方法</p></li><li><p>结果</p></li><li><p>讨论</p></li><li><p>参考文献</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数1</title>
    <link href="/2024/04/29/xianxindaishu/"/>
    <url>/2024/04/29/xianxindaishu/</url>
    
    <content type="html"><![CDATA[<p>神经元的工程实现使用了矩阵来作为工程学上的实现。线性代数研究向量空间和线性映射的理论。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ol><li>行列式 ，行列式的概念和基本性质 行列式按行（列）展开定理 。</li><li>矩阵 ， 矩阵的概念， 矩阵的线性运算， 矩阵的乘法， 方阵的幂， 方阵乘积的行列式， 矩阵的转置，逆矩阵的概念和性质， 矩阵可逆的充分必要条件， 伴随矩阵，矩阵的初等变换，初等矩阵，矩阵的秩，矩阵的等价 分块矩阵及其运算。 </li><li>向量 ，向量的概念， 向量的线性组合和线性表示， 向量组的线性相关与线性无关， 向量组的极， 大线性无关组， 等价向量组， 向量组的秩， 向量组的秩与矩阵的秩之间的关系， 向量的内积 ，线性无关向量组的的正交规范化方法 。</li><li>线性方程组 ，线性方程组的克拉默（Cramer）法则， 齐次线性方程组有非零解的充分必要条件， 非齐次线性方程组有解的充分必要条件， 线性方程组解的性质和解的结构， 齐次线性方程组的基础解系和通解， 非齐次线性方程组的通解 。</li><li>矩阵的特征值和特征向量， 矩阵的特征值和特征向量的概念、性质， 相似矩阵的概念及性质， 矩阵可相似对角化的充分必要条件及相似对角矩阵， 实对称矩阵的特征值、特征向量及其相似对角矩阵 。</li><li>二次型 ，二次型及其矩阵表示，合同变换与合同矩阵，二次型的秩，惯性定理，二次型的标准形和规范形， 用正交变换和配方法化二次型为标准形，二次型及其矩阵的正定性。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>有意思的句子</title>
    <link href="/2024/04/29/juzhi4/"/>
    <url>/2024/04/29/juzhi4/</url>
    
    <content type="html"><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>腰有十文，必振衣作响；</p><p>每遇美人，必急色登床；</p><p>相逢故交，必装逼摆阔；</p><p>每与人言，必谈及贵戚；</p><p>施人小惠，必广布于众；</p><p>与人言谈，必刁言逞才；</p><p>当人前称兄道弟，背人后揭人隐私；</p><p>借钱时觍颜如乞，拖款时蛮横如王。</p><p>—— 改述自林语堂</p><h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><p>小有姿色，则矫揉造作；</p><p>每逢故友，必揭人情史；</p><p>屡踩前任，且喋喋不休；</p><p>偶购名牌，必通告全网；</p><p>严人宽己，常自怜自艾；</p><p>妒人姻缘，必暗中破坏；</p><p>用你时姐妹情深，不用你音讯全无；</p><p>当人前惺惺作态，背人后口吐芬芳。</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高等数学</title>
    <link href="/2024/04/29/gaodengshuxue1/"/>
    <url>/2024/04/29/gaodengshuxue1/</url>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/105704401">我在知乎学数学</a><br><a href="https://www.zhihu.com/question/336322284/answer/918067537">什么是微积分</a></p><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>最近需要解析梯度下降算法，对于矩阵搞不清楚怎么进行多矩阵的梯度下降，算起来是补充基础了。</p><p><img src="/pic/gaodengshuxue1.jpg" alt="数学的图"></p><h2 id="高等数学研究了什么问题？"><a href="#高等数学研究了什么问题？" class="headerlink" title="高等数学研究了什么问题？"></a>高等数学研究了什么问题？</h2><p>高等数学是由微积分学，较深入的代数学、几何学以及它们之间的交叉内容所形成的一门基础学科。主要内容包括：数列、极限、微积分、空间解析几何与线性代数、级数、常微分方程。初等数学研究的是常量与匀变量，高等数学研究的是非匀变量。</p><ol><li><p>函数、极限、连续。<br>函数的概念是什么？函数的有界性、单调性、周期性和奇偶性是什么？复合函数、反函数、分段函数和隐函数。基本初等函数的性质。函数关系的建立。<br>数列极限与函数极限的定义及其性质，函数的左极限与右极限，无穷小量和无穷大量的概念及其关系，无穷小量的性质及无穷小量的比较。极限的存在准则：单调有界性准则和夹逼准则，两个重要极限。<br>函数连续的概念，函数间断点的类型，初等函数的连续性，闭区间上连续函数的性质。</p></li><li><p>一元函数的微分的问题。什么是微分？导数、微分。<br>导数和微分的概念，导数的几何意义和物理意义，函数的可导性与连续性之间的关系。<br>平面曲线的切线和法线，导数和微分的四则运算，基本初等函数的导数，复合函数、反函数、隐函数以及参数方程所确定的函数的微分法，高阶段导数，一阶段微分形式的不变性，微分中值定理（罗尔中值定理，拉格朗日中值定理，泰勒定理，柯西中值定理），洛必达法则，函数单调性的判别，函数的极值，函数的凹凸性，拐点以及渐近线，函数图形的描绘，函数的最大小值。<br>弧微分，曲率的概念，曲率圆与曲率半径。</p></li><li><p>一元函数积分学。<br>原函数和不定积分的概念，不定积分的基本性质，基本积分公式，定积分的概念和基本性质，定积分中值定理，积分上限的函数及其导数，牛顿-莱布尼茨（Newton-Leibniz）公式，不定积分和定积分的换元积分法与分部积分法。<br>有理函数、三角函数的有理式和简单无理函数的积分，反常（广义）积分，定积分的应用（平面图形的面积，平面曲线的弧长，旋转体的体积及侧面积，平行截面积为已知的立体体积、功、引力、压力、质心、形心等）及函数平均值。</p></li><li><p>多元函数微分学。<br>多元函数的概念，二元函数的几何意义，二元函数的极限与连续的概念，有界闭区间上二元连续函数的性质，多元函数的偏导数和全微分，多元复合函数、隐函数的求导法，二阶偏导数，多元函数的极值和条件极值、最大值和最小值，二重积分的概念、基本性子和计算。</p></li><li><p>常微分方程。<br>常微分方程的基本概念，变量可分离的微分方程，齐次微分方程，一阶线性微分方程 ，可降阶的高阶微分方程，线性微分方程解的性质及解的结构定理，二阶常系数齐次线性微分方程， 高于二阶的某些常系数齐次线性微分方程，简单的二阶常系数非齐次线性微分方程 ，微分方程的简单应用 </p></li><li><p>级数</p></li></ol><h2 id="填坑"><a href="#填坑" class="headerlink" title="填坑"></a>填坑</h2><h3 id="函数的概念"><a href="#函数的概念" class="headerlink" title="函数的概念"></a>函数的概念</h3><p>在数学中，函数是一种对应关系，它将一个集合中的每个元素（称为自变量）映射到另一个集合中的唯一元素（称为因变量）。函数通常用一个符号表示，例如 f(x)，表示自变量为 x 时对应的因变量值。<br>函数的概念包括以下几个要点：</p><p>定义域（Domain）： 函数的定义域是指所有可能作为自变量的值的集合。函数在定义域内有定义。<br>值域（Range）： 函数的值域是指所有可能作为因变量的值的集合。对于实函数（实数到实数的映射），值域是函数的所有可能取值的集合。<br>图像（Graph）： 函数的图像是在坐标系中表示函数的一种方式，横轴表示自变量，纵轴表示因变量。函数的图像可以帮助我们直观地理解函数的性质。<br>对应关系（Correspondence）： 函数是一种对应关系，它确保了每个自变量都有唯一的因变量与之对应。<br>函数的表示方式： 函数可以通过公式、图表、数据表等方式来表示。常见的函数包括多项式函数、指数函数、对数函数、三角函数等。</p><p>注： 补充概念，数学中的“元”指未知数，常见的一元一次，二元一次。数学中的“项”代表由数与未知数还有运算符号组成的基本算术单元。“次”就是方程中未知数的乘方数。<br><img src="/pic/gaodengshuxue2.png" alt="一元二次方程组"></p><h3 id="函数的性质。"><a href="#函数的性质。" class="headerlink" title="函数的性质。"></a>函数的性质。</h3><p>函数的有界性、单调性、周期性和奇偶性是什么？</p><p>有界性（Boundedness）：一个函数在某个区间内是有界的，意味着存在一个常数M，使得函数的取值始终在一个特定的范围内，即|f(x)| ≤ M，对于所有的x在给定的区间内成立。如果一个函数既有上界又有下界，则称该函数在该区间内是有界的。<br>函数的有界性判断方法？</p><p>单调性（Monotonicity）：一个函数在某个区间内是单调的，意味着函数的值随着自变量的增加而增加或者随着自变量的减少而减少。如果函数在区间上满足f(x1) ≤ f(x2)对于任意x1 &lt; x2或者f(x1) ≥ f(x2)对于任意x1 &lt; x2，则称函数在该区间内是单调递增或单调递减的。<br>函数单调性判断方法？</p><ol><li>定义法</li><li>导数法</li></ol><p>周期性（Periodicity）：一个函数在某个区间内是周期的，意味着存在一个正数T，使得对于所有的x在该区间内，有f(x+T) &#x3D; f(x)。即函数在一个周期内重复。</p><p>奇偶性（Odd and Even）：一个函数的奇偶性指的是函数的对称性。如果对于所有的x在定义域内，有f(-x) &#x3D; -f(x)，则称函数是奇函数。如果对于所有的x在定义域内，有f(-x) &#x3D; f(x)，则称函数是偶函数。</p><h3 id="复合函数"><a href="#复合函数" class="headerlink" title="复合函数"></a>复合函数</h3><p>设函数y&#x3D;f(u)的定义域为Du，函数u&#x3D;g(x)的定义域为Dg，值域Rg。若Du和Rg的交集不为空集，则称函数y&#x3D;f[g(x) ]为函数y&#x3D;f(u)与函数u&#x3D;g(x )的复合函数，它的定义域为{x|x属于Dg，g(x)属于Du}。<br>例如：如果有函数f(x) &#x3D; x^2 和g(x) &#x3D; 2x +1,那么他们的复合函数f(g(x))就是先计算g(x)，得到2x+1，然后将2x+1作为f(x)的输入，最终得到f(g(x)) &#x3D; (2x+1)^2<br>复合函数的性质：</p><ol><li>结合律：即(f。g)。h&#x3D;f。(g。h)，其中。表示复合。</li><li>交换律：一般情况下，复合函数不满足交换律，即一般情况下f。g&#x3D;&#x2F;&#x3D;g。f</li><li>单位元：对于每个函数f(x)，都存在一个单位元函数e(x) &#x3D; x ，使得f。e &#x3D; f和e。f &#x3D; f</li></ol><h3 id="反函数"><a href="#反函数" class="headerlink" title="反函数"></a>反函数</h3><p>反函数是指一个函数的逆运算，如果函数f将集合A中的元素映射到集合B中的元素，那么f的反函数f^-1将集合B中的元素映射回集合A中的元素，使得f^-1(f(x))&#x3D;x对于所有x成立。<br>例如：如果有函数f(x) &#x3D; 2x ,它将实数集合中的每一个数映射到其自身的两倍，那么它的反函数就是f^-1(x) &#x3D; x&#x2F;2，将任意实数x映射回其的一半。反函数在数学中可以用来解决函数的逆运算问题。</p><h3 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h3><p>分段函数是指由多个部分组成的函数，每个部分在定义域的不同区间内具有不同的表达式。例如，绝对值函数就是一个常见的分段函数，它在正数区间和负数区间的表达式是不同的。分段函数用两个或两个以上的 式子表示，分段函数不是一个函数，判断分段函数的值域、运算、性质时，都需要在相应的定义域范围内进行。<br><a href="https://zhuanlan.zhihu.com/p/662140765">参考博客</a></p><h3 id="隐函数"><a href="#隐函数" class="headerlink" title="隐函数"></a>隐函数</h3><p>隐函数是由隐式方程所隐含定义的函数。设F（x,y）是某个定义域上的函数。如果存在定义域上的子集D，使得对每个x属于D，存在相应的y满足F(x,y)&#x3D;0，则称方程确定了一个隐函数。记为y&#x3D;y(x)。 [2]显函数是用y&#x3D;f(x)来表示的函数，显函数是相对于隐函数来说的。<br>如果在方程F(x,y)&#x3D;0中，当x取某区间内的任一值时，相应地总有满足此方程唯一的y值存在，那么方程F(x,y)&#x3D;0在该区间内确定了一个一元隐函数。类似若有一个三元方程F(x,y,z)&#x3D;0所确定的二元函数z&#x3D;f(x,)存在，则有可能确定一个二元隐函数。<br>例如，圆的方程 x^2 + y^2 &#x3D; r^2 中就包含了一个隐函数关系。<br><a href="https://www.zhihu.com/people/wo-zai-kan-19">参考博客</a></p><h3 id="基本初等函数的性质"><a href="#基本初等函数的性质" class="headerlink" title="基本初等函数的性质"></a>基本初等函数的性质</h3><p>基本初等函数包括常数函数、幂函数、指数函数、对数函数、三角函数和反三角函数等。它们具有一些常见的性质，如奇偶性、周期性、单调性等。例如，正弦函数是一个奇函数，具有周期性和单调性。（反三，对，幂，指，三）<br><img src="/pic/jbcdhs1.png" alt="常数函数"><br><img src="/pic/jbcdhs2.png" alt="幂函数"><br><img src="/pic/jbcdhs3.png" alt="指数函数"><br><img src="/pic/jbcdhs4.png" alt="对数函数"><br><img src="/pic/jbcdhs5.png" alt="三角函数-正弦函数"><br><img src="/pic/jbcdhs6.png" alt="三角函数-余弦函数"><br><img src="/pic/jbcdhs7.png" alt="三角函数-正切函数"><br><img src="/pic/jbcdhs8.png" alt="三角函数-余切函数"><br><img src="/pic/jbcdhs9.png" alt="反三角函数-反正弦函数"><br><img src="/pic/jbcdhs10.png" alt="反三角函数-反余弦函数"><br><img src="/pic/jbcdhs11.png" alt="反三角函数-反正切函数"><br><img src="/pic/jbcdhs12.png" alt="反三角函数-反余切函数"><br><a href="https://blog.csdn.net/chaotiantian/article/details/115029466">参考博客</a><br>注:初等函数是指可以由有限次常数函数、幂函数、指数函数、对数函数、三角函数和反三角函数通过有限次四则运算和复合运算（即函数的和、差、积、商以及函数的复合）构成的函数。</p><h3 id="函数关系的建立："><a href="#函数关系的建立：" class="headerlink" title="函数关系的建立："></a>函数关系的建立：</h3><p>建立函数关系通常涉及到确定函数的表达式或规律，以描述输入和输出之间的关系。这可以通过观察规律、实验数据或者解决实际问题来完成。例如，建立两个变量之间的线性关系可以通过观察数据点来确定斜率和截距，从而建立线性函数的关系。、</p><h2 id="极限"><a href="#极限" class="headerlink" title="极限"></a>极限</h2><p>数列极限与函数极限的定义及其性质，函数的左极限与右极限，无穷小量和无穷大量的概念及其关系，无穷小量的性质及无穷小量的比较。极限的存在准则：单调有界性准则和夹逼准则，两个重要极限。</p><h3 id="数列极限与函数极限的定义及其性质："><a href="#数列极限与函数极限的定义及其性质：" class="headerlink" title="数列极限与函数极限的定义及其性质："></a>数列极限与函数极限的定义及其性质：</h3><p>数列极限的定义： 对于数列{an}，如果对于任何给定的正数E，存在正整数N，使得当n&gt;N时，有|an-A| &lt; E ,称数列{an}收敛于A，记住lim n 趋近于无穷an &#x3D; A<br>函数极限的定义： 设函数f(x)在点x0的某个去心领域内有定义，如果对于任意给定的正数E，存在正数&amp;，使得当0 &lt; |x - x0| &lt; &amp; 时，有|f(x) - A | &lt; E,则称函数f(x)在点x0处的极限为A，记住lim x——&gt;f(x) &#x3D; A<br>性质：数列和函数极限都具有唯一性、局部有界性、四则运算性质。<br><a href="https://zhuanlan.zhihu.com/p/350844135"></a></p><p>计算函数极限的方法有，带入计算，等价无穷小（泰勒展开的一阶展开。），泰勒展开，洛必达（只能乘除不能加减。）<br><img src="/pic/tailezhankai.png" alt="等价无穷小泰勒展开"><br><img src="/pic/tailezhankai2.png" alt="泰勒展开"><br><img src="/pic/tailezhankai3.png" alt="泰勒展开"></p><h3 id="背景补充"><a href="#背景补充" class="headerlink" title="背景补充"></a>背景补充</h3><p>微积分：分为微积与微分。 微积： 由无数个无穷小的面积组成的面积S，对应一元函数微分学。微分学的基本概念是导数，导数代表斜率，斜率刚好就是这条直线和夹角的正切值，就是说直线和x轴的夹角越大，就倾斜越大。但是对于曲线来讲，曲线跟直线不同，完全可以在不同的地方的倾斜程度是不一样的，所以，我们不能说一条曲线的倾斜程度（”斜率”）而只能说曲线在某个点的倾斜程度。故此，需要引入一个概念，切线观地看，就是刚好在这点“碰到”曲线的直线。因为切线是直线，所以切线有斜率，于是我们就可以用切线的斜率代表曲线在这点的倾斜程度。利用无穷小定义了一点上的切线，我们就可以理所当然地用过这点切线的斜率来表示曲线在这点的倾斜度了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文思路——数据预处理</title>
    <link href="/2024/04/28/paper-idear2/"/>
    <url>/2024/04/28/paper-idear2/</url>
    
    <content type="html"><![CDATA[<h2 id="目前的数据预处理有"><a href="#目前的数据预处理有" class="headerlink" title="目前的数据预处理有"></a>目前的数据预处理有</h2><p>This may be time-domain features(时域特征)<a href="dx.doi.org/10.3390/diagnostics11081437">45——2021年</a></p><ol><li>absolute band power(绝对功率谱)</li><li>Discrete Wavelet Transform (离散小波变换)<a href="https://ieeexplore.ieee.org/document/9857825">37-2022-IEEE</a></li><li>permutation entropy (排列熵) or spectral entropy (熵谱)[21-2021-Complexity of EEG dynamics for early diagnosis of Alzheimer’s disease using permutation entropy neuromarker,’’ C]</li><li>coherence anaylysis features (相干性分析) such as spectrall coherece(光谱相干性)</li><li>RBP (注：按照下面的频率划分，the shape of data is [T , B ,C]&#x3D;[T, 5, C]，Delta: 0.5 – 4 Hz Theta: 4 – 8 Hz Alpha: 8 – 13 Hz Beta: 13-25 Hz Gamma: 25-45 Hz,或许这个数据预处理的方法会生效 )</li><li>spectral coherence connectivity (光谱相干性，光谱相干性建立在PSD，PSD是功率谱密度)</li><li>FFT (Fast Fourier Transform) 快速傅里叶变换</li></ol><h2 id="数据预处理组合思路"><a href="#数据预处理组合思路" class="headerlink" title="数据预处理组合思路"></a>数据预处理组合思路</h2><ol><li>Morlet Wavelet Transform ——&gt; RBP</li><li>Welch PSD ——&gt; SSC</li></ol><h2 id="需要注意的前提知识。"><a href="#需要注意的前提知识。" class="headerlink" title="需要注意的前提知识。"></a>需要注意的前提知识。</h2><ol><li>AD patients may exhaibit changes in the EEG signal, such as reduced(减少) alpha power (Alpha: 8 – 13 Hz ) and increased (增加) theta power (: 4 – 8 Hz).<a href="https://www.sciencedirect.com/science/article/abs/pii/S1388245721005976">39-2021-Clinical Neurophysiology-3区 </a><br>It can be visually observed that AD group has lower delta connectivity than CN group in multiple brain locations.This finding is supported by the literature  [53-2016]<a href="https://www.sciencedirect.com/science/article/pii/S1388245715009839()">https://www.sciencedirect.com/science/article/pii/S1388245715009839()</a></li><li>Train, validation and test sets are created.</li><li>The time frequency transforms and the feature extraction steps were implemented in Python 3.10 using the MNE library.</li><li>GFlops (计算量)</li><li>hyperparameters (超参数)</li></ol><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="怎么计算模型的计算量。"><a href="#怎么计算模型的计算量。" class="headerlink" title="怎么计算模型的计算量。"></a>怎么计算模型的计算量。</h3><h2 id="看论文的心态"><a href="#看论文的心态" class="headerlink" title="看论文的心态"></a>看论文的心态</h2><p>Because it is an English paper, there is a kind of resistance. Take your time.<br>由于是英文论文，有种抗拒的心态。慢慢看吧。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><p>FFT快速傅里叶变换<a href="https://zhuanlan.zhihu.com/p/347091298">参考博客</a></p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import os<br>import numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-built_in">from</span> tqdm import tqdm<br><br><span class="hljs-comment"># 定义输入和输出文件夹路径</span><br>input_folder = <span class="hljs-string">&#x27;data_cut_npy/AD&#x27;</span>  <span class="hljs-comment"># 输入文件夹路径，包含要进行 FFT 变换的 .npy 文件</span><br>output_folder = <span class="hljs-string">&#x27;data_FFT_npy/AD&#x27;</span>  <span class="hljs-comment"># 输出文件夹路径，用于保存变换后的数据</span><br><br><span class="hljs-comment"># 确保输出文件夹存在</span><br>os.makedirs(output_folder, exist_ok=True)<br><br><span class="hljs-comment"># 获取输入文件夹中的所有 .npy 文件</span><br>file_list = os.listdir(input_folder)<br>npy_files = [<span class="hljs-built_in">file</span> <span class="hljs-keyword">for</span> <span class="hljs-built_in">file</span> <span class="hljs-keyword">in</span> file_list <span class="hljs-keyword">if</span> <span class="hljs-built_in">file</span>.endswith(<span class="hljs-string">&#x27;.npy&#x27;</span>)]<br><br><span class="hljs-comment"># 遍历每个 .npy 文件进行 FFT 变换并保存</span><br><span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> tqdm(npy_files, desc=<span class="hljs-string">&#x27;Processing&#x27;</span>, unit=<span class="hljs-string">&#x27;file&#x27;</span>):<br>    <span class="hljs-comment"># 读取 .npy 文件</span><br>    file_path = os.path.join(input_folder, file_name)<br>    data = np.<span class="hljs-built_in">load</span>(file_path)<br><br>    <span class="hljs-comment"># 对数据中的每一行进行 FFT 变换</span><br>    fft_data = np.apply_along_axis(np.fft.fft, axis=<span class="hljs-number">0</span>, arr=data)<br><br>    <span class="hljs-comment"># 获取 FFT 结果的幅值</span><br>    fft_magnitude = np.<span class="hljs-built_in">abs</span>(fft_data)<br><br>    <span class="hljs-comment"># 构造输出文件路径</span><br>    output_file_name = file_name.<span class="hljs-built_in">replace</span>(<span class="hljs-string">&#x27;.npy&#x27;</span>, <span class="hljs-string">&#x27;_fft.npy&#x27;</span>)<br>    output_file_path = os.path.join(output_folder, output_file_name)<br><br>    <span class="hljs-comment"># 保存 FFT 结果</span><br>    np.save(output_file_path, fft_magnitude)<br><br></code></pre></td></tr></table></figure></li><li><p>RBP 按频率划分[T, C, B]</p></li></ol><h2 id="什么是时域和频域"><a href="#什么是时域和频域" class="headerlink" title="什么是时域和频域"></a>什么是时域和频域</h2><p><a href="https://zhuanlan.zhihu.com/p/401681076">参考资料</a><br>原始的EEG数据是由很多个样本点数所构成的一个有限的离散的时间序列数据。至于样本点数的多少，则由采样率所决定，比如采样率为1000Hz，那么每秒就有1000个数据样本点。其中，每个样本点数据代表的是脑电波幅的大小，物理学上称为电压值，单位为伏特（V），由于脑电信号通常较弱，所以更常使用的单位为微伏（uV）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> mne<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_and_save_set_files</span>(<span class="hljs-params">input_folder, output_folder</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_data</span>(<span class="hljs-params">raw</span>):<br>        <span class="hljs-comment"># 获取信号数据</span><br>        data = raw.get_data() <span class="hljs-comment"># shape: (n_channels, n_samples)</span><br><br>        <span class="hljs-comment"># 定义频率范围</span><br>        freq_ranges = &#123;<br>            <span class="hljs-string">&#x27;Delta&#x27;</span>: (<span class="hljs-number">0.5</span>, <span class="hljs-number">4</span>),<br>            <span class="hljs-string">&#x27;Theta&#x27;</span>: (<span class="hljs-number">4</span>, <span class="hljs-number">8</span>),<br>            <span class="hljs-string">&#x27;Alpha&#x27;</span>: (<span class="hljs-number">8</span>, <span class="hljs-number">13</span>),<br>            <span class="hljs-string">&#x27;Beta&#x27;</span>: (<span class="hljs-number">13</span>, <span class="hljs-number">25</span>),<br>            <span class="hljs-string">&#x27;Gamma&#x27;</span>: (<span class="hljs-number">25</span>, <span class="hljs-number">45</span>)<br>        &#125;<br><br>        <span class="hljs-comment"># 初始化频带划分后的数据</span><br>        data_bands = []<br><br>        <span class="hljs-comment"># 对每个频带进行处理</span><br>        <span class="hljs-keyword">for</span> _, (fmin, fmax) <span class="hljs-keyword">in</span> freq_ranges.items():<br>            <span class="hljs-comment"># 使用 mne.filter 函数进行滤波</span><br>            filtered_data = mne.<span class="hljs-built_in">filter</span>.filter_data(data, raw.info[<span class="hljs-string">&#x27;sfreq&#x27;</span>], fmin, fmax)<br><br>            <span class="hljs-comment"># 将滤波后的数据存储到列表中</span><br>            data_bands.append(filtered_data)<br><br>        <span class="hljs-comment"># 将列表转换为numpy数组</span><br>        data_bands = np.array(data_bands)<br><br>        <span class="hljs-keyword">return</span> data_bands<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">find_set_files</span>(<span class="hljs-params">root_folder</span>):<br>        set_files = []<br>        <span class="hljs-keyword">for</span> root, dirs, files <span class="hljs-keyword">in</span> os.walk(root_folder):<br>            <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>                <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&#x27;.set&#x27;</span>):<br>                    set_files.append(os.path.join(root, file))<br>        <span class="hljs-keyword">return</span> set_files<br><br>    <span class="hljs-comment"># 找到所有的 .set 文件</span><br>    set_files = find_set_files(input_folder)<br><br>    <span class="hljs-keyword">for</span> file_path <span class="hljs-keyword">in</span> tqdm(set_files, desc=<span class="hljs-string">&#x27;Processing files&#x27;</span>):<br>        <span class="hljs-comment"># 读取 .set 文件</span><br>        raw = mne.io.read_raw_eeglab(file_path, preload=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># 处理数据</span><br>        processed_data = process_data(raw)<br><br>        <span class="hljs-comment"># 构造新的文件路径</span><br>        npy_file_name = os.path.basename(file_path).replace(<span class="hljs-string">&#x27;.set&#x27;</span>, <span class="hljs-string">&#x27;.npy&#x27;</span>)<br>        npy_file_path = os.path.join(output_folder, npy_file_name)<br><br>        <span class="hljs-comment"># 保存为 .npy 文件</span><br>        np.save(npy_file_path, processed_data)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 指定您的 AD 文件夹路径和保存 .npy 文件的新文件夹路径</span><br>    ad_folder = <span class="hljs-string">r&#x27;C:/Users/Administrator/Desktop/RBP/data/FDT&#x27;</span><br>    output_folder = <span class="hljs-string">r&#x27;C:/Users/Administrator/Desktop/RBP/data_npy/FDT&#x27;</span><br><br>    <span class="hljs-comment"># 处理并保存数据</span><br>    process_and_save_set_files(ad_folder, output_folder)<br><br></code></pre></td></tr></table></figure><h2 id="赫兹-HZ-的定义是什么？"><a href="#赫兹-HZ-的定义是什么？" class="headerlink" title="赫兹(HZ)的定义是什么？"></a>赫兹(HZ)的定义是什么？</h2><p>Hz 是频率的单位。频率是指电脉冲，交流电波形，电磁波，声波和机械的振动周期循环时，1秒钟重复的次数。1Hz代表每秒钟周期震动1次，60Hz代表每秒周期震动60次。<br>对于声音，人类的听觉范围为20Hz～20000Hz，低于这个范围叫做次声波，高于这个范围的叫做超声波。</p><p>AD交集</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 原始通道列表</span><br>original_channels = [<span class="hljs-string">&#x27;PO3&#x27;</span>, <span class="hljs-string">&#x27;PO7&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;P7&#x27;</span>, <span class="hljs-string">&#x27;CP1&#x27;</span>, <span class="hljs-string">&#x27;CP5&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;T7&#x27;</span>, <span class="hljs-string">&#x27;FC5&#x27;</span>, <span class="hljs-string">&#x27;FC1&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;AF3&#x27;</span>, <span class="hljs-string">&#x27;FP1&#x27;</span>, <span class="hljs-string">&#x27;FP2&#x27;</span>, <span class="hljs-string">&#x27;AF4&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;FC2&#x27;</span>, <span class="hljs-string">&#x27;FC6&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T8&#x27;</span>, <span class="hljs-string">&#x27;CP6&#x27;</span>, <span class="hljs-string">&#x27;CP2&#x27;</span>, <span class="hljs-string">&#x27;P8&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;PO8&#x27;</span>, <span class="hljs-string">&#x27;PO4&#x27;</span>, <span class="hljs-string">&#x27;OZ&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>]<br><br><span class="hljs-comment"># 所需通道列表</span><br>channels = [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;T3&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T4&#x27;</span>, <span class="hljs-string">&#x27;T5&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;T6&#x27;</span>, <span class="hljs-string">&#x27;O1&#x27;</span>, <span class="hljs-string">&#x27;O2&#x27;</span>]<br><br>channels2 = [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;F11&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;F12&#x27;</span>, <span class="hljs-string">&#x27;FT11&#x27;</span>, <span class="hljs-string">&#x27;FC3&#x27;</span>, <span class="hljs-string">&#x27;FCz&#x27;</span>, <span class="hljs-string">&#x27;FC4&#x27;</span>, <span class="hljs-string">&#x27;FT12&#x27;</span>, <span class="hljs-string">&#x27;T7&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T8&#x27;</span>, <span class="hljs-string">&#x27;CP3&#x27;</span>, <span class="hljs-string">&#x27;CPz&#x27;</span>, <span class="hljs-string">&#x27;CP4&#x27;</span>, <span class="hljs-string">&#x27;P7&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;P8&#x27;</span>, <span class="hljs-string">&#x27;O1&#x27;</span>, <span class="hljs-string">&#x27;Oz&#x27;</span>, <span class="hljs-string">&#x27;O2&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>]<br><span class="hljs-comment"># 找出交集</span><br>intersection = list(<span class="hljs-built_in">set</span>(original_channels) &amp; <span class="hljs-built_in">set</span>(channels) &amp; <span class="hljs-built_in">set</span>(channels2))<br><br><span class="hljs-comment"># 打印交集通道</span><br><span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;交集通道: &#123;intersection&#125;&quot;</span>)<br><span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;交集通道数量: &#123;len(intersection)&#125;&quot;</span>)<br><br></code></pre></td></tr></table></figure><p>检查通道</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> mne<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">check_electrode_order</span>(<span class="hljs-params">folder_path, expected_orders</span>):<br>    <span class="hljs-comment"># 获取文件夹中的所有EDF文件</span><br>    edf_files = [file <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> os.listdir(folder_path) <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&#x27;.edf&#x27;</span>)]<br>    <br>    <span class="hljs-comment"># 初始化匹配计数</span><br>    match_counts = &#123;<span class="hljs-built_in">tuple</span>(order): <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> order <span class="hljs-keyword">in</span> expected_orders&#125;<br>    <br>    <span class="hljs-comment"># 循环检查每个EDF文件的电极顺序</span><br>    <span class="hljs-keyword">for</span> edf_file <span class="hljs-keyword">in</span> edf_files:<br>        edf_file_path = os.path.join(folder_path, edf_file)<br><br>        <span class="hljs-comment"># 读取EDF文件</span><br>        raw = mne.io.read_raw_edf(edf_file_path, preload=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># 获取当前EDF文件的电极顺序</span><br>        current_order = raw.ch_names<br><br>        <span class="hljs-comment"># 检查电极顺序是否符合任何一个预期顺序</span><br>        matched = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">for</span> order <span class="hljs-keyword">in</span> expected_orders:<br>            <span class="hljs-keyword">if</span> current_order == order:<br>                match_counts[<span class="hljs-built_in">tuple</span>(order)] += <span class="hljs-number">1</span><br>                matched = <span class="hljs-literal">True</span><br>                <span class="hljs-keyword">break</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> matched:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;edf_file&#125;</span>: Electrode order is incorrect. Current order: <span class="hljs-subst">&#123;current_order&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-keyword">for</span> order, count <span class="hljs-keyword">in</span> match_counts.items():<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Order <span class="hljs-subst">&#123;<span class="hljs-built_in">list</span>(order)&#125;</span> matches <span class="hljs-subst">&#123;count&#125;</span> files.channal is <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>(order))&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 替换为实际的文件夹路径和期望的电极顺序</span><br>folder_path = <span class="hljs-string">&quot;data/NC&quot;</span><br>expected_orders = [<br>    [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;T3&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T4&#x27;</span>, <span class="hljs-string">&#x27;T5&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;T6&#x27;</span>, <span class="hljs-string">&#x27;O1&#x27;</span>, <span class="hljs-string">&#x27;O2&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>],<br>    [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;T7&#x27;</span>, <span class="hljs-string">&#x27;T8&#x27;</span>, <span class="hljs-string">&#x27;P7&#x27;</span>, <span class="hljs-string">&#x27;P8&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;FC1&#x27;</span>, <span class="hljs-string">&#x27;FC2&#x27;</span>, <span class="hljs-string">&#x27;CP1&#x27;</span>, <span class="hljs-string">&#x27;CP2&#x27;</span>, <span class="hljs-string">&#x27;FC5&#x27;</span>, <span class="hljs-string">&#x27;FC6&#x27;</span>, <span class="hljs-string">&#x27;CP5&#x27;</span>, <span class="hljs-string">&#x27;CP6&#x27;</span>, <span class="hljs-string">&#x27;AF3&#x27;</span>, <span class="hljs-string">&#x27;AF4&#x27;</span>, <span class="hljs-string">&#x27;PO3&#x27;</span>, <span class="hljs-string">&#x27;PO4&#x27;</span>, <span class="hljs-string">&#x27;PO7&#x27;</span>, <span class="hljs-string">&#x27;PO8&#x27;</span>, <span class="hljs-string">&#x27;Oz&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>],<br>    [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;F11&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;F12&#x27;</span>, <span class="hljs-string">&#x27;FT11&#x27;</span>, <span class="hljs-string">&#x27;FC3&#x27;</span>, <span class="hljs-string">&#x27;FCz&#x27;</span>, <span class="hljs-string">&#x27;FC4&#x27;</span>, <span class="hljs-string">&#x27;FT12&#x27;</span>, <span class="hljs-string">&#x27;T7&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T8&#x27;</span>, <span class="hljs-string">&#x27;CP3&#x27;</span>, <span class="hljs-string">&#x27;CPz&#x27;</span>, <span class="hljs-string">&#x27;CP4&#x27;</span>, <span class="hljs-string">&#x27;P7&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;P8&#x27;</span>, <span class="hljs-string">&#x27;O1&#x27;</span>, <span class="hljs-string">&#x27;Oz&#x27;</span>, <span class="hljs-string">&#x27;O2&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>],<br>    [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;F11&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;F12&#x27;</span>, <span class="hljs-string">&#x27;FT11&#x27;</span>, <span class="hljs-string">&#x27;FC3&#x27;</span>, <span class="hljs-string">&#x27;FCz&#x27;</span>, <span class="hljs-string">&#x27;FC4&#x27;</span>, <span class="hljs-string">&#x27;FT12&#x27;</span>, <span class="hljs-string">&#x27;T7&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T8&#x27;</span>, <span class="hljs-string">&#x27;CP3&#x27;</span>, <span class="hljs-string">&#x27;CPz&#x27;</span>, <span class="hljs-string">&#x27;CP4&#x27;</span>, <span class="hljs-string">&#x27;M1&#x27;</span>, <span class="hljs-string">&#x27;M2&#x27;</span>, <span class="hljs-string">&#x27;P7&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;P8&#x27;</span>, <span class="hljs-string">&#x27;O1&#x27;</span>, <span class="hljs-string">&#x27;Oz&#x27;</span>, <span class="hljs-string">&#x27;O2&#x27;</span>, <span class="hljs-string">&#x27;Trigger&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>],<br>    [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;T7&#x27;</span>, <span class="hljs-string">&#x27;T8&#x27;</span>, <span class="hljs-string">&#x27;P7&#x27;</span>, <span class="hljs-string">&#x27;P8&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;Oz&#x27;</span>, <span class="hljs-string">&#x27;FC1&#x27;</span>, <span class="hljs-string">&#x27;FC2&#x27;</span>, <span class="hljs-string">&#x27;CP1&#x27;</span>, <span class="hljs-string">&#x27;CP2&#x27;</span>, <span class="hljs-string">&#x27;FC5&#x27;</span>, <span class="hljs-string">&#x27;FC6&#x27;</span>, <span class="hljs-string">&#x27;CP5&#x27;</span>, <span class="hljs-string">&#x27;CP6&#x27;</span>, <span class="hljs-string">&#x27;ECG&#x27;</span>, <span class="hljs-string">&#x27;AF3&#x27;</span>, <span class="hljs-string">&#x27;AF4&#x27;</span>, <span class="hljs-string">&#x27;PO3&#x27;</span>, <span class="hljs-string">&#x27;PO4&#x27;</span>, <span class="hljs-string">&#x27;PO7&#x27;</span>, <span class="hljs-string">&#x27;PO8&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>],<br>    [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;FCz&#x27;</span>, <span class="hljs-string">&#x27;FC3&#x27;</span>, <span class="hljs-string">&#x27;FC4&#x27;</span>, <span class="hljs-string">&#x27;FT7&#x27;</span>, <span class="hljs-string">&#x27;FT8&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T3&#x27;</span>, <span class="hljs-string">&#x27;T4&#x27;</span>, <span class="hljs-string">&#x27;CPz&#x27;</span>, <span class="hljs-string">&#x27;CP3&#x27;</span>, <span class="hljs-string">&#x27;CP4&#x27;</span>, <span class="hljs-string">&#x27;TP7&#x27;</span>, <span class="hljs-string">&#x27;TP8&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;T5&#x27;</span>, <span class="hljs-string">&#x27;T6&#x27;</span>, <span class="hljs-string">&#x27;Oz&#x27;</span>, <span class="hljs-string">&#x27;O1&#x27;</span>, <span class="hljs-string">&#x27;O2&#x27;</span>, <span class="hljs-string">&#x27;HEOL&#x27;</span>, <span class="hljs-string">&#x27;HEOR&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>],<br>    [<span class="hljs-string">&#x27;PO3&#x27;</span>, <span class="hljs-string">&#x27;PO7&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;P7&#x27;</span>, <span class="hljs-string">&#x27;CP1&#x27;</span>, <span class="hljs-string">&#x27;CP5&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;T7&#x27;</span>, <span class="hljs-string">&#x27;FC5&#x27;</span>, <span class="hljs-string">&#x27;FC1&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;AF3&#x27;</span>, <span class="hljs-string">&#x27;FP1&#x27;</span>, <span class="hljs-string">&#x27;FP2&#x27;</span>, <span class="hljs-string">&#x27;AF4&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;FC2&#x27;</span>, <span class="hljs-string">&#x27;FC6&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T8&#x27;</span>, <span class="hljs-string">&#x27;CP6&#x27;</span>, <span class="hljs-string">&#x27;CP2&#x27;</span>, <span class="hljs-string">&#x27;P8&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;PO8&#x27;</span>, <span class="hljs-string">&#x27;PO4&#x27;</span>, <span class="hljs-string">&#x27;OZ&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>],<br>    [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;AF3&#x27;</span>, <span class="hljs-string">&#x27;AF4&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;FC5&#x27;</span>, <span class="hljs-string">&#x27;FC1&#x27;</span>, <span class="hljs-string">&#x27;FC2&#x27;</span>, <span class="hljs-string">&#x27;FC6&#x27;</span>, <span class="hljs-string">&#x27;T7&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T8&#x27;</span>, <span class="hljs-string">&#x27;CP5&#x27;</span>, <span class="hljs-string">&#x27;CP1&#x27;</span>, <span class="hljs-string">&#x27;CP2&#x27;</span>, <span class="hljs-string">&#x27;CP6&#x27;</span>, <span class="hljs-string">&#x27;P7&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;P8&#x27;</span>, <span class="hljs-string">&#x27;PO7&#x27;</span>, <span class="hljs-string">&#x27;PO3&#x27;</span>, <span class="hljs-string">&#x27;PO4&#x27;</span>, <span class="hljs-string">&#x27;PO8&#x27;</span>, <span class="hljs-string">&#x27;OZ&#x27;</span>, <span class="hljs-string">&#x27;AFZ&#x27;</span>, <span class="hljs-string">&#x27;Status&#x27;</span>]<br>]<br><br><span class="hljs-comment"># 执行检查</span><br>check_electrode_order(folder_path, expected_orders)<br><br></code></pre></td></tr></table></figure><p>数据预处理- 留一法验证</p><ol><li><p>随机划分数据集。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs lua">import <span class="hljs-built_in">os</span><br>import shutil<br>import <span class="hljs-built_in">random</span><br><br># 指定包含数据文件的文件夹路径<br>data_folder = <span class="hljs-string">&quot;data_npy\FDT&quot;</span><br><br># 指定用于保存训练集和测试集的文件夹路径<br>train_folder = <span class="hljs-string">&quot;train\FDT&quot;</span><br>test_folder = <span class="hljs-string">&quot;test\FDT&quot;</span><br><br># 创建保存训练集和测试集的文件夹<br><span class="hljs-built_in">os</span>.makedirs(train_folder, exist_ok=True)<br><span class="hljs-built_in">os</span>.makedirs(test_folder, exist_ok=True)<br><br># 遍历数据文件夹中的文件<br><span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> <span class="hljs-built_in">os</span>.listdir(data_folder):<br>    # 以<span class="hljs-number">8</span>:<span class="hljs-number">2</span>的比例将文件分配到训练集或测试集<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">random</span>.<span class="hljs-built_in">random</span>() &lt; <span class="hljs-number">0.8</span>:<br>        shutil.copy(<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(data_folder, file_name), train_folder)<br>    <span class="hljs-keyword">else</span>:<br>        shutil.copy(<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(data_folder, file_name), test_folder)<br><br># 打印训练集和测试集的文件数量<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练集大小:&quot;</span>, <span class="hljs-built_in">len</span>(<span class="hljs-built_in">os</span>.listdir(train_folder)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试集大小:&quot;</span>, <span class="hljs-built_in">len</span>(<span class="hljs-built_in">os</span>.listdir(test_folder)))<br><br></code></pre></td></tr></table></figure></li><li><p>剪切数据集代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># 指定包含原始npy文件的文件夹路径列表</span><br>input_folders = [<span class="hljs-string">&#x27;test/AD&#x27;</span>, <span class="hljs-string">&#x27;test/CN&#x27;</span>]  <span class="hljs-comment"># 输入文件夹列表</span><br>output_folders = [<span class="hljs-string">&#x27;data_npy_cut1/test/AD&#x27;</span>, <span class="hljs-string">&#x27;data_npy_cut1/test/CN&#x27;</span>]  <span class="hljs-comment"># 对应的输出文件夹列表</span><br><br><span class="hljs-comment"># 确定剪切后的数据长度</span><br>cut_length = <span class="hljs-number">2500</span><br><br><span class="hljs-comment"># 确保输出文件夹存在</span><br><span class="hljs-keyword">for</span> output_folder <span class="hljs-keyword">in</span> output_folders:<br>    os.makedirs(output_folder, exist_ok=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 遍历文件夹列表</span><br><span class="hljs-keyword">for</span> input_folder, output_folder <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(input_folders, output_folders):<br>    <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> os.listdir(input_folder):<br>        <span class="hljs-keyword">if</span> file_name.endswith(<span class="hljs-string">&#x27;.npy&#x27;</span>):<br>            <span class="hljs-comment"># 加载原始npy文件</span><br>            data = np.load(os.path.join(input_folder, file_name))<br>            <br>            <span class="hljs-comment"># 确定剪切的段数</span><br>            num_cuts = data.shape[<span class="hljs-number">1</span>] // cut_length<br><br>            <span class="hljs-comment"># 使用tqdm显示进度条</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(num_cuts), desc=<span class="hljs-string">f&#x27;Processing <span class="hljs-subst">&#123;file_name&#125;</span> in <span class="hljs-subst">&#123;input_folder&#125;</span>&#x27;</span>, unit=<span class="hljs-string">&#x27;cut&#x27;</span>):<br>                cut_data = data[:, i * cut_length : (i + <span class="hljs-number">1</span>) * cut_length]<br>                np.save(os.path.join(output_folder, <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;file_name[:-<span class="hljs-number">4</span>]&#125;</span>_cut_<span class="hljs-subst">&#123;i&#125;</span>.npy&#x27;</span>), cut_data)<br><br></code></pre></td></tr></table></figure></li></ol><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>mne库的文档链接<a href="https://mne.tools/dev/api/python_reference.html">链接</a><br>pywt库的<br>3. 连续小波变换 + RBP + 绘制图像的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.signal <span class="hljs-keyword">import</span> welch<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_psd</span>(<span class="hljs-params">signal, fs, nperseg</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    使用Welch方法计算功率谱密度（PSD）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">    - signal: 输入信号（二维数组：通道数 x 样本数）。</span><br><span class="hljs-string">    - fs: 采样频率（Hz）。</span><br><span class="hljs-string">    - nperseg: 每段的长度（样本数）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">    - f: 频率数组。</span><br><span class="hljs-string">    - psd: 功率谱密度（PSD），形状为（通道数 x 频率数）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    psd_list = []<br>    <span class="hljs-keyword">for</span> ch_signal <span class="hljs-keyword">in</span> signal:<br>        f, psd_ch = welch(ch_signal, fs, nperseg=nperseg)<br>        psd_list.append(psd_ch)<br>    <span class="hljs-keyword">return</span> f, np.array(psd_list)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_rbp</span>(<span class="hljs-params">f, psd, freq_bands</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算相对波动指数（RBP）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">    - f: 频率数组。</span><br><span class="hljs-string">    - psd: 功率谱密度（PSD），形状为（通道数 x 频率数）。</span><br><span class="hljs-string">    - freq_bands: 频段列表，每个频段是一个元组（开始频率，结束频率）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">    - rbp: 相对波动指数（RBP），形状为（通道数 x 频段数）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    total_psd = np.<span class="hljs-built_in">sum</span>(psd, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>    rbp = []<br>    <span class="hljs-keyword">for</span> (f_start, f_end) <span class="hljs-keyword">in</span> freq_bands:<br>        band_power = np.<span class="hljs-built_in">sum</span>(psd[:, (f &gt;= f_start) &amp; (f &lt;= f_end)], axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>        rbp.append(band_power / total_psd)<br>    <span class="hljs-keyword">return</span> np.hstack(rbp)<br><br><span class="hljs-comment"># 示例频段（可以根据实际需求调整）</span><br>freq_bands = [(<span class="hljs-number">0.5</span>, <span class="hljs-number">4</span>), (<span class="hljs-number">4</span>, <span class="hljs-number">8</span>), (<span class="hljs-number">8</span>, <span class="hljs-number">12</span>), (<span class="hljs-number">12</span>, <span class="hljs-number">30</span>), (<span class="hljs-number">30</span>, <span class="hljs-number">50</span>)]<br><br><span class="hljs-comment"># 处理一个文件夹中的所有 .npy 文件</span><br>input_folder = <span class="hljs-string">&#x27;data_npy_cut/test/AD/&#x27;</span>  <span class="hljs-comment"># 输入文件夹路径</span><br>output_folder = <span class="hljs-string">&#x27;PSD-RBP/test/AD&#x27;</span>  <span class="hljs-comment"># 输出文件夹路径</span><br><br><span class="hljs-comment"># 创建输出文件夹（如果不存在）</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(output_folder):<br>    os.makedirs(output_folder)<br><br><span class="hljs-comment"># 处理文件夹中的所有 .npy 文件</span><br><span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> os.listdir(input_folder):<br>    <span class="hljs-keyword">if</span> filename.endswith(<span class="hljs-string">&#x27;.npy&#x27;</span>):<br>        input_file_path = os.path.join(input_folder, filename)<br>        output_file_path = os.path.join(output_folder, filename.split(<span class="hljs-string">&#x27;.&#x27;</span>)[<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27;_rbp.npy&#x27;</span>)<br>        <br>        <span class="hljs-comment"># 加载输入数据</span><br>        signal = np.load(input_file_path)<br><br>        <span class="hljs-comment"># 检查输入信号的大小是否为 [19, 2500]</span><br>        <span class="hljs-keyword">assert</span> signal.shape == (<span class="hljs-number">19</span>, <span class="hljs-number">2500</span>), <span class="hljs-string">f&quot;Expected input signal shape to be (19, 2500), but got <span class="hljs-subst">&#123;signal.shape&#125;</span>&quot;</span><br><br>        <span class="hljs-comment"># 设置采样频率和每段的长度</span><br>        fs = <span class="hljs-number">500</span>  <span class="hljs-comment"># 采样频率（Hz）</span><br>        nperseg = <span class="hljs-number">128</span>  <span class="hljs-comment"># 每段的长度（样本数）</span><br><br>        <span class="hljs-comment"># 计算PSD</span><br>        f, psd = calculate_psd(signal, fs, nperseg)<br><br>        <span class="hljs-comment"># 计算RBP</span><br>        rbp = calculate_rbp(f, psd, freq_bands)<br><br>        <span class="hljs-comment"># 将RBP保存到新的 .npy 文件</span><br>        np.save(output_file_path, rbp)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Saved <span class="hljs-subst">&#123;output_file_path&#125;</span>&#x27;</span>)<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 加载 .npy 文件</span><br>npy_file_path = <span class="hljs-string">&#x27;output.npy&#x27;</span>  <span class="hljs-comment"># 替换为实际的 .npy 文件路径</span><br>rbp_data = np.load(npy_file_path)<br><span class="hljs-built_in">print</span>(rbp_data.shape)<br><span class="hljs-comment"># 检查数据形状</span><br><span class="hljs-keyword">assert</span> rbp_data.shape == (<span class="hljs-number">19</span>, <span class="hljs-number">5</span>), <span class="hljs-string">f&quot;Expected RBP data shape to be (19, 5), but got <span class="hljs-subst">&#123;rbp_data.shape&#125;</span>&quot;</span><br><br><span class="hljs-comment"># 绘制 RBP 数据</span><br>channels = [<span class="hljs-string">&#x27;Fp1&#x27;</span>, <span class="hljs-string">&#x27;Fp2&#x27;</span>, <span class="hljs-string">&#x27;F7&#x27;</span>, <span class="hljs-string">&#x27;F3&#x27;</span>, <span class="hljs-string">&#x27;Fz&#x27;</span>, <span class="hljs-string">&#x27;F4&#x27;</span>, <span class="hljs-string">&#x27;F8&#x27;</span>, <span class="hljs-string">&#x27;T3&#x27;</span>, <span class="hljs-string">&#x27;C3&#x27;</span>, <span class="hljs-string">&#x27;Cz&#x27;</span>, <span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-string">&#x27;T4&#x27;</span>, <span class="hljs-string">&#x27;T5&#x27;</span>, <span class="hljs-string">&#x27;P3&#x27;</span>, <span class="hljs-string">&#x27;Pz&#x27;</span>, <span class="hljs-string">&#x27;P4&#x27;</span>, <span class="hljs-string">&#x27;T6&#x27;</span>, <span class="hljs-string">&#x27;O1&#x27;</span>, <span class="hljs-string">&#x27;O2&#x27;</span>]  <span class="hljs-comment"># 替换为实际的通道名称</span><br>freq_bands = [<span class="hljs-string">&#x27;0.5-4 Hz&#x27;</span>, <span class="hljs-string">&#x27;4-8 Hz&#x27;</span>, <span class="hljs-string">&#x27;8-12 Hz&#x27;</span>, <span class="hljs-string">&#x27;12-30 Hz&#x27;</span>, <span class="hljs-string">&#x27;30-50 Hz&#x27;</span>]  <span class="hljs-comment"># 替换为实际的频段名称</span><br><br>fig, ax = plt.subplots(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">8</span>))<br>cax = ax.matshow(rbp_data, cmap=<span class="hljs-string">&#x27;viridis&#x27;</span>)<br><br><span class="hljs-comment"># 设置颜色条</span><br>fig.colorbar(cax)<br><br><span class="hljs-comment"># 设置通道名称</span><br>ax.set_xticks(np.arange(<span class="hljs-built_in">len</span>(freq_bands)))<br>ax.set_xticklabels(freq_bands, rotation=<span class="hljs-number">45</span>, ha=<span class="hljs-string">&#x27;left&#x27;</span>)<br>ax.set_yticks(np.arange(<span class="hljs-built_in">len</span>(channels)))<br>ax.set_yticklabels(channels)<br><br><span class="hljs-comment"># 设置标签</span><br>plt.xlabel(<span class="hljs-string">&#x27;Frequency Bands&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Channels&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Relative Band Power (RBP) Heatmap&#x27;</span>)<br><br><span class="hljs-comment"># 显示图像</span><br>plt.tight_layout()  <span class="hljs-comment"># 调整布局以防止标签重叠</span><br>plt.show()<br><br></code></pre></td></tr></table></figure><ol start="4"><li><p>CWT + RBP</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pywt<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># 采样频率</span><br>fs = <span class="hljs-number">500</span><br><br><span class="hljs-comment"># 输入和输出文件夹路径列表</span><br>input_folders = [<span class="hljs-string">&#x27;data_npy_cut/train/AD/&#x27;</span>, <span class="hljs-string">&#x27;data_npy_cut/train/CN/&#x27;</span>,<span class="hljs-string">&#x27;data_npy_cut/train/FDT/&#x27;</span>]<br>output_folders = [<span class="hljs-string">&#x27;CWT-RBP/train/AD&#x27;</span>, <span class="hljs-string">&#x27;CWT-RBP/train/CN&#x27;</span>,<span class="hljs-string">&#x27;CWT-RBP/train/FDT&#x27;</span>]<br><br><br><span class="hljs-comment"># 确保文件夹数量相同</span><br><span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(input_folders) == <span class="hljs-built_in">len</span>(output_folders), <span class="hljs-string">&quot;输入和输出文件夹数量不匹配&quot;</span><br><br><span class="hljs-comment"># 创建新文件夹</span><br><span class="hljs-keyword">for</span> output_folder <span class="hljs-keyword">in</span> output_folders:<br>    os.makedirs(output_folder, exist_ok=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 小波函数</span><br>wavelet = <span class="hljs-string">&#x27;morl&#x27;</span><br><br><span class="hljs-comment"># 定义 CWT 的频率范围</span><br>freq_ranges = [(<span class="hljs-number">0.5</span>, <span class="hljs-number">4</span>), (<span class="hljs-number">4</span>, <span class="hljs-number">8</span>), (<span class="hljs-number">8</span>, <span class="hljs-number">13</span>), (<span class="hljs-number">13</span>, <span class="hljs-number">25</span>), (<span class="hljs-number">25</span>, <span class="hljs-number">45</span>)]<br>scales = np.arange(<span class="hljs-number">1</span>, <span class="hljs-number">128</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_rbp</span>(<span class="hljs-params">cwt_coeffs, freq_band, scales, fs</span>):<br>    min_scale = np.<span class="hljs-built_in">min</span>(np.where((fs / (scales * <span class="hljs-number">2</span>)) &lt;= freq_band[<span class="hljs-number">1</span>]))<br>    max_scale = np.<span class="hljs-built_in">max</span>(np.where((fs / (scales * <span class="hljs-number">2</span>)) &gt;= freq_band[<span class="hljs-number">0</span>]))<br>    band_power = np.<span class="hljs-built_in">sum</span>(np.<span class="hljs-built_in">abs</span>(cwt_coeffs[min_scale:max_scale + <span class="hljs-number">1</span>, :])**<span class="hljs-number">2</span>, axis=<span class="hljs-number">0</span>)<br>    total_power = np.<span class="hljs-built_in">sum</span>(np.<span class="hljs-built_in">abs</span>(cwt_coeffs)**<span class="hljs-number">2</span>, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> band_power / total_power<br><br><span class="hljs-comment"># 遍历输入文件夹</span><br><span class="hljs-keyword">for</span> input_folder, output_folder <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(input_folders, output_folders):<br>    <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> tqdm(os.listdir(input_folder)):<br>        <span class="hljs-keyword">if</span> file_name.endswith(<span class="hljs-string">&#x27;.npy&#x27;</span>):<br>            <span class="hljs-comment"># 读取 .npy 文件</span><br>            data = np.load(os.path.join(input_folder, file_name))<br>            <br>            <span class="hljs-comment"># 提取不同频段的数据</span><br>            data_freq_bands = []<br>            <span class="hljs-keyword">for</span> ch_data <span class="hljs-keyword">in</span> data:<br>                ch_data_freq_band = []<br>                <span class="hljs-comment"># 计算 CWT 系数</span><br>                cwt_coeffs, _ = pywt.cwt(ch_data, scales, wavelet, sampling_period=<span class="hljs-number">1</span>/fs)<br>                <span class="hljs-keyword">for</span> fmin, fmax <span class="hljs-keyword">in</span> freq_ranges:<br>                    <span class="hljs-comment"># 计算频段的相对功率谱密度 (RBP)</span><br>                    rbp = calculate_rbp(cwt_coeffs, (fmin, fmax), scales, fs)<br>                    ch_data_freq_band.append(rbp)<br>                data_freq_bands.append(ch_data_freq_band)<br>            <br>            <span class="hljs-comment"># 重塑数据形状为 [5, 19, 2500]</span><br>            data_freq_bands = np.array(data_freq_bands)<br>            data_freq_bands = np.transpose(data_freq_bands, (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>))  <span class="hljs-comment"># [5, 19, 2500]</span><br>            <br>            <span class="hljs-comment"># 修改文件名，添加处理方法标记</span><br>            output_file_name = os.path.splitext(file_name)[<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27;_cwt_rbp.npy&#x27;</span><br>            output_file_path = os.path.join(output_folder, output_file_name)<br>            <br>            <span class="hljs-comment"># 保存处理后的数据</span><br>            np.save(output_file_path, data_freq_bands)<br><br></code></pre></td></tr></table></figure></li><li><p>FTBT + BRP<br>FTBT</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.fftpack <span class="hljs-keyword">import</span> fft, ifft<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># 采样频率</span><br>fs = <span class="hljs-number">500</span><br><br><span class="hljs-comment"># 输入和输出文件夹路径列表</span><br>input_folders = [<span class="hljs-string">&#x27;data_npy_cut/train/AD/&#x27;</span>, <span class="hljs-string">&#x27;data_npy_cut/train/CN/&#x27;</span>,<span class="hljs-string">&#x27;data_npy_cut/train/FDT/&#x27;</span>]<br>output_folders = [<span class="hljs-string">&#x27;FBFT/train/AD&#x27;</span>, <span class="hljs-string">&#x27;FBFT/train/CN&#x27;</span>,<span class="hljs-string">&#x27;FBFT/train/FDT&#x27;</span>]<br><br><span class="hljs-comment"># 确保文件夹数量相同</span><br><span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(input_folders) == <span class="hljs-built_in">len</span>(output_folders), <span class="hljs-string">&quot;输入和输出文件夹数量不匹配&quot;</span><br><br><span class="hljs-comment"># 创建新文件夹</span><br><span class="hljs-keyword">for</span> output_folder <span class="hljs-keyword">in</span> output_folders:<br>    os.makedirs(output_folder, exist_ok=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fbft</span>(<span class="hljs-params">signal</span>):<br>    N = <span class="hljs-built_in">len</span>(signal)<br>    forward_fft = fft(signal)<br>    backward_fft = fft(ifft(forward_fft))<br>    fbft_result = forward_fft + backward_fft<br>    <span class="hljs-keyword">return</span> fbft_result<br><br><span class="hljs-comment"># 遍历输入文件夹</span><br><span class="hljs-keyword">for</span> input_folder, output_folder <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(input_folders, output_folders):<br>    <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> tqdm(os.listdir(input_folder)):<br>        <span class="hljs-keyword">if</span> file_name.endswith(<span class="hljs-string">&#x27;.npy&#x27;</span>):<br>            <span class="hljs-comment"># 读取 .npy 文件</span><br>            data = np.load(os.path.join(input_folder, file_name))<br>            <br>            <span class="hljs-comment"># 对每个通道的数据进行 FBFT 操作</span><br>            fbft_data = []<br>            <span class="hljs-keyword">for</span> ch_data <span class="hljs-keyword">in</span> data:<br>                fbft_ch_data = fbft(ch_data)<br>                fbft_data.append(fbft_ch_data)<br>            <br>            <span class="hljs-comment"># 将结果转换为 numpy 数组</span><br>            fbft_data = np.array(fbft_data)<br>            <br>            <span class="hljs-comment"># 修改文件名，添加处理方法标记</span><br>            output_file_name = os.path.splitext(file_name)[<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27;_fbft.npy&#x27;</span><br>            output_file_path = os.path.join(output_folder, output_file_name)<br>            <br>            <span class="hljs-comment"># 保存处理后的数据</span><br>            np.save(output_file_path, fbft_data)<br><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## FTBT+BRP</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.fftpack <span class="hljs-keyword">import</span> fft, ifft<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># 采样频率</span><br>fs = <span class="hljs-number">500</span><br><br><span class="hljs-comment"># 输入和输出文件夹路径列表</span><br>input_folders = [<span class="hljs-string">&#x27;data_npy_cut/test/AD/&#x27;</span>, <span class="hljs-string">&#x27;data_npy_cut/test/CN/&#x27;</span>]<br>output_folders = [<span class="hljs-string">&#x27;FBFT-RBP/test/AD&#x27;</span>, <span class="hljs-string">&#x27;FBFT-RBP/test/CN&#x27;</span>]<br><br><span class="hljs-comment"># 确保文件夹数量相同</span><br><span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(input_folders) == <span class="hljs-built_in">len</span>(output_folders), <span class="hljs-string">&quot;输入和输出文件夹数量不匹配&quot;</span><br><br><span class="hljs-comment"># 创建新文件夹</span><br><span class="hljs-keyword">for</span> output_folder <span class="hljs-keyword">in</span> output_folders:<br>    os.makedirs(output_folder, exist_ok=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 定义频率范围</span><br>freq_ranges = [(<span class="hljs-number">0.5</span>, <span class="hljs-number">4</span>), (<span class="hljs-number">4</span>, <span class="hljs-number">8</span>), (<span class="hljs-number">8</span>, <span class="hljs-number">13</span>), (<span class="hljs-number">13</span>, <span class="hljs-number">25</span>), (<span class="hljs-number">25</span>, <span class="hljs-number">45</span>)]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_rbp</span>(<span class="hljs-params">f, psd, freq_band</span>):<br>    f_start, f_end = freq_band<br>    band_power = np.<span class="hljs-built_in">sum</span>(psd[(f &gt;= f_start) &amp; (f &lt;= f_end)])<br>    total_power = np.<span class="hljs-built_in">sum</span>(psd)<br>    <span class="hljs-keyword">return</span> band_power / total_power<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fbft</span>(<span class="hljs-params">signal</span>):<br>    N = <span class="hljs-built_in">len</span>(signal)<br>    forward_fft = fft(signal)<br>    backward_fft = fft(ifft(forward_fft))<br>    psd = np.<span class="hljs-built_in">abs</span>(forward_fft[:N//<span class="hljs-number">2</span>]) ** <span class="hljs-number">2</span> + np.<span class="hljs-built_in">abs</span>(backward_fft[:N//<span class="hljs-number">2</span>]) ** <span class="hljs-number">2</span><br>    freqs = np.fft.fftfreq(N, d=<span class="hljs-number">1</span>/fs)[:N//<span class="hljs-number">2</span>]<br>    <span class="hljs-keyword">return</span> freqs, psd<br><br><span class="hljs-comment"># 遍历输入文件夹</span><br><span class="hljs-keyword">for</span> input_folder, output_folder <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(input_folders, output_folders):<br>    <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> tqdm(os.listdir(input_folder)):<br>        <span class="hljs-keyword">if</span> file_name.endswith(<span class="hljs-string">&#x27;.npy&#x27;</span>):<br>            <span class="hljs-comment"># 读取 .npy 文件</span><br>            data = np.load(os.path.join(input_folder, file_name))<br>            <br>            <span class="hljs-comment"># 提取不同频段的数据</span><br>            data_freq_bands = []<br>            <span class="hljs-keyword">for</span> ch_data <span class="hljs-keyword">in</span> data:<br>                ch_data_freq_band = []<br>                <span class="hljs-comment"># 计算 FBFT 系数</span><br>                freqs, psd = fbft(ch_data)<br>                <span class="hljs-keyword">for</span> fmin, fmax <span class="hljs-keyword">in</span> freq_ranges:<br>                    <span class="hljs-comment"># 计算频段的相对功率谱密度 (RBP)</span><br>                    rbp = calculate_rbp(freqs, psd, (fmin, fmax))<br>                    ch_data_freq_band.append(rbp)<br>                data_freq_bands.append(ch_data_freq_band)<br>            <br>            <span class="hljs-comment"># 重塑数据形状为 [5, 19, 2500]</span><br>            data_freq_bands = np.array(data_freq_bands)<br>            data_freq_bands = np.transpose(data_freq_bands, (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># [5, 19]</span><br>            <br>            <span class="hljs-comment"># 修改文件名，添加处理方法标记</span><br>            output_file_name = os.path.splitext(file_name)[<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27;_fbft_rbp.npy&#x27;</span><br>            output_file_path = os.path.join(output_folder, output_file_name)<br>            <br>            <span class="hljs-comment"># 保存处理后的数据</span><br>            np.save(output_file_path, data_freq_bands)<br><br></code></pre></td></tr></table></figure></li></ol><p>划分数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> csv<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>train_path = <span class="hljs-string">&quot;train_data.csv&quot;</span><br>val_path = <span class="hljs-string">&quot;test_data.csv&quot;</span><br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_data_text</span>(<span class="hljs-params">path,train_percent = <span class="hljs-number">0.9</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;建立数据data列表,划分数据集&quot;&quot;&quot;</span><br>    f_train = <span class="hljs-built_in">open</span>(train_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-comment">#</span><br>    <span class="hljs-comment">#并将文件对象赋给了变量 f_train。open(train_path, &quot;w&quot;, newline=&#x27;&#x27;)</span><br>    <span class="hljs-comment">#  的意思是以写入模式打开名为 train_path 的文件，</span><br>    <span class="hljs-comment"># 并且在写入文本时不插入额外的换行符（newline=&#x27;&#x27;）。open是python的内置函数</span><br>    f_val = <span class="hljs-built_in">open</span>(val_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&#x27;&#x27;</span>)<br>    train_writer = csv.writer(f_train)<br>    <span class="hljs-comment"># 创建了一个 CSV writer 对象 train_writer，</span><br>    <span class="hljs-comment"># 用于向文件对象 f_train 中写入 CSV 格式的数据。csv.writer() 接受一个文件对象作为参数，</span><br>    <span class="hljs-comment"># 并返回一个 CSV writer 对象，可以使用该对象的方法将数据写入文件。</span><br>    val_writer = csv.writer(f_val)<br>    <span class="hljs-comment"># enumerate() 是 Python 内置函数，</span><br>    <span class="hljs-comment"># 用于将一个可迭代对象（如列表、元组、字符串等）</span><br>    <span class="hljs-comment"># 组合为一个索引序列，同时列出数据和数据下标。</span><br>    <span class="hljs-comment"># 函数返回一个枚举对象，其中每个元素是一个包含索引和对应元素的元组。</span><br>    <span class="hljs-keyword">for</span> cls,dirname <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(os.listdir(path)):<br>        flist = os.listdir(os.path.join(path,dirname))<br>        <span class="hljs-comment"># os.path.join(path, dirname) 是一个函数，用于将多个路径组合成一个完整的路径。</span><br>        <span class="hljs-comment"># 它会根据当前操作系统的规则使用正确的路径分隔符来连接路径。</span><br>        np.random.shuffle(flist)<br>        <span class="hljs-comment"># np.random.shuffle(flist) 是 NumPy 库中的一个函数，用于随机打乱列表 flist 中的元素顺序。</span><br>        <span class="hljs-comment"># 这个函数会改变原始列表的顺序，使得列表中的元素随机排列。</span><br>        fnum = <span class="hljs-built_in">len</span>(flist)<br>        <span class="hljs-comment"># len函数返回</span><br>        <span class="hljs-keyword">for</span> i,filename <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(flist):<br>            <span class="hljs-comment"># 使用了 enumerate() 函数来遍历列表 flist 中的元素，同时获取元素的索引和值。具体来说，</span><br>            <span class="hljs-comment"># enumerate(flist) 返回一个枚举对象，其中每个元素是一个元组，包含元素在列表中的索引和元素的值。</span><br>            <span class="hljs-keyword">if</span> i &lt; fnum * train_percent:<br>                train_writer.writerow([os.path.join(path,dirname,filename),<span class="hljs-built_in">str</span>(cls)])<br>            <span class="hljs-comment"># 是将一个包含两个元素的列表写入CSV文件的操作。这个列表包含两个元素：</span><br>            <span class="hljs-comment"># os.path.join(path,dirname,filename) 返回一个完整的文件路径，其中 path 是主目录路径，dirname 是子目录路径，filename 是文件名。这个路径表示要写入CSV文件的文件的完整路径。</span><br>            <span class="hljs-comment"># str(cls) 将整数 cls 转换为字符串，cls 表示类别编号。</span><br>            <span class="hljs-keyword">else</span>:<br>                val_writer.writerow([os.path.join(path,dirname,filename),<span class="hljs-built_in">str</span>(cls)])<br>    f_train.close()<br>    <span class="hljs-comment"># f_train.close() 是关闭文件 f_train 的方法。在使用完文件后，</span><br>    <span class="hljs-comment"># 应该调用这个方法来关闭文件，以释放资源并确保文件被正确关闭。</span><br>    f_val.close()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    create_data_text(<span class="hljs-string">&quot;FFT_data&quot;</span>)<br>    <br></code></pre></td></tr></table></figure><h2 id="填坑"><a href="#填坑" class="headerlink" title="填坑"></a>填坑</h2><p>train-loss 和test-loss之间的关系<br>变化趋势分析：train loss 不断下降，test loss不断下降，说明网络仍在学习;（最好的）train loss 不断下降，test loss趋于不变，说明网络过拟合；train loss 趋于不变，test loss不断下降，说明数据集100%有问题;（检查dataset）train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;（减少学习率）train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题；（最不好的情况）train_loss 不断下降， test_loss 不断上升，和第2种情况类似说明网络过拟合了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>兰亭集序</title>
    <link href="/2024/04/28/juzhi3/"/>
    <url>/2024/04/28/juzhi3/</url>
    
    <content type="html"><![CDATA[<center><font size = 6> 兰亭集序 / 兰亭序<font><center><center >魏晋：王羲之<center><font size = 5>永和九年，岁在癸丑，暮春之初，会于会稽山阴之兰亭，修禊事也。群贤毕至，少长咸集。此地有崇山峻岭，茂林修竹；又有清流激湍，映带左右，引以为流觞曲水，列坐其次。虽无丝竹管弦之盛，一觞一咏，亦足以畅叙幽情。<p>是日也，天朗气清，惠风和畅，仰观宇宙之大，俯察品类之盛，所以游目骋怀，足以极视听之娱，信可乐也。</p><p>夫人之相与，俯仰一世，或取诸怀抱，悟言一室之内；或因寄所托，放浪形骸之外。虽趣舍万殊，静躁不同，当其欣于所遇，暂得于己，快然自足，不知老之将至。及其所之既倦，情随事迁，感慨系之矣。向之所欣，俯仰之间，已为陈迹，犹不能不以之兴怀。况修短随化，终期于尽。古人云：“死生亦大矣。”岂不痛哉！(不知老之将至 一作：曾不知老之将至)</p><p>每览昔人兴感之由，若合一契，未尝不临文嗟悼，不能喻之于怀。固知一死生为虚诞，齐彭殇为妄作。后之视今，亦犹今之视昔。悲夫！故列叙时人，录其所述，虽世殊事异，所以兴怀，其致一也。后之览者，亦将有感于斯文。</p><font><div style="text-align: left;"><font size = 3>译文：永和九年，时在癸丑之年，三月上旬，我们会集在会稽郡山阴城的兰亭，为了做禊礼这件事。诸多贤士能人都汇聚到这里，年长、年少者都聚集在这里。兰亭这个地方有高峻的山峰，茂盛高密的树林和竹丛；又有清澈激荡的水流，在亭子的左右辉映环绕，我们把水引来作为飘传酒杯的环形渠水，排列坐在曲水旁边，虽然没有管弦齐奏的盛况，但喝着酒作着诗，也足够来畅快表达幽深内藏的感情了。<p>这一天，天气晴朗，和风习习，抬头纵观广阔的天空，俯看观察大地上繁多的万物，用来舒展眼力，开阔胸怀，足够来极尽视听的欢娱，实在很快乐。</p><p>人与人相互交往，很快便度过一生。有的人在室内畅谈自己的胸怀抱负；就着自己所爱好的事物，寄托自己的情怀，不受约束，放纵无羁的生活。虽然各有各的爱好，安静与躁动各不相同，但当他们对所接触的事物感到高兴时，一时感到自得，感到高兴和满足，竟然不知道衰老将要到来。等到对于自己所喜爱的事物感到厌倦，心情随着当前的境况而变化，感慨随之产生了。过去所喜欢的东西，转瞬间，已经成为旧迹，尚且不能不因为它引发心中的感触，况且寿命长短，听凭造化，最后归结于消灭。古人说：“死生毕竟是件大事啊。”怎么能不让人悲痛呢？</p><p>每当我看到前人兴怀感慨的原因，与我所感叹的好像符契一样相合，没有不面对着他们的文章而嗟叹感伤的，在心里又不能清楚地说明。本来知道把生死等同的说法是不真实的，把长寿和短命等同起来的说法是妄造的。后人看待今人，也就像今人看待前人。可悲呀！所以一个一个记下当时与会的人，录下他们所作的诗篇。纵使时代变了，事情不同了，但触发人们情怀的原因，他们的思想情趣是一样的。后世的读者，也将对这次集会的诗文有所感慨。</p></div size = 3><font><div style="text-align: left;"><font size = 3></div size = 3><font>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔13</title>
    <link href="/2024/04/28/ganwu13/"/>
    <url>/2024/04/28/ganwu13/</url>
    
    <content type="html"><![CDATA[<h2 id="实事求是"><a href="#实事求是" class="headerlink" title="实事求是"></a>实事求是</h2><p>重新写一下实事求是，做事最重要的是什么？清醒，明白自己在干嘛，明白自己做事是为了进步的，没有进步这件事就不需要去做。以进步为导向，以完成这个目标为目的，不畏惧目标的远大，以实事求是的态度，对一个不懂的东西，要去研究它，了解它，等研究透了，知其然，知其所以然。等了解这项工作后才能评价它，在这之后能不能做我能清楚的明白，在没有做之前不需要情绪上的波动，不需要什么自卑，崇拜。<br>实事求是的条件是分析现实的条件，分析条件的组成，但这只是一个思路，具体到做又有实践中需要注意的，但是知和行是一体两面的（感觉对这个理论还是很混沌，没有很好的阐述出来）。实践是检验真理的唯一标准，明白这句话需要这句清晰的知道实践，真理，标准的范畴。<br>今天看到一篇博客，觉得是实事求举得例子是举得很好的，这里放个链接(<a href="https://www.zhihu.com/question/365148040/answer/968807103">https://www.zhihu.com/question/365148040/answer/968807103</a>)</p><p>对于我来讲以现实为角度，如同子曰：为政以德譬，如北辰居其所而众星共之。当人把北极星的位置确定后，执持这位置相应就可以定出其他星星位置；当人从现实出发分析把握了现实关系的逻辑结构后，还不能把握大道吗？一切都从也只能从现实出发，现实在什么阶段，什么位次，是必须首要分析的问题。<br>哲学家们只是用不用的方式解释世界,问题在于改变世界</p><h2 id="精神的独立"><a href="#精神的独立" class="headerlink" title="精神的独立"></a>精神的独立</h2><p>人不仅要在物质上独立，更要在精神上独立。畏难情绪，不验证的验证，对于不验证验证了什么？验证了自己能不能做这件事，验证了自己不能做这件事。这样的精神气质就不对了，很难提升境界了。人云亦云，亦步亦趋，这样的精神是不行的，精神上畏难，即使再努力也是没有用的。不要认为任何人是不可超越的，始终相信不断的积累是可以达到他们的境界的。无论是智力不如别人，还是学的不够扎实。精神上站起来，认识到自己，实践是认识的来源，认识是改变的第一步，是成长的第一步。</p><h2 id="教育的作用"><a href="#教育的作用" class="headerlink" title="教育的作用"></a>教育的作用</h2><p>教育，读书，明理，解决问题。中国的大学教育被大众大众赋予了太多的神话，认为读了大学什么问题就解决了，上了大学一切就都解决了。我想这样的想法或多或少是不恰当的，把自己的未来放置于虚幻的未来，不立足于当下，立足于现实，想来是不恰当的。我认为教育的作用是去启发自我的本性，培养自我的能力，让自己能够去迎接挑战，而不是去等，去守株待兔，等待那虚幻的未来。知识付出一点时间和精力就能够获取，但是对于心理和人格的成长教育是欠缺的，这欠缺的反而需要学生自己去寻找，自己去开启心智的成长。<br>谈精神之独立，鲁迅是谈的好的，为此我在这挖一个坑（读鲁迅的杂文集）</p><h2 id="对死亡抱有希望真的会有解脱吗？"><a href="#对死亡抱有希望真的会有解脱吗？" class="headerlink" title="对死亡抱有希望真的会有解脱吗？"></a>对死亡抱有希望真的会有解脱吗？</h2><p>时常的，在我闲下来的时候，从高处往下看，总会幻想跳下去后，一切问题就都解决了。将解决问题的最终方式诉诸于死亡是我最后解决问题的办法，因为生命最终都会消逝，或早或晚没有什么不好，有的只有快慢。人的这一生，如昙花一现。如草木春绿秋枯，如曦月东升西落。死亡好像没有什么不好，但是我认为没有考虑到的是，人出生下来就应该背负一定的责任，对自己负责，对家人负责。自己亲手了结自己的生命是对自己不负责的表现，对家人不负责的表现。烦恼总是有的，但是为什么不换个想法，这件事最好会达到什么程度，做不好我会遭受什么，大部分时候我感到困惑，感到不解的来源是我的情绪，然而情绪在这种时候是最没有用处的东西，被情绪所裹挟是没有任何益处的，控制自己的情绪又是困难的，诉诸于死亡是无法之法。或许可以在感受情绪的同时，去想想最坏的结果，最坏的结果会导致我死亡，会影响我的生活，亦或者会失去什么，预演之后我能接受最坏的结果，再回来看最坏的结果也不过如此，更重要的是最坏的结果还没有发生，还有去改变的机会，这时就付诸一切的努力去改变。接受最坏的结果，尽力改变我接受的最坏的结果，即使最后失败了，我也欣然接受，而不是陷入不证之证，把希望诉诸于死亡。</p><h2 id="做事闲谈"><a href="#做事闲谈" class="headerlink" title="做事闲谈"></a>做事闲谈</h2><p>最开始开始写随笔是4月5日，现在已经积累了13篇随笔了，回头看看好像也没有什么难的，好像慢慢的就有了，我完成网站的构建，一开始我对于建网站只是有一个很模糊的想法，只是想了一下，慢慢的我就开始接触域名的购买，框架的使用，跳转，其中虽然有困难，也好像困惑了很久，但是也都解决了，所以请不要担心你行动的结果——仅仅关注行动本身就好了。行动的结果会自然而然地产生。问题不是用来忧虑的，而是用来解决的，对待问题不应该感到忧虑，而应该感到快乐，快乐的是又有进步的空间，又有一道难题需要攻破。或许当时很忧虑，后来回头看看，回首向来萧瑟处，归去，也无风雨也无晴。每个人都真正会从事一生的事业，那就是：学习和成长！</p><h2 id="意义的来源"><a href="#意义的来源" class="headerlink" title="意义的来源"></a>意义的来源</h2><p>在随笔11中我写道“人生的意义是什么？”回答：“每个人都有不同的答案。”，我的答案是什么？每天进步一点点。<br>认识到死亡给我的意义。明天和意外哪个先来，谁也不知道。死亡不不可以控制的，但是我想心由境转，境由心生。认识到死亡是存在的一段时间，我认为一切是无意义的，死亡到来之时所有的一切都将消散，与我而言没有意义。于浩歌狂热之际中寒，于天上看见深渊。一天无意义，一生无意义。生命呀，你到底是为了什么？活着你到底是为了什么，活着就是为了活着，活着就是为了感受活着，活着就是每一天，每一刻去感受活着，在面对一个个挑战时有勇气去战胜自己，死亡赋予我战胜困难的勇气，在过程中感到意义，感到快乐，活着就是有意义，有意义就是好好活。不以物喜，不以自悲。</p><p>注： 今天重新看了之前的随笔，发现了许多错别字，这里挖个坑（后面慢慢修改，因为看和改完全是两种条件。改的条件是需要电脑和我发现错别字。）</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch基础</title>
    <link href="/2024/04/27/deeplearnbook3-1/"/>
    <url>/2024/04/27/deeplearnbook3-1/</url>
    
    <content type="html"><![CDATA[<p>参考书目：《Python深度学习基于PyTorch》</p><h2 id="Pytorch-基础"><a href="#Pytorch-基础" class="headerlink" title="Pytorch 基础"></a>Pytorch 基础</h2><p>Pytorch采用python语言接口实现编程，非常容易上手。它就像带GPU的Numpy，与Python一样都属于动态框架。PyTorch继承了Torch灵活、动态的编程环境和用户友好的界面，支持以快速灵活的方式构建动态神经网络，还允许在训练过程中快速更改代码而不妨碍其性能，支持动态图形等尖端AI模型的能力，是快速实验的理想选择。</p><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>Pytorch是建立在Torch库上的python包，目的在于加速深度学习的应用。它包含了多维张量的数据结构以及基于其上的多种数学操作。动态计算图<br>PyTorch 主要由4个包组成：<br>torch：类似于Numpy的通用数组库，可将张量类型转换为torch.cuda.Tensor.Float，并在GPU上进行计算。<br>torch.autograd: 用于构建计算图形并自动获取梯度的包<br>torch.nn: 具有共享层和损失函数的神经网络库。<br>torch.ptim： 具有通用优化算法（如SGD，Adam等）的优化包</p><h2 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h2><p>pytorch的GPU版本 的安装配置有一点繁琐，这里阐述一下需要的装备</p><ol><li>电脑（装有显卡，本台电脑是GTX-3080），安装GPU的驱动（如英伟达的NVDIA）以及CUDA，cuDNN计算框架。安装GPU驱动的时候就会安装CUDA，cuDNN的安装要去官网查找对应版本。</li><li>软件miniconda ，python的环境包管理</li><li>VS_code</li></ol><h2 id="Numpy和Tensor"><a href="#Numpy和Tensor" class="headerlink" title="Numpy和Tensor"></a>Numpy和Tensor</h2><p>前面说到深度学习的最主要的东西是矩阵，深度学习就是一个大的函数。Tensor是numpy的Pytorch中的实现(这么说不知道行不行，但我是这么认为的)，pytorch中的Tensor可以是零维（又称为一个标量或一个数）、一维、二维以及多维数组。Tensor可以把产生的Tensor放置在GPU中进行加速计算。<br>对Tensor的操作很多，从接口的角度可以划分为两类，<br>torch.function 如 torch.sum、torch.add<br>tensor.function ，如tensor.view、tensor.add等<br>这些操作对于大部分Tensor都是等价的，比如torch.add(x)与x.add(y)等价（注：前提是x的dtype是tensor）</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import torch<br>x = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[1 , 2]</span>)<br>y =torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[3,4]</span>)<br>z = x<span class="hljs-selector-class">.add</span>(y)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(z)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x)</span></span><br>x<span class="hljs-selector-class">.add_</span>(y)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x)</span></span><br></code></pre></td></tr></table></figure><p>Tensor创建的方式</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"># <span class="hljs-built_in">Tensor</span>(*size) 直接从参数构建一个张量<br></code></pre></td></tr></table></figure><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="仍然没有搞明白使用梯度下降算法来怎么对矩阵中的参数进行更新的方法？"><a href="#仍然没有搞明白使用梯度下降算法来怎么对矩阵中的参数进行更新的方法？" class="headerlink" title="仍然没有搞明白使用梯度下降算法来怎么对矩阵中的参数进行更新的方法？"></a>仍然没有搞明白使用梯度下降算法来怎么对矩阵中的参数进行更新的方法？</h3><p>目前网络上的梯度下降多使用线性函数来进行距离，没有推导过程，而我又不会推导，死循环了。所以我想知道怎么使用矩阵来实现梯度下降，进而实现SGD，Adam，Rsomp等梯度下降优化函数。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——ResNet</title>
    <link href="/2024/04/27/deeplearnpaper4/"/>
    <url>/2024/04/27/deeplearnpaper4/</url>
    
    <content type="html"><![CDATA[<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/101332297">信念网络</a><br><a href="https://zhuanlan.zhihu.com/p/56961832">论文分析</a><br><a href="https://zhuanlan.zhihu.com/p/159162779">论文中文翻译</a><br><a href="https://arxiv.org/pdf/1512.03385">论文原文</a><br><a href="https://www.bilibili.com/video/BV1P3411y7nn/">李沐读论文</a></p><h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h2 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;resnet in pytorch</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Deep Residual Learning for Image Recognition</span><br><span class="hljs-string">    https://arxiv.org/abs/1512.03385v1</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicBlock</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Basic Block for resnet 18 and resnet 34</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment">#BasicBlock and BottleNeck block</span><br>    <span class="hljs-comment">#have different output size</span><br>    <span class="hljs-comment">#we use class attribute expansion</span><br>    <span class="hljs-comment">#to distinct</span><br>    expansion = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, stride=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-comment">#residual function</span><br>        self.residual_function = nn.Sequential(<br>            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels * BasicBlock.expansion)<br>        )<br><br>        <span class="hljs-comment">#shortcut</span><br>        self.shortcut = nn.Sequential()<br><br>        <span class="hljs-comment">#the shortcut output dimension is not the same with residual function</span><br>        <span class="hljs-comment">#use 1*1 convolution to match the dimension</span><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> in_channels != BasicBlock.expansion * out_channels:<br>            self.shortcut = nn.Sequential(<br>                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(out_channels * BasicBlock.expansion)<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> nn.ReLU(inplace=<span class="hljs-literal">True</span>)(self.residual_function(x) + self.shortcut(x))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BottleNeck</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Residual block for resnet over 50 layers</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    expansion = <span class="hljs-number">4</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, stride=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.residual_function = nn.Sequential(<br>            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels * BottleNeck.expansion),<br>        )<br><br>        self.shortcut = nn.Sequential()<br><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> in_channels != out_channels * BottleNeck.expansion:<br>            self.shortcut = nn.Sequential(<br>                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(out_channels * BottleNeck.expansion)<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> nn.ReLU(inplace=<span class="hljs-literal">True</span>)(self.residual_function(x) + self.shortcut(x))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNet</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, block, num_block, num_classes=<span class="hljs-number">3</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        self.in_channels = <span class="hljs-number">64</span><br><br>        self.conv1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>))<br>        <span class="hljs-comment">#we use a different inputsize than the original paper</span><br>        <span class="hljs-comment">#so conv2_x&#x27;s stride is 1</span><br>        self.conv2_x = self._make_layer(block, <span class="hljs-number">64</span>, num_block[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)<br>        self.conv3_x = self._make_layer(block, <span class="hljs-number">128</span>, num_block[<span class="hljs-number">1</span>], <span class="hljs-number">2</span>)<br>        self.conv4_x = self._make_layer(block, <span class="hljs-number">256</span>, num_block[<span class="hljs-number">2</span>], <span class="hljs-number">2</span>)<br>        self.conv5_x = self._make_layer(block, <span class="hljs-number">512</span>, num_block[<span class="hljs-number">3</span>], <span class="hljs-number">2</span>)<br>        self.avg_pool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.fc = nn.Linear(<span class="hljs-number">512</span> * block.expansion, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, block, out_channels, num_blocks, stride</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;make resnet layers(by layer i didnt mean this &#x27;layer&#x27; was the</span><br><span class="hljs-string">        same as a neuron netowork layer, ex. conv layer), one layer may</span><br><span class="hljs-string">        contain more than one residual block</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            block: block type, basic block or bottle neck block</span><br><span class="hljs-string">            out_channels: output depth channel number of this layer</span><br><span class="hljs-string">            num_blocks: how many blocks per layer</span><br><span class="hljs-string">            stride: the stride of the first block of this layer</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            return a resnet layer</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># we have num_block blocks per layer, the first block</span><br>        <span class="hljs-comment"># could be 1 or 2, other blocks would always be 1</span><br>        strides = [stride] + [<span class="hljs-number">1</span>] * (num_blocks - <span class="hljs-number">1</span>)<br>        layers = []<br>        <span class="hljs-keyword">for</span> stride <span class="hljs-keyword">in</span> strides:<br>            layers.append(block(self.in_channels, out_channels, stride))<br>            self.in_channels = out_channels * block.expansion<br><br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output = self.conv1(x)<br>        output = self.conv2_x(output)<br>        output = self.conv3_x(output)<br>        output = self.conv4_x(output)<br>        output = self.conv5_x(output)<br>        output = self.avg_pool(output)<br>        output = output.view(output.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        output = self.fc(output)<br><br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet18</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 18 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BasicBlock, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet34</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 34 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BasicBlock, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet50</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 50 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BottleNeck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet101</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 101 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BottleNeck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">23</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet152</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 152 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BottleNeck, [<span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">36</span>, <span class="hljs-number">3</span>])<br><br><br><br><span class="hljs-comment"># import torch</span><br><span class="hljs-comment"># from torchsummary import summary</span><br><br><span class="hljs-comment"># # Instantiate the ResNet model (choose the variant you want, e.g., resnet18())</span><br><span class="hljs-comment"># model = resnet18()</span><br><br><span class="hljs-comment"># # Move the model to the device (e.g., GPU if available)</span><br><span class="hljs-comment"># device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="hljs-comment"># model.to(device)</span><br><br><span class="hljs-comment"># # Print the model summary</span><br><span class="hljs-comment"># summary(model, (1, 33, 1025))  # Adjust the input size (channels, height, width) as needed</span><br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——GoogleNet</title>
    <link href="/2024/04/27/deeplearnpaper3/"/>
    <url>/2024/04/27/deeplearnpaper3/</url>
    
    <content type="html"><![CDATA[<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>2014年<br><a href="https://zhuanlan.zhihu.com/p/482776152">GoogleNet解读</a><br><a href="https://blog.csdn.net/Jwenxue/article/details/107788765">论文翻译中文</a><br><a href="https://arxiv.org/pdf/1409.4842v1">论文原文</a><br><a href="https://www.jianshu.com/p/6a9e33e34571">论文中文翻译</a></p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;google net in pytorch</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">[1] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,</span><br><span class="hljs-string">    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Going Deeper with Convolutions</span><br><span class="hljs-string">    https://arxiv.org/abs/1409.4842v1</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-comment">#1x1conv branch</span><br>        self.b1 = nn.Sequential(<br>            nn.Conv2d(input_channels, n1x1, kernel_size=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n1x1),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>        <span class="hljs-comment">#1x1conv -&gt; 3x3conv branch</span><br>        self.b2 = nn.Sequential(<br>            nn.Conv2d(input_channels, n3x3_reduce, kernel_size=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n3x3_reduce),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(n3x3_reduce, n3x3, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n3x3),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>        <span class="hljs-comment">#1x1conv -&gt; 5x5conv branch</span><br>        <span class="hljs-comment">#we use 2 3x3 conv filters stacked instead</span><br>        <span class="hljs-comment">#of 1 5x5 filters to obtain the same receptive</span><br>        <span class="hljs-comment">#field with fewer parameters</span><br>        self.b3 = nn.Sequential(<br>            nn.Conv2d(input_channels, n5x5_reduce, kernel_size=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n5x5_reduce),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(n5x5_reduce, n5x5, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n5x5, n5x5),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(n5x5, n5x5, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n5x5),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>        <span class="hljs-comment">#3x3pooling -&gt; 1x1conv</span><br>        <span class="hljs-comment">#same conv</span><br>        self.b4 = nn.Sequential(<br>            nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>            nn.Conv2d(input_channels, pool_proj, kernel_size=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(pool_proj),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GoogleNet</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_class=<span class="hljs-number">3</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.prelayer = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">192</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>        )<br><br>        <span class="hljs-comment">#although we only use 1 conv layer as prelayer,</span><br>        <span class="hljs-comment">#we still use name a3, b3.......</span><br>        self.a3 = Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, <span class="hljs-number">96</span>, <span class="hljs-number">128</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)<br>        self.b3 = Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">192</span>, <span class="hljs-number">32</span>, <span class="hljs-number">96</span>, <span class="hljs-number">64</span>)<br><br>        <span class="hljs-comment">##&quot;&quot;&quot;In general, an Inception network is a network consisting of</span><br>        <span class="hljs-comment">##modules of the above type stacked upon each other, with occasional</span><br>        <span class="hljs-comment">##max-pooling layers with stride 2 to halve the resolution of the</span><br>        <span class="hljs-comment">##grid&quot;&quot;&quot;</span><br>        self.maxpool = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br><br>        self.a4 = Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, <span class="hljs-number">96</span>, <span class="hljs-number">208</span>, <span class="hljs-number">16</span>, <span class="hljs-number">48</span>, <span class="hljs-number">64</span>)<br>        self.b4 = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, <span class="hljs-number">112</span>, <span class="hljs-number">224</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        self.c4 = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        self.d4 = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, <span class="hljs-number">144</span>, <span class="hljs-number">288</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        self.e4 = Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br><br>        self.a5 = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br>        self.b5 = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">48</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br><br>        <span class="hljs-comment">#input feature size: 8*8*1024</span><br>        self.avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.dropout = nn.Dropout2d(p=<span class="hljs-number">0.4</span>)<br>        self.linear = nn.Linear(<span class="hljs-number">1024</span>, num_class)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.prelayer(x)<br>        x = self.maxpool(x)<br>        x = self.a3(x)<br>        x = self.b3(x)<br><br>        x = self.maxpool(x)<br><br>        x = self.a4(x)<br>        x = self.b4(x)<br>        x = self.c4(x)<br>        x = self.d4(x)<br>        x = self.e4(x)<br><br>        x = self.maxpool(x)<br><br>        x = self.a5(x)<br>        x = self.b5(x)<br><br>        <span class="hljs-comment">#&quot;&quot;&quot;It was found that a move from fully connected layers to</span><br>        <span class="hljs-comment">#average pooling improved the top-1 accuracy by about 0.6%,</span><br>        <span class="hljs-comment">#however the use of dropout remained essential even after</span><br>        <span class="hljs-comment">#removing the fully connected layers.&quot;&quot;&quot;</span><br>        x = self.avgpool(x)<br>        x = self.dropout(x)<br>        x = x.view(x.size()[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)<br>        x = self.linear(x)<br><br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">googlenet</span>():<br>    <span class="hljs-keyword">return</span> GoogleNet()<br><br><br><span class="hljs-comment"># import torch</span><br><span class="hljs-comment"># from torchsummary import summary</span><br><br><span class="hljs-comment"># # Instantiate the GoogleNet model</span><br><span class="hljs-comment"># model = googlenet()</span><br><br><span class="hljs-comment"># # Move the model to the device (e.g., GPU if available)</span><br><span class="hljs-comment"># device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="hljs-comment"># model.to(device)</span><br><br><span class="hljs-comment"># # Print the model summary</span><br><span class="hljs-comment"># summary(model, (1, 33, 1025))  # Adjust the input size (channels, height, width) as needed</span><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python——flask后端代码开发</title>
    <link href="/2024/04/26/tiankeng5/"/>
    <url>/2024/04/26/tiankeng5/</url>
    
    <content type="html"><![CDATA[<p>看别人的代码是必要的</p><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>最近我们在研究如何将直接的模型部署在网站上面，所以我们打算写一个python工程来放置对模型部署，使用的flask来进行模型和前端的交互。<br>大概思路是这样的，前端传入一个文件，经过flask传输到服务器，触发处理求取，模型处理完成后返回模型处理结果。<br>增加功能是用户可以自己选择模型种类和通道种类。（用户可以自由选取自己想处理的通道对应的模型）</p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><p>世界上没有技术驱动的公司，不论是google、facebook，还是阿里腾讯、阿里。技术不是源头，需求才是，因此一切技术问题，都要服从产品交付和市场反馈。所以，任何公司，都不可能以技术去驱动自身。人可以以技术驱动自己进步，但公司不行。以解决问题为导向，我需要解决什么问题，然后了解这个问题，有什么解决问题的思路。<br>资本富集的地方，人都得加班，加班的本质，是人跟着机器跑、人跟着钱跑；更为本质地说，资本富集的地方，人作为劳动力，也是资本的一种。即，人是资本而不是人本身。IT是工科，不是理科，和IT行业相似度最高的行业是盖楼房。真的，相似度相当惊人。<br>一个程序员，应该花80%的时间做代码设计、画UML图、画时序图，20%的时间写code和debug；菜鸟程序员的这个比例恰好是反的。一句话，不论这个需求有多紧急，你都一定要“想好再动手”；“想好”的标志就是设计文档写好了；文档一旦写好，写代码就是纯粹的无脑工作。<br>英语，很重要。能否使用英语查阅资料，是区分技术人员水平的重要指示之一。寄希望于“有人迟早会翻译成中文”的人是愚蠢的、是会被淘汰的。<br>工作要有热情。</p><p>智商决定你的起点情商决定你能走多远爬多高；混职场，靠的是情商。情商高就是：别人愿意和你一起工作、你有问题的时候别人愿意帮你。智商有时候可以稍微弥补一下情商但不起决定性的作用。</p><h3 id="跨域"><a href="#跨域" class="headerlink" title="跨域"></a>跨域</h3><p>http:&#x2F;&#x2F; (协议)<br><a href="http://hostname(主机名)：port（端口号）">http://hostname(主机名)：port（端口号）</a> 这三个东西有一个不同就叫做跨域，跨域的本质是浏览器的安全保护。<br>怎么解决，后端允许跨域。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔12</title>
    <link href="/2024/04/26/ganwu12/"/>
    <url>/2024/04/26/ganwu12/</url>
    
    <content type="html"><![CDATA[<h2 id="当我感觉烦躁时我该怎么办？"><a href="#当我感觉烦躁时我该怎么办？" class="headerlink" title="当我感觉烦躁时我该怎么办？"></a>当我感觉烦躁时我该怎么办？</h2><p>感到烦躁是自己告诉自己应该休息了，大脑到达负荷了。出去走走，小睡一会，放松放松。<br>精力越是消极、情绪越低落，表现就越糟糕；相反，精力越积极、情绪越高涨，表现也会越高效。精力主要来自体能、思维、意志、情感</p><ol><li>锻炼，适当的锻炼会对精力有很大的帮助，间歇性训练，每一次哪怕只是维持1分钟左右，都会产生出乎意料的积极影响。俯卧撑微行为计划</li><li>呼吸，恢复精力时有技巧的，三次一组吸气，三次一组吸气、也就是，把一次吸气分为三次，把呼气分成六次，这样通过深度、平静、有节奏地呼吸会激发精力，带来放松、精力不够的时候可以尝试做一下这个动作。</li><li>食物，早餐非常重要，它不仅能提高血糖水平，还能强力推动机体新陈代谢。</li><li>充足的睡眠，充足的睡眠是最重要的精力恢复来源。中午小睡一会，能快速恢复精力。随时准备20分钟的小睡。</li><li>情绪，满足和安全感的活动都能够激发正面情感、能够恢复精力，但是很多人会觉得看电视，拿着Ipad追剧，看综艺也能带来满足感，看电视带来了只是暂时的恢复，时间长了、反而让人消耗精力。要去多做一些能够有社交性的活动，比如骑自行车、参加读书会、听音乐会等等，因为可以和其它人交流，这样满足感会时间会持续的长一些。</li><li>思维精力恢复的关键呢，就是让大脑能有间歇地休息。创造性需要投入和抽离、思考和放松、活跃与休息之间有节奏的交替进行。</li></ol><h2 id="我的精力恢复方式"><a href="#我的精力恢复方式" class="headerlink" title="我的精力恢复方式"></a>我的精力恢复方式</h2><ol><li>抖腿</li><li>小睡</li><li>做一个俯卧撑。</li><li>吃点坚果</li><li>多喝水</li><li>在进步本上记录时间点</li></ol><p>你对自己从事一生的事业了解之匮乏是难以想象的。”每个人都真正会从事一生的事业，那就是：学习和成长！</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑——软编码</title>
    <link href="/2024/04/26/tiankeng4/"/>
    <url>/2024/04/26/tiankeng4/</url>
    
    <content type="html"><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>今天想重新复写一下模型结构，让模型能够自适应的去适应在不同的数据集，实现软编码。<br>首先介绍一下硬编码<br><a href="https://blog.csdn.net/weixin_44943389/article/details/134928228">参考博客</a><br>硬编码是指将具体的数值、路径、参数等直接写入程序代码中，而不通过变量或配置文件来表示。这样的做法使得程序中的这些数值和参数变得固定，不容易修改，且缺乏灵活性。硬编码的值通常被称为”魔法数”（Magic Numbers）或”魔法字符串”，因为它们没有直观的含义，只能通过查看代码来了解。<br>例如，以下是一个硬编码的示例，其中数值 10 直接出现在代码中：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss">for <span class="hljs-selector-tag">i</span> in <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Iteration&quot;</span>, i)<br></code></pre></td></tr></table></figure><p>软编码（Softcoding）：</p><p>软编码是指通过变量、配置文件、参数等方式将具体数值或参数抽象出来，而不是直接写入代码。通过软编码，程序变得更加灵活，可以更容易地进行修改和维护，且适应性更强。</p><p>使用软编码的例子：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs maxima"><span class="hljs-built_in">iterations</span> = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">iterations</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Iteration&quot;</span>, i)<br></code></pre></td></tr></table></figure><p>硬编码：将具体数值、参数等直接写入程序代码中，缺乏灵活性，不易修改和维护。</p><p>软编码：通过变量、配置文件等方式将数值或参数抽象出来，使得程序更具灵活性，易于修改和维护。</p><h2 id="我的解决办法"><a href="#我的解决办法" class="headerlink" title="我的解决办法"></a>我的解决办法</h2><ol><li><p>在foward中进行重新赋值（有问题），问题就是没有前向训练过程中都重新创建了一个linear，这个linear层的参数没有训练，相当于随机（注：只是我猜的，没有验证，挖个坑在这）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">IntegratedNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(IntegratedNet, self).__init__()<br>        self.linear = <span class="hljs-literal">None</span> <span class="hljs-comment"># 在初始类中先第一一个self.linear=None ，而后在forward中重新定义</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">if</span> self.linear <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            self.linear = nn.Linear(in_features=x.size(<span class="hljs-number">1</span>), out_features=<span class="hljs-number">1</span>)<br>        <br>        x = self.linear(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 创建模型实例</span><br>model = IntegratedNet()<br><br><span class="hljs-comment"># 创建输入数据</span><br>x = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">512</span>, <span class="hljs-number">64</span>)<br><br><span class="hljs-comment"># 前向传播</span><br>output = model(x)<br><br><span class="hljs-comment"># 打印输出的形状</span><br><span class="hljs-built_in">print</span>(output.size())<br><br></code></pre></td></tr></table></figure></li><li><p>第二种解决方法在__init__中留下一个接口，在调用这个模型时直接重新赋值。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">class</span> IntegratedNet(nn.Module):<br>    <span class="hljs-attribute">def</span> __init__(self, input_size=<span class="hljs-number">3</span>, mlp_dim=<span class="hljs-number">512</span>, mlp_ratio=<span class="hljs-number">4</span>,<br>                 <span class="hljs-attribute">dims</span>=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],in_feature=<span class="hljs-number">64</span>):<br>        <span class="hljs-attribute">super</span>(IntegratedNet, self).__init__()<br><br><span class="hljs-attribute">model</span> = IntegratedNet(input_size=<span class="hljs-number">2</span>,in_feature=<span class="hljs-number">209</span>)  # 全部重新赋值，实现<br><span class="hljs-attribute">device</span> = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> if torch.cuda.is_available() else <span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-attribute">model</span> = model.to(device)<br><br><span class="hljs-comment"># 打印模型结构摘要</span><br><span class="hljs-attribute">summary</span>(model, (<span class="hljs-number">2</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3333</span>))<br><br></code></pre></td></tr></table></figure></li><li><p>重新构建网络结构，加入自适应池化层。</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《定风波》</title>
    <link href="/2024/04/25/juzhi2/"/>
    <url>/2024/04/25/juzhi2/</url>
    
    <content type="html"><![CDATA[<center> <font size = 6>《定风波》 <font></center>  三月七日，沙湖道中遇雨。雨具先去，同行皆狼狈，余独不觉。已而遂晴，故作此词。  莫听穿林打叶声，何妨吟啸且徐行。竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生。  料峭春风吹酒醒，微冷，山头斜照却相迎。回首向来萧瑟处，归去，也无风雨也无晴。<p><font size = 3><font>译文：三月七日，在沙湖道上赶上了下雨。雨具先前被带走了，同行的人都觉得很狼狈，只有我不这么觉得。过了一会儿天晴了，就创作了这首词。不用注意那穿林打叶的雨声，不妨一边吟咏长啸着，一边悠然地行走。竹杖和草鞋轻捷得胜过骑马，有什么可怕的？一身蓑衣任凭风吹雨打，照样过我的一生。春风微凉，将我的酒意吹醒，寒意初上，山头初晴的斜阳却应时相迎。回头望一眼走过来遇到风雨的地方，回去吧，对我来说，既无所谓风雨，也无所谓天晴。</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>喜欢的一些句子</title>
    <link href="/2024/04/25/juzhi1/"/>
    <url>/2024/04/25/juzhi1/</url>
    
    <content type="html"><![CDATA[<ol><li><p>于浩歌狂热之际中寒，于天上看见深渊，于一切眼中看见无所有，于无所希望中得救 —— 鲁迅《墓碣文》</p></li><li><p>但是太阳，他每时每刻都是夕阳也都是旭日。 当他熄灭着走下山去收尽苍凉残照之际，正是他在另一面，燃烧着爬上山巅布散烈烈朝辉之时。那一天，我也将沉静着走下山去，扶着我的拐杖。有一天，在某一处山洼里，势必会跑上来一个欢蹦的孩子，抱着他的玩具。——史铁生</p></li><li><p>生命就是这样一个过程，一个不断超越自身局限的过程，这就是命运，任何人都是一样。在这过程中，我们遭遇痛苦、超越局限、从而感受幸福。所以一切人都是平等的，我们毫不特殊。——史铁生 《病隙碎笔》</p></li><li><p>只要你不停的向上走，一级级楼梯就没有尽头，在你向上走的脚下，它们也在向上长。——卡夫卡《律师》</p></li><li><p>找到属于自己的意义，赋予生命目的，每一天都像向日葵朝向太阳一样，充满方向和意义的活，是人类能活出的最好样子，它治愈我们的根本恐惧。</p></li><li><p>成功就是用自己喜欢的方式过一生。这句话分三部分。首先要知道自己喜欢什么，其次要有追逐它的勇气，追到了，还需要一生不渝的毅力。</p></li><li><p>“我来到这个世界，不是为了繁衍后代，而是来看花怎么开，水怎么流，太阳怎么升起，夕阳如何落下。我活在世上，无非是想要明白些道理，遇见有趣的事。生命是一场偶然，我在其中寻找因果。”生命对于每个人，它的意义是不一样的，每个人都是宇宙中一个独特的存在。</p></li><li><p>世界上只有一种英雄主义，那就是认清生活的真相后依旧热爱生活。</p></li><li><p>一切的一切都是个人的选择，自己支付代价，自己承担后果，旁人没什么评价的资格。世界上很多事是你不能细想也不能过分纠结的，太纷杂的想法会如同重重的枷锁，束缚住你的脚步，等你回头看的时候，你发现其实走错路没什么可怕的，反而是踟蹰不决让自己停留在原地，丧失了人生很多重要的体验。</p></li><li><p>愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光，就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。此后如竟没有炬火：我便是唯一的光。倘若有了炬火，出了太阳，我们自然心悦诚服的消失。不但毫无不平，而且还要随喜赞美这炬火或太阳；因为他照了人类，连我都在内。——鲁迅《热风·随感录四十一》</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔11</title>
    <link href="/2024/04/25/ganwu11/"/>
    <url>/2024/04/25/ganwu11/</url>
    
    <content type="html"><![CDATA[<p>“人生的意义是什么？”<br>回答：“每个人都有不同的答案。”</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文思路</title>
    <link href="/2024/04/25/paper_idear/"/>
    <url>/2024/04/25/paper_idear/</url>
    
    <content type="html"><![CDATA[<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><ol><li>傅里叶变换 （时间域变换为频域）</li><li>归一化（零归一化，批归一化，层归一化）</li><li>数据增强操作。</li></ol><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ol><li>startRule</li><li>ReLU</li></ol><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><ol><li>编码器——解码器</li><li>残差连接（基于ResNet）</li><li>MLP</li><li>双输入卷积神经网络。</li></ol><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ol><li>交叉熵损失函数</li></ol><h2 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h2><ol><li>SGD</li><li>RMSprop</li></ol><h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><p>怎么进行评估，<br><img src="/pic/predict.jpg" alt="二分类示意图"><br>True Positives (TP)：正类别样本中被正确预测为正类别的数量。True Negatives (TN)：负类别样本中被正确预测为负类别的数量。False Positives (FP)：负类别样本中被错误预测为正类别的数量。False Negatives (FN)：正类别样本中被错误预测为负类别的数量。</p><ol><li>ACC （准确率） ( TP+TN )  &#x2F; (TP+TN+FP+FN)</li><li>pression (精确率) (TP&#x2F;TP+FP) </li><li>Recall (Sensitivity，灵敏度) (TP &#x2F; TP+FN )</li><li>F1-score (F1 值) （2 x (precision x Recall)&#x2F;(precision + Recall )）</li><li>Specificity (特异性) （TN &#x2F; (TN + FP )）</li><li>混淆矩阵</li></ol><h2 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h2><ol><li>双输入卷积神经网络 （傅里叶信号+归一化信号）</li></ol><h2 id="实验对比设置"><a href="#实验对比设置" class="headerlink" title="实验对比设置"></a>实验对比设置</h2><ol><li>横向实验： 3个数据集，其中包含为1个私有数据集，2个公开数据集。（注：私有数据集中的数据使用不同仪器采集，可以分为很多类。公开数据集本身为一个数据集，但是有两个阶段数据，一个为包含了伪迹信号的数据，一个为未包含伪迹信号的数据集。）</li><li>纵向实验: 模型对比，AlexNet，CNN，googleNet，shuffleNet，ResNet（注：对比模型不足。）</li></ol><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><ol><li>双数据输入，对数据采用不同的数据预处理，比如FFT(傅里叶变换)，小波变换，零归一化.</li><li>在公开数据集上验证处理不同的数据预处理方法对实验结果的好坏。</li><li>对比私有数据集，验证自己模型的稳健性（鲁棒性）。</li><li>使用图像处理的模型结构。</li><li>公开数据集的二分类结果，对比论文。</li><li>验证选取合适的数据预处理方法是合适的。(注：数据预处理+深度学习模型架构。)</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python自定义包的层级引用</title>
    <link href="/2024/04/24/tiankeng3/"/>
    <url>/2024/04/24/tiankeng3/</url>
    
    <content type="html"><![CDATA[<h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><p>今天debug的时候自定义了一个函数，使用了start主函数来引用processing函数，processing函数引用了同级文件夹中的python文件中的dataset函数，在运行processing的时候，test是通过的，但是在使用start函数来调用processing函数，processing函数函数调用dataset函数时就出现了报错，提示找不到这个包。（注：这里需要指明的是start函数放置在根文件夹中，processing函数放置在processing文件夹中）问题就在于python文件的文件运行路径的出错。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs stylus">│  start<span class="hljs-selector-class">.py</span><br>│<br>├─static<br>│  │  __init__<span class="hljs-selector-class">.py</span><br>│  │<br>│  ├─model<br>│  │      MLPForMer<span class="hljs-selector-class">.pth</span><br>│  │<br>│  ├─processing<br>│  │  │  dataset<span class="hljs-selector-class">.py</span><br>│  │  │  net<span class="hljs-selector-class">.py</span><br>│  │  │  processing<span class="hljs-selector-class">.py</span><br>│  │  │  __init__<span class="hljs-selector-class">.py</span><br>│  │  │<br>│  │  └─__pycache__<br>│  │          dataset<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │          net<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │          processing<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │          __init__<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │<br>│  ├─result<br>│  │      average_probabilities<span class="hljs-selector-class">.csv</span><br>│  │      average_probabilities<span class="hljs-selector-class">.png</span><br>│  │<br>│  ├─tmp<br>│  │      <span class="hljs-number">1</span><span class="hljs-selector-class">.edf</span><br>│  │<br>│  └─__pycache__<br>│          __init__<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│<br>└─templates<br>        upload.html<br></code></pre></td></tr></table></figure><h2 id="填坑"><a href="#填坑" class="headerlink" title="填坑"></a>填坑</h2><p>对待这种问题目前我知道的有两种方法</p><ol><li>第一种方法在processing文件中明确的所以绝对引用的方法,因为问题是出现在processing中的。</li></ol><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> <span class="hljs-keyword">static</span>.processing.net <span class="hljs-keyword">import</span> * <span class="hljs-comment"># 这里引用是相对于start函数的位置</span><br><br></code></pre></td></tr></table></figure><ol start="2"><li>第二种方法，在__init__文件中给出直接引用<br>1.相对引用package需要采用from 相对位置 import package_name的方式。因为相对位置只能写在from和import中间。<br>2.from . import * 只会检索当前目录下的module，而不会导入package。</li></ol><h3 id="挖坑-1"><a href="#挖坑-1" class="headerlink" title="挖坑"></a>挖坑</h3><h3 id="windown怎么打印树状图？"><a href="#windown怎么打印树状图？" class="headerlink" title="windown怎么打印树状图？"></a>windown怎么打印树状图？</h3><p>使用<code>tree</code>来打印文件夹<br>使用<code>tree /f</code>来打印文件目录，如上面的文件目录结构。</p><h3 id="init-文件的作用是什么？"><a href="#init-文件的作用是什么？" class="headerlink" title="__init__文件的作用是什么？"></a>__init__文件的作用是什么？</h3><p>作为包的标识：</p><ol><li>当一个目录包含__init__.py文件时，Python会将该目录视为一个包，而不仅仅是一个普通的目录。这使得包内的模块可以被正确导入和使用。</li><li><strong>init</strong>.py文件可以是一个空文件，也可以包含初始化包的代码，比如设置包的属性、导入子模块等。</li></ol><p>初始化包：</p><ol><li>在包被导入时，<strong>init</strong>.py文件会在包内的其他模块之前被执行。这使得可以在__init__.py中执行一些初始化操作，比如设置包级别的变量、执行必要的初始化代码等。</li><li>这也可以用于在导入包时自动执行一些操作，比如注册插件、加载配置等。·</li></ol><h2 id="填坑-1"><a href="#填坑-1" class="headerlink" title="填坑"></a>填坑</h2><h3 id="居中显示"><a href="#居中显示" class="headerlink" title="居中显示"></a>居中显示</h3><p>可以使用center标签，或者使用div标签，或者使用p标签，或者h标签都是可以的</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">center</span>&gt;</span> <span class="hljs-tag">&lt;&gt;</span>数据结构和算法是居中展示，使用center标签<span class="hljs-tag">&lt;/<span class="hljs-name">center</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">center</span>&gt;</span>数据结构和算法是居中展示，使用div标签<span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">align</span>=<span class="hljs-string">&quot;center&quot;</span>&gt;</span>数据结构和算法是居中展示，使用p标签<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">h5</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;text-align:center&quot;</span>&gt;</span>数据结构和算法是居中展示，使用h标签<span class="hljs-tag">&lt;/<span class="hljs-name">h5</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="给改文字大小"><a href="#给改文字大小" class="headerlink" title="给改文字大小"></a>给改文字大小</h3><p>使用font标签，字体使用face，颜色使用color，尺寸使用size。<br>颜色可以使用字母比如red，black，blue，yellow等，也可以是十六进制表示比如#0000ff或者#F025AB等等<br>size 是从1到7，数字越小字体越小，浏览器默认是3<br>这几个属性可以都设置，也可以只设置其中的1到2个</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">&lt;font <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;黑体&quot;</span>&gt;我是黑体字体&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;微软雅黑&quot;</span>&gt;我是微软雅黑字体&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;STCAIYUN&quot;</span>&gt;我是华文彩字体云&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">color</span>=red <span class="hljs-attribute">size</span>=3 <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;黑体&quot;</span>&gt;我是红色，黑色字体，大小是3&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">color</span>=#F025AB <span class="hljs-attribute">size</span>=5&gt;我的颜色是#F025AB，大小是5&lt;/font&gt;<br><br></code></pre></td></tr></table></figure><h3 id="生成requirements-txt文件"><a href="#生成requirements-txt文件" class="headerlink" title="生成requirements.txt文件"></a>生成requirements.txt文件</h3><ol><li>如果你使用了虚拟环境（virtualenv）来管理项目依赖，可以在激活虚拟环境后运行pip freeze &gt; requirements.txt命令来生成requirements.txt文件。</li><li>使用pipreqs：pipreqs是一个可以根据Python代码中的import语句生成requirements.txt文件的工具。你可以通过以下命令安装pipreqs：<code>pip install pipreqs</code>, 然后在项目的根目录运行以下命令：<code>pipreqs .</code> 这将在当前目录下生成一个requirements.txt文件，其中包含了项目所需的所有包及其版本信息。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔10</title>
    <link href="/2024/04/24/ganwu10/"/>
    <url>/2024/04/24/ganwu10/</url>
    
    <content type="html"><![CDATA[<h3 id="怎么才能快乐？"><a href="#怎么才能快乐？" class="headerlink" title="怎么才能快乐？"></a>怎么才能快乐？</h3><p>不以物喜，不以己悲。因为外物而带来的快乐是会失去的，喜悦要来源于自己的内心。对我而言，战胜一个又一个困难的过程是有意思的，可能这件事我不是很感兴趣，但是令我开心的是解决问题的过程。我知道自己现在无法解决这个问题，但是慢慢的去做，在做的过程中我发现自己爱上了这个感觉，爱上了解决问题的过程，就像米哈里所说的心流状态，即使是一点点进步我就会产生一点点发自内心的喜悦。</p><p>一呼一吸，一言一行。花开花落，云卷云舒。感受过程，提升自己。一句我从小听到的话，一句很普通的话，隐含着巨大的道理——“每天进步一点点”,这句话在我曾经就读的小学校园的门口就能看到。进步是令人快乐的，这种快乐不是来源于外物，而是来源于自己的内心。胡适先生为“中国科学社”写社歌，最后几句歌词就是:我们唱天行有常，我们唱致知穷理。怕什么真理无穷，进一寸有一寸的欢喜。1934年，他写《“九·一八”的第三周年纪念告全国的青年》。其中说:“努力一分，就有一分的效果。努力百分，就有百分的效果。”</p><p>以勇气来迎接人生的每一个挑战。这个挑战不一定很宏大，可能它就是今天我要8点起床，晚上11点睡觉，可能就是我今天要做一个俯卧撑，跑一圈操场，一个普普通通的挑战。改变总是开始于微小的，即使是写一个project也是从新建文件开始的。积极向上，把每一次挑战看作一次进步的机会，即使失败了又有什么问题，我想这个过程中一定是快乐的。苦难不值得被歌颂，认清苦难的现实和战胜苦难的勇气才值得被歌颂。</p><p>踏上自我成长的道路，每一个过程都是令人快乐的。</p><h3 id="今天冲浪的感受。"><a href="#今天冲浪的感受。" class="headerlink" title="今天冲浪的感受。"></a>今天冲浪的感受。</h3>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像处理-数据预处理</title>
    <link href="/2024/04/23/deeplearnbook3/"/>
    <url>/2024/04/23/deeplearnbook3/</url>
    
    <content type="html"><![CDATA[<h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><p>在深度学习中，图像数据通常以多维数组（在Python中通常使用Numpy数组）的形式表示，这个数组的形状（shape）取决于图像的维度和颜色通道数。<br>灰度图像：对于灰度图像（也就是黑白图像），shape通常是两维的，表示图像的高度和宽度。例如，一个256x256像素的灰度图像的shape将是(256, 256)。灰度图像的像素值通常在0到255之间，其中0表示黑色，255表示白色，中间的值表示不同的灰度级别。这是因为每个像素通常由8位（一个字节）表示，所以可以有256（即$2^8$）个不同的可能值。然而，这并不是唯一的表示方式。有时，为了方便计算，我们可能会将像素值归一化到0到1之间。在这种情况下，0仍然表示黑色，1表示白色，中间的值表示不同的灰度级别。<br>彩色图像：对于彩色图像，通常使用RGB（红，绿，蓝）三个颜色通道，所以shape是三维的。例如，一个256x256像素的RGB彩色图像的shape将是(256, 256, 3)。这里的3代表三个颜色通道。彩色图像通常由三个颜色通道组成：红色（R），绿色（G）和蓝色（B）。每个通道的像素值通常在0到255之间，其中0表示该颜色的完全缺失，255表示该颜色的最大强度。所以，一个RGB颜色图像的像素值范围在理论上是0到255的三维空间，即(0,0,0)到(255,255,255)。同样，有时我们也会将每个颜色通道的像素值归一化到0到1之间。在这种情况下，(0,0,0)表示黑色，(1,1,1)表示白色，其他值表示不同的颜色。需要注意的是，虽然RGB是最常用的颜色空间，但也有其他的颜色空间，如HSV（色相，饱和度，亮度）或者CMYK（青色，品红，黄色，黑色），它们的取值范围可能会有所不同。<br>图像批量：在深度学习中，我们通常会一次处理多个图像，这就是所谓的批量（batch）。在这种情况下，图像数据的shape将是四维的：(批量大小, 高度, 宽度, 颜色通道数)。例如，如果我们有32个256x256像素的RGB图像，那么这个批量的shape将是(32, 256, 256, 3)。</p><h2 id="显示彩色图像"><a href="#显示彩色图像" class="headerlink" title="显示彩色图像"></a>显示彩色图像</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> cv2 # opencv中按BGR排布，蓝绿红<br><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">img</span> = np.zeros((<span class="hljs-number">5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>),dtype=np.uint8)<br><br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>) # 蓝色区块，位于（<span class="hljs-number">0</span> x轴，<span class="hljs-number">0</span> y轴，<span class="hljs-number">0</span>通道位置） 注： 如下图，对于<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>) # 绿色区块，位于（<span class="hljs-number">1</span> x轴， <span class="hljs-number">1</span> y轴，<span class="hljs-number">1</span>通道位置 ） <br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>) # <br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>) # 中间的白色区块。 <br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),<span class="hljs-number">255</span>) # <br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">4</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>)<br><br><span class="hljs-attribute">cv2</span>.namedWindow(&#x27;img&#x27;,cv2.WINDOW_NORMAL)<br><span class="hljs-attribute">cv2</span>.resizeWindow(&#x27;img&#x27;,<span class="hljs-number">500</span>,<span class="hljs-number">500</span>)<br><span class="hljs-attribute">cv2</span>.imshow(&#x27;img&#x27;,img)<br><span class="hljs-attribute">cv2</span>.waitKey()<br><span class="hljs-attribute">cv2</span>.destroyAllWindows()<br></code></pre></td></tr></table></figure><p><img src="/pic/sdxxtx1.png" alt="代码结果"></p><h2 id="对图像进行截取操作"><a href="#对图像进行截取操作" class="headerlink" title="对图像进行截取操作"></a>对图像进行截取操作</h2><p>剪裁图片<br><img src="/pic/kongfu_panda.jpg" alt="功夫熊猫"></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs scss">import cv2<br>import matplotlib<span class="hljs-selector-class">.pyplot</span> as plt<br>def <span class="hljs-built_in">show_plt</span>(path):<br>    # 显示原始图片<br>    image_path = path<br>    image = plt.<span class="hljs-built_in">imread</span>(image_path)<br>    plt.<span class="hljs-built_in">imshow</span>(image)<br>    plt.<span class="hljs-built_in">axis</span>(<span class="hljs-string">&#x27;off&#x27;</span>)<br>    plt.<span class="hljs-built_in">show</span>()<br><br>def <span class="hljs-built_in">mian</span>(path):<br>    img = cv2.<span class="hljs-built_in">imread</span>(path)<br>    if img is not None:<br>        ROI1 = img[<span class="hljs-number">79</span>:<span class="hljs-number">510</span>,<span class="hljs-number">345</span>:<span class="hljs-number">670</span>,:] # 可以看出 先是y轴，而后是x轴，最后是通道<br>        ROI2 = img[<span class="hljs-number">70</span>:<span class="hljs-number">340</span>,<span class="hljs-number">35</span>:<span class="hljs-number">250</span>,:] <br>        ROI3 = img[<span class="hljs-number">227</span>:<span class="hljs-number">499</span>,<span class="hljs-number">213</span>:<span class="hljs-number">356</span>,:]<br>        ROI4 = img[<span class="hljs-number">250</span>:<span class="hljs-number">510</span>,<span class="hljs-number">605</span>:<span class="hljs-number">751</span>,:]<br>        ROI5 = img[<span class="hljs-number">53</span>:<span class="hljs-number">421</span>,<span class="hljs-number">675</span>:<span class="hljs-number">969</span>,:]<br><br>        cv2.<span class="hljs-built_in">imshow</span>(<span class="hljs-string">&#x27;ROI1&#x27;</span>,ROI1)<br>        cv2.<span class="hljs-built_in">imshow</span>(<span class="hljs-string">&#x27;ROI2&#x27;</span>,ROI2)<br>        cv2.<span class="hljs-built_in">imshow</span>(<span class="hljs-string">&#x27;ROI3&#x27;</span>,ROI3)<br>        cv2.<span class="hljs-built_in">imshow</span>(<span class="hljs-string">&#x27;ROI4&#x27;</span>,ROI4)<br>        cv2.<span class="hljs-built_in">imshow</span>(<span class="hljs-string">&#x27;ROI5&#x27;</span>,ROI5)<br>        key = cv2.<span class="hljs-built_in">waitKey</span>(<span class="hljs-number">0</span>) # 等待按键，<br>        if key == <span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;q&#x27;</span>):<br>            cv2.<span class="hljs-built_in">destroyAllWindows</span>() # 关闭窗口<br>    else:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;failed to load image&#x27;</span>)<br><br>if __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-built_in">show_plt</span>(<span class="hljs-string">&#x27;1/kongfu_panda.jpg&#x27;</span>)<br>    <span class="hljs-built_in">mian</span>(<span class="hljs-string">&#x27;1/kongfu_panda.jpg&#x27;</span>)<br><br></code></pre></td></tr></table></figure><p>结果图片<br><img src="/pic/tpcl1.png" alt="剪切结果"></p><p><img src="/pic/tpcl2.png" alt="处理图片"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>(<span class="hljs-params">path</span>):<br>    img = cv2.imread(path)<br>    <span class="hljs-built_in">print</span>(img.shape)<br>    <span class="hljs-keyword">if</span> img <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        face = np.random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,(<span class="hljs-number">600</span>,<span class="hljs-number">445</span>,<span class="hljs-number">3</span>)) <span class="hljs-comment"># 随机产生掩盖矩阵</span><br>        img[<span class="hljs-number">50</span>:<span class="hljs-number">650</span>,<span class="hljs-number">364</span>:<span class="hljs-number">809</span>,:] = face <span class="hljs-comment"># 将图片的重新赋值</span><br>        cv2.namedWindow(<span class="hljs-string">&#x27;Data Masking&#x27;</span>,<span class="hljs-number">0</span>)<br>        cv2.resizeWindow(<span class="hljs-string">&#x27;Data Masking&#x27;</span>,<span class="hljs-number">500</span>,<span class="hljs-number">500</span>) <span class="hljs-comment"># 对现实的图片进行缩放，缩放到（500，500）</span><br>        cv2.imshow(<span class="hljs-string">&#x27;Data Masking&#x27;</span>,img)<br>        cv2.waitKey() <span class="hljs-comment"># 这样写的原因是保持图片的一直显示,否则一闪而逝</span><br>        cv2.destroyAllWindows()<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;falied to load image&#x27;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main(<span class="hljs-string">&quot;1/police_story.png&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/pic/tpcl3.png" alt="处理图片"></p><p>显示RGB通道的图片内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<br><br>lena = cv2.imread(<span class="hljs-string">&#x27;1/lena_color.jpg&#x27;</span>)<br><br><span class="hljs-keyword">if</span> lena <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    cv2.imshow(<span class="hljs-string">&#x27;lean&#x27;</span>,lena)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;img.shape&#x27;</span>,img.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;img.size&#x27;</span>,img.size)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;omg.dtype&#x27;</span>,img.dtype)<br>    b = lena[:,:,<span class="hljs-number">0</span>]<br>    g = lena[:,:,<span class="hljs-number">1</span>]<br>    r = lena[:,:,<span class="hljs-number">2</span>]<br>    cv2.imshow(<span class="hljs-string">&#x27;r&#x27;</span>,r)<br>    cv2.imshow(<span class="hljs-string">&#x27;g&#x27;</span>,g)<br>    cv2.imshow(<span class="hljs-string">&#x27;b&#x27;</span>,b)<br>    img1 = cv2.resize(img,(<span class="hljs-number">600</span>,<span class="hljs-number">600</span>)) <span class="hljs-comment"># 调整图像本身的大小</span><br>     img1 = cv2.merge([g,r,b]) <span class="hljs-comment"># cv2.merge可以调整BGR图像通道为自定义的[g,r,b]</span><br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;failed to load image&#x27;</span>)<br><br>cv2.waitKey()<br>cv2.destroyAllWindows()<br><span class="hljs-comment"># 可以看到基本的信息都包含，只不过是灰度的图像</span><br></code></pre></td></tr></table></figure><p><img src="/pic/tpcl5.png" alt="通道显示"></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br>import cv2<br><br>img1=np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[178,83,29]</span>,<span class="hljs-selector-attr">[202,200,158]</span>,<span class="hljs-selector-attr">[27,177,162]</span>],dtype=np.uint8)<br>img2=np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[26,48,57]</span>,<span class="hljs-selector-attr">[52,153,8]</span>,<span class="hljs-selector-attr">[10,232,7]</span>],dtype=np.uint8)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;img1\n&quot;</span>, img1)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;img1\n&quot;</span>, img2)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;img1+img2\n&quot;</span>,img1+img2)</span></span><br><br><br>cv2<span class="hljs-selector-class">.namedWindow</span>(<span class="hljs-string">&#x27;img&#x27;</span>,cv2.WINDOW_NORMAL)<br>cv2<span class="hljs-selector-class">.imshow</span>(<span class="hljs-string">&quot;img&quot;</span>,img1+img2)<br>cv2<span class="hljs-selector-class">.resizeWindow</span>(<span class="hljs-string">&#x27;img&#x27;</span>,<span class="hljs-number">500</span>,<span class="hljs-number">500</span>)<br>key = cv2<span class="hljs-selector-class">.waitKey</span>()<br><span class="hljs-keyword">if</span> key == <span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;q&#x27;</span>):<br>    cv2<span class="hljs-selector-class">.destroyAllWindows</span>()<br></code></pre></td></tr></table></figure><p><img src="/pic/tpcl6.png" alt="3x3矩阵显示"></p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><ol><li>二值化处理：这是最基本的阈值处理方法。对于每个像素，我们选择一个阈值。如果像素值大于阈值，我们将其设置为一个值（通常是白色），如果像素值小于或等于阈值，我们将其设置为另一个值（通常是黑色）。这样我们就得到了一个二值图像。</li><li>反二值化处理：这是二值化处理的反向操作。如果像素值大于阈值，我们将其设置为一个值（通常是黑色），如果像素值小于或等于阈值，我们将其设置为另一个值（通常是白色）。</li><li>截断阈值处理：对于每个像素，如果其值大于阈值，我们将其设置为阈值。如果像素值小于或等于阈值，我们保持其原值不变。</li><li>超阈值零处理：对于每个像素，如果其值大于阈值，我们保持其原值不变。如果像素值小于或等于阈值，我们将其设置为零。</li><li>低阈值零处理：这是超阈值零处理的反向操作。如果像素值大于阈值，我们将其设置为零。如果像素值小于或等于阈值，我们保持其原值不变。</li><li>自适应阈值处理：这是一种更复杂的方法，它不使用固定的阈值。相反，它根据像素周围的小区域计算阈值。因此，对于同一张图片上的不同区域，我们可以有不同的阈值。这对于当图像的光照条件变化很大时，例如，一半是明亮的，一半是暗淡的图像，非常有用。</li></ol><h2 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h2><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs smali">import cv2<br><br><span class="hljs-comment"># 读取图像</span><br>image = cv2.imread(&#x27;e1.jpg&#x27;, cv2.IMREAD_GRAYSCALE)<br><br><span class="hljs-comment"># 二值化处理</span><br>_, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)<br><br><span class="hljs-comment"># 反二值化处理</span><br>_, binary_inv_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)<br><br><span class="hljs-comment"># 截断阈值处理</span><br>_, trunc_image = cv2.threshold(image, 127, 255, cv2.THRESH_TRUNC)<br><br><span class="hljs-comment"># 超阈值零处理</span><br>_, tozero_inv_image = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO_INV)<br><br><span class="hljs-comment"># 低阈值零处理</span><br>_, tozero_image = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO)<br><br><span class="hljs-comment"># 自适应阈值处理</span><br>adaptive_image = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)<br><br><span class="hljs-comment"># 保存处理后的图像</span><br>cv2.imwrite(&#x27;binary_image.jpg&#x27;, binary_image)<br>cv2.imwrite(&#x27;binary_inv_image.jpg&#x27;, binary_inv_image)<br>cv2.imwrite(&#x27;trunc_image.jpg&#x27;, trunc_image)<br>cv2.imwrite(&#x27;tozero_inv_image.jpg&#x27;, tozero_inv_image)<br>cv2.imwrite(&#x27;tozero_image.jpg&#x27;, tozero_image)<br>cv2.imwrite(&#x27;adaptive_image.jpg&#x27;, adaptive_image)<br><br></code></pre></td></tr></table></figure><p><img src="/pic/e1.jpg" alt="原图"><br><img src="/pic/binary_image.jpg" alt="二值化处理图像"><br><img src="/pic/binary_inv_image.jpg" alt="反二值化处理图像"><br><img src="/pic/trunc_image.jpg" alt="截断阈值处理图像"><br><img src="/pic/tozero_inv_image.jpg" alt="超阈值处理图像"><br><img src="/pic/tozero_image.jpg" alt="低阈值零处理图像"><br><img src="/pic/adaptive_image.jpg" alt="自适应阈值处理图像"></p><h3 id="挖更大的坑，opencv库。"><a href="#挖更大的坑，opencv库。" class="headerlink" title="挖更大的坑，opencv库。"></a>挖更大的坑，opencv库。</h3><h3 id="彩色图像怎么转换为二维图像的？"><a href="#彩色图像怎么转换为二维图像的？" class="headerlink" title="彩色图像怎么转换为二维图像的？"></a>彩色图像怎么转换为二维图像的？</h3><p>首先灰度图像中的一个像素点的范围为0-255，彩色图像可以理解为3个灰度图重合。</p><h3 id="需要深度解析代码中的含义，比如一个参数有什么用处。"><a href="#需要深度解析代码中的含义，比如一个参数有什么用处。" class="headerlink" title="需要深度解析代码中的含义，比如一个参数有什么用处。"></a>需要深度解析代码中的含义，比如一个参数有什么用处。</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑——经典网络结构——AlexNet</title>
    <link href="/2024/04/23/tiankeng2/"/>
    <url>/2024/04/23/tiankeng2/</url>
    
    <content type="html"><![CDATA[<h2 id="问题1，卷积是什么？作用什么？"><a href="#问题1，卷积是什么？作用什么？" class="headerlink" title="问题1，卷积是什么？作用什么？"></a>问题1，卷积是什么？作用什么？</h2><p>卷积（Convolution）是一种数学运算，常用于信号处理和图像处理领域。在信号处理中，卷积用于将输入信号与卷积核（也称为滤波器）进行运算，产生输出信号。<br>卷积的作用有以下几个方面：</p><ol><li>信号滤波：卷积可以用于信号滤波，通过将输入信号与合适的卷积核进行卷积运算，可以实现对信号的滤波操作。滤波可以用于去除信号中的噪声、平滑信号、强调信号中的某些频率成分等。</li><li>特征提取：在图像处理中，卷积可以用于特征提取。通过将图像与不同的卷积核进行卷积运算，可以提取出图像中的不同特征，例如边缘、纹理、角点等。这些特征可以用于图像识别、目标检测和图像处理中的其他任务。</li><li>信号压缩：卷积可以用于信号压缩。通过将输入信号与适当的卷积核进行卷积运算，可以将信号表示转换为另一种表示形式，通常具有更紧凑的表示。这种表示形式可以用于信号压缩和数据压缩。</li><li>卷积神经网络：卷积神经网络（Convolutional Neural Network，CNN）是一种基于卷积运算的深度学习模型，广泛应用于图像识别、计算机视觉和自然语言处理等领域。卷积在 CNN 中用于提取图像或文本的特征，并通过多层卷积和池化操作来实现对输入数据的高级表示和分类。如果输入数据为图片，那么卷积层的作用就是提取图片中的信息，这些信息被称为图像特征，这些特征是由图像中的每个像素通过组合或者独立的方式所体现，比如图片的纹理特征、颜色特征、空间特征。</li></ol><p>卷积的操作过程：<br>请参考<a href="https://blog.csdn.net/weipf8/article/details/103917202">参考博客</a>。image的图片大小为5x5，卷积核为3x3，输出的特征的大小为3x3<br>特征图计算公式：一般情况下,输入的图片矩阵以及后面的卷积核,特征图矩阵都是方阵,这里设输入矩阵大小为w,卷和核大小为k,步幅为s,补零层数为p,则卷积后产生的特征图大小计算公式为:W &#x3D; （w+2p-k）&#x2F;s + 1. 比如说上面5x5的图片与3x3的卷积核进行卷积操作，特征图的大小为： W &#x3D; （5 + 2*0 -3）&#x2F;1 + 1 &#x3D;3<br>特征图相对与下一层的卷积层是图片。<br>卷积核的参数量计算，卷积核尺寸： K， 前一层的通道数：Cin 当前层的卷积核的个数： Cout 。单个卷积核的参数量： params kernel &#x3D; Cin x K x K, 有</p><p>假设有卷积神经网络，输入为大小224<em>224的RGB图，第一层为卷积层，有12个大小为5</em>5的卷积核，填充为2，步长为4。该层参数共有（  912    ）个。计算过程权重参数量：每个卷积核有 75 （5 x 5 x 3）个权重参数，共有12 个卷积核，所以权重参数量为 75×12&#x3D;900.偏置参数量：每个卷积核有一个偏置项，共有 12 个卷积核，所以偏置参数量为 12。<br><a href="https://blog.csdn.net/Together_CZ/article/details/115494176">执行卷积的过程的动态图</a><br>关于卷积其实还有很多问题，比如说输入一张（3x255x255）的图片，输入后经过卷积后输出的特征图大小为： 要考虑卷积核的大小（kernel size ） 步幅（stride），边界填充（padding） 计算公式入上式所示。<br>。1x1卷积为什么可以实现升维和降维。）<br>1x1 卷积可以实现升维和降维的原因在于：（通道数可以自定义数量）<br>升维：当输入特征图的通道数较少时，可以使用 1x1 卷积来增加通道数，从而增加网络的表示能力。这是因为 1x1 卷积可以将输入特征图中的每个通道与卷积核中的权重相乘并求和，从而生成一个新的特征图。<br>降维：当需要减少特征图的通道数时，可以使用 1x1 卷积并调整输出通道数为所需的值。通过调整卷积核中的输出通道数，可以实现特征图通道数的降维。</p><h2 id="问题2，池化是什么？作用是什么？"><a href="#问题2，池化是什么？作用是什么？" class="headerlink" title="问题2，池化是什么？作用是什么？"></a>问题2，池化是什么？作用是什么？</h2><p>池化（Pooling）是一种常用的操作，通常与卷积神经网络（CNN）结合使用。池化操作通过对输入数据的局部区域进行聚合或采样来减小数据的空间尺寸，从而减少参数数量、降低计算量，并提取出输入数据的重要特征。</p><p>池化的作用有以下几个方面</p><ol><li>降采样：池化操作可以减小输入数据的空间尺寸，从而降低后续层的计算复杂度。通过降低数据的维度，池化可以在保留重要特征的同时减少冗余信息，提高计算效率。</li><li>平移不变性：池化操作具有一定的平移不变性。在图像处理中，通过对局部区域进行池化操作，可以使得输入图像在平移、旋转和缩放等变换下具有一定的不变性。这对于图像识别和目标检测等任务是有益的。</li><li>特征提取：池化操作可以提取输入数据的重要特征。通过对局部区域进行池化，池化操作会选择区域中的最大值（最大池化）或平均值（平均池化）作为输出值，从而提取出输入数据的显著特征。这有助于减少数据的维度，并保留重要的特征信息。</li><li>减少过拟合：池化操作可以在一定程度上减少过拟合。通过减小数据的空间尺寸，池化操作可以降低模型的参数数量，从而减少过拟合的风险。此外，池化操作还可以通过丢弃一些冗余信息来提高模型的泛化能力。</li></ol><p>池化的种类</p><ol><li>最大池化（Max Pooling）：最大池化是一种常见的池化操作。在最大池化中，输入数据的局部区域被分割成不重叠的块，然后在每个块中选择最大值作为输出。最大池化可以提取出输入数据的显著特征，同时减小数据的空间尺寸。</li><li>平均池化（Average Pooling）：平均池化是另一种常见的池化操作。在平均池化中，输入数据的局部区域被分割成不重叠的块，然后计算每个块中元素的平均值作为输出。平均池化可以平滑输入数据并减小数据的空间尺寸。</li><li>自适应池化（Adaptive Pooling）：自适应池化是一种具有灵活性的池化操作。与最大池化和平均池化不同，自适应池化不需要指定池化窗口的大小，而是根据输入数据的尺寸自动调整池化窗口的大小。这使得自适应池化可以适应不同尺寸的输入数据。</li><li>全局池化（Global Pooling）：全局池化是一种特殊的池化操作，它将整个输入数据的空间尺寸缩减为一个单一的值或向量。全局池化可以通过对输入数据的所有位置进行池化操作，从而提取出输入数据的全局特征。常见的全局池化有全局平均池化（Global Average Pooling）和全局最大池化（Global Max Pooling）。</li></ol><h2 id="问题3，全连接是什么？作用是什么？"><a href="#问题3，全连接是什么？作用是什么？" class="headerlink" title="问题3，全连接是什么？作用是什么？"></a>问题3，全连接是什么？作用是什么？</h2><p>的是神经网络中的一种连接方式，也称为密集连接（dense connection）。在全连接中，每个神经元都与前一层的所有神经元相连。这意味着前一层的每个神经元的输出都将作为输入传递给下一层的每个神经元。<br>全连接层的作用是将输入数据进行线性变换，并应用激活函数来产生输出。这种连接方式允许神经网络学习输入数据中的复杂关系，从而实现各种任务，例如分类、回归等。<br><img src="/pic/tiankeng1.png" alt="全连接示意图"><br>请问如上DNN神经网络共有几层： 5层<br>请问该DNN神经网络用来解决二分类问题，那么最后一层的激活函数是 Sigmoid<br>请问如上所示的DNN神经网络的第一个隐藏层有多少个参数： 2 x 3 + 3 &#x3D; 9 （前一层输入量 乘以 后一层的神经元数量 + 偏执项。）</p><h2 id="问题4，AlexNet论文使用的loss函数是什么？"><a href="#问题4，AlexNet论文使用的loss函数是什么？" class="headerlink" title="问题4，AlexNet论文使用的loss函数是什么？"></a>问题4，AlexNet论文使用的loss函数是什么？</h2><p>CrossEntropy交叉损失函数： </p><p><a href="https://blog.csdn.net/qq_44629163/article/details/124348366">参考博客</a></p><h2 id="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"><a href="#问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？" class="headerlink" title="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"></a>问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？</h2><p>AlexNet论文中使用的梯度优化算法是随机梯度下降（Stochastic Gradient Descent，SGD）。在训练过程中，SGD通过计算损失函数关于网络参数的梯度，并根据该梯度更新参数，以使损失函数最小化。<br><a href="https://blog.csdn.net/qq_58146842/article/details/121280968">SGD参考博客</a></p><h2 id="问题6，AlexNet论文中使用的评价指标是什么？"><a href="#问题6，AlexNet论文中使用的评价指标是什么？" class="headerlink" title="问题6，AlexNet论文中使用的评价指标是什么？"></a>问题6，AlexNet论文中使用的评价指标是什么？</h2><p>错误率 ： ACC 的相反数，计算方法为1-ACC</p><h2 id="问题7，AlexNet中的创新点是什么？"><a href="#问题7，AlexNet中的创新点是什么？" class="headerlink" title="问题7，AlexNet中的创新点是什么？"></a>问题7，AlexNet中的创新点是什么？</h2><ol><li>ReLU激活函数的引入，采样非线性单元（ReLU）的深度卷积神经网络训练时间要比tanh单元要快几倍。而时间开销是进行模型训练过程中的很重要的因数。同时ReLU有效的防止了过拟合的现象。</li><li>层叠池化操作，以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize&#x3D;2×2，stride&#x3D;2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize &gt; stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。</li><li>Dropout操作， Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。</li><li>网络层数更深，与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。当然，这种思考的重要节点出现在VGG网络（下一篇博文VGG论文中将会讲到）。</li></ol><h2 id="问题8，优化函数的具体实现是什么？"><a href="#问题8，优化函数的具体实现是什么？" class="headerlink" title="问题8，优化函数的具体实现是什么？"></a>问题8，优化函数的具体实现是什么？</h2><p>请参考博客：<br><a href="https://www.bilibili.com/video/BV1Fu4y1N7Qu/?spm_id_from=333.337.search-card.all.click&vd_source=c2d8f28374ac78ec2f99b6321e56032a">以逻辑回归为例讲的梯度下降算法矩阵化</a><br><a href="https://www.bilibili.com/video/BV1eK42147wr/?p=29&vd_source=c2d8f28374ac78ec2f99b6321e56032a">结合看</a><br><a href="https://www.bilibili.com/video/BV1JK411k7ah/?spm_id_from=333.337.search-card.all.click&vd_source=c2d8f28374ac78ec2f99b6321e56032a">代码实现</a></p><h2 id="问题10，什么是过拟合合和欠拟合？"><a href="#问题10，什么是过拟合合和欠拟合？" class="headerlink" title="问题10，什么是过拟合合和欠拟合？"></a>问题10，什么是过拟合合和欠拟合？</h2><p>过拟合：过拟合指的是模型在训练数据上表现良好，但在未见过的测试数据上表现较差的情况。过拟合通常发生在模型过于复杂或训练数据过少的情况下。当模型过度学习了训练数据中的噪声或特定的样本特征时，会导致过拟合问题。在过拟合的情况下，模型可能会过度拟合训练数据，导致泛化能力较差，无法很好地适应新的、未见过的数据。<br>欠拟合：欠拟合指的是模型在训练数据上表现不佳，无法捕捉数据中的足够的信息和结构，导致模型过于简单或不够复杂。欠拟合通常发生在模型复杂度过低或训练数据量不足的情况下。在欠拟合的情况下，模型可能无法捕捉数据中的关键特征或模式，导致训练误差和测试误差都较高。<br><a href="https://zhuanlan.zhihu.com/p/72038532">参考博客</a></p><h2 id="问题9，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"><a href="#问题9，论文中怎么证明，层叠池化可以有效防止过拟合的发生？" class="headerlink" title="问题9，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"></a>问题9，论文中怎么证明，层叠池化可以有效防止过拟合的发生？</h2><p>验对比：作者可能会设计实验，对比使用层叠池化和不使用层叠池化的模型在同一数据集上的性能表现。通过比较两种模型的训练误差和测试误差，可以观察到使用层叠池化的模型是否在测试数据上表现更好，从而证明其能够有效防止过拟合。<br>交叉验证：作者可能会使用交叉验证来评估模型的泛化能力。通过在不同的训练集和测试集上多次进行实验，可以更客观地评估模型的性能，并观察使用层叠池化的模型是否具有更好的泛化能力。<br>可视化分析：作者可能会对模型的训练过程进行可视化分析，比如绘制训练损失曲线和验证损失曲线。通过观察损失曲线的变化趋势，可以了解模型是否存在过拟合问题，并观察是否使用层叠池化的模型更加稳定。</p><h2 id="问题10，-softMax的机制是怎么样的？"><a href="#问题10，-softMax的机制是怎么样的？" class="headerlink" title="问题10， softMax的机制是怎么样的？"></a>问题10， softMax的机制是怎么样的？</h2><p>Softmax函数是一种常用的激活函数，通常用于多分类问题中的输出层。Softmax函数可以将一个具有任意实数值的向量转换成一个概率分布，使得各个元素的值都在 (0, 1) 范围内，并且所有元素的和为1。</p><p><a href="https://zhuanlan.zhihu.com/p/105722023">参考博客</a></p><h2 id="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？"><a href="#问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？" class="headerlink" title="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？"></a>问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？</h2><p>Dropout是一种常用的正则化技术，用于减少神经网络的过拟合现象。其运行机制如下：</p><p>训练阶段：在每次训练迭代时，以概率 p 将神经网络中的某些神经元（或者称为节点）临时从网络中删除（置为零）。这样，在每次迭代中，都会随机删除一部分神经元，从而导致每次迭代得到的网络结构都不同。<br>预测阶段：在预测阶段，不再使用Dropout，而是使用所有的神经元，但需要对每个神经元的输出值乘以 p，以保持期望的输出值不变。</p><h2 id="问题16，-什么是超参数？"><a href="#问题16，-什么是超参数？" class="headerlink" title="问题16， 什么是超参数？"></a>问题16， 什么是超参数？</h2><p>超参数（Hyperparameters）是机器学习模型训练过程中的配置参数，其值不能通过训练过程自动学习，而是需要人工设置。与模型的参数（例如权重和偏置）不同，超参数通常用于控制模型的结构、学习过程的行为和性能调优。</p><p>一些常见的超参数包括：<br>学习率（Learning Rate）：用于控制每次参数更新的步长。<br>迭代次数（Number of Iterations&#x2F;Epochs）：训练模型时数据集遍历的次数。<br>批量大小（Batch Size）：每次迭代中用于更新参数的样本数量。<br>网络结构参数：例如隐藏层的数量、每个隐藏层的神经元数量、卷积核大小等。<br>正则化参数：用于控制模型的复杂度，例如L1和L2正则化的权重。<br>优化算法参数：例如动量（momentum）、adam的参数等。<br>损失函数参数：例如softmax交叉熵的参数、权重类别平衡等。</p><h2 id="问题17，-什么是监督学习和无监督学习，半监督学习？"><a href="#问题17，-什么是监督学习和无监督学习，半监督学习？" class="headerlink" title="问题17， 什么是监督学习和无监督学习，半监督学习？"></a>问题17， 什么是监督学习和无监督学习，半监督学习？</h2><p>监督学习（Supervised Learning）是一种机器学习任务，其目标是从有标签的数据中学习出一个输入到输出的映射关系，即从输入数据预测出相应的输出标签。在监督学习中，训练数据包括了输入和对应的输出标签，模型通过学习输入和输出之间的关系来进行预测。典型的监督学习任务包括分类和回归。</p><p>无监督学习（Unsupervised Learning）是一种机器学习任务，其目标是从没有标签的数据中学习出数据的内在结构或者特征表示，而无需事先给定标签信息。在无监督学习中，训练数据只包括输入数据，没有对应的输出标签。无监督学习可以用于聚类、降维、异常检测等任务。</p><p>半监督学习（Semi-Supervised Learning）是介于监督学习和无监督学习之间的一种学习方式，其目标是利用少量有标签数据和大量无标签数据来训练模型。在半监督学习中，训练数据同时包括有标签的数据和无标签的数据。半监督学习可以通过结合监督学习和无监督学习的方法，利用无标签数据的信息来提升模型性能，尤其在标注数据有限或者成本较高的情况下具有重要意义。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑-PyTorch基础——Numpy</title>
    <link href="/2024/04/23/tiankeng1/"/>
    <url>/2024/04/23/tiankeng1/</url>
    
    <content type="html"><![CDATA[<h3 id="向量和数组之间的关系是什么？向量的定义是什么？"><a href="#向量和数组之间的关系是什么？向量的定义是什么？" class="headerlink" title="向量和数组之间的关系是什么？向量的定义是什么？"></a>向量和数组之间的关系是什么？向量的定义是什么？</h3><p>在数学科物理中，向量被定义为具有大小和方向量。例如速度是一个向量，因为它不仅有大小（数独），还有方向（行进的方向）。<br>数组是编程中的一种基本数据结构，用于存储一组有序的元素。这些元素可以是任何类型，如整形、浮点数、字符串等。<br>标量（scalar）是零维只有大小，没有方向的量，如1，2，3<br>向量（Vector）是一维只有大小和方向的量，如（1，2）。（计算方向的公式为：）<br>矩阵（Matrix）是二维的向量，[[1, 2], [2, 3]]<br>张量（Tensor） 按照任意维排列的一堆数字的推广。矩阵不过是三维张量下的一个二维切面。要在三维张量下找到零维张量需要三个维度的坐标来定位。（注：张量可以是多维的）</p><h3 id="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"><a href="#矩阵是什么，作用是什么？如何实现矩阵的加减乘除" class="headerlink" title="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"></a>矩阵是什么，作用是什么？如何实现矩阵的加减乘除</h3><ol><li>矩阵是一个二维数组，由行和列的元素组成。在数学中，矩阵通常用大写字母表示，如 A，B 等，矩阵中的元素通常用小写字母表示，如aij​，表示矩阵 A 的第 i 行第 j 列的元素。</li><li>矩阵可以用来表示线性变换，解决线性方程组，或者表示图形的变换。在数据科学和机器学习中，矩阵通常用于存储和操作大量的数据。</li></ol><h4 id="实现矩阵的加减乘除。"><a href="#实现矩阵的加减乘除。" class="headerlink" title="实现矩阵的加减乘除。"></a>实现矩阵的加减乘除。</h4><p>加法：两个矩阵相加，只有在它们的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相加的结果。例如，如果A &#x3D; aij 和B &#x3D; bij 是同样大小的矩阵，那么它们的和C &#x3D; [ cij ]是矩阵 ,其中cij &#x3D; aij + bij。对应相加<br>减法：矩阵的减法与加法类似，只有在两个矩阵的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相减的结果。<br>乘法：矩阵的乘法比较复杂。如果A 是一个 m×n 的矩阵，B 是一个n×p 的矩阵，那么它们的乘积 AB 是一个 m×p 的矩阵，其元素由A 的行和 B 的列的对应元素的乘积之和给出。<br>除法：在矩阵中，通常不直接定义除法。但是，我们可以通过乘以逆矩阵来实现类似的效果。如果A是一个可逆的（也就是说，存在一个矩阵 （A-1）使得，A（A-1） &#x3D; （A-1）A &#x3D; I其中 𝐼I 是单位矩阵），那么我们可以定义B&#x2F;A为（BA-1），即是B矩阵除以A矩阵等于B乘以A矩阵的转置。但是，请注意，不是所有的矩阵都是可逆的。 </p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs makefile">import numpy as np<br><br><span class="hljs-comment"># 创建两个矩阵</span><br>A = np.array([[1, 2], [3, 4]])<br>B = np.array([[5, 6], [7, 8]])<br><br><span class="hljs-comment"># 矩阵加法</span><br>C = A + B<br><br><span class="hljs-comment"># 矩阵减法</span><br>D = A - B<br><br><span class="hljs-comment"># 矩阵乘法</span><br>E = np.dot(A, B)<br><br><span class="hljs-comment"># 矩阵除法（通过乘以逆矩阵）</span><br>F = np.dot(A, np.linalg.inv(B)) <br><br></code></pre></td></tr></table></figure><h3 id="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"><a href="#傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）" class="headerlink" title="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"></a>傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）</h3><h4 id="基本介绍。"><a href="#基本介绍。" class="headerlink" title="基本介绍。"></a>基本介绍。</h4><p>傅里叶变换是一种在数学、物理和工程中广泛使用的数学变换，它可以将一个函数或信号从其原始的时间或空间表示转换为频率表示。这对于许多应用都非常有用，因为它可以揭示信号的频率成分，这在原始的时间或空间表示中可能不明显。<br>傅里叶变换的基本思想是，任何函数都可以表示为一系列正弦波和余弦波的叠加。换句话说，我们可以将一个复杂的信号分解为一系列更简单的正弦波和余弦波。</p><h4 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h4><p>傅里叶变换的基本原理是将一个函数或信号从其原始的时间或空间表示转换为频率表示。这是通过将函数表示为一系列正弦波和余弦波的叠加来实现的。<br><img src="/pic/fly1.jpg" alt="傅里叶变换示意图"></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># 创建一个简单的信号</span><br><span class="hljs-attribute">t</span> = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">500</span>)<br><span class="hljs-attribute">f</span> = np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">50</span> * t) + <span class="hljs-number">0</span>.<span class="hljs-number">5</span> * np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">120</span> * t)<br><br><span class="hljs-comment"># 绘制原始信号</span><br><span class="hljs-attribute">plt</span>.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-attribute">plt</span>.plot(t, f)<br><span class="hljs-attribute">plt</span>.title(&#x27;Original Signal&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Time&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Amplitude&#x27;)<br><br><span class="hljs-comment"># 计算傅里叶变换</span><br><span class="hljs-attribute">F</span> = np.fft.fft(f)<br><br><span class="hljs-comment"># 计算频率</span><br><span class="hljs-attribute">freq</span> = np.fft.fftfreq(t.shape[-<span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 绘制频谱</span><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-attribute">plt</span>.plot(freq, np.abs(F))<br><span class="hljs-attribute">plt</span>.title(&#x27;Frequency Spectrum&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Frequency&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Magnitude&#x27;)<br><br><span class="hljs-attribute">plt</span>.tight_layout()<br><span class="hljs-attribute">plt</span>.show()<br><br></code></pre></td></tr></table></figure><h3 id="什么是对象？-封装，继承，多态是什么？"><a href="#什么是对象？-封装，继承，多态是什么？" class="headerlink" title="什么是对象？ 封装，继承，多态是什么？"></a>什么是对象？ 封装，继承，多态是什么？</h3><p>什么是对象？<br>在面向对象编程（Object-Oriented Programming，OOP）中，对象是类的实例。类是一种抽象的概念，用于描述具有相似属性和行为的对象的集合。对象是类的具体实现，它具有类定义的属性和方法。<br>对象可以看作是现实世界中的实体或概念在程序中的表示。每个对象都有自己的状态（属性）和行为（方法），并且可以与其他对象进行交互。</p><p>封装<br>封装是面向对象编程的一种重要概念，它将数据和操作数据的方法捆绑在一起，形成一个称为类的单个实体。封装隐藏了数据的内部实现细节，只暴露对外部可见的接口。这样可以保护数据的完整性，并提供更好的代码组织和维护性。<br>通过封装，对象的内部状态可以被保护起来，只能通过公共接口进行访问和修改。这样可以防止对数据的不合理访问和修改，增加了代码的安全性和可靠性。</p><p>继承<br>继承是面向对象编程中的另一个重要概念，它允许一个类继承另一个类的属性和方法。继承创建了一个类的层次结构，其中一个类（称为子类或派生类）可以从另一个类（称为父类或基类）继承属性和方法。<br>通过继承，子类可以继承父类的特性，并且可以添加自己的特定特性。这样可以实现代码的重用和扩展，减少了重复编写代码的工作量。</p><p>多态<br>多态是面向对象编程中的另一个重要概念，它允许使用统一的接口来处理不同的对象类型。多态性允许同一个方法在不同的对象上产生不同的行为。<br>通过多态，可以编写通用的代码，可以处理多个不同类型的对象，而无需针对每种类型编写特定的代码。这提高了代码的灵活性和可扩展性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 封装示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model</span>):<br>        self.brand = brand<br>        self.model = model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 Car 对象并访问其信息</span><br>my_car = Car(<span class="hljs-string">&quot;Toyota&quot;</span>, <span class="hljs-string">&quot;Corolla&quot;</span>)<br>my_car.display_info()<br><br><span class="hljs-comment"># 继承示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ElectricCar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model, battery_capacity</span>):<br>        <span class="hljs-built_in">super</span>().__init__(brand, model)<br>        self.battery_capacity = battery_capacity<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Electric Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>, Battery Capacity: <span class="hljs-subst">&#123;self.battery_capacity&#125;</span> kWh&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 ElectricCar 对象并访问其信息</span><br>my_electric_car = ElectricCar(<span class="hljs-string">&quot;Tesla&quot;</span>, <span class="hljs-string">&quot;Model S&quot;</span>, <span class="hljs-number">100</span>)<br>my_electric_car.display_info()<br><br><span class="hljs-comment"># 多态示例</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_car_info</span>(<span class="hljs-params">car</span>):<br>    car.display_info()<br><br><span class="hljs-comment"># 使用 show_car_info 函数展示不同类型的车辆信息</span><br>show_car_info(my_car)<br>show_car_info(my_electric_car)<br><br></code></pre></td></tr></table></figure><h3 id="python中的不同代码高亮表示什么？"><a href="#python中的不同代码高亮表示什么？" class="headerlink" title="python中的不同代码高亮表示什么？"></a>python中的不同代码高亮表示什么？</h3><p>在Python的IDLE编程环境中，不同颜色的文本表示不同的含义。以下是IDLE中常见的颜色及其含义：<br>黑色：普通的代码文本。<br>蓝色：关键字，例如if、else、for、while等。<br>绿色：字符串文本。<br>红色：语法错误或代码中的错误。<br>紫色：函数和方法的名称。<br>棕色：数字。<br>橙色：内置函数和模块的名称。<br>灰色：注释。</p><h3 id="github上传远端仓库"><a href="#github上传远端仓库" class="headerlink" title="github上传远端仓库"></a>github上传远端仓库</h3>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——VGG</title>
    <link href="/2024/04/22/deeplearnpaper2/"/>
    <url>/2024/04/22/deeplearnpaper2/</url>
    
    <content type="html"><![CDATA[<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>2014年<br><a href="https://blog.csdn.net/C_chuxin/article/details/82833070">中英文对照翻译</a><br><a href="https://zhuanlan.zhihu.com/p/460777014">VGG论文解读</a><br><a href="https://arxiv.org/pdf/1409.1556">原文</a><br><a href="https://zhuanlan.zhihu.com/p/107884876">VGG论文解读</a></p><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>VGG是牛津大学的Visual Geometry Group的团队在ILSVRC 2014上的相关工作。在这项工作中，主要研究卷积网络深度对大规模图像识别准确率的影响。其主要的贡献是对使用非常小的卷积滤波器（3 X 3）的体系架构来增加网络深度进行彻底的评估。实验结果表明将网络的深度提升至16-19个权重层可以实现对现有技术的显著改进。其在2014年的 imageNet 大规模视觉挑战赛（ILSVRC - 2014）中取得亚军。（冠军是 GoogleNet，预告下一篇是GoogleNet）</p><h2 id="VGG原理"><a href="#VGG原理" class="headerlink" title="VGG原理"></a>VGG原理</h2><p>VGG原理<br>相比于 LeNet 网络，VGG 网络的一个改进点是将 大尺寸的卷积核 用 多个小尺寸的卷积核 代替。</p><p>比如：VGG使用 2个3X3的卷积核 来代替 5X5的卷积核，3个3X3的卷积核 代替7X7的卷积核。</p><p>这样做的好处是：</p><ol><li>在保证相同感受野的情况下，多个小卷积层堆积可以提升网络深度，增加特征提取能力（非线性层增加）。</li><li>参数更少。比如 1个大小为5的感受野 等价于 2个步长为1，3X3大小的卷积核堆叠。（即1个5X5的卷积核等于2个3X3的卷积核）。而1个5X5卷积核的参数量为 5<em>5</em>C^2。而2个3X3卷积核的参数量为 2<em>3</em>3*C^2。很显然，18C^2 &lt; 25C^2。</li><li>3X3卷积核更有利于保持图像性质。</li></ol><p>VGG缺点：</p><p>VGG耗费更多计算资源，并且使用了更多的参数（这里不是3x3卷积的锅），导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。<br>注：这里参数量的计算，忽略了偏置。并且假设 输入和输出通道数都为C。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>VGGNet以下6种不同结构，我们以通常所说的VGG-16(即下图D列)为例，展示其结构示意图<br><img src="/pic/VGG.png" alt="VGG_6种模型结构"><br><img src="/pic/VGG_16.png" alt="VGG_16模型结构图"><br><img src="/pic/VGG_16_chanshu.png" alt="VGG参数图"><br><img src="/pic/VGG_chanshu.png" alt="VGG16参数图"></p><h2 id="摘要-Abstract"><a href="#摘要-Abstract" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><p>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Ourmain contribution is a thorough evaluation of networks of increasing depth usingan architecture withvery small (3×3) convolution filters, which shows that a significant improvementon the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. Wehave made our two best-performing ConvNet models publicly available to facili-tate further research on the use of deep visual representations in computer vision.</p><p>在这项工作中，我们研究了卷积网络深度对其在大规模图像识别设置中的准确性的影响。我们的主要贡献是使用一个非常小的(3×3)卷积filter的架构对增加深度的网络进行了彻底的评估，这表明通过将深度提升到16 - 19个weight层，可以显著改善先前的配置。这些发现是我们提交ImageNet挑战赛2014的基础，我们的团队分别获得了本地化和分类的第一名和第二名。我们还展示了我们的成果可以很好地推广到其他数据集，在这些数据集上他们可以得到最优结果。我们已经公开了两个性能最好的卷积神经网络模型，以促进在计算机视觉中使用深度视觉表示的进一步研究。</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>训练和之前的AlexNet整体类似，使用小批量梯度下降，参数方面：batch设为256，动量设为0.9，除最后一层外的全连接层也都使用了丢弃率0.5的dropout。learning rate最初设为0.01,权重衰减系数为5×10^-4。对于权重层采用了随机初始化，初始化为均值0，方差0.01的正态分布。 训练的图像数据方面，为了增加数据集，和AlexNet一样，这里也采用了随机水平翻转和随机RGB色差进行数据扩增。对经过重新缩放的图片随机排序并进行随机剪裁得到固定尺寸大小为224×224的训练图像。</p><h2 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h2><p><img src="/pic/VGG_train_result.webp" alt="对比结果"><br>通过表格间各个网络的对比发现如下结论：</p><p>总体来说卷积网络越深，损失越小，效果越好。<br>C优于B，表明多增加的非线性relu有效<br>D优于C，表明了卷积层filter对于捕捉空间特征有帮助。<br>E深度达到19层后达到了损失的最低点，但是对于其他更大型的数据集来说，可能更深的模型效果更好。<br>B和同类型filter size为5×5的网络进行了对比，发现其top-1错误率比B高7%，表明小尺寸filter效果更好。<br>在训练中，采用浮动尺度效果更好，因为这有助于学习分类目标在不同尺寸下的特征。</p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="使用VGG来实现垃圾的40分类"><a href="#使用VGG来实现垃圾的40分类" class="headerlink" title="使用VGG来实现垃圾的40分类"></a>使用VGG来实现垃圾的40分类</h3><ol><li>第一步准备训练集，固定数据集（当然也可以不固定数据集，但是在对比实验中一定要固定数据集划分）<br>utils.py文件,这个文件的作业是产生2个csv文件，固定训练集和测试集<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs lua">import <span class="hljs-built_in">os</span><br>import csv<br>import numpy as np<br>train_path = <span class="hljs-string">&quot;train_data.csv&quot;</span><br>val_path = <span class="hljs-string">&quot;val_data.csv&quot;</span><br><br>train_percent = <span class="hljs-number">0.9</span><br><br>def create_data_txt(<span class="hljs-built_in">path</span>):<br>    f_train = <span class="hljs-built_in">open</span>(train_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&quot;&quot;</span>)<br>    f_val = <span class="hljs-built_in">open</span>(val_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&quot;&quot;</span>)<br>    train_writer = csv.writer(f_train)<br>    val_writer = csv.writer(f_val)<br><br>    <span class="hljs-keyword">for</span> cls,dirname <span class="hljs-keyword">in</span> enumerate(<span class="hljs-built_in">os</span>.listdir(<span class="hljs-built_in">path</span>)):<br>        flist = <span class="hljs-built_in">os</span>.listdir(<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>,dirname))<br>        np.<span class="hljs-built_in">random</span>.shuffle(flist)<br>        fnum = <span class="hljs-built_in">len</span>(flist)<br>        <span class="hljs-keyword">for</span> i,filename <span class="hljs-keyword">in</span> enumerate(flist):<br>            <span class="hljs-keyword">if</span> i &lt; fnum*train_percent:<br>                train_writer.writerow([<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>,dirname,filename),str(cls)])<br>            <span class="hljs-keyword">else</span>:<br>                val_writer.writerow([<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>, dirname, filename), str(cls)])<br><br>    f_train.<span class="hljs-built_in">close</span>()<br>    f_val.<span class="hljs-built_in">close</span>()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    create_data_txt(<span class="hljs-string">&quot;data_garbage&quot;</span>)<br><br></code></pre></td></tr></table></figure></li></ol><p>dataset.py 文件根据utisl.py文件来对数据进行数据预处理操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms,utils<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset,DataLoader<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>train_tf = transforms.Compose([<br>    <span class="hljs-comment"># transforms.RandomResizedCrop(size=(224,224), scale=(0.9,1.1)),</span><br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    transforms.RandomRotation(<span class="hljs-number">10</span>),<br>    transforms.ColorJitter(brightness=(<span class="hljs-number">0.9</span>,<span class="hljs-number">1.1</span>),contrast=(<span class="hljs-number">0.9</span>,<span class="hljs-number">1.1</span>)),<br>    <span class="hljs-comment"># transforms.Resize((50,50)),</span><br>    transforms.ToTensor(),<br>])<br><br>val_tf = transforms.Compose([<br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>    <span class="hljs-comment"># transforms.Grayscale(1),</span><br>    transforms.ToTensor(),<br>])<br><br><span class="hljs-comment">#自定义数据集</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Animals_dataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,istrain=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-keyword">if</span> istrain:<br>            f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;train_data.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;val_data.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>)<br>        self.dataset = f.readlines()<br>        f.close()<br>        self.istrain = istrain<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.dataset)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        data = self.dataset[index]<br>        img_path = data.split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">0</span>]<br>        cls = <span class="hljs-built_in">int</span>(data.split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">1</span>])<br><br>        img_data = Image.<span class="hljs-built_in">open</span>(img_path).convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>        <span class="hljs-keyword">if</span> self.istrain:<br>            dst = train_tf(img_data)<br>        <span class="hljs-keyword">else</span>:<br>            dst =val_tf(img_data)<br><br>        <span class="hljs-keyword">return</span> dst,torch.tensor(cls)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">visulization</span>():<br>    train_dataset = Animals_dataset(<span class="hljs-literal">True</span>)<br>    train_dataloader = DataLoader(train_dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>    examples = <span class="hljs-built_in">enumerate</span>(train_dataloader)<br>    batch_index,(data, lable) = <span class="hljs-built_in">next</span>(examples)<br>    <span class="hljs-built_in">print</span>(data.shape)<br><br>    grid = utils.make_grid(data)<br>    plt.imshow(grid.numpy().transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>    plt.show()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    visulization()<br></code></pre></td></tr></table></figure><p>train.py 训练模型的代码</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim,nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> dataset <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>m = nn.Softmax(dim=<span class="hljs-number">1</span>)<br>def train(<span class="hljs-keyword">method</span>=&quot;normal&quot;,ckpt_path=&quot;&quot;):<br>    # 数据集和数据加载器<br>    train_dataset = Animals_dataset(<span class="hljs-keyword">True</span>)<br>    train_dataloader = DataLoader(train_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-keyword">True</span>)<br>    val_dataset = Animals_dataset(<span class="hljs-keyword">False</span>)<br>    val_dataloader = DataLoader(val_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-keyword">False</span>)<br><br>    #模型<br>    device = torch.device(&quot;cuda&quot; <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> &quot;cpu&quot;)#系统自己决定有啥训练<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">method</span>==&quot;normal&quot;:<br>        model = models.vgg16(num_classes=<span class="hljs-number">40</span>,dropout=<span class="hljs-number">0.45</span>).<span class="hljs-keyword">to</span>(device)<br>    elif <span class="hljs-keyword">method</span>==&quot;step1&quot;:<br>        model=models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> model.parameters():<br>            i.requires_grad=<span class="hljs-keyword">False</span><br>        model.classifier=nn.Sequential(<br>            nn.Linear(<span class="hljs-number">512</span>*<span class="hljs-number">7</span>*<span class="hljs-number">7</span>,<span class="hljs-number">2048</span>),<br>            nn.ReLU(<span class="hljs-keyword">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.35</span>),<br>            nn.Linear(<span class="hljs-number">2048</span>,<span class="hljs-number">1024</span>),<br>            nn.ReLU(<span class="hljs-keyword">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.35</span>),<br>            nn.Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">40</span>)<br>        )<br>        model.<span class="hljs-keyword">to</span>(device)<br>    elif <span class="hljs-keyword">method</span>==&quot;step2&quot;:<br>        model=models.vgg16()<br>        model.classifier=nn.Sequential(<br>            nn.Linear(<span class="hljs-number">512</span> * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">2048</span>),<br>            nn.ReLU(<span class="hljs-keyword">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.35</span>),<br>            nn.Linear(<span class="hljs-number">2048</span>, <span class="hljs-number">1024</span>),<br>            nn.ReLU(<span class="hljs-keyword">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.35</span>),<br>            nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">40</span>)<br>        )<br>        model.load_state_dict(torch.<span class="hljs-keyword">load</span>(&quot;model/vgg16_step1_trush.pth&quot;))<br>        model.<span class="hljs-keyword">to</span>(device)<br>    print(&quot;train on &quot;,device)<br>    #损失函数（二分类交叉熵）<br>    loss_fn = nn.CrossEntropyLoss()<br><br>    #优化器<br>    optimizer = optim.SGD(model.parameters(),lr=<span class="hljs-number">0.01</span>,momentum=<span class="hljs-number">0.9</span>)<br><br>    #断点恢复<br>    start_epoch = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">if</span> ckpt_path != &quot;&quot;:<br>        <span class="hljs-keyword">checkpoint</span> = torch.<span class="hljs-keyword">load</span>(ckpt_path)<br>        model.load_state_dict(<span class="hljs-keyword">checkpoint</span>[&quot;net&quot;])<br>        optimizer.load_state_dict(<span class="hljs-keyword">checkpoint</span>[&quot;optimizer&quot;])<br>        start_epoch = <span class="hljs-keyword">checkpoint</span>[&quot;epoch&quot;] + <span class="hljs-number">1</span><br><br>    #训练<br>    train_loss_arr = []<br>    train_acc_arr = []<br>    val_loss_arr = []<br>    val_acc_arr = []<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):<br>        train_loss_total = <span class="hljs-number">0</span> #所有batch的loss累加值<br>        train_acc_total = <span class="hljs-number">0</span> #所有batch的acc累加值<br>        val_loss_total = <span class="hljs-number">0</span><br>        val_acc_total = <span class="hljs-number">0</span><br><br>        model.train()#标志此时为训练状态，启用dropout随机失活，否则不启用<br>        <span class="hljs-keyword">for</span> i,(train_x,train_y) <span class="hljs-keyword">in</span> enumerate(train_dataloader):<br>            train_x = train_x.<span class="hljs-keyword">to</span>(device)<br>            train_y = train_y.<span class="hljs-keyword">to</span>(device)<br><br>            #前向传播<br>            train_y_pred = model(train_x)<br>            train_loss = loss_fn(train_y_pred,train_y)<br>            train_acc = (m(train_y_pred).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]==train_y).sum()/train_y.shape[<span class="hljs-number">0</span>]<br>            train_loss_total += train_loss.data.item()<br>            train_acc_total += train_acc.data.item()<br>            #反向传播<br>            train_loss.backward()<br>             #梯度下降<br>            optimizer.step()<br>            optimizer.zero_grad()<br><br>            print(&quot;epoch:&#123;&#125; train_loss:&#123;&#125; train_acc:&#123;&#125;&quot;.format(epoch, train_loss.data.item(), train_acc.data.item()))<br><br>        train_loss_arr.append(train_loss_total / len(train_dataloader)) #平均值<br>        train_acc_arr.append(train_acc_total / len(train_dataloader))<br><br>        #测试集<br>        <span class="hljs-keyword">for</span> j, (val_x, val_y) <span class="hljs-keyword">in</span> enumerate(val_dataloader):<br>            val_x = val_x.<span class="hljs-keyword">to</span>(device)<br>            val_y = val_y.<span class="hljs-keyword">to</span>(device)<br>            #前向传播<br>            val_y_pred,_,_ = model(val_x)<br>            val_loss = loss_fn(val_y_pred,val_y)<br>            val_acc = (m(val_y_pred).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]==val_y).sum()/val_y.shape[<span class="hljs-number">0</span>]<br>            val_loss_total += val_loss.data.item()<br>            val_acc_total += val_acc.data.item()<br><br>        val_loss_arr.append(val_loss_total / len(val_dataloader))  # 平均值<br>        val_acc_arr.append(val_acc_total / len(val_dataloader))<br>        print(&quot;epoch:&#123;&#125; val_loss:&#123;&#125; val_acc:&#123;&#125;&quot;.format(epoch, val_loss_arr[<span class="hljs-number">-1</span>], val_acc_arr[<span class="hljs-number">-1</span>]))<br>        #保存模型（断点连续）<br>        <span class="hljs-keyword">checkpoint</span>=&#123;<br>            &quot;net&quot;:model.state_dict(),<br>            &quot;optimizer&quot;:optimizer.state_dict(),<br>            &quot;epoch&quot;:epoch<br>        &#125;<br>        torch.save(<span class="hljs-keyword">checkpoint</span>,&quot;checkpoint/ckpt.pth&quot;)<br><br><br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>) #画布一分为二,<span class="hljs-number">1</span>行<span class="hljs-number">2</span>列，用第一个<br>    plt.title(&quot;loss&quot;)<br>    plt.plot(train_loss_arr,&quot;r&quot;,label = &quot;train&quot;)<br>    plt.plot(val_loss_arr,&quot;b&quot;,label = &quot;val&quot;)<br>    plt.legend()<br><br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  # 画布一分为二,<span class="hljs-number">1</span>行<span class="hljs-number">2</span>列，用第一个<br>    plt.title(&quot;acc&quot;)<br>    plt.plot(train_acc_arr, &quot;r&quot;, label=&quot;train&quot;)<br>    plt.plot(val_acc_arr, &quot;b&quot;, label=&quot;val&quot;)<br>    plt.legend()<br>    plt.savefig(&quot;loss/loss_acc_vgg.png&quot;)<br><br>    plt.<span class="hljs-keyword">show</span>()<br><br>    #保存模型<br>    #<span class="hljs-number">1.</span>torch.save()<br>    #<span class="hljs-number">2.</span>文件的后缀名：.pt、.pth、.pkl<br>    torch.save(model.state_dict(),&quot;model/vgg_trush.pth&quot;)<br>    print(&quot;保存模型成功!&quot;)<br><br><br><span class="hljs-keyword">if</span> __name__ == &quot;__main__&quot;:<br>    train()<br><br><br></code></pre></td></tr></table></figure><p>test.py测试模型的代码</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch.cuda<br><br><span class="hljs-keyword">from</span> torchvision import models<br>import os<br><span class="hljs-keyword">from</span> torch import nn<br><span class="hljs-keyword">from</span> dataset import *<br><span class="hljs-keyword">from</span> PIL import Image<br><span class="hljs-keyword">from</span> torch.utils.data import DataLoader<br><span class="hljs-keyword">from</span> dataset import *<br><span class="hljs-keyword">from</span> sklearn.metrics import recall_score, f1_score, precision_score, confusion_matrix<br><span class="hljs-keyword">from</span> matplotlib import rcParams<br>rcParams[<span class="hljs-string">&#x27;font.family&#x27;</span>] = <span class="hljs-string">&#x27;SimHei&#x27;</span><br><br>m = nn.Softmax(<span class="hljs-attribute">dim</span>=1)<br>labels = os.listdir(<span class="hljs-string">&quot;data_garbage&quot;</span>)<br><br>def evaluate():<br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    model = models.googlenet(<span class="hljs-attribute">num_classes</span>=40).to(device)<br>    model.load_state_dict(torch.load(<span class="hljs-string">&quot;model/vgg_trush.pth&quot;</span>))<br>    model.eval()<br><br>    img = Image.open(<span class="hljs-string">&quot;tests/5.jpg&quot;</span>)<br>    dst = val_tf(img).<span class="hljs-keyword">to</span>(device)<br>    dst = torch.unsqueeze(dst, <span class="hljs-attribute">dim</span>=0)   # (1, 3, 224, 224)<br>    y_hat = model(dst)<br><br>    values = m(y_hat).sort(<span class="hljs-attribute">dim</span>=1, <span class="hljs-attribute">descending</span>=<span class="hljs-literal">True</span>)[0][0]<br>    index = m(y_hat).sort(<span class="hljs-attribute">dim</span>=1, <span class="hljs-attribute">descending</span>=<span class="hljs-literal">True</span>)[1][0]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(5):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;:&#125; - &#123;:.5f&#125;&quot;</span>.format(labels[index[i]], values[i]))<br><br>    plt.imshow(img)<br>    plt.show()<br><br>def val():<br>    # 数据集和数据加载器<br>    val_dataset = Animals_dataset(<span class="hljs-literal">False</span>)<br>    val_data_loader = DataLoader(val_dataset, <span class="hljs-attribute">batch_size</span>=128, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">drop_last</span>=<span class="hljs-literal">True</span>)<br><br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    model = models.googlenet(<span class="hljs-attribute">num_classes</span>=40).to(device)<br>    model.load_state_dict(torch.load(<span class="hljs-string">&quot;model/vgg_trush.pth&quot;</span>))<br>    model.eval()<br><br>    val_y_total = []<br>    val_y_pred_total = []<br>    <span class="hljs-keyword">for</span> val_x, val_y <span class="hljs-keyword">in</span> val_data_loader:<br>        val_x = val_x.<span class="hljs-keyword">to</span>(device)<br>        val_y_pred = model(val_x).cpu()<br><br>        val_y_total.extend(val_y.cpu().numpy())    # 将列表中的数据取出来追加<br>        val_y_pred_total.extend(m(val_y_pred).max(<span class="hljs-attribute">dim</span>=1)[1].cpu().numpy())<br><br>    p = precision_score(val_y_total, val_y_pred_total, <span class="hljs-attribute">average</span>=<span class="hljs-string">&quot;weighted&quot;</span>)<br>    recall = recall_score(val_y_total, val_y_pred_total, <span class="hljs-attribute">average</span>=<span class="hljs-string">&quot;weighted&quot;</span>)<br>    f1 = f1_score(val_y_total, val_y_pred_total, <span class="hljs-attribute">average</span>=<span class="hljs-string">&quot;weighted&quot;</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;precision: &#123;:.5f&#125;, recall=&#123;:.5f&#125;, f1=&#123;:.5f&#125;&quot;</span>.format(p, recall, f1))<br><br>    cm = confusion_matrix(val_y_total, val_y_pred_total)<br><br>    plt.imshow(cm, <span class="hljs-attribute">cmap</span>=plt.cm.Blues)<br>    plt.xticks(range(40), <span class="hljs-attribute">labels</span>=labels)<br>    plt.yticks(range(40), <span class="hljs-attribute">labels</span>=labels)<br><br>    plt.colorbar()<br>    plt.xlabel(<span class="hljs-string">&quot;预测值&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;真实值&quot;</span>)<br>    thresh = cm.mean()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(40):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(40):<br>            <span class="hljs-built_in">info</span> = cm[j, i]<br>            plt.text(i, j, info, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;white&quot;</span> <span class="hljs-keyword">if</span> info&gt;thresh <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;black&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;confusion_matrix.jpg&quot;</span>)<br>    plt.show()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    evaluate()<br><br><br></code></pre></td></tr></table></figure><h3 id="什么是感受野？"><a href="#什么是感受野？" class="headerlink" title="什么是感受野？"></a>什么是感受野？</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑博客总目录</title>
    <link href="/2024/04/22/tiankeng/"/>
    <url>/2024/04/22/tiankeng/</url>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="PyTorch基础——Numpy"><a href="#PyTorch基础——Numpy" class="headerlink" title="PyTorch基础——Numpy"></a>PyTorch基础——Numpy</h2><h2 id="经典网络结构——AlexNet"><a href="#经典网络结构——AlexNet" class="headerlink" title="经典网络结构——AlexNet"></a>经典网络结构——AlexNet</h2>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔9</title>
    <link href="/2024/04/22/ganwu9/"/>
    <url>/2024/04/22/ganwu9/</url>
    
    <content type="html"><![CDATA[<h1 id="夜晚思考"><a href="#夜晚思考" class="headerlink" title="夜晚思考"></a>夜晚思考</h1><p>作为一个大学生，住在寝室是很正常的。2024&#x2F;4&#x2F;21的夜晚不是很寻常，床下的键盘声和电脑的光亮让我难以入睡。思绪浮想联翩，世界毁灭了，我要毁灭了。情绪在波动，心脏在疼痛。我该怎么去停止这键盘声和光亮从而让我安静的入眠。<br>人总是以为自己是站在道德的高点，很不幸的告诉我自己，当自以为在道德高点时，我其实已经没有了道德。以自己最大的恶意去揣测他人，已经不道德了。键盘声和光亮真的不能让我入睡吗？还是自己不让自己睡觉，自己的情绪，自己禁锢自己。意识和情绪不是一体的，控制自己的情绪。<br>我总是有两个自己，一个以恶意揣测别人，一个则想怎么去解决这个问题。逃离，争吵，苦恼，毁灭世界。思想斗争吧，预演所有情况吧，一个小时后，问题不能被解决，反而越来越难受。沟通一下吧。起身，正坐。问道：“兄弟，你有什么重要的事情需要去完成吗？” 答曰：“作业没有写完，正在写作业。”听之，甚觉羞愧，焕然冰释。<br>自己禁锢自己，被情绪裹挟，人啊人啊。<br><img src="/pic/ganwu9.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔8</title>
    <link href="/2024/04/21/ganwu8/"/>
    <url>/2024/04/21/ganwu8/</url>
    
    <content type="html"><![CDATA[<h1 id="做事中什么最重要？"><a href="#做事中什么最重要？" class="headerlink" title="做事中什么最重要？"></a>做事中什么最重要？</h1><p>明白自己在干什么，明白此时此刻我在干什么。为什么说这是最重要的?拿做数学题来讲吧，要做一道数学题，必须先看题，了解题目的内容，获取前提条件。获取了前提条件之后，有两种可能，会做和不会做，会做的过程中，最好的感觉是，看了一眼题目之后就胸有成竹了，虽然不知道这道题的答案是什么，但是清楚这道题应该怎么去解答，应该在什么部分注意。这是最好的状态，很清晰的明白自己在做什么的状态。对于第二种，不知道该怎么解决的，首先得清楚自己不会做，然后去分析自己哪里不会，而不是自己不会就不会，这样的心态是大忌，不要傲慢，没有人生下来就会所有事情，都是从不会到会的，对待自己会别人不会的题，不要傲慢，不要鄙视他人，因为我也是从不会到会的。了解自己哪里不会了之后，去找到解决方法，补足不会的点，然后就能解决这道数学题了。这是第二个情况的解决方法，在整个流程中，我都清晰自己在干什么，而不是迷迷糊糊的不知道自己在干什么。<br>再举一个例子，拿科研来讲，一篇论文需要注意什么？第一，创新点。第二，实验设计。第三，文章表述。这三点的重要性不分先后。我想的，我做的，我写出来的是三种东西。在这个过程中需要清晰的认识自己在做什么，如果很清晰的知道，胸有成竹的，就可以去做，如果是第二种情况，那就慢慢来补充自己欠缺的知识。需要说明的是，胸有成竹和存在不足的情况可能会周期交互，一段时间的胸有成竹和一段时间的不足。但是在解决不足的过程却是胸有成竹的，清楚的明白自己需要干什么。</p><h2 id="《胸有成竹》-苏轼"><a href="#《胸有成竹》-苏轼" class="headerlink" title="《胸有成竹》-苏轼"></a>《胸有成竹》-苏轼</h2><p>竹之始生，一寸之萌耳，而节叶具焉；自蜩蝮蛇蚹，以至于剑拔十寻者，生而有之也。<br>今画者乃节节而为之，叶叶而累之，岂复有竹乎？故画竹必先得成竹于胸中，执笔熟视，乃见其所欲画者，急起从之，振笔直遂，以追其所见，如兔起鹘落，少纵则逝矣。与可之教予如此。予不能然也，而心识其所以然。夫既心识其所以然，而不能然者，内外不一，心手不相应，不学之过也。故凡有见于中，而操之不熟者，平居自视了然，而临时忽焉丧之，岂独竹乎？子由为《墨竹赋》以遗与可曰：“庖丁，解牛者也，而养生者取之；轮扁，斫轮者也，而读书者与之。今夫夫子之托于斯竹也，而予以为有道者则非耶？”子由未尝画也，故得其意而已。若予者，岂独得其意，并得其法。</p><p>译文：竹子开始生出时，只是一寸高的萌芽而已，但节、叶都具备了。从蝉破壳而出、蛇长出鳞一样的状态，直至像剑拔出鞘一样长到八丈高，都是一生长出来就有的。如今画竹的人都是一节节地画它，一叶叶地堆积它，这样哪里还会有完整的、活生生的竹子呢？所以画竹必定要心里先有完整的竹子形象，拿起笔来仔细看去，就看到了自己所想画的竹子，急速起身跟住它，动手作画，一气呵成，以追上自己所见到的，如兔子跃起奔跑、隼俯冲下搏，稍一放松就消失了。与可告诉我的是如此。我不能做到这样，但心里明白这样做的道理。既然心里明白这样做的道理，但不能做到这样，是由于内外不一，心与手不相适应，没有学习的过错。所以凡是在心中有了构思，但是做起来不熟练的，平常自己认为很清楚，可事到临头忽然又忘记了，这种现象难道仅仅是画竹有吗？ 　子由写了篇《墨竹赋》，把它送给与可，说：“丁厨子，是杀牛的，但讲求养生的人从他的行动中悟出了道理；轮匠扁，是造车轮的，但读书的人赞成他讲的道理。如今您寄托意蕴在这幅竹画上，我认为您是深知道理的人，难道不是吗？”子由没有作过画，所以只得到了他的意蕴。象我这样的人，哪里仅仅是得到与可的意蕴，并且也得到了与可的方法。</p><p>自注： 我觉得苏轼这篇说说很好，但是需要补充的是，这里以做事的态度讨论，其他角度碍于自己的层次有限暂时不讨论，胸有成竹是最好的做事状态。竹子一开始是具备了节和叶，但是只是一寸高的萌芽的，要从一寸长的萌芽成长到高耸，需要很多条件都满足，竹子的成长也是一点点，一节一节来长得。很多时候做事都不是胸有成竹的状态， 一开始都是懵懂不知道的，只有在做的得心应手时才有胸有成竹的状态，在做事之前已作好充分准备，对事情的成功已有了十分的把握；最开始的竹子都是矮小的，高耸的竹子都是一点点成长的。苏轼说的没错的是故画竹必先得成竹于胸中，执笔熟视，乃见其所欲画者，急起从之，振笔直遂，以追其所见，如兔起鹘落，少纵则逝矣。但是他没有说的是这是大佬做事的境界，小白不都是从今画者乃节节而为之，叶叶而累之吗？<br>故凡有见于中，而操之不熟者，平居自视了然，而临时忽焉丧之，岂独竹乎？这句叫人不要傲慢，知行合一。<br>子由为《墨竹赋》以遗与可曰：“庖丁，解牛者也，而养生者取之；轮扁，斫轮者也，而读书者与之。今夫夫子之托于斯竹也，而予以为有道者则非耶？”子由未尝画也，故得其意而已。若予者，岂独得其意，并得其法。这句法则都是相同的，方法是法则的具体体现。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——AlexNet</title>
    <link href="/2024/04/21/deeplearnpaper/"/>
    <url>/2024/04/21/deeplearnpaper/</url>
    
    <content type="html"><![CDATA[<p>我给自己挖了很多坑没有去填，只能慢慢填了，今天先填第一个坑。<br><a href="https://blog.csdn.net/guzhao9901/article/details/118552085">本人参考博客1-</a><br><a href="https://zhuanlan.zhihu.com/p/618545757">本人参考博客2-</a><br><a href="https://blog.csdn.net/hongbin_xu/article/details/80271291">本人参考博客3-AlexNet的翻译</a><br><a href="https://blog.csdn.net/ARYAD/article/details/107687362">本人参考的博客-模型结构发展简史</a></p><h1 id="AlexNet-介绍"><a href="#AlexNet-介绍" class="headerlink" title="AlexNet 介绍"></a>AlexNet 介绍</h1><p><a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">论文原文链接</a><br>AlexNet是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年ImageNet图像分类竞赛中提出的一种经典的卷积神经网络。AlexNet在 ImageNet 大规模视觉识别竞赛中取得了优异的成绩，把深度学习模型在比赛中的正确率提升到一个前所未有的高度。因此，它的出现对深度学习发展具有里程碑式的意义。<br><a href="https://github.com/aaron-xichen/pytorch-playground">可以参考的github仓库</a></p><ol><li>AlexNet的输入为RGB三通道大小的图像，图像的shape可以表述为（227x227x3）。AlexNet共包含5个卷积层（包含3个池化）和3个全连接层。其中每个卷积层都包含卷积核、偏置项、ReLU激活函数和局部响应归一化（LRN）模块。第1，2，5个卷积层后面都跟着一个最大池化层，后三个层为全连接层。最终的输出层为softmax（这里有一个很有意思的知识，softmax怎么将网络输出转化为概率值，后面再说。）</li></ol><p><img src="/pic/paper_Alex_1.png" alt="AlexNet模型结构图"><br>这里需要指出的是，在网络设计上并非上图所示，上图包含了GPU通信的部分。这是因为当时的GPU内存的限制引起的，作者使用了两块GPU进行计算<br>废话不多说，直接上代码。代码来源为《动手深度学习》</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn, optim<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-meta"># 上面的部分为引入包操作，介绍一下上面引入的包的作用。</span><br><span class="hljs-meta"># time：这是python中的内置的模块，用于处理时间相关的操作。可以用来获取当前的时间，或者在程序中添加延迟。</span><br><span class="hljs-meta"># torch：这是pytorch库的主要部分，一个用于机器学习和深度学习的开源库。提高高效的张量（多维数组）计算（类似于Numpy）的方式，同时支持GPU计算（基于CUDA和CUDNN）</span><br><span class="hljs-meta"># torch.nn 是pytorch中的一个子模块，提供构建神经网络所需要的各种工具和组件。</span><br><span class="hljs-meta"># torch.optim也是pytorch中的一个子模块，提供各种优化算法，比如SGD，Adam和RMSProp等（这里给自己挖个坑）</span><br><span class="hljs-meta"># torch.torchvision，一个与PyTorch关联的库，专门用于处理图像和视频的计算机视觉任务。它提供许多预训练的模型，如ResNet，VGG和AlexNet等，同时还有常见的数据集，如ImageNet，CIFAR10/100，MNIST等。</span><br><br><span class="hljs-title">device</span> = torch.device(&#x27;cuda&#x27; <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> &#x27;cpu&#x27;)<br><span class="hljs-meta"># 这一句的作用是选取GPU训练还是CPU训练。</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">AlexNet</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">AlexNet</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.conv = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding（输出通道数，输出通道数，卷积核大小，步长，填充，这里又有坑，关于卷积后特征图应该怎么计算？）</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(), # <span class="hljs-type">ReLU</span>激活函数</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2), # kernel_size, stride 最大池化，3x3的池化层，步长为2.意思是一个3x3的二维矩阵，按照最大值来输出最大特征。</span><br><span class="hljs-class">            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(96, 256, 5, 1, 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2),</span><br><span class="hljs-class">            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span><br><span class="hljs-class">            # 前两个卷积层后不使用池化层来减小输入的高和宽</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(256, 384, 3, 1, 1), # 第三个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(384, 384, 3, 1, 1), # 第四个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(384, 256, 3, 1, 1), # 第五个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2)</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">         # 这里全连接层的输出个数比<span class="hljs-type">LeNet</span>中的大数倍。使用丢弃层来缓解过拟合</span><br><span class="hljs-class">        self.fc = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Linear</span>(256*5*5, 4096), # 线性层，256*5*5为输入大小，4096为输出大小。</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout</span>(0.5), # 随机失活，<span class="hljs-type">AlexNet</span>的主要创新点之一。这里失活率为0.5</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(4096, 4096),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout</span>(0.5),</span><br><span class="hljs-class">            # 输出层。由于这里使用<span class="hljs-type">Fashion</span>-<span class="hljs-type">MNIST</span>，所以用类别数为10，而非论文中的1000</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(4096, 10), # 输出类为10.</span><br><span class="hljs-class">        )</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-title">img</span>): # 前向传播，forward在代码中需要自定义。在这里可以加入残差等操作。</span><br><span class="hljs-class">        feature = self.conv(<span class="hljs-title">img</span>)</span><br><span class="hljs-class">        output = self.fc(<span class="hljs-title">feature</span>.<span class="hljs-title">view</span>(<span class="hljs-title">img</span>.<span class="hljs-title">shape</span>[0], -1))</span><br><span class="hljs-class">        return output</span><br></code></pre></td></tr></table></figure><ol start="2"><li>背景介绍，在AlexNet网络问世之前，大量的学者在进行图像分类、分割、识别的操作时，主要是通过对图像提取特征或特征+机器学习的方法，手工提取特征是非常难的事情，即特征工程。为了提升准确率或减少人工复杂度等种种原因。因此，学界一直认为，特征是不是可以进行学习？如果可以学习，特征之间的表示方法是什么？例如第一层为线或是点特征，第二层为线与点组成的初步特征，第三层为局部特征）？从这一思想出发，特征可学习且自动组合并给出结果，这是典型的“end-to-end” 。</li></ol><h1 id="论文阅读："><a href="#论文阅读：" class="headerlink" title="论文阅读："></a>论文阅读：</h1><p>先阐述一下论文结构</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">0</span>. 标题（title）<br><span class="hljs-attribute">0</span>.<span class="hljs-number">5</span>. 摘要（Abstract）<br><span class="hljs-attribute">1</span>. 介绍（Introduction）<br><span class="hljs-attribute">2</span>. 数据集（The Dataset）<br><span class="hljs-attribute">3</span>. 网络结构（The Architecture）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">1</span> ReLU非线性单元（ReLU Nonlinearity）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">2</span> 多GPU训练（Training <span class="hljs-literal">on</span> Multiple GPU）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">3</span> 局部响应和归一化（Local Response Normalization）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">4</span> 层叠池化（Overlapping Pooling）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">5</span> 整体结构（Overall Architecture）<br><span class="hljs-attribute">4</span>. 减少过拟合（Reducing Overfitting） <br><span class="hljs-attribute">4</span>.<span class="hljs-number">1</span> 数据增强 （Data Augmentation）<br><span class="hljs-attribute">4</span>.<span class="hljs-number">2</span> 随机失活 （Dropout）<br><span class="hljs-attribute">5</span>. 学习细节 （Details of learning）<br><span class="hljs-attribute">6</span>. 结果 （Results）<br><span class="hljs-attribute">6</span>.<span class="hljs-number">1</span> 定性评估（Qualitative Evacuation）<br><span class="hljs-attribute">6</span>.<span class="hljs-number">2</span> 讨论（Discussion） <br></code></pre></td></tr></table></figure><p>这里需要说明的是由于markdown的限制和本人技术能力的欠缺。在这篇博文中不放公式，如果想看公式，请去看原论文，数学的公式才是最简洁的表达方式，前提是能够看懂，看懂了之后就像打开新世界的大门。感觉就像我有一双滑板鞋，我走到那就穿到哪。<br>0. 标题论文标题为<br>ImageNet Classification with Deep Convolutional Neural Networks<br>摘要： 我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。（开门见山，直接说干了什么。）在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率（这里使用的是错误率的评估指标，和我目前使用的Acc，Presion，Recall，召回率评估指标不一样。），这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成（这里挖一个坑，参数量和神经元数量的评估指标不一样。），其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。（这里挖个坑softMax的机制是怎么样的？）为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法（dropout，随机丢弃的机制是什么？），该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模式的一个变种，取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。</p><ol><li><p>介绍：目前，机器学习方法对物体识别非常重要。为了改善他们的表现（前提条件就是之前的表现不是很好），我们可以收集更大的数据集，训练更强大的模型，并使用更好的技术来防止过拟合。（这里指出减少过拟合的方法有增大数据集，更改模型结构，使用更好的优化技术。）直到最近，标记好图像的数据集相对还较小——大约上万的数量级（例如，NORB，Caltech-101&#x2F;256和CIFAR-10&#x2F;100）。使用这种规模的数据集可以很好地解决简单的识别任务，特别是如果他们增加了保留标签转换（label-preserving transformations）。例如，目前MNIST数字识别任务的最低错误率（&lt;0.3％）基本达到了人类的识别水平。但是物体在现实环境中可能表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。事实上，小图像数据集的缺点已是众所周知（例如，Pinto），但直到最近才可以收集到数百万的标记数据集。新的大型数据集包括LabelMe ，其中包含数十万个完全分割的图像，以及ImageNet ，其中包含超过15,000万个超过22,000个类别的高分辨率图像。（目前的研究的对象，研究现状。）<br>要从数百万图像中学习数千个类别，我们需要一个具有强大学习能力的模型。（这里暗示这篇文章的模型大小非常大，但是现在看来入门级把，毕竟是12年前的文章了，开山鼻祖了。）然而，物体识别任务的巨大复杂性意味着即使是像ImageNet这样大的数据集也不能完美地解决这个问题，所以我们的模型也需要使用很多先验知识来弥补我们数据集不足的问题。卷积神经网络（CNN）就构成了一类这样的模型（卷积神经网络可以获取先验的知识，来弥补数据集不足的问题，后面是卷积神经网络为什么能够实现获取先验知识。）。它们的容量可以通过改变它们的深度和宽度来控制，并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。<br>尽管CNN具有很好的质量，并且尽管其局部结构的效率相对较高，但将它们大规模应用于高分辨率图像时仍然显得非常昂贵。幸运的是，当前的GPU可以用于高度优化的二维卷积，能够加速许多大型CNN的训练，并且最近的数据集（如ImageNet）包含足够多的标记样本来训练此类模型，而不会出现严重的过度拟合。<br>本文的具体贡献如下：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。（codeing能力还是有的，我的目标就是能够实现自己的想法，Talk is cheap. Show me the code.这句话真的是令人兴奋）我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3章节。我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。（这点可能验证了特征是层级表示的，）最后，网络的规模主要受限于目前GPU上可用的内存量以及我们可接受的训练时间。我们的网络需要在两块GTX 580 3GB GPU上花费五到六天的时间来训练。我们所有的实验都表明，通过等待更快的GPU和更大的数据集出现，我们的结果可以进一步完善。</p></li><li><p>数据集：ImageNet是一个拥有超过1500万个已标记高分辨率图像的数据集，大概有22,000个类别。（对数据集有一个基本介绍，保证权威性，说明没有造假）图像都是从网上收集，并使用Amazon-Mechanical Turk群智工具人工标记（人工智能，人工越多越智能，找不到工作就去打标签，打标签的特点就是不费脑子，一坐坐一天。ImageNet是李飞飞<a href="https://baike.baidu.com/item/%E6%9D%8E%E9%A3%9E%E9%A3%9E/7448630">放个连接</a>。从2010年起，作为Pascal视觉对象挑战赛的一部分，这是每年举办一次的名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。 ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。<br>ILSVRC-2010是ILSVRC中的唯一可以使用测试集标签的版本，因此这也正是我们进行大部分实验的版本。由于我们也在ILSVRC-2012比赛中引入了我们的模型，因此在第6部分中，我们也会给出此版本数据集的结果，尽管这个版本的测试集标签不可用。在ImageNet上，习惯上使用两种错误率：top-1和top-5，其中top-5错误率是正确标签不在被模型认为最可能的五个标签之中的测试图像的百分率。（原来错误率来源于这个比赛）<br>ImageNet由可变分辨率的图像组成，而我们的系统需要固定的输入尺寸。因此，我们将图像下采样到256×256的固定分辨率。给定一个矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心的256×256的图片。除了将每个像素中减去训练集的像素均值之外，我们没有以任何其他方式对图像进行预处理。所以我们在像素的（中心）原始RGB值上训练了我们的网络。</p></li><li><p>图（前文放的模型结构图）概括了我们所提出网络的结构。它包含八个学习层——五个卷积层和三个全连接层。下面，我们将描述一些所提出网络框架中新颖或不寻常的地方。 3.1-3.4节按照我们对它们重要性的估计进行排序，其中最重要的是第一个。<br>RelU：对一个神经元模型的输出的常规套路是，给他接上一个激活函数：（tanh（x）的公式，）就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数（ReLU的公式f(x)&#x3D;max(0,x)注：因为ReLU的公式比较简单所以这里放一下)如慢得多。根据Nair和Hinton的说法[20]（这篇论文相当于为ReLU背书了，就相当于我的理论依据），我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。这个结果从图1中可以看出来（实验证明来了，填坑，使用了实验证明），该图展示了对于一个特定的四层CNN，CIFAR-10数据集训练中的误差率达到25%所需要的迭代次数。从这张图的结果可以看出，如果我们使用传统的饱和神经元模型来训练CNN，那么我们将无法为这项工作训练如此大型的神经网络。我们并不是第一个考虑在CNN中替换掉传统神经元模型的(继续理论证明，巨大的论文阅读量，)。例如，Jarrett等人[11]声称，非线性函数在他们的对比度归一化问题上，再接上局部均值池化单元，在Caltech-101数据集上表现的非常好。然而，在这个数据集中，主要担心的还是防止过拟合，所以他们观察到的效果与我们在使用ReLU时观察到的训练集的加速能力还是不一样。加快训练速度对大型数据集上训练的大型模型的性能有很大的影响。<br>在多个GPU上训练：单个GTX 580 GPU只有3GB内存（当时GPU的内存确实小，不过也挺厉害了。有时候真觉得自己跟不上时代了，对时间没有一点感觉，12年24年对我有什么区别？），这限制了可以在其上训练的网络的最大尺寸。事实证明，120万个训练样本足以训练那些因规模太大而不适合使用一个GPU训练的网络。因此，我们将网络分布在两个GPU上。目前的GPU很适合于跨GPU并行化操作，因为它们能够直接读写对方的内存，而无需通过主机内存。我们采用的并行化方案基本上将半个内核（或神经元）放在各个GPU上，（有种左右脑的感觉）——另外还有一个技巧：GPU只在某些层间进行通信。这意味着，例如，第3层的内核从第2层的所有内核映射（kernel maps）中获取输入。然而，第4层中的内核又仅从位于同一GPU上的第3层中的那些内核映射获取输入。选择连接模式对于交叉验证是一个不小的问题，但这使得我们能够精确调整通信量，直到它的计算量的达到可接受的程度。由此产生的架构有点类似于Cire¸san等人使用的“柱状”CNN[5]，除了我们的每列不是独立的之外（见图2）。与一个GPU上训练的每个卷积层只有一半的内核数量的网络相比，该方案分别将我们的top-1和top-5错误率分别降低了1.7％和1.2％。双GPU网络的训练时间比单GPU网络更少。<br>局部响应归一化：ReLU具有理想的属性，它们不需要对输入进行归一化来防止它们饱和。如果至少有一些训练实例为ReLU产生了正的输入，那么这个神经元就会学习。然而，我们还是发现下面的这种归一化方法有助于泛化。设aix,y表示第i个内核计算(x,y)位置的ReLU非线性单元的输出，而响应归一化（Local Response Normalization）的输出值定义为bix,y其中，（公式）求和部分公式中的 n表示同一个位置下与该位置相邻的内核映射的数量，而N表示这一层所有的内核数（即通道数）。内核映射的顺序当然是任意的，并且在训练之前就已经定好了。这种响应归一化实现了一种模仿真实神经元的横向抑制，从而在使用不同内核计算的神经元输出之间产生较大的竞争。常数k都是超参数（hyper-parameters），它们的值都由验证集决定。我们取 k&#x3D;2。我们在某些层的应用ReLU后再使用这种归一化方法（参见第3.5节）。这个方案与Jarrett等人[11]的局部对比归一化方案有些相似之处，但我们的被更准确地称为“亮度归一化”，因为我们没有减去均值。响应归一化将我们的top-1和top-5的错误率分别降低了1.4％和1.2％。我们还验证了这种方案在CIFAR-10数据集上的有效性：没有进行归一化的四层CNN实现了13％的测试错误率，而进行了归一化的则为11％。<br>层叠池化：CNN中的池化层汇集了相同内核映射中相邻神经元组的输出。在传统方法中，相邻池化单元之间互不重叠（例如[17,11,4]）。更准确地说，一个池化层可以被认为是由一些间隔为s个像素的池化单元组成的网格，每个都表示了一个以池化单元的位置为中心的大小为z×z的邻域。如果我们令s &#x3D; z，我们就可以得到CNN中常用的传统的局部池化。<br>整体结构：现在我们已经准备好描述CNN的整体架构了。如图2所示，这个网络包含了八层权重;前五个是卷积层，其余三个为全连接层。最后的全连接层的输出被送到1000维的softmax函数，其产生1000个类的预测。我们的网络最大化多项逻辑回归目标，这相当于在预测的分布下最大化训练样本中正确标签对数概率的平均值。第二，第四和第五个卷积层的内核仅与上一层存放在同一GPU上的内核映射相连（见图2）。第三个卷积层的内核连接到第二层中的所有内核映射。全连接层中的神经元连接到前一层中的所有神经元。响应归一化层紧接着第一个和第二个卷积层。 在3.4节中介绍的最大池化层，后面连接响应归一化层以及第五个卷积层。将ReLU应用于每个卷积层和全连接层的输出。第一个卷积层的输入为224×224×3的图像，对其使用96个大小为11×11×3、步长为4（步长表示内核映射中相邻神经元感受野中心之间的距离）的内核来处理输入图像。第二个卷积层将第一个卷积层的输出（响应归一化以及池化）作为输入，并使用256个内核处理图像，每个内核大小为5×5×48。第三个、第四个和第五个卷积层彼此连接而中间没有任何池化或归一化层。第三个卷积层有384个内核，每个的大小为3×3×256，其输入为第二个卷积层的输出。第四个卷积层有384个内核，每个内核大小为3×3×192。第五个卷积层有256个内核，每个内核大小为3×3×192。全连接层各有4096个神经元。</p></li><li><p>减少过拟合。我们的神经网络架构拥有6000万个参数。尽管ILSVRC的1000个类别使得每个训练样本从图像到标签的映射被限制在了10 bit之内，但这不足以保证训练这么多参数而不出现过拟合。下面，我们将介绍对付过度拟合的两个方法。<br>数据增强： 减小过拟合的最简单且最常用的方法就是，使用标签保留转换（label-preserving transformations，例如[25,4,5]），人为地放大数据集。我们采用两种不同形式的数据增强方法，它们都允许通过很少的计算就能从原始图像中生成转换图像，所以转换后的图像不需要存储在硬盘上。在我们实现过程中，转换后的图像是使用CPU上的Python代码生成的，在生成这些转换图像的同时，GPU还在训练上一批图像数据。所以这些数据增强方案实际上是很高效的。<br>数据增强的第一种形式包括平移图像和水平映射。我们通过从256×256图像中随机提取224×224的图像块（及其水平映射）并在这些提取的图像块上训练我们的网络来做到这一点。这使我们的训练集的规模增加了2048倍，尽管由此产生的训练样本当然还是高度相互依赖的。如果没有这个方案，我们的网络就可能会遭受大量的的过拟合，可能会迫使我们不得不使用更小的网络。在测试时，网络通过提取5个224×224的图像块（四个角块和中心块）以及它们的水平映射（因此总共包括10个块）来进行预测，并求网络的softmax层的上的十个预测结果的均值。第二种形式的数据增强包括改变训练图像中RGB通道的灰度。具体而言，我们在整个ImageNet训练集的图像的RGB像素值上使用PCA。对于每个训练图像，我们添加多个通过PCA找到的主成分，大小与相应的特征值成比例，乘以一个随机值，该随机值属于均值为0、标准差为0.1的高斯分布。因此，对于每个图像的RGB像素有：Ixy&#x3D;[IRxy IGxy IBxy]T（自己去看论文中的公式），我们加入如下的值：[p1 p2 p3] [α1λ1 α2λ2 α3λ3]T其中， pi和 λi分别是3x3的RGB协方差矩阵的第 i个特征向量和第i个的特征值，而 αi是前面所说的随机值。对于一张特定图像中的所有像素，每个 αi只会被抽取一次，知道这张图片再次用于训练时，才会重新提取随机变量。这个方案近似地捕捉原始图像的一些重要属性，对象的身份不受光照的强度和颜色变化影响。这个方案将top-1错误率降低了1％以上。<br>Dropout： 结合许多不同模型的预测结果是减少测试错误率的一种非常成功的方法[1,3]，但对于已经花费数天时间训练的大型神经网络来说，它似乎成本太高了。然而，有一种非常有效的模型组合方法，在训练期间，只需要消耗1&#x2F;2的参数。这个新发现的技术叫做“Dropout”[10]，它会以50%的概率将隐含层的神经元输出置为0。以这种方法被置0的神经元不参与网络的前馈和反向传播。因此，每次给网络提供了输入后，神经网络都会采用一个不同的结构，但是这些结构都共享权重。这种技术减少了神经元的复杂适应性，因为神经元无法依赖于其他特定的神经元而存在。因此，它被迫学习更强大更鲁棒的功能，使得这些神经元可以与其他神经元的许多不同的随机子集结合使用。在测试时，我们试着使用了所有的神经元，并将它们的输出乘以0.5。这与采用大量dropout的网络产生的预测结果分布的几何均值近似。我们在图2中的前两个全连接层上使用了dropout。没有dropout，我们的网络会出现严重的过拟合。Dropout大概会使达到收敛的迭代次数翻倍。</p></li><li><p>训练细节。我们使用随机梯度下降法来训练我们的模型，每个batch有128个样本，动量（momentum）为0.9，权重衰减（weight decay）为0.0005。我们发现这种较小的权重衰减对于模型的训练很重要。换句话说，权重衰减在这里不仅仅是一个正则化方法：它减少了模型的训练误差。权重ω的更新法则是：（自己看公式去）<br>我们使用标准差为0.01、均值为0的高斯分布来初始化各层的权重。我们使用常数1来初始化了网络中的第二个、第四个和第五个卷积层以及全连接层中的隐含层中的所有偏置参数。这种初始化权重的方法通过向ReLU提供了正的输入，来加速前期的训练。我们使用常数0来初始化剩余层中的偏置参数。<br>我们对所有层都使用相同的学习率，在训练过程中又手动进行了调整。我们遵循的启发式方法是：以当前的学习速率训练，验证集上的错误率停止降低时，将学习速率除以10.学习率初始时设为0.01，并且在终止前减少3次。我们使用120万张图像的训练集对网络进行了大约90次迭代的训练，这在两块NVIDIA GTX 580 3GB GPU上花费了大约5到6天的时间。（这里说明了优化函数，超参数设置。这里挖个坑，什么是超参数？）</p></li><li><p>结果：我们在ILSVRC-2010上取得的结果如表1所示。我们的网络的top-1和top-5测试集错误率分别为37.5％和17.0％。在ILSVRC-2010比赛期间取得的最佳成绩是47.1％和28.2％，其方法是对六种不同的稀疏编码模型所产生的预测结果求平均[2]。此后公布的最佳结果为45.7％、25.7％，其方法是对两种经过密集采样的特征[24]计算出来的Fisher向量（FV）训练的两个分类器取平均值。我们的网络实现了37.5％和17.0％的前1和前5个测试集错误率5。在ILSVRC-2010比赛期间取得的最佳成绩是47.1％和28.2％，其中一种方法是对六种针对不同特征进行训练的稀疏编码模型所产生的预测进行平均[2]，此后最佳公布结果为45.7％， 25.7％，其中一种方法是：对两个在不同取样密度的Fisher向量上训练的分类器取平均。（纵向对比了，相同数据集，不同模型。）<br>我们还在ILSVRC-2012竞赛中使用了我们的模型，并在表2中给出了我们的结果。由于ILSVRC-2012测试集标签未公开，因此我们无法给出我们测试过的所有模型在测试集上的错误率。在本节的其余部分中，我们将验证集和测试集的错误率互换，因为根据我们的经验，它们之间的差值不超过0.1％（见表2）。本文描述的CNN的top-5错误率达到了18.2％。对五个相似CNN的预测结果计算均值，得到的错误率为16.4％。单独一个CNN，在最后一个池化层之后，额外添加第六个卷积层，对整个ImageNet Fall 2011 release(15M images, 22K categories)进行分类，然后在ILSVRC-2012上“微调”（fine-tuning）网络，得到的错误率为16.6％。对整个ImageNet Fall 2011版本的数据集下预训练的两个CNN，求他们输出的预测值与前面提到的5个不同的CNN输出的预测值的均值，得到的错误率为15.3％。比赛的第二名达到了26.2％的top-5错误率，他们的方法是：对几个在特征取样密度不同的Fisher向量上训练的分类器的预测结果取平均的方法[7]。<br>最后，我们还在ImageNet Fall 2009版本的数据集上提交了错误率，总共有10,184个类别和890万张图像。在这个数据集中，我们遵循文献中的使用一半图像用于训练，一半图像用于测试的惯例。由于没有建立测试集，所以我们的拆分方法有必要与先前作者使用的拆分方法不同，但这并不会对结果产生显著的影响。我们在这个数据集上的top-1和top-5错误率分别是67.4％和40.9％，是通过前面描述的网络获得的，但是在最后的池化层上还有额外的第6个卷积层。该数据集此前公布的最佳结果是78.1％和60.9％[19]。<br>定性评估：图3（自己看论文去）显示了由网络的两个数据连接层学习得到的卷积内核。（网络结构还可以画出来，也是挺有意思的。）该网络已经学习到许多频率和方向提取的内核，以及各种色块。请注意两个GPU所展现的不同特性，这也是3.5节中介绍的限制互连的结果。GPU1上的内核在很大程度上与颜色无关，然而GPU2上的内核在很大程度上都于颜色有关。这种特异性在每次迭代期间都会发生，并且独立于任何特定的随机权重初始化过程（以GPU的重新编号为模）。<br>在图4（自己看论文去，图4展示了一堆实验结果）的左边，我们通过计算8张测试图像的top-5预测来定性评估网络的训练结果。请注意，即使是偏离中心的物体，如左上角的螨虫，也可以被网络识别出来。大多数top-5的标签都显得比较合理。例如，只有其他类型的猫才被认为是豹子的可能标签。在某些情况下（栅栏、樱桃），照片的关注点存在模糊性，不知道到底该关注哪个。另一个研究可视化的网络的方法是，考虑由最后一个4096维隐含层中的图像的特征的激活函数输出值。如果两幅图像产生有的欧氏距离，我们可以认为高层次的神经网络认为它们是相似的。图4显示了测试集中的5个图像和来袭训练集的6个图像，这些图像根据这种度量方法来比较它们中的哪一个与其最相似。请注意，在像素层次上，待检测的训练图像通常不会与第一列中的查询图像有较小的L2距离。例如，检索到的狗和大象有各种不同的姿势。我们在补充材料中提供了更多测试图像的结果。通过使用欧式距离来计算两个4096维实值向量的相似性，效率不高，但是通过训练自编码器可以将这些向量压缩为较短的二进制码，能够使其更高效。与应用自编码器到原始像素[14]相比，这应该是更好的图像检索方法。它不使用图像标签，因此更秦翔宇检索具有相似图案边缘的图像，不管它们的图像语义是否相似。</p></li><li><p>讨论：我们的研究结果表明，一个大的深层卷积神经网络能够在纯粹使用监督学习（这里有个概念，监督学习和无监督学习，半监督学习。挖个坑）的情况下，在极具挑战性的数据集上实现破纪录的结果。值得注意的是，如果移除任何一个卷积层，网络的性能就会下降。例如，删除任何中间层的结果会导致网络性能的top-1错误率下降2%。因此网络的深度对于实现我们的结果真的很重要。（基本上后面的深度学习的思路就是堆网络结构）<br>为了简化我们的实验，我们没有使用任何无监督的预训练方法，尽管这样可能会有所帮助，特别是如果我们获得了足够的计算能力来显著地增加网络的大小而不会相应地增加已标记数据的数量。到目前为止，我们的结果已经获得了足够的进步，因为我们已经使网络更大，并且训练了更长时间。但我们仍然有很大的空间去优化网络，使之能够像人类的视觉系统一样感知。最后，我们希望对视频序列使用非常大的深度卷积神经网路，其中时间结构提供了非常有用的信息，这些信息往往在静态图像中丢失了，或者说不太明显。</p></li></ol><h1 id="个人感觉"><a href="#个人感觉" class="headerlink" title="个人感觉"></a>个人感觉</h1><p>论文很短，内容很多。展现在论文中的，没有展现在论文中的。学习的过程中既有鲜花也有荆棘，这是客观的条件，我承认有人会有论语中的天生的智慧，看待世界的方式就不一样，这是现实，但是那又有什么？不管怎么样先把下面的坑填了。还有就是博客中难免有错别字，记得更改。+</p><hr><h2 id="title-填坑——经典网络结构——AlexNetdate-2024-04-23-10-48-00tags-填坑"><a href="#title-填坑——经典网络结构——AlexNetdate-2024-04-23-10-48-00tags-填坑" class="headerlink" title="title: 填坑——经典网络结构——AlexNetdate: 2024-04-23 10:48:00tags: 填坑"></a>title: 填坑——经典网络结构——AlexNet<br>date: 2024-04-23 10:48:00<br>tags: 填坑</h2><h2 id="问题1，卷积是什么？作用什么？"><a href="#问题1，卷积是什么？作用什么？" class="headerlink" title="问题1，卷积是什么？作用什么？"></a>问题1，卷积是什么？作用什么？</h2><p>卷积（Convolution）是一种数学运算，常用于信号处理和图像处理领域。在信号处理中，卷积用于将输入信号与卷积核（也称为滤波器）进行运算，产生输出信号。<br>卷积的作用有以下几个方面：</p><ol><li>信号滤波：卷积可以用于信号滤波，通过将输入信号与合适的卷积核进行卷积运算，可以实现对信号的滤波操作。滤波可以用于去除信号中的噪声、平滑信号、强调信号中的某些频率成分等。</li><li>特征提取：在图像处理中，卷积可以用于特征提取。通过将图像与不同的卷积核进行卷积运算，可以提取出图像中的不同特征，例如边缘、纹理、角点等。这些特征可以用于图像识别、目标检测和图像处理中的其他任务。</li><li>信号压缩：卷积可以用于信号压缩。通过将输入信号与适当的卷积核进行卷积运算，可以将信号表示转换为另一种表示形式，通常具有更紧凑的表示。这种表示形式可以用于信号压缩和数据压缩。</li><li>卷积神经网络：卷积神经网络（Convolutional Neural Network，CNN）是一种基于卷积运算的深度学习模型，广泛应用于图像识别、计算机视觉和自然语言处理等领域。卷积在 CNN 中用于提取图像或文本的特征，并通过多层卷积和池化操作来实现对输入数据的高级表示和分类。如果输入数据为图片，那么卷积层的作用就是提取图片中的信息，这些信息被称为图像特征，这些特征是由图像中的每个像素通过组合或者独立的方式所体现，比如图片的纹理特征、颜色特征、空间特征。</li></ol><p>卷积的操作过程：<br>请参考<a href="https://blog.csdn.net/weipf8/article/details/103917202">参考博客</a>。image的图片大小为5x5，卷积核为3x3，输出的特征的大小为3x3<br>特征图计算公式：一般情况下,输入的图片矩阵以及后面的卷积核,特征图矩阵都是方阵,这里设输入矩阵大小为w,卷和核大小为k,步幅为s,补零层数为p,则卷积后产生的特征图大小计算公式为:W &#x3D; （w+2p-k）&#x2F;s + 1. 比如说上面5x5的图片与3x3的卷积核进行卷积操作，特征图的大小为： W &#x3D; （5 + 2*0 -3）&#x2F;1 + 1 &#x3D;3<br>特征图相对与下一层的卷积层是图片。<br>卷积核的参数量计算，卷积核尺寸： K， 前一层的通道数：Cin 当前层的卷积核的个数： Cout 。单个卷积核的参数量： params kernel &#x3D; Cin x K x K, 有</p><p>假设有卷积神经网络，输入为大小224<em>224的RGB图，第一层为卷积层，有12个大小为5</em>5的卷积核，填充为2，步长为4。该层参数共有（  912    ）个。计算过程权重参数量：每个卷积核有 75 （5 x 5 x 3）个权重参数，共有12 个卷积核，所以权重参数量为 75×12&#x3D;900.偏置参数量：每个卷积核有一个偏置项，共有 12 个卷积核，所以偏置参数量为 12。<br><a href="https://blog.csdn.net/Together_CZ/article/details/115494176">执行卷积的过程的动态图</a><br>关于卷积其实还有很多问题，比如说输入一张（3x255x255）的图片，输入后经过卷积后输出的特征图大小为： 要考虑卷积核的大小（kernel size ） 步幅（stride），边界填充（padding） 计算公式入上式所示。<br>。1x1卷积为什么可以实现升维和降维。）<br>1x1 卷积可以实现升维和降维的原因在于：（通道数可以自定义数量）<br>升维：当输入特征图的通道数较少时，可以使用 1x1 卷积来增加通道数，从而增加网络的表示能力。这是因为 1x1 卷积可以将输入特征图中的每个通道与卷积核中的权重相乘并求和，从而生成一个新的特征图。<br>降维：当需要减少特征图的通道数时，可以使用 1x1 卷积并调整输出通道数为所需的值。通过调整卷积核中的输出通道数，可以实现特征图通道数的降维。</p><h2 id="问题2，池化是什么？作用是什么？"><a href="#问题2，池化是什么？作用是什么？" class="headerlink" title="问题2，池化是什么？作用是什么？"></a>问题2，池化是什么？作用是什么？</h2><p>池化（Pooling）是一种常用的操作，通常与卷积神经网络（CNN）结合使用。池化操作通过对输入数据的局部区域进行聚合或采样来减小数据的空间尺寸，从而减少参数数量、降低计算量，并提取出输入数据的重要特征。</p><p>池化的作用有以下几个方面</p><ol><li>降采样：池化操作可以减小输入数据的空间尺寸，从而降低后续层的计算复杂度。通过降低数据的维度，池化可以在保留重要特征的同时减少冗余信息，提高计算效率。</li><li>平移不变性：池化操作具有一定的平移不变性。在图像处理中，通过对局部区域进行池化操作，可以使得输入图像在平移、旋转和缩放等变换下具有一定的不变性。这对于图像识别和目标检测等任务是有益的。</li><li>特征提取：池化操作可以提取输入数据的重要特征。通过对局部区域进行池化，池化操作会选择区域中的最大值（最大池化）或平均值（平均池化）作为输出值，从而提取出输入数据的显著特征。这有助于减少数据的维度，并保留重要的特征信息。</li><li>减少过拟合：池化操作可以在一定程度上减少过拟合。通过减小数据的空间尺寸，池化操作可以降低模型的参数数量，从而减少过拟合的风险。此外，池化操作还可以通过丢弃一些冗余信息来提高模型的泛化能力。</li></ol><p>池化的种类</p><ol><li>最大池化（Max Pooling）：最大池化是一种常见的池化操作。在最大池化中，输入数据的局部区域被分割成不重叠的块，然后在每个块中选择最大值作为输出。最大池化可以提取出输入数据的显著特征，同时减小数据的空间尺寸。</li><li>平均池化（Average Pooling）：平均池化是另一种常见的池化操作。在平均池化中，输入数据的局部区域被分割成不重叠的块，然后计算每个块中元素的平均值作为输出。平均池化可以平滑输入数据并减小数据的空间尺寸。</li><li>自适应池化（Adaptive Pooling）：自适应池化是一种具有灵活性的池化操作。与最大池化和平均池化不同，自适应池化不需要指定池化窗口的大小，而是根据输入数据的尺寸自动调整池化窗口的大小。这使得自适应池化可以适应不同尺寸的输入数据。</li><li>全局池化（Global Pooling）：全局池化是一种特殊的池化操作，它将整个输入数据的空间尺寸缩减为一个单一的值或向量。全局池化可以通过对输入数据的所有位置进行池化操作，从而提取出输入数据的全局特征。常见的全局池化有全局平均池化（Global Average Pooling）和全局最大池化（Global Max Pooling）。</li></ol><h2 id="问题3，全连接是什么？作用是什么？"><a href="#问题3，全连接是什么？作用是什么？" class="headerlink" title="问题3，全连接是什么？作用是什么？"></a>问题3，全连接是什么？作用是什么？</h2><p>的是神经网络中的一种连接方式，也称为密集连接（dense connection）。在全连接中，每个神经元都与前一层的所有神经元相连。这意味着前一层的每个神经元的输出都将作为输入传递给下一层的每个神经元。<br>全连接层的作用是将输入数据进行线性变换，并应用激活函数来产生输出。这种连接方式允许神经网络学习输入数据中的复杂关系，从而实现各种任务，例如分类、回归等。<br><img src="/pic/tiankeng1.png" alt="全连接示意图"><br>请问如上DNN神经网络共有几层： 5层<br>请问该DNN神经网络用来解决二分类问题，那么最后一层的激活函数是 Sigmoid<br>请问如上所示的DNN神经网络的第一个隐藏层有多少个参数： 2 x 3 + 3 &#x3D; 9 （前一层输入量 乘以 后一层的神经元数量 + 偏执项。）</p><h2 id="问题4，AlexNet论文使用的loss函数是什么？"><a href="#问题4，AlexNet论文使用的loss函数是什么？" class="headerlink" title="问题4，AlexNet论文使用的loss函数是什么？"></a>问题4，AlexNet论文使用的loss函数是什么？</h2><p>CrossEntropy交叉损失函数： </p><p><a href="https://blog.csdn.net/qq_44629163/article/details/124348366">参考博客</a></p><h2 id="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"><a href="#问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？" class="headerlink" title="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"></a>问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？</h2><p>AlexNet论文中使用的梯度优化算法是随机梯度下降（Stochastic Gradient Descent，SGD）。在训练过程中，SGD通过计算损失函数关于网络参数的梯度，并根据该梯度更新参数，以使损失函数最小化。<br><a href="https://blog.csdn.net/qq_58146842/article/details/121280968">SGD参考博客</a></p><h2 id="问题6，AlexNet论文中使用的评价指标是什么？"><a href="#问题6，AlexNet论文中使用的评价指标是什么？" class="headerlink" title="问题6，AlexNet论文中使用的评价指标是什么？"></a>问题6，AlexNet论文中使用的评价指标是什么？</h2><p>错误率 ： ACC 的相反数，计算方法为1-ACC</p><h2 id="问题7，AlexNet中的创新点是什么？"><a href="#问题7，AlexNet中的创新点是什么？" class="headerlink" title="问题7，AlexNet中的创新点是什么？"></a>问题7，AlexNet中的创新点是什么？</h2><ol><li>ReLU激活函数的引入，采样非线性单元（ReLU）的深度卷积神经网络训练时间要比tanh单元要快几倍。而时间开销是进行模型训练过程中的很重要的因数。同时ReLU有效的防止了过拟合的现象。</li><li>层叠池化操作，以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize&#x3D;2×2，stride&#x3D;2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize &gt; stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。</li><li>Dropout操作， Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。</li><li>网络层数更深，与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。当然，这种思考的重要节点出现在VGG网络（下一篇博文VGG论文中将会讲到）。</li></ol><h2 id="问题8，优化函数的具体实现是什么？"><a href="#问题8，优化函数的具体实现是什么？" class="headerlink" title="问题8，优化函数的具体实现是什么？"></a>问题8，优化函数的具体实现是什么？</h2><p>请参考博客：<br><a href="https://www.bilibili.com/video/BV1Fu4y1N7Qu/?spm_id_from=333.337.search-card.all.click&vd_source=c2d8f28374ac78ec2f99b6321e56032a">以逻辑回归为例讲的梯度下降算法矩阵化</a><br><a href="https://www.bilibili.com/video/BV1eK42147wr/?p=29&vd_source=c2d8f28374ac78ec2f99b6321e56032a">结合看</a><br><a href="https://www.bilibili.com/video/BV1JK411k7ah/?spm_id_from=333.337.search-card.all.click&vd_source=c2d8f28374ac78ec2f99b6321e56032a">代码实现</a></p><h2 id="问题10，什么是过拟合合和欠拟合？"><a href="#问题10，什么是过拟合合和欠拟合？" class="headerlink" title="问题10，什么是过拟合合和欠拟合？"></a>问题10，什么是过拟合合和欠拟合？</h2><p>过拟合：过拟合指的是模型在训练数据上表现良好，但在未见过的测试数据上表现较差的情况。过拟合通常发生在模型过于复杂或训练数据过少的情况下。当模型过度学习了训练数据中的噪声或特定的样本特征时，会导致过拟合问题。在过拟合的情况下，模型可能会过度拟合训练数据，导致泛化能力较差，无法很好地适应新的、未见过的数据。<br>欠拟合：欠拟合指的是模型在训练数据上表现不佳，无法捕捉数据中的足够的信息和结构，导致模型过于简单或不够复杂。欠拟合通常发生在模型复杂度过低或训练数据量不足的情况下。在欠拟合的情况下，模型可能无法捕捉数据中的关键特征或模式，导致训练误差和测试误差都较高。<br><a href="https://zhuanlan.zhihu.com/p/72038532">参考博客</a></p><h2 id="问题9，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"><a href="#问题9，论文中怎么证明，层叠池化可以有效防止过拟合的发生？" class="headerlink" title="问题9，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"></a>问题9，论文中怎么证明，层叠池化可以有效防止过拟合的发生？</h2><p>验对比：作者可能会设计实验，对比使用层叠池化和不使用层叠池化的模型在同一数据集上的性能表现。通过比较两种模型的训练误差和测试误差，可以观察到使用层叠池化的模型是否在测试数据上表现更好，从而证明其能够有效防止过拟合。<br>交叉验证：作者可能会使用交叉验证来评估模型的泛化能力。通过在不同的训练集和测试集上多次进行实验，可以更客观地评估模型的性能，并观察使用层叠池化的模型是否具有更好的泛化能力。<br>可视化分析：作者可能会对模型的训练过程进行可视化分析，比如绘制训练损失曲线和验证损失曲线。通过观察损失曲线的变化趋势，可以了解模型是否存在过拟合问题，并观察是否使用层叠池化的模型更加稳定。</p><h2 id="问题10，-softMax的机制是怎么样的？"><a href="#问题10，-softMax的机制是怎么样的？" class="headerlink" title="问题10， softMax的机制是怎么样的？"></a>问题10， softMax的机制是怎么样的？</h2><p>Softmax函数是一种常用的激活函数，通常用于多分类问题中的输出层。Softmax函数可以将一个具有任意实数值的向量转换成一个概率分布，使得各个元素的值都在 (0, 1) 范围内，并且所有元素的和为1。</p><p><a href="https://zhuanlan.zhihu.com/p/105722023">参考博客</a></p><h2 id="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？"><a href="#问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？" class="headerlink" title="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？"></a>问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？</h2><p>Dropout是一种常用的正则化技术，用于减少神经网络的过拟合现象。其运行机制如下：</p><p>训练阶段：在每次训练迭代时，以概率 p 将神经网络中的某些神经元（或者称为节点）临时从网络中删除（置为零）。这样，在每次迭代中，都会随机删除一部分神经元，从而导致每次迭代得到的网络结构都不同。<br>预测阶段：在预测阶段，不再使用Dropout，而是使用所有的神经元，但需要对每个神经元的输出值乘以 p，以保持期望的输出值不变。</p><h2 id="问题16，-什么是超参数？"><a href="#问题16，-什么是超参数？" class="headerlink" title="问题16， 什么是超参数？"></a>问题16， 什么是超参数？</h2><p>超参数（Hyperparameters）是机器学习模型训练过程中的配置参数，其值不能通过训练过程自动学习，而是需要人工设置。与模型的参数（例如权重和偏置）不同，超参数通常用于控制模型的结构、学习过程的行为和性能调优。</p><p>一些常见的超参数包括：<br>学习率（Learning Rate）：用于控制每次参数更新的步长。<br>迭代次数（Number of Iterations&#x2F;Epochs）：训练模型时数据集遍历的次数。<br>批量大小（Batch Size）：每次迭代中用于更新参数的样本数量。<br>网络结构参数：例如隐藏层的数量、每个隐藏层的神经元数量、卷积核大小等。<br>正则化参数：用于控制模型的复杂度，例如L1和L2正则化的权重。<br>优化算法参数：例如动量（momentum）、adam的参数等。<br>损失函数参数：例如softmax交叉熵的参数、权重类别平衡等。</p><h2 id="问题17，-什么是监督学习和无监督学习，半监督学习？"><a href="#问题17，-什么是监督学习和无监督学习，半监督学习？" class="headerlink" title="问题17， 什么是监督学习和无监督学习，半监督学习？"></a>问题17， 什么是监督学习和无监督学习，半监督学习？</h2><p>监督学习（Supervised Learning）是一种机器学习任务，其目标是从有标签的数据中学习出一个输入到输出的映射关系，即从输入数据预测出相应的输出标签。在监督学习中，训练数据包括了输入和对应的输出标签，模型通过学习输入和输出之间的关系来进行预测。典型的监督学习任务包括分类和回归。</p><p>无监督学习（Unsupervised Learning）是一种机器学习任务，其目标是从没有标签的数据中学习出数据的内在结构或者特征表示，而无需事先给定标签信息。在无监督学习中，训练数据只包括输入数据，没有对应的输出标签。无监督学习可以用于聚类、降维、异常检测等任务。</p><p>半监督学习（Semi-Supervised Learning）是介于监督学习和无监督学习之间的一种学习方式，其目标是利用少量有标签数据和大量无标签数据来训练模型。在半监督学习中，训练数据同时包括有标签的数据和无标签的数据。半监督学习可以通过结合监督学习和无监督学习的方法，利用无标签数据的信息来提升模型性能，尤其在标注数据有限或者成本较高的情况下具有重要意义。</p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。（请移步深度学习：图像处理-数据预处理部分）"><a href="#关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。（请移步深度学习：图像处理-数据预处理部分）" class="headerlink" title="关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。（请移步深度学习：图像处理-数据预处理部分）"></a>关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。（请移步深度学习：图像处理-数据预处理部分）</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch基础——Numpy</title>
    <link href="/2024/04/21/deeplearnbook2/"/>
    <url>/2024/04/21/deeplearnbook2/</url>
    
    <content type="html"><![CDATA[<p>第一部分</p><h1 id="Numpy基础"><a href="#Numpy基础" class="headerlink" title="Numpy基础"></a>Numpy基础</h1><h2 id="什么是Numpy？"><a href="#什么是Numpy？" class="headerlink" title="什么是Numpy？"></a>什么是Numpy？</h2><p>NumPy（Numerical Python的简称）是一个开源的Python库，用于进行科学计算。它提供了一个强大的N维数组对象，以及大量的函数用于处理这些数组。NumPy的主要功能包括：</p><ol><li>多维数组对象：NumPy的核心功能是其多维数组对象（ndarray）。这是一个快速、灵活的容器，可以容纳大量同类型数据，使你能够对这些数据进行数学运算。</li><li>广播功能：NumPy提供了广播功能，这是一种强大的机制，允许NumPy在执行算术运算时处理不同形状的数组。</li><li>数学函数：NumPy提供了大量的数学函数，可以对数组中的元素进行各种数学运算，如加、减、乘、除、平方根等。</li><li>线性代数：NumPy包含了线性代数函数库，可以进行矩阵乘法、求逆、解线性方程，傅里叶变换等操作。</li><li>随机数生成：NumPy提供了生成各种随机数的功能，如均匀分布、正态分布等。</li><li>更方便的读取\写入磁盘上的阵列数据和操作存储映像文件的工具。<br>NumPy是许多科学计算库（如Pandas、Matplotlib、SciPy等）的基础库，也是机器学习和数据科学中常用的库。在深度学习中图像、声音、文本等输入数据最终都要转换为数组或矩阵，NumPy的多维数组可以用来表示向量、矩阵和张量，这些都是深度学习算法的基本构成元素。</li></ol><h2 id="为什么要使用Numpy"><a href="#为什么要使用Numpy" class="headerlink" title="为什么要使用Numpy"></a>为什么要使用Numpy</h2><p>实际上python包含多个数据类型，数值类型（int，float，complex）、布尔类型（bool）、字符串（str）、列表（list）、元组（tuple）、字典（dict，{‘name’:’chenli’,’age’: 114514}）、集合（set{}）。<br>python包含这么多的数据类型，其中的列表（list）和数组（array）为什么不能用？原因是对于大数据来说，这些结构有很多不足。比如由于列表的元素可以是任何对象，因此列表中所保存的是对象的指针。例如为了存储[1,2,3],就需要3个指针和3个整数对象，这样对于数值运算来讲，严重浪费了计算机中的内存和CPU或GPU的算力。对于array，它可以直接保存数值，但是它不支持多维，并且对应操作的函数也不多，因此也不适合。</p><h2 id="怎么使用Numpy？"><a href="#怎么使用Numpy？" class="headerlink" title="怎么使用Numpy？"></a>怎么使用Numpy？</h2><p>第一步使用 <code>pip install numpy </code>来下载numpy库。</p><ol><li><p>生成Numpy数组。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>lst1= <span class="hljs-string">[[3.14, 2.17,0,1,2],[1,2,3,4,5]]</span> # 一个二维列表<br>nd1 = np.array(lst1)# 使用np.array()将列表数据类型转换成np数据类型<br><span class="hljs-built_in">print</span>(nd1) # 显示的结果为一个二维列表<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(nd1)) # 显示结果为&lt;class <span class="hljs-string">&#x27;numpy.ndarray&#x27;</span>&gt;    <br></code></pre></td></tr></table></figure></li><li><p>numpy中的random模块生成数组</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">nd1</span> = np.random.random([<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]) # 产生一个[<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]ndarray，范围为<span class="hljs-number">0</span>-<span class="hljs-number">1</span>之间的随机数。<br><span class="hljs-attribute">np</span>.random.seed(<span class="hljs-number">123</span>) # 为了每次生成同一份数据，可以指定一个随机种子，而后生成的随机数据是固定的。<br><span class="hljs-attribute">nd2</span> = np.random.random(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 产生一个[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]的ndarray<br><span class="hljs-attribute">np</span>.random.shuffle(nd2) # 随机打乱nd2中的数据。<br><span class="hljs-attribute">nd3</span> = np.random.uniform(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 生成均匀分布的随机数<br><span class="hljs-attribute">nd4</span> = np.random.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 生成标准正态分布的随机数 <br><span class="hljs-attribute">nd5</span> = np.random.randint(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 生成随机的整数<br></code></pre></td></tr></table></figure></li><li><p>numpy中创建特定形状的多维数组</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">nd1</span> = np.zeros([<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]) # 生成全是<span class="hljs-number">0</span>的<span class="hljs-number">3</span>x3的矩阵，np.zeros()生成全是<span class="hljs-number">0</span>的ndarray<br><span class="hljs-attribute">nd2</span> = np.zeros_like(nd1) # 以ndrr相同维度创建元素为<span class="hljs-number">0</span>的数组<br><span class="hljs-attribute">nd3</span> = np.ones_like(nd1) # 以nd1维度创建元素全是<span class="hljs-number">1</span>的数组<br><span class="hljs-attribute">nd4</span> = np.empty_like(nd1) # 以nd1维度创建一个空数组<br><span class="hljs-attribute">nd5</span> = np.eye(<span class="hljs-number">5</span>) # 该函数用于创建一个<span class="hljs-number">5</span>x5的矩阵，对角线为<span class="hljs-number">1</span>，其余为<span class="hljs-number">0</span><br><span class="hljs-attribute">nd6</span> = np.full((<span class="hljs-number">3</span>,<span class="hljs-number">5</span>),<span class="hljs-number">666</span>) # 创建<span class="hljs-number">3</span>x5的元素全为<span class="hljs-number">666</span>的数组，<span class="hljs-number">666</span>为指定值<br></code></pre></td></tr></table></figure></li><li><p>保存使用numpy生成的数据</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import numpy <span class="hljs-keyword">as</span> np<br>nd = np.<span class="hljs-built_in">random</span>.<span class="hljs-built_in">random</span>([<span class="hljs-number">5</span>,<span class="hljs-number">5</span>])<br>np.savetxt(X=nd,fname=<span class="hljs-string">&#x27;test.txt&#x27;</span>) <span class="hljs-comment"># 保存nd，文件名称为test.txt</span><br>nd2 = np.loadtxt(<span class="hljs-string">&#x27;test.txt&#x27;</span>) <span class="hljs-comment"># 从test.txt中加载数据</span><br></code></pre></td></tr></table></figure></li><li><p>利用arange、linspace函数来生成数组<br>arange是numpy模块中的函数，格式为：<code>arange([start],stop[,step],dtype=None)</code><br>其中start与stop用来限定范围，step是步长默认为1，step可以为小数。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">nd</span> = np.arange(<span class="hljs-number">10</span>) # np中的内容为[<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span> <span class="hljs-number">8</span> <span class="hljs-number">9</span>]<br><span class="hljs-attribute">nd1</span> = np.arange(<span class="hljs-number">1</span>，<span class="hljs-number">4</span>，<span class="hljs-number">0</span>.<span class="hljs-number">5</span>) # np中的内容为[<span class="hljs-number">1</span>.<span class="hljs-number">0</span> <span class="hljs-number">1</span>.<span class="hljs-number">5</span> <span class="hljs-number">2</span>.<span class="hljs-number">0</span> <span class="hljs-number">2</span>.<span class="hljs-number">5</span> <span class="hljs-number">3</span>.<span class="hljs-number">0</span> <span class="hljs-number">3</span>.<span class="hljs-number">5</span> <span class="hljs-number">4</span>.<span class="hljs-number">0</span>]<br><span class="hljs-attribute">nd2</span> = np.aramge(<span class="hljs-number">9</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>) # nd2中的内容为[<span class="hljs-number">9</span> <span class="hljs-number">8</span> <span class="hljs-number">7</span> <span class="hljs-number">6</span> <span class="hljs-number">5</span> <span class="hljs-number">4</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>linspace也是numpy中常用的函数，格式为：<code>np.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None)</code><br>linspace可以根据输入数据的指定范围以及等份数量，自动生成线性分量。endpoint（包含终点）默认为True。等分量num默认为50，如果将retstep设置为True，则会返回一个带步长的ndarray</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">print</span>(np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>)) # 产生<span class="hljs-number">10</span>个数，间隔为<span class="hljs-number">0</span>.<span class="hljs-number">111111</span> <br></code></pre></td></tr></table></figure></li><li><p>获取数据。<br>数据生成后，如何读取数据，常用数据的的方法。（类似于python数据切片的方法）</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">np</span>.random.seed(<span class="hljs-number">2024</span>)<br><span class="hljs-attribute">nd</span> = np.random.random([<span class="hljs-number">10</span>]) # 产生一维的<span class="hljs-number">10</span>个数据点<br><span class="hljs-attribute">nd</span>[<span class="hljs-number">3</span>]  # 获取指定位置的数据，获取第<span class="hljs-number">4</span>个元素<br><span class="hljs-attribute">nd</span>[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>] # 截取一段数据<br><span class="hljs-attribute">nd</span>[<span class="hljs-number">1</span>:<span class="hljs-number">6</span>:<span class="hljs-number">2</span>] # 获取固定间隔的数据<br><span class="hljs-attribute">nd</span>[::-<span class="hljs-number">2</span>] # 倒序取数<br><span class="hljs-attribute">nd2</span> = np.arange(<span class="hljs-number">25</span>).reshape([<span class="hljs-number">5</span>,<span class="hljs-number">5</span>]) # 产生一个<span class="hljs-number">25</span>个数据点的一维数据，而后通过reshape进行形状重整为[<span class="hljs-number">5</span>,<span class="hljs-number">5</span>]<br><span class="hljs-attribute">nd2</span>[:,<span class="hljs-number">1</span>:<span class="hljs-number">3</span>] # 截取多维数组中，指定的列，读取第<span class="hljs-number">2</span>，<span class="hljs-number">3</span>列。<br></code></pre></td></tr></table></figure></li><li><p>Numpy的算术运算<br>在机器学习和深度学习中，涉及大量的数组或矩阵运算，这里介绍两种常用的运算。一种是对应元素相乘，又称逐元乘法（Element-Wisr Product）运算符为np.multiply()或*。一种是点积或内积元素，运算符为np.dot()</p></li></ol><p>逐元乘法</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">a</span> = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[-<span class="hljs-number">1</span>,<span class="hljs-number">4</span>]])<br><span class="hljs-attribute">b</span> = np.array([[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])<br><span class="hljs-attribute">a</span>*b<br><br><span class="hljs-comment"># 结果为array([2,0],[-3,16]) , 运算过程为 1*2 = 2， 2*0 = 0 ， -1*3 = -3， 4*4 =16</span><br><span class="hljs-comment"># 另外一种写法，np.multiply(a,b) 结果和上面一样</span><br><span class="hljs-comment"># Numpy数组不仅可以和数组进行对应元素相乘，还可以和单一数值（或称为标量）来进行运算</span><br></code></pre></td></tr></table></figure><p>点积运算（Dot Product）又可以称为内积，在Numpy用np.dot表示</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs lua">X1 = np.array(<span class="hljs-string">[[1,2],[3,4]]</span>)<br>X2 = np.array(<span class="hljs-string">[[5,6,7],[8,9,10]]</span>)<br>X3 = np.dot(X1,X2)<br><span class="hljs-built_in">print</span>(X3)<br># 结果为<span class="hljs-string">[[21,24,27],[47,54,61]]</span><br># 运算方法，<span class="hljs-number">1</span>*<span class="hljs-number">5</span>+<span class="hljs-number">2</span>*<span class="hljs-number">8</span> = <span class="hljs-number">21</span>, <span class="hljs-number">1</span>*<span class="hljs-number">6</span> + <span class="hljs-number">2</span>*<span class="hljs-number">9</span>=<span class="hljs-number">24</span>, <span class="hljs-number">1</span>*<span class="hljs-number">7</span>+<span class="hljs-number">2</span>*<span class="hljs-number">7</span>=<span class="hljs-number">27</span> <br># <span class="hljs-number">3</span>*<span class="hljs-number">5</span>+<span class="hljs-number">4</span>*<span class="hljs-number">8</span> = <span class="hljs-number">47</span>,<span class="hljs-number">3</span>*<span class="hljs-number">6</span>+<span class="hljs-number">4</span>*<span class="hljs-number">9</span>=<span class="hljs-number">54</span>, <span class="hljs-number">3</span>*<span class="hljs-number">7</span>+<span class="hljs-number">4</span>*<span class="hljs-number">10</span>=<span class="hljs-number">61</span><br></code></pre></td></tr></table></figure><ol start="8"><li>数组变形<br>在机器学习和深度学习任务中，通常需要将处理好的数据以模型能够接收的格式进行输入，然后进行一系列运算，最终返回一个处理结果。然而由于不同模型所接受的输入格式不一样，往往需要先对其进行一系列变形和运算，从而将数据处理成符合模型要求的格式。<br>更改数组的形状。方法有<figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs tap"><span class="hljs-comment"># arr.reshape(),将向量arr维度进行改变，不修改向量本身</span><br><span class="hljs-comment"># arr.resize(), 重新将向量arr维度进行改变，修改向量本身</span><br><span class="hljs-comment"># arr.T ,对向量进行转置</span><br><span class="hljs-comment"># arr.ravel ，对向量arr进行展平，即将多维数组变成1维数组，不会产生原数组的副本</span><br><span class="hljs-comment"># arr.flatten，对向量arr进行展平，即将多维数值变成1维数组，返回原数组的副本</span><br><span class="hljs-comment"># arr.squeeze(), 只能对维度为1的进行降维。对多维数组使用时不会进行任何报错，但是不会产生任何影响</span><br><span class="hljs-comment"># arr.transpose，对高维矩阵进行轴转换</span><br>import numpy as np<br><br>arr = np.arange(10)<br>print(arr.reshape(2,5))<br><span class="hljs-comment"># 指定维度时可以只指定行数和列数，其他用-1代替</span><br>print(arr.reshape([5,-1]))<br>print(arr)<br>print(arr.reshape([-1,5]))<br><br><span class="hljs-comment">#结果[[0 1 2 3 4]</span><br> [5<span class="hljs-number"> 6 </span>7<span class="hljs-number"> 8 </span>9]]<br>[[0 1]<br> [2 3]<br> [4 5]<br> [6 7]<br> [8 9]]<br>arr [0<span class="hljs-number"> 1 </span>2<span class="hljs-number"> 3 </span>4<span class="hljs-number"> 5 </span>6<span class="hljs-number"> 7 </span>8 9] <span class="hljs-comment"># 没有对arr本身进行修改</span><br>[[0<span class="hljs-number"> 1 </span>2<span class="hljs-number"> 3 </span>4]<br> [5<span class="hljs-number"> 6 </span>7<span class="hljs-number"> 8 </span>9]]<br></code></pre></td></tr></table></figure></li></ol><p>使用resize来修改向量维度，</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>arr = np.arange(<span class="hljs-number">10</span>)<br><span class="hljs-built_in">print</span>(arr)<br>arr.resize(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>) <span class="hljs-comment"># 直接对向量修改，不像arr.reshape没有对向量进行修改。</span><br><span class="hljs-built_in">print</span>(arr)<br></code></pre></td></tr></table></figure><p>向量转置 ,T</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><br>arr = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">12</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)#注这里重新进行赋值了，reshape保存了下来<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr.T)</span></span><br></code></pre></td></tr></table></figure><p>向量展平.ravel</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br>arr = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">6</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">2</span>,-<span class="hljs-number">1</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;按照列优先，展平&#x27;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr.ravel(<span class="hljs-string">&#x27;F&#x27;</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;按照行优先，展平&#x27;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr.ravel()</span></span>)<br></code></pre></td></tr></table></figure><p>矩阵转换为向量，这种需求经常出现在卷积神经网络与全连接之间，用于数据的展平,flatten</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs maxima">import numpy as <span class="hljs-built_in">np</span><br>a = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">floor</span>(<span class="hljs-number">10</span>*<span class="hljs-built_in">np</span>.<span class="hljs-built_in">random</span>.<span class="hljs-built_in">random</span>(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(a.<span class="hljs-built_in">flatten</span>()) # 需要注意的是一些方法是直接对向量进行修改，一些没有对向量进行修改，这点需要注意。<br></code></pre></td></tr></table></figure><p>降维操作，我也是经常使用的一个方法，squeeze 主要用来降维，把矩阵中含有1的维度去掉，在pytorch中还有一种与之相反的一种操作，torch.unsqueeze()</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">arr</span> =np.arange(<span class="hljs-number">3</span>).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)<br><span class="hljs-attribute">print</span>(arr.shape) # <span class="hljs-number">3</span>,<span class="hljs-number">1</span><br><span class="hljs-attribute">print</span>(arr.squeeze().shape) # (<span class="hljs-number">3</span>,)<br><span class="hljs-attribute">arr1</span> = np.arange(<span class="hljs-number">6</span>).reshpe(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br><span class="hljs-attribute">print</span>(arr1.shape) # (<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br><span class="hljs-attribute">print</span>(arr1.squeeze().shape) #(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>高维转换，transpose，这个在深度学习中经常使用，比如把RGB转换为GBR</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">arr</span> = np.arange(<span class="hljs-number">24</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">print</span>(arr.shape) # (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>) 注意这里为什么shape不用加括号，因为shape是一个np的属性，而不是方法。在构建一个class时，有属性和方法的区别，这里挖个坑<br><span class="hljs-attribute">print</span>(arr.transpose().shape) # (<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><ol start="9"><li>合并数组<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># <span class="hljs-built_in">np</span>.<span class="hljs-built_in">append</span> ,内存占用大<br># <span class="hljs-built_in">np</span>.concatenate 没有内存占用问题<br># <span class="hljs-built_in">np</span>.stack, 沿着新的轴加入一系列数组<br># <span class="hljs-built_in">np</span>.hstack ，堆栈数组垂直顺序（行）<br># <span class="hljs-built_in">np</span>.vstack ,堆栈数组垂直顺序（列）<br># 对于<span class="hljs-built_in">append</span> 和concatenate ，待合并的数组必须有相同的行数或列数（满足一个即可）<br></code></pre></td></tr></table></figure></li></ol><p>append合并一维数组</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[1,2,3]</span>)<br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[4,5,6]</span>)<br>c = np<span class="hljs-selector-class">.append</span>(<span class="hljs-selector-tag">a</span>,b)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(c)</span></span><br></code></pre></td></tr></table></figure><p>合并多维数组</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>a = np.arange(4).reshape(2,2)<br>b = np.arange(4).reshape(2,2)<br>c = np.append(a,b,<span class="hljs-attribute">axis</span>=0)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;按行合并后的结果&#x27;</span>)<br><span class="hljs-built_in">print</span>(c)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;合并后数据维度&#x27;</span>，c.shape) <br><span class="hljs-comment"># 按例合并</span><br>d = np.append(a,b,<span class="hljs-attribute">axis</span>=1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;按列进行合并&#x27;</span>)<br><span class="hljs-built_in">print</span>(d)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;合并后的数据维度&#x27;</span>,d.shape)<br></code></pre></td></tr></table></figure><p>concatenate沿指定轴连接数组或矩阵</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[1,2]</span>,<span class="hljs-selector-attr">[3,4]</span>])<br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[5,6]</span>)<br>c = np<span class="hljs-selector-class">.concatenate</span>((<span class="hljs-selector-tag">a</span>,b),axis=<span class="hljs-number">0</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(c)</span></span><br>d = np<span class="hljs-selector-class">.concatenate</span>((<span class="hljs-selector-tag">a</span>,<span class="hljs-selector-tag">b</span>.T),axis=<span class="hljs-number">1</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(d)</span></span><br></code></pre></td></tr></table></figure><p>stack 沿着固定轴堆积或矩阵</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.array(<span class="hljs-string">[[1,2],[3,4]]</span>)<br>b = np.array(<span class="hljs-string">[[5,6],[7,8]]</span>)<br><span class="hljs-built_in">print</span>(np.stack((a,b),axis=<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><ol start="11"><li><p>通用函数<br>sqrt ,计算序列化数据的平方根<br>sin,cos 三角函数<br>abs ， 计算序列化数据的绝对值<br>dot， 矩阵运算<br>log,log10,log2， 对数函数<br>exp, 指数函数<br>cumsum,cumproduct , 累计求和，求积<br>sum ,对一个序列化数据进行求和<br>mean ， 计算均值<br>median ，计算中位数<br>std , 计算标准差<br>var ， 计算方差</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import <span class="hljs-selector-tag">time</span> <br>import math<br>import numpy as np<br><br>x = <span class="hljs-selector-attr">[i * 0.001 for i in np.arange(1000000)]</span><br>start = <span class="hljs-selector-tag">time</span><span class="hljs-selector-class">.clock</span>()<br><span class="hljs-keyword">for</span> <span class="hljs-selector-tag">i</span> , t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(x):<br>    x<span class="hljs-selector-attr">[i]</span> = math<span class="hljs-selector-class">.sin</span>(t)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;math.sin:&#x27;</span>,time.clock()</span></span>-start)<br><br>x = <span class="hljs-selector-attr">[i*0.001 for i in np.arange(100000)]</span><br>x = np<span class="hljs-selector-class">.array</span>(x)<br>start =<span class="hljs-selector-tag">time</span><span class="hljs-selector-class">.clock</span>()<br>np<span class="hljs-selector-class">.sin</span>(x)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;numpy.sin:&#x27;</span>,time.clock()</span></span>-start)<br></code></pre></td></tr></table></figure></li><li><p>广播机制<br>Numpy 中的Universal functional 中要求输入的数组shape是一致的，当数组的shape不相等时，则会使用广播机制。但是使用广播机制需要满足一定的规则，否则将出错。<br>1） 让所有输入数组都向其中的shape最长的数组看齐，不足的部分则通过在前面加1补齐。<br>2） 输出数组的shape是输出数组shape的各个轴上的最大值</p></li></ol><ol start="3"><li>如果输入数组的某个轴和输出数组的对于长度相同或者某个轴的长度为1时，这个数组能被用来计算，否则出错<br>4） 当输入数组的某个轴的长度为1时，沿着此轴运算时都用（或复制）此轴上的第一组值。</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br>A  = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">0</span>,<span class="hljs-number">40</span>,<span class="hljs-number">10</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>)<br>B = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">0</span>,<span class="hljs-number">3</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;A矩阵的形状:&#123;&#125;,B矩阵的形状：&#123;&#125;&#x27;</span>.format(A.shape,B.shape)</span></span>)<br>C = A + B<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;C矩阵的形状：&#123;&#125;&#x27;</span>.format(C.shape)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(C)</span></span><br></code></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Numpy是深度学习的入门库，学习是很重要的。这里只是列举了一些主要内容，如果想要了解更多的内容，可以登录Numpy官网（<a href="http://www.numpy.org/%EF%BC%89%E6%9F%A5%E7%9C%8B%E6%9B%B4%E5%A4%9A%E7%9A%84%E5%86%85%E5%AE%B9%E3%80%82">http://www.Numpy.org/）查看更多的内容。</a></p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="向量和数组之间的关系是什么？向量的定义是什么？"><a href="#向量和数组之间的关系是什么？向量的定义是什么？" class="headerlink" title="向量和数组之间的关系是什么？向量的定义是什么？"></a>向量和数组之间的关系是什么？向量的定义是什么？</h3><p>在数学科物理中，向量被定义为具有大小和方向量。例如速度是一个向量，因为它不仅有大小（数独），还有方向（行进的方向）。<br>数组是编程中的一种基本数据结构，用于存储一组有序的元素。这些元素可以是任何类型，如整形、浮点数、字符串等。<br>标量（scalar）是零维只有大小，没有方向的量，如1，2，3<br>向量（Vector）是一维只有大小和方向的量，如（1，2）。（计算方向的公式为：）<br>矩阵（Matrix）是二维的向量，[[1, 2], [2, 3]]<br>张量（Tensor） 按照任意维排列的一堆数字的推广。矩阵不过是三维张量下的一个二维切面。要在三维张量下找到零维张量需要三个维度的坐标来定位。（注：张量可以是多维的）</p><h3 id="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"><a href="#矩阵是什么，作用是什么？如何实现矩阵的加减乘除" class="headerlink" title="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"></a>矩阵是什么，作用是什么？如何实现矩阵的加减乘除</h3><ol><li>矩阵是一个二维数组，由行和列的元素组成。在数学中，矩阵通常用大写字母表示，如 A，B 等，矩阵中的元素通常用小写字母表示，如aij​，表示矩阵 A 的第 i 行第 j 列的元素。</li><li>矩阵可以用来表示线性变换，解决线性方程组，或者表示图形的变换。在数据科学和机器学习中，矩阵通常用于存储和操作大量的数据。</li></ol><h4 id="实现矩阵的加减乘除。"><a href="#实现矩阵的加减乘除。" class="headerlink" title="实现矩阵的加减乘除。"></a>实现矩阵的加减乘除。</h4><p>加法：两个矩阵相加，只有在它们的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相加的结果。例如，如果A &#x3D; aij 和B &#x3D; bij 是同样大小的矩阵，那么它们的和C &#x3D; [ cij ]是矩阵 ,其中cij &#x3D; aij + bij。对应相加<br>减法：矩阵的减法与加法类似，只有在两个矩阵的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相减的结果。<br>乘法：矩阵的乘法比较复杂。如果A 是一个 m×n 的矩阵，B 是一个n×p 的矩阵，那么它们的乘积 AB 是一个 m×p 的矩阵，其元素由A 的行和 B 的列的对应元素的乘积之和给出。<br>除法：在矩阵中，通常不直接定义除法。但是，我们可以通过乘以逆矩阵来实现类似的效果。如果A是一个可逆的（也就是说，存在一个矩阵 （A-1）使得，A（A-1） &#x3D; （A-1）A &#x3D; I其中 𝐼I 是单位矩阵），那么我们可以定义B&#x2F;A为（BA-1），即是B矩阵除以A矩阵等于B乘以A矩阵的转置。但是，请注意，不是所有的矩阵都是可逆的。 </p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs makefile">import numpy as np<br><br><span class="hljs-comment"># 创建两个矩阵</span><br>A = np.array([[1, 2], [3, 4]])<br>B = np.array([[5, 6], [7, 8]])<br><br><span class="hljs-comment"># 矩阵加法</span><br>C = A + B<br><br><span class="hljs-comment"># 矩阵减法</span><br>D = A - B<br><br><span class="hljs-comment"># 矩阵乘法</span><br>E = np.dot(A, B)<br><br><span class="hljs-comment"># 矩阵除法（通过乘以逆矩阵）</span><br>F = np.dot(A, np.linalg.inv(B)) <br><br></code></pre></td></tr></table></figure><h3 id="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"><a href="#傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）" class="headerlink" title="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"></a>傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）</h3><h4 id="基本介绍。"><a href="#基本介绍。" class="headerlink" title="基本介绍。"></a>基本介绍。</h4><p>傅里叶变换是一种在数学、物理和工程中广泛使用的数学变换，它可以将一个函数或信号从其原始的时间或空间表示转换为频率表示。这对于许多应用都非常有用，因为它可以揭示信号的频率成分，这在原始的时间或空间表示中可能不明显。<br>傅里叶变换的基本思想是，任何函数都可以表示为一系列正弦波和余弦波的叠加。换句话说，我们可以将一个复杂的信号分解为一系列更简单的正弦波和余弦波。</p><h4 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h4><p>傅里叶变换的基本原理是将一个函数或信号从其原始的时间或空间表示转换为频率表示。这是通过将函数表示为一系列正弦波和余弦波的叠加来实现的。<br><img src="/pic/fly1.jpg" alt="傅里叶变换示意图"></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># 创建一个简单的信号</span><br><span class="hljs-attribute">t</span> = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">500</span>)<br><span class="hljs-attribute">f</span> = np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">50</span> * t) + <span class="hljs-number">0</span>.<span class="hljs-number">5</span> * np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">120</span> * t)<br><br><span class="hljs-comment"># 绘制原始信号</span><br><span class="hljs-attribute">plt</span>.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-attribute">plt</span>.plot(t, f)<br><span class="hljs-attribute">plt</span>.title(&#x27;Original Signal&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Time&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Amplitude&#x27;)<br><br><span class="hljs-comment"># 计算傅里叶变换</span><br><span class="hljs-attribute">F</span> = np.fft.fft(f)<br><br><span class="hljs-comment"># 计算频率</span><br><span class="hljs-attribute">freq</span> = np.fft.fftfreq(t.shape[-<span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 绘制频谱</span><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-attribute">plt</span>.plot(freq, np.abs(F))<br><span class="hljs-attribute">plt</span>.title(&#x27;Frequency Spectrum&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Frequency&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Magnitude&#x27;)<br><br><span class="hljs-attribute">plt</span>.tight_layout()<br><span class="hljs-attribute">plt</span>.show()<br><br></code></pre></td></tr></table></figure><h3 id="什么是对象？-封装，继承，多态是什么？"><a href="#什么是对象？-封装，继承，多态是什么？" class="headerlink" title="什么是对象？ 封装，继承，多态是什么？"></a>什么是对象？ 封装，继承，多态是什么？</h3><p>什么是对象？<br>在面向对象编程（Object-Oriented Programming，OOP）中，对象是类的实例。类是一种抽象的概念，用于描述具有相似属性和行为的对象的集合。对象是类的具体实现，它具有类定义的属性和方法。<br>对象可以看作是现实世界中的实体或概念在程序中的表示。每个对象都有自己的状态（属性）和行为（方法），并且可以与其他对象进行交互。</p><p>封装<br>封装是面向对象编程的一种重要概念，它将数据和操作数据的方法捆绑在一起，形成一个称为类的单个实体。封装隐藏了数据的内部实现细节，只暴露对外部可见的接口。这样可以保护数据的完整性，并提供更好的代码组织和维护性。<br>通过封装，对象的内部状态可以被保护起来，只能通过公共接口进行访问和修改。这样可以防止对数据的不合理访问和修改，增加了代码的安全性和可靠性。</p><p>继承<br>继承是面向对象编程中的另一个重要概念，它允许一个类继承另一个类的属性和方法。继承创建了一个类的层次结构，其中一个类（称为子类或派生类）可以从另一个类（称为父类或基类）继承属性和方法。<br>通过继承，子类可以继承父类的特性，并且可以添加自己的特定特性。这样可以实现代码的重用和扩展，减少了重复编写代码的工作量。</p><p>多态<br>多态是面向对象编程中的另一个重要概念，它允许使用统一的接口来处理不同的对象类型。多态性允许同一个方法在不同的对象上产生不同的行为。<br>通过多态，可以编写通用的代码，可以处理多个不同类型的对象，而无需针对每种类型编写特定的代码。这提高了代码的灵活性和可扩展性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 封装示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model</span>):<br>        self.brand = brand<br>        self.model = model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 Car 对象并访问其信息</span><br>my_car = Car(<span class="hljs-string">&quot;Toyota&quot;</span>, <span class="hljs-string">&quot;Corolla&quot;</span>)<br>my_car.display_info()<br><br><span class="hljs-comment"># 继承示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ElectricCar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model, battery_capacity</span>):<br>        <span class="hljs-built_in">super</span>().__init__(brand, model)<br>        self.battery_capacity = battery_capacity<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Electric Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>, Battery Capacity: <span class="hljs-subst">&#123;self.battery_capacity&#125;</span> kWh&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 ElectricCar 对象并访问其信息</span><br>my_electric_car = ElectricCar(<span class="hljs-string">&quot;Tesla&quot;</span>, <span class="hljs-string">&quot;Model S&quot;</span>, <span class="hljs-number">100</span>)<br>my_electric_car.display_info()<br><br><span class="hljs-comment"># 多态示例</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_car_info</span>(<span class="hljs-params">car</span>):<br>    car.display_info()<br><br><span class="hljs-comment"># 使用 show_car_info 函数展示不同类型的车辆信息</span><br>show_car_info(my_car)<br>show_car_info(my_electric_car)<br><br></code></pre></td></tr></table></figure><h3 id="python中的不同代码高亮表示什么？"><a href="#python中的不同代码高亮表示什么？" class="headerlink" title="python中的不同代码高亮表示什么？"></a>python中的不同代码高亮表示什么？</h3><p>在Python的IDLE编程环境中，不同颜色的文本表示不同的含义。以下是IDLE中常见的颜色及其含义：<br>黑色：普通的代码文本。<br>蓝色：关键字，例如if、else、for、while等。<br>绿色：字符串文本。<br>红色：语法错误或代码中的错误。<br>紫色：函数和方法的名称。<br>棕色：数字。<br>橙色：内置函数和模块的名称。<br>灰色：注释。</p><h3 id="怎么对通道数位置的npy数据进行叠加"><a href="#怎么对通道数位置的npy数据进行叠加" class="headerlink" title="怎么对通道数位置的npy数据进行叠加"></a>怎么对通道数位置的npy数据进行叠加</h3><p>思路分析，<br>x &#x3D; np.array([1 ,  2]) # shape为(1,2)<br>y &#x3D; np.array([3 ,  4]) # shape为(1,2)<br>怎么叠加np为[[1 , 2],[3 ,4]] #shape为(2,2)</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">x</span> = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])  # shape为(<span class="hljs-number">2</span>,)<br><span class="hljs-attribute">y</span> = np.array([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])  # shape为(<span class="hljs-number">2</span>,)<br><br><span class="hljs-attribute">z</span> = np.vstack((x, y))  # 叠加x和y，得到z<br><span class="hljs-attribute">print</span>(z)  # 输出结果为[[<span class="hljs-number">1</span> <span class="hljs-number">2</span>]<br>          <span class="hljs-comment">#           [3 4]]</span><br><span class="hljs-attribute">print</span>(z.shape)  # 输出结果为(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br></code></pre></td></tr></table></figure><p>堆栈数组垂直顺序（行）</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">x</span> = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])  # shape为(<span class="hljs-number">2</span>,)<br><span class="hljs-attribute">y</span> = np.array([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])  # shape为(<span class="hljs-number">2</span>,)<br><br><span class="hljs-attribute">z</span> = np.hstack((x, y))  # 堆叠x和y，得到z<br><span class="hljs-attribute">print</span>(z)  # 输出结果为[<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span>]<br><span class="hljs-attribute">print</span>(z.shape)  # 输出结果为(<span class="hljs-number">4</span>,)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch介绍</title>
    <link href="/2024/04/19/deeplearnbook1/"/>
    <url>/2024/04/19/deeplearnbook1/</url>
    
    <content type="html"><![CDATA[<p>在深度学习，要永远抱着学徒的心。<br>本人参考书目为《Python深度学习基于PyTorch》 <a href="http://www.feiguyunai.com/">下载链接</a> <a href="https://github.com/Wumg3000/feiguyunai">使用下载链接——github</a></p><h1 id="目前深度学习的框架有什么？"><a href="#目前深度学习的框架有什么？" class="headerlink" title="目前深度学习的框架有什么？"></a>目前深度学习的框架有什么？</h1><ol><li>TensorFlow ：由Google开发的开源深度学习框架，提供了灵活性和高性能计算能力。TensorFlow 2.x版本引入了更加易用的Keras API作为主要接口。<a href="https://github.com/tensorflow/tensorflow">TensorFlow的github链接</a></li><li>PyTorch ：由Facebook开发的开源深度学习框架，以动态计算图的方式进行建模，易于调试和学习。PyTorch在研究领域广泛应用。<a href="https://github.com/pytorch/pytorch">PyTorch的github链接</a></li><li>Keras：最初作为独立的深度学习框架，现在已经成为TensorFlow的高级API。Keras提供了简单易用的接口，适合快速搭建深度学习模型。 <a href="https://github.com/keras-team/keras">Keras的github链接</a></li><li>MXNet：由Apache软件基金会支持的深度学习框架，具有高度可扩展性和灵活性。MXNet支持动态和静态计算图。[MXNet的github链接]（<a href="https://github.com/apache/mxnet%EF%BC%89">https://github.com/apache/mxnet）</a></li><li>CNTK (Microsoft Cognitive Toolkit)：由微软开发的深度学习框架，提供了高效的性能和多GPU支持。 <a href="https://github.com/microsoft/CNTK">CNTK的github链接</a></li><li>PaddlePaddle（百度飞桨）。这是一个由百度开发的开源深度学习平台，它为深度学习研究人员和开发者提供了丰富的API，支持多种模型结构，可以用来创建各种深度学习模型。[百度飞浆的链接]（<a href="https://www.paddlepaddle.org.cn/%EF%BC%89">https://www.paddlepaddle.org.cn/）</a></li></ol><h1 id="为什么要学习PyTorch？"><a href="#为什么要学习PyTorch？" class="headerlink" title="为什么要学习PyTorch？"></a>为什么要学习PyTorch？</h1><ol><li>pytorch是动态计算图，用法更接近python，并且pytoch与python共同使用了numpy的命令，降低了学习的门槛，比TensorFlow更容易上手</li><li>pytorch需要定义网络层、参数更新等关键步骤，有助于学习深度学习的核心（根据梯度更新参数。）</li><li>pytorch的流行仅次于TensorFlow。在github上的stareed为77.7K （此数据截止到2024&#x2F;4&#x2F;19日）</li><li>pytorch的动态图机制在调试方面非常简单，如果计算图运行出错，马上可以跟踪到问题。pytorch的调试和python一样，可以通过断点检查来解决问题。</li></ol><h1 id="解释一下这本书的结构"><a href="#解释一下这本书的结构" class="headerlink" title="解释一下这本书的结构"></a>解释一下这本书的结构</h1><ol><li>第一部分：介绍深度学习的基石Numpy，介绍PyTorch基础于pytorch构建神经网络的工具箱和数据处理工具。</li><li>第二部分：这本书的核心内容，包括机器学习的流程，常用算法和技巧等内容。实现了基于卷积神经网络的多个视觉处理实例，实现了多个自然语言处理、时间序列方面的实例。介绍了编码器——解码器模型、带注意力的编码器——解码器模型、对抗式生成器以及多种衍生生成器。（注：这里阐述一下关于深度学习、机器学习、人工智能之间的关系。人工智能包含机器学习，机器学习包含深度学习）</li><li>第三部分：实战部分，这部分在介绍相关原理、架构的基础上，使用了pytoch实现了多个深度学习典型实例，比如人脸识别、迁移学习、数据增强、中英文互译、生成式网络实例、模型迁移、强化学习、深度强化学习等实例。</li></ol><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="什么是python的断点检查？"><a href="#什么是python的断点检查？" class="headerlink" title="什么是python的断点检查？"></a>什么是python的断点检查？</h3><h3 id="什么是动态计算图？"><a href="#什么是动态计算图？" class="headerlink" title="什么是动态计算图？"></a>什么是动态计算图？</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔7</title>
    <link href="/2024/04/19/ganwu7/"/>
    <url>/2024/04/19/ganwu7/</url>
    
    <content type="html"><![CDATA[<audio controls>  <source src="https://github.com/changjingzhi/changjingzhi.github.io/blob/master/pic/shueibi7.mp3" type="audio/mp3">  Your browser does not support the audio element.</audio># what is important in life？（生命中什么最重要？）<h3 id="Today-I-can’t-help-but-ask-myself-what-is-most-important-in-life-I-don’t-think-I-can-answer-the-question-because-I-don’t-know-how-to-answer-the-question-But-I-think-I-may-have-a-clue-to-this-problem-thinking-from-my-20-years-of-experience-I-often-worry-about-what-I-don’t-have-but-the-result-of-worrying-is-only-worrying-itself-which-does-nothing-to-change-the-status-quo-or-makes-the-result-worse-I-thought-I-needed-to-get-rid-of-this-worry-so-I-saw-death-I-saw-history-the-flow-of-the-past-the-men-I-have-learned-that-nothing-is-permanent-in-the-face-of-death-and-that-time-diminishes-everything-So-I-saw-the-past-the-future-in-the-past-regret-and-anxiety-about-the-future-saw-the-present-the-past-is-gone-the-future-future-instead-of-falling-into-the-past-regret-and-anxiety-about-the-future-why-not-change-a-mentality-to-spend-the-present-Life-has-no-meaning-instead-of-blindly-looking-for-the-meaning-of-the-castle-in-the-air-it-is-better-to-spend-the-present-with-gratitude-for-the-past-and-hope-for-the-future-Look-to-the-future-based-on-the-present-not-to-worry-about-the-future-the-future-is-not-to-worry-but-to-create-For-the-present-attitude-not-happy-with-things-not-sad-Lief-is-short-kiss-slowly-laugh-insanely-love-truly-and-forgive-quickly"><a href="#Today-I-can’t-help-but-ask-myself-what-is-most-important-in-life-I-don’t-think-I-can-answer-the-question-because-I-don’t-know-how-to-answer-the-question-But-I-think-I-may-have-a-clue-to-this-problem-thinking-from-my-20-years-of-experience-I-often-worry-about-what-I-don’t-have-but-the-result-of-worrying-is-only-worrying-itself-which-does-nothing-to-change-the-status-quo-or-makes-the-result-worse-I-thought-I-needed-to-get-rid-of-this-worry-so-I-saw-death-I-saw-history-the-flow-of-the-past-the-men-I-have-learned-that-nothing-is-permanent-in-the-face-of-death-and-that-time-diminishes-everything-So-I-saw-the-past-the-future-in-the-past-regret-and-anxiety-about-the-future-saw-the-present-the-past-is-gone-the-future-future-instead-of-falling-into-the-past-regret-and-anxiety-about-the-future-why-not-change-a-mentality-to-spend-the-present-Life-has-no-meaning-instead-of-blindly-looking-for-the-meaning-of-the-castle-in-the-air-it-is-better-to-spend-the-present-with-gratitude-for-the-past-and-hope-for-the-future-Look-to-the-future-based-on-the-present-not-to-worry-about-the-future-the-future-is-not-to-worry-but-to-create-For-the-present-attitude-not-happy-with-things-not-sad-Lief-is-short-kiss-slowly-laugh-insanely-love-truly-and-forgive-quickly" class="headerlink" title="Today, I can’t help but ask myself what is most important in life. I don’t think I can answer the question because I don’t know how to answer the question. But I think I may have a clue to this problem, thinking from my 20 years of experience, I often worry about what I don’t have, but the result of worrying is only worrying itself, which does nothing to change the status quo, or makes the result worse. I thought I needed to get rid of this worry, so I saw death, I saw history, the flow of the past, the men. I have learned that nothing is permanent in the face of death, and that time diminishes everything. So I saw the past, the future, in the past regret and anxiety about the future saw the present, the past is gone, the future future, instead of falling into the past regret and anxiety about the future, why not change a mentality to spend the present. Life has no meaning, instead of blindly looking for the meaning of the castle in the air, it is better to spend the present with gratitude for the past and hope for the future. Look to the future based on the present, not to worry about the future, the future is not to worry, but to create. For the present attitude, not happy with things, not sad. Lief is short, kiss slowly, laugh insanely, love truly and forgive quickly."></a>Today, I can’t help but ask myself what is most important in life. I don’t think I can answer the question because I don’t know how to answer the question. But I think I may have a clue to this problem, thinking from my 20 years of experience, I often worry about what I don’t have, but the result of worrying is only worrying itself, which does nothing to change the status quo, or makes the result worse. I thought I needed to get rid of this worry, so I saw death, I saw history, the flow of the past, the men. I have learned that nothing is permanent in the face of death, and that time diminishes everything. So I saw the past, the future, in the past regret and anxiety about the future saw the present, the past is gone, the future future, instead of falling into the past regret and anxiety about the future, why not change a mentality to spend the present. Life has no meaning, instead of blindly looking for the meaning of the castle in the air, it is better to spend the present with gratitude for the past and hope for the future. Look to the future based on the present, not to worry about the future, the future is not to worry, but to create. For the present attitude, not happy with things, not sad. Lief is short, kiss slowly, laugh insanely, love truly and forgive quickly.</h3><h3 id="今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！"><a href="#今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！" class="headerlink" title="今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！"></a>今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！</h3>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《诫子书》</title>
    <link href="/2024/04/18/jzs/"/>
    <url>/2024/04/18/jzs/</url>
    
    <content type="html"><![CDATA[<h1 id="诫子书"><a href="#诫子书" class="headerlink" title="诫子书"></a>诫子书</h1><h2 id="夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。"><a href="#夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。" class="headerlink" title="夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。"></a>夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。</h2><p>自注：从论语中我们可以得到，圣人的任务是将人不知的世界改造成人不愠的世界。而君子最开始是人不知的，从人不知的群体中诞生的，那么怎么才能从人不知的群体中成长为君子。我想戒子书中给出了一个思路。<br>第一要有志向，但是志向不是说凭空随便想一个就行了，它必须要基于自身条件实现的可能性，不然随便设立的志向即无实现的可能，也把自己的精力给浪费了。那么怎么才能有一个好的志向？在我面前看来，不要期许有什么好的志向，先把眼前要做的做好，基于现在，现实，充分认识到现实存在条件，然后一天天的打算，一天天的实现自己的打算，在实践的过程中不断深化自己做事的观点，想法，我想慢慢的根植于自己内心的想法就能变成志向，而且这个志向会更有实现的可能性。（认知来源于实践，理论指导实践。）<br>第二要对事情的实现减少期望感，为什么要这样说？首先我们讨论事情的实现。怎么才能实现一件事情？构成这件事的基本条件满足时，那么这件事情就已经实现了。比如吃饭，吃饭有人，要有饭，构成吃饭的动作，那么吃饭这件事就可以开始了。那么怎么中断吃饭这件事勒？抽取吃饭的条件就行了，将吃饭的人给抽取，吃饭这件事就不能进行了。所以完成一件事很难，因为要满足各种条件，前提条件满足了，那么这件事就可能完成，为什么是可能完成，因为在事情发展的过程中还会存在各种各样的影响因素，使构成事情的基本条件被抽取，这样事情就不能够发展了。站在这个角度上，不对事情抱有期待感是有道理的。<br>第三要认识到积累的重要性，千里之行，始于足下。可能我看到这个人有很多奖项，很多我没有的条件。但是我想我没有看到的是，人家背后的付出，只看到人家怎么怎么样，没有看到人家付出时的艰辛。经济学上讲，要想收获什么，就必须付出什么。时间，金钱，必须要拿自己拥有的去换取自己想要的。为什么这里要谈积累的重要性？举个例子，拿这个博客来说把，最开始肯定是从第一篇开始的，不可能从第n篇开始吧。而这么多博文的内容，其背后必然是时间，知识的积累才能诞生的。</p><p><img src="/pic/jzs1.jpg" alt="路是靠走出来的，不是靠想出来的"></p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习之开篇</title>
    <link href="/2024/04/16/deeplearn1/"/>
    <url>/2024/04/16/deeplearn1/</url>
    
    <content type="html"><![CDATA[<h1 id="为什么要开这个坑"><a href="#为什么要开这个坑" class="headerlink" title="为什么要开这个坑"></a>为什么要开这个坑</h1><ol><li>因为我是人工智能专业的学生</li><li>在学习的过程中起到记录和反思的作用</li><li>单纯想开</li></ol><h1 id="开这个坑，打算怎么填坑"><a href="#开这个坑，打算怎么填坑" class="headerlink" title="开这个坑，打算怎么填坑"></a>开这个坑，打算怎么填坑</h1><p>慢慢填呗，图难于其易，为大于其细。天下难事必作于易，天下大事必作于细。是以圣人终不为大，故能成其大。</p><h1 id="这个坑的流程是什么？"><a href="#这个坑的流程是什么？" class="headerlink" title="这个坑的流程是什么？"></a>这个坑的流程是什么？</h1><ol><li>先讲pytorch</li><li>基于pytorch实现一些经典网络，完成一些案例</li><li>看论文，介绍一些经典的论文，比如AlexNet，VGG，CNN，GoolgeNet，Unet，shuffleNet</li><li>开始跑yolo，更改网络模块。</li></ol><p>注： 这里只是一个流程，在开始后会有更多的坑需要去填的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>峨眉山</title>
    <link href="/2024/04/14/tupian2/"/>
    <url>/2024/04/14/tupian2/</url>
    
    <content type="html"><![CDATA[<h1 id="清晨的树"><a href="#清晨的树" class="headerlink" title="清晨的树"></a>清晨的树</h1><p><img src="/pic/e1.jpg"><br><img src="/pic/e2.jpg"><br><img src="/pic/e3.jpg"><br><img src="/pic/e4.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>图片</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论语</title>
    <link href="/2024/04/14/luwnyu/"/>
    <url>/2024/04/14/luwnyu/</url>
    
    <content type="html"><![CDATA[<p>注： 本博客参考的论语注解来源于缠中说禅，仅作为学习使用。<br>论语不仅是一本修身的学说，更是一本治世的学说。<br>修身，齐家，平天下。<br>修身（定、静、安、虑、得）</p><h1 id="第一句"><a href="#第一句" class="headerlink" title="第一句"></a>第一句</h1><ol><li>子曰：学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？</li></ol><p>问：什么是学？<br>答：闻“圣人之道”、见“圣人之道”、“对照”“圣人”、在现实社会中不断地“校对”。<br>问：谁学？<br>答：君子。<br>问：学什么？<br>答：成“圣人”之道。<br>问：学了能成什么？<br>答：“圣人”。</p><p>闻，见，学，行<br>闻圣人之道，见圣人之道，学圣人之道，行圣人之道<br>行“圣人之道”的人就是要使得“不知之人”变得“不愠”，使得“不知之世界”变得“不愠”。<br>人不知，人不相，人不愠。</p><p>2024&#x2F;5&#x2F;2注： 人不知：有一种人，不愿意付出努力，希望坐享其成，或者通过捷径来取得成果，即使知道努力是通往外界的必经因素，人的天性是懒惰的，通过自律或者其他外界因素才能让自己足够努力。在这种情况下看到他人付出努力取得成功了，为了让自己不显得那么失败，于是就寻找虚幻的优越感，例如努力的人是因为智商不够才需要努力（或者因为别的不足才需要努力），虚构出别人不如自己的假象，人并不关心真相，只关心如何让自己获得优越感，如何维持自己的傲慢。</p><p>注:人不相，见相非相，即见如来。见性，见智，凡所有相，皆是虚妄，若见诸相非相，既见如来。世界呈现在我们面前的是表象，非相才是本质。一切的现象都是由非相这个条件构成的，非相指一切事物形成的条件，肉眼是看不到的，什么条件产生什么结果，这个叫因果，这个因果本身的来源无从探究。要达到看清事物的本质的境界，首先向内求解，看到自己的本性，清楚自己的条件，这就是觉性，又可称为自我觉醒。之后再通过自己的觉性的智慧看待事物，做符合条件的事物及发展规律的事情，才可能达到与之相应的结果。若执着于事物的表象，不断在这个世界里沉迷见色（色：包括但不限于，酒、色、财、气），一旦执着于这些表现，当然会着像、着礼（人情世俗中的世俗），开始执着于礼的形式，在这种形式下产生各种各样的观点，立场，分别，执着。源于各种各样的概念，这些概念又将事物划分成了各种各样的等级，人的观念自然出现了各种各样鉴别，亲疏，好恶，从而相，而忽视了本质，一直处于人不知，达不到人不相的境界了，。</p><h1 id="第二句"><a href="#第二句" class="headerlink" title="第二句"></a>第二句</h1><ol start="2"><li>子曰：朝闻道夕死，可矣！<br>子曰： 朝闻，夕死。可矣！</li></ol><p>君子慎独，一旦开始走上圣人的道路，就要严格的要求自己。无论什么情况，与大家在一起时与自己一个人在一起时所表现出来的状态是一样的。<br>解释：君子从“闻其道”开始，无论任何地方，无论条件恶劣还是优越，甚至出生入死，都要不断地“固守”，“承担”“圣人之道”之行直到最终成就“不愠的世界”而不退转，只有这样，才可以行“圣人之道”呀。</p><p>自注： 立志走上成为圣人的这条道路上，就要时时刻刻牢记自己内心的要求，以现实为标准。时时刻刻的固守，达到内圣外王的境界。</p><h1 id="第三句"><a href="#第三句" class="headerlink" title="第三句"></a>第三句</h1><ol start="3"><li>子在川上曰：逝者如斯夫，不舍昼夜。</li></ol><p>解释：孔子在河流的源头，抚今追昔、满怀感慨，自告且忠告所有决心开始“见、学、行”“圣人之道”的君子：“立志“见、学、行”“圣人之道”的君子，就要像这江水一样，从“闻其道”的源头开始，后浪推前浪，生生不息、前赴后继，无论任何时候、任何地方，无论条件恶劣还是优越，甚至出生入死，都要不断地“固守”，“承担”“圣人之道”之行直到最终成就“不愠的世界”而不退转。”这里必须明确，这话既是孔子自己的感慨，也是对所有有志于圣人之道的人的忠告和勉励。</p><h1 id="第四句"><a href="#第四句" class="headerlink" title="第四句"></a>第四句</h1><ol start="4"><li>子曰：人能弘道，非道弘人。<br>就是“道”不是目的，只有“人”才是目的，只有现实中的“人”才是目的，一切以打着虚无飘渺的所谓“道”为目的，以现实的“人”为手段的所谓“闻、见、学、行”“圣人之道”，都是《论语》背道而驰的。</li></ol><p>注：孔子说：“人能够把道发扬光大，不是道能把人发扬光大。”</p><h1 id="第五句"><a href="#第五句" class="headerlink" title="第五句"></a>第五句</h1><ol start="5"><li>子曰：攻乎异端，斯害也己。</li></ol><p>对于行“圣人之道”的君子，“异端”只不过是“别为一端行非圣人之道”的“不知”者，如果没有这种人，“圣人之道”之行就成了无源之水。“不知”，如同米；“不愠”，如同饭；<br>在我看来，这句话更像是说要允许矛盾的存在，不能只允许一种情况的存在。毕竟按照矛盾的观点来看，正是矛盾才使事物能够不断地发展。圣人也是从不知者中诞生的，如果不允许不知者的话那么就不可能诞生知者了。<br>那么问题来了，圣人的作用是什么？人不知，人不愠。天下大同。对于“别为一端行非圣人之道”的“不知”者，行“圣人之道”的君子不是要攻打他们、消灭他们，而是要如把“米”煮成“饭”般把他们从“不知”者变成“不愠”者，变成行“圣人之道”的君子，把“不知”的世界变成“不愠”的世界，只有这样，才算是真行“圣人之道”。</p><h1 id="第六句"><a href="#第六句" class="headerlink" title="第六句"></a>第六句</h1><ol start="6"><li>子曰：道，不同、不相为谋。<br>子曰： 道，不同，不相为谋。<br>不相： 不以外在为判断条件，要看其本质。类似于见相非相，即见如来（此中有真意，欲辩以忘言）<br>解释：<br>道，圣人之道，就如同大河，大河是不会去“选择”的、也不会去强迫“一致”，是“不相”、“不同”的。 “圣人之道”之“谋”，就是“不同”、“不相”。最常见的以“相”相之就是所谓的“以貌取人”，延伸下去，根据思想、观点、意识形态、经济水平等等，都是以“相”相之，都不是“不相”，是和“圣人之道”相违的。</li></ol><p>注：“你得允许一部分人先高雅起来，一部分人后高雅起来，一部分人怎么也高雅不起来。”</p><h1 id="第七句"><a href="#第七句" class="headerlink" title="第七句"></a>第七句</h1><ol start="7"><li>子曰：有教无类。<br>自注：学问就一定有好坏之分吗？什么是检验的标准，实践是检验真理的唯一标准。现实是检验真理的唯一标准。</li></ol><h1 id="第八句"><a href="#第八句" class="headerlink" title="第八句"></a>第八句</h1><ol start="8"><li>子曰：士志於道，而耻恶衣恶食者，未足与议也！<br>注：“耻恶衣恶食者”，就是“相”如果一个人，立志要行“圣人之道”，却把人分为“好衣好食”、“恶衣恶食”两类人，也就是以贫富划分人，而选择以“恶衣恶食”也就是穷人为耻，远离他们，那这种人谈论的“圣人之道”只是羊头狗肉的勾当。为什么？因为他不能“不相”。</li></ol><h1 id="第九句"><a href="#第九句" class="headerlink" title="第九句"></a>第九句</h1><ol start="9"><li>子曰：贤哉，回也！一箪食，一瓢饮，在陋巷，人不堪其忧，回也不改其乐。贤哉，回也！<br>自注：真评和加贫。真有本事和假有本事。<br>颜回这个“安贫乐道”的典型，并不是故意去“贫”，并不是故意要“恶衣恶食”，也不是如某些宗教所教唆的故意去苦行，这些都是严重地“相”了，这些都是和君子谋“圣人之道”所必须坚持的“不相”原则背道而驰的。</li></ol><h1 id="第十句"><a href="#第十句" class="headerlink" title="第十句"></a>第十句</h1><ol start="10"><li>子曰：贫而无怨难；富而无骄易。</li></ol><p>这句有多种解释：第一种，从人不知的社会到人不相的社会，过程中，贫穷而不怨恨是困难的，富贵而不骄横是容易的。<br>问题是贫穷真的会使人怨恨吗？富贵真的会使人不骄横吗？在人不知的社会中（最初始的人不知），现实却又是穷人经常乐呵呵，富人却骄横无理。（穷乐呵，为富不仁。）<br>第二种解释，在任何“人不知”的社会中，都体现为“贫而怨难；富而骄易。</p><h1 id="第十一句"><a href="#第十一句" class="headerlink" title="第十一句"></a>第十一句</h1><ol start="11"><li>子贡曰：“贫而无谄，富而无骄，何如？”子曰：“可也；未若贫而乐，富而好礼者也。”</li></ol><p>“贫而谄”不得，最终就会“贫而怨难”，因“怨”有“仇”而“敌对”甚至“造反”，但“造反”成功的马上又成为“富而骄”，又有新的“贫而谄”，结果不断循环，都逃不出这个“贫而谄，富而骄”的“人不知”社会。<br>儒家看穿了这个“贫而谄，富而骄”的恶性循环，知道在这里打圈圈是没用的，而要打破这个恶性循环的办法，只有通过“人不相”而达到“人不愠”，最终摆脱“贫而谄，富而骄”的“人不知”的恶性循环。要实现这个打破，首先就要实现“贫而无谄，富而无骄”的“人不相”，为此，就必须要实现对“贫富”之相的“不相”，达到“人不相”。为什么实现对“贫富”之相的“不相”，就能实现“人不相”？是因为只要存在人与人的地方，就必然会出现各种方面的“贫富”之相，消灭这种“贫富”之相、将之抹平是不可能的，唯一办法就是使之“不相”，使得各种“贫富”之相能平等地存在，实现其“不同”，容纳各种“不同”而成其大，最终成就其“大同”。儒家、《论语》认为，这种“大同社会”的实现是当下的，是可以现世实现的，这种看法是由儒家的入世以及现世精神所决定的。<br>就像天幕红尘中说的，你要允许你得允许一部分人先高雅起来，一部分人后高雅起来，一部分人怎么也高雅不起来。” </p><h1 id="第十二句"><a href="#第十二句" class="headerlink" title="第十二句"></a>第十二句</h1><ol start="12"><li>子曰：齐一变，至於鲁；鲁一变，至於道。<br>在孔子时代是打着以“仁”以“德”治国的典型，号称传承着被孔子当成典范的周公之仁德。以“仁”以“德”治国，强调善的力量，对于一个习惯于以恶为前提的“人不知”世界是不可想象的，相比“齐式”国家模式，“鲁式”国家模式的出现是一种进步，所以才有“齐一变，至於鲁”的说法。</li></ol><h1 id="第十三句"><a href="#第十三句" class="headerlink" title="第十三句"></a>第十三句</h1><ol start="13"><li>子曰：放于利而行，多怨。<br>“放于利而行，多怨。”就是无论放弃还是放纵“利”而行，都会产生“多怨”的结果。 其实，现在的人对于这句话，肯定会更容易理解。计划经济年代，都是放弃“利”而行，结果是“多怨”；而市场经济年代，放纵“利”而行，结果还是“多怨”。这“人不知”社会的总规律，绝对不能放弃或放纵“利”而行，要充分把握其“利”，所谓用其刃而不被其刃所伤。行“圣人之道”的君子，首先要是“知人”，如果自己都还“不知”，又如何去让“人不知”之相“不相”？一事不知，儒者之耻，不尽量用这世界上的知识武装自己，是没资格当儒者的。</li></ol><h1 id="第十四句"><a href="#第十四句" class="headerlink" title="第十四句"></a>第十四句</h1><ol start="14"><li>子曰：好勇疾贫，乱也。人而不仁，疾之已甚，乱也。<br>“人不知”社会中同时存在的两种乱相：“贫者”，好勇斗狠；“富者”，为富不仁，被过分享乐之病急速传染，所谓纸醉金迷、醉生梦死。在“人不知”的社会，单纯的道德说教是没意义的，在“利”面前，所有的道德说教都苍白无力。这种“利”的“贫富”之相的严重对立，使得“富者”因得其“利”而放纵无度，而“贫者”因不得其“利”而不平。就算是一个懦夫，当“利”的“贫富”之相严重对立形成的落差储备到了足够大势能后，懦夫也会成为“勇夫”的。这样，自然就有了“好勇疾贫，乱也。人而不仁，疾之已甚，乱也。”。这种图景在“人不知”的社会随处可见、无处不在，《论语》早在两千多年前就已总结出来了。</li></ol><h1 id="第十五句"><a href="#第十五句" class="headerlink" title="第十五句"></a>第十五句</h1><ol start="15"><li>子曰：善人为邦百年，亦可以胜残去杀矣。诚哉是言也！<br>“胜残”、“去杀”，是两个意思相仿的词并列而成，简单说就是“战胜残暴、制止杀戮”；“善人”，就是“使人善”，“善”就是好的意思。“善人”和“胜残去杀”，其并列是一体的，如两腋之于人，双翼之于鸟，钱币的两面之于钱币。“善人、胜残去杀”，才可能“为邦百年”，让国家长治久安。“胜残去杀”，是针对“人而不仁，疾之已甚”，是针对为富不仁的“富者”，包括贼王暴君、贪官污吏、奸商恶霸等等，所谓杀一暴君而救亿万者乃真大仁矣；“善人”，是针对“好勇疾贫”的“贫者”，改善他们的生存条件、扩展他们的生存空间、提高他们的生存能力等等，都可以归之于“善人”之数。但必须强调的是，站在人和社会的整体角度，没有一个人是在任何方面都是“富”者，也没有一个人在任何方面都是“贫”者，但对于现实中的国家来说，经济、社会地位、权力等角度的“贫富”之相才最具有现实力量，这点也是不能忽视的。</li></ol><h1 id="第十六句"><a href="#第十六句" class="headerlink" title="第十六句"></a>第十六句</h1><ol start="16"><li>子曰：如有王者，必世而后仁。<br>一般来解答这一句为大致意思就成了“如果有称王的，一定要经过一世三十年，才能行其仁政。”国家长治久安的六字箴言“善人、胜残去杀”，而现实中，在“人不知”世界里，这六字箴言又有几人能办到？办不到，就必然是“城头变换大王旗”，中国历史上，这种改朝换代的事情，难道还不司空见惯？这种恶性循环中，有一个规律，就是本章的“如有王者，必世而后仁。”“王”，不一定需要有人当皇帝，如资本主义的确立也是一种王，其后到处贩卖的“民主、自由”就是“必世而后仁”了。</li></ol><h1 id="第十七句"><a href="#第十七句" class="headerlink" title="第十七句"></a>第十七句</h1><ol start="17"><li>子适卫，冉有仆。子曰：“庶矣哉！”冉有曰：“既庶矣，又何加焉？”曰：“富之。”曰：“既富矣，又何加焉？”曰：“教之。”<br>不同的社会，有不同的“庶、富、教”发展程度。而“全面发展的自由人的联合体”，就是“庶、富、教”充分发展所呈现的面貌。只有自由人，才会有多样性，才会有“不相”而“不同”，才有真正的“庶”；只有全面发展，才有真正的“富”；由“庶”而“富”，充分发展而形成“全面发展的自由人的联合体”所构成的社会结构，这才是真正的“教”。“庶、富、教”，就是不同成其大而大同。“庶、富”的发展水平，决定了“教”的发展水平，只有“庶、富”充分发展，才有“教”的充分发展，“庶、富”对应的是“人不相”，而“教”的充分发展最终对应的就是“人不愠”，只有“全面发展的自由人的联合体”构成的“教”，才是构成“人不愠”世界的社会结构基础。而只有“人不愠”，才是真正的“善人”。</li></ol><h1 id="第十八句"><a href="#第十八句" class="headerlink" title="第十八句"></a>第十八句</h1><ol start="18"><li>子曰：善人、教民七年，亦可以即戎矣。<br>通常解释为孔子说：“善人教导训练百姓七年时间，就可以叫他们去作战了”。“善人教民七年，亦可以即戎矣。”的通常断句是错的，应该是“善人、教民七年，亦可以即戎矣。”这一章是在彰显“善人”之道的力量，“教”的力量，文明的力量。“善人”之道，就是“圣人之道”一个具体过程中体现的具体形式，“圣人之道”最终要使得“人不知”的世界变成“人不愠”的世界，当然需要融合、同化那些未开化的、文明程度比较低的人、民族和国家，这是“人不知”世界一个很大的组成部分。如果说上一章更侧重于“善人”之道在国家范围的应用，那这一章就指出，“善人”之道在全世界实现的必然性，而只有在全世界的实现，才算真正的“善人”之道。孔子认为，作为“圣人之道”低级阶段的“善人”之道的实现也只能是一个全球性事件，大同，只能是大同天下，而不可能是某一国的大同，必然要“即戎”而达到天下大同。</li></ol><h1 id="第十九句"><a href="#第十九句" class="headerlink" title="第十九句"></a>第十九句</h1><ol start="19"><li>子曰：以不教民战，是谓弃之。<br>通常解释为“以不教民战”解释成“用不经教练的民众去临战阵”孔子说：“用没有经过军事训练的老百姓去打仗，这是有意让他们去送死。”。滑稽，断句应为，以不教，民战，是谓弃之。不行“善人”之道，那只能用“残、杀”，用所谓的白色恐怖来压制，企图让人民战栗、恐惧而治理国家。用“残、杀”企图使民众战栗、恐惧而治理国家的，就是遗弃、背叛民众，而最终也将被民众所遗弃。</li></ol><h1 id="第二十句"><a href="#第二十句" class="headerlink" title="第二十句"></a>第二十句</h1><ol start="20"><li>哀公问社於宰我。宰我对曰：“夏后氏以松，殷人以柏，周人以栗，曰，使民战栗。”子闻之，曰：“成事不说，遂事不谏，既往不咎。”<br>鲁哀公向孔子的弟子宰我问“土地神的祭祀”，宰我自作聪明道：“夏代用松木，殷代用柏木，而周代用栗木是为了借谐音使民战栗。”孔子听到，就告戒：“正成的事不要妄加评议，即成的事就不要徒劳劝告，已成的事就不要再生灾祸。”<br>“成事”，不是指已成的事，而是指正成的事，也就是在萌芽状态的，这时候，还需要观察，不能妄加评议，胡乱定性；“遂事”，马上就要成的事，已经无可挽回的，就不要徒费口舌去劝告了，这样只能产生怨恨；“既往”，已经过去的已成的事，要“不咎”，“咎”的本义是灾祸，已经成的事，如果错了，就不要错上加错，再生灾祸。这句话针对事物发展的三个不同阶段应该采取的态度。</li></ol><h1 id="第二十一句"><a href="#第二十一句" class="headerlink" title="第二十一句"></a>第二十一句</h1><ol start="21"><li>子曰：夷狄之有君、不如，诸夏之亡也。<br>“夷狄之有君、不如，诸夏之亡也。”的意思是：未开化的、文明程度比较低的人、民族和国家，虽然有他们自己的国体、政体，但由于没有遵从、依照文明程度比较高的人、民族和国家的政体、国体，而被后者所轻视。只要有不同的人、民族、国家同时存在，就必然有“诸夏”、“夷狄”之分，对于民族、国家来说，任何不行“圣人之道”的，无论是“齐式”的“王霸之道”还是“鲁式”的“仁德”之道，都必然会有“先进”对“落后”的轻视、压榨。一个国家、民族，如果不行“善人”之道，用“残、杀”企图让别国、别的民族战栗、恐惧而治理世界，就是遗弃、背叛各国、各民族，而最终也将被各国、各民族所遗弃。一个现成的例子，就是美国</li></ol><h1 id="第二十二句"><a href="#第二十二句" class="headerlink" title="第二十二句"></a>第二十二句</h1><ol start="22"><li>子曰：为政以德譬，如北辰居其所而众星共之。<br>这一章，其实就是上一章所说““圣人之道”、“善人之道”是大道，更是现实之道，无位可本，又何来“本位”？正因为无位可本，才可以无所位而生其本、无所本而生其位。这，才是真正的大道、现实之道。”的进一步展开。何谓“为政以德譬，如北辰居其所而众星共之。”？就是“无所位而生其本、无所本而生其位”。 只有明白了这句话，才可能真正明白马克思意义上的“具体问题具体分析”，也才可能真正明白何谓“为政以德譬，如北辰居其所而众星共之”。当人把北极星的位置确定后，执持这位置相应就可以定出其他星星位置；当人从现实出发分析把握了现实关系的逻辑结构后，“孰敢不正？</li></ol><h1 id="第二十三句"><a href="#第二十三句" class="headerlink" title="第二十三句"></a>第二十三句</h1><ol start="23"><li>季康子问政於孔子，孔子对曰：政者，正也，子帅以正，孰敢不正？<br>季康子，鲁国大夫，向孔子问政。“政者，正也”，为政，就是要立行“圣人之道”而成就之这一逻辑支点；“子帅以正，孰敢不正？”为政的人，遵循现实的逻辑，从现实出发，行“圣人之道”而成就之，其它问题就会以此为基础相应地找到解决的办法。这里必须要明确的是，现实，是最底层的支点，行“圣人之道”而成就之这个逻辑支点必须也必然在现实支点之上，离开现实，无所谓“圣人之道”。“圣人之道”，不是离开现实的乌托邦，那种把“圣人之道”装扮成某种口号、旗帜、目标，以此而驱使人，让人为此而折腾，都和“圣人之道”、《论语》、孔子毫无关系。人不是现实的奴隶，现实必须是人参与其中的，没有了人，也无所谓现实，更无所谓现实逻辑。 现实之于人，按其逻辑，有着各种不同的选择，究竟如何去选择，就构成了各色各样的政治。各种政治结构的逻辑支点，都来自现实，这逻辑支点也如同北极星，一旦确立，其它就以此为基础相应地构建。</li></ol><h1 id="第二十四句"><a href="#第二十四句" class="headerlink" title="第二十四句"></a>第二十四句</h1><ol start="24"><li>子曰：不在其位，不谋其政。<br>“不在其位，不谋其政”，就是“不谋不在其位之政”，不谋划与现实变化的位次不符的政事、政治关系、政治制度、上层建筑、生产关系等等。一切都从也只能从现实出发，现实在什么阶段，什么位次，是必须首要分析的问题。</li></ol><h1 id="第二十五句"><a href="#第二十五句" class="headerlink" title="第二十五句"></a>第二十五句</h1><ol start="25"><li>子曰：“不在其位，不谋其政。”曾子曰：“君子思不出其位。”</li></ol><h1 id="第二十六句"><a href="#第二十六句" class="headerlink" title="第二十六句"></a>第二十六句</h1><ol start="26"><li>子曰：“不患，无位；患，所以立。不患，莫己知求，为可知也。”<br>“无所位而生其本、无所本而生其位”，即所“立”、即所“止”、即所“位”。有所“立”，则“立”其“有”，其“有”必有其“位”<br>儒家，内圣、外王，“不在其位，不谋其政”的外王，是和“不患，无位；患，所以立。不患，莫己知求，为可知也。”的内圣相互相成的。这是参悟儒家之说的大关键。</li></ol><p>缠中说禅白话直译<br>子曰：“不患，无位；患，所以立。不患，莫己知求，为可知也。”<br>孔子说：“不患”，无位次；“患”，以“不患”的“无位次”而“位次”。“不患”，不以自己“所知”来选择，就是“能知”。</p><h1 id="第二十七句"><a href="#第二十七句" class="headerlink" title="第二十七句"></a>第二十七句</h1><ol start="27"><li>子曰：不患人之不己知；患其不能也。<br>通译：不要担心别人不了解自己，应该担心的是自己不了解别人。<br>正因为“不明了”的现实，所以才有了自己不断“明了”自己的可能，所以才有了“明了”的可能，不明白这一点，是不可能明白何谓“内圣”的。</li></ol><p>缠中说禅白话直译<br>子曰：“不患人之不己知；患其不能也。”<br>孔子说：不患别人或自己不明了自己，患别人或自己不能明了自己啊。</p><h1 id="第二十八句"><a href="#第二十八句" class="headerlink" title="第二十八句"></a>第二十八句</h1><ol start="28"><li>子曰：“不患人之不己知；患不知人也。”<br>通译：“不要担心别人不了解自己，应该担心的是自己不了解别人。”<br>缠中说禅白话直译<br>子曰：“不患人之不己知；患不知人也。”<br>孔子说：不患人不明了自己，患“人不知”的世界啊。</li></ol><h1 id="第二十九句"><a href="#第二十九句" class="headerlink" title="第二十九句"></a>第二十九句</h1><ol start="29"><li>子曰：“性相，近也；习相，远也。”</li></ol><p>缠中说禅白话直译<br>子曰：性相，近也；习相，远也。<br>孔子说：以性性相，缠附呀；以习习相，深奥啊。</p><h1 id="第三十句"><a href="#第三十句" class="headerlink" title="第三十句"></a>第三十句</h1><ol start="30"><li>子曰：人无远虑，必有近忧。<br>“远”，深远、深奥，同于“习相，远也”，和“习相”相关，脱离“习相”无所谓深远、深奥，不过幻想而已。“习相”，先要明其“相”，明其“相”必先明其“相”之位次，明其“相”之位次，必对其“相”的当下逻辑关系有一明确把握。<br>“虑”，审察、思虑、谋划。“虑”，不是哈姆雷特式的，而是审察、思虑、谋划的统一，三者缺一不可，而最终必须落在行动上，没有行动的“虑”也不过是幻想而已。<br>人的行为，必须从其苗头下手，不想吃恶果，最简单的方法就是不要种下其种子，忧患、祸患的种子一旦缠附，一有机会就会萌芽，就要结果。别以为可以用任何方法可以消除这种子，种子一旦种下就是无位次的，准确说，相对于现实系统来说，种子是无位次的，任何现实的把戏都消灭不了种子，种子不一定在眼前发芽，但不发芽只是机会不成熟，一旦成熟，逃都逃不掉，眼前看不到、没迹象的忧患、祸患，往往才是致命的。而这，才是真正的“近忧”。</li></ol><p>缠中说禅白话直译<br>子曰：人无远虑，必有近忧。<br>孔子说：人没有深远的审察、思虑、谋划，必然缠附祸患。</p><h1 id="第三十一句"><a href="#第三十一句" class="headerlink" title="第三十一句"></a>第三十一句</h1><ol start="31"><li>子曰：众，恶之，必察焉；众，好之，必察焉。</li></ol><p>缠中说禅白话直译<br>子曰：众，恶之，必察焉；众，好之，必察焉。<br>孔子说：一切现象，当被认为是恶的就会被厌恶，对此必须摈弃一切厌恶当下直观；一切现象，当被认为是好的就会被喜好，对此必须摈弃一切喜好当下直观。（恶并不是恶，好的并不一定是好的。）</p><h1 id="第三十二句"><a href="#第三十二句" class="headerlink" title="第三十二句"></a>第三十二句</h1><ol start="32"><li>子曰：视，其所以；观，其所由；察，其所安。人焉廋哉？人焉廋哉？<br>视”，人与认识对象之间的看，相当于感性以及康德规定性判断力所连接的知性与理性所构成的高级人类认识能力，也就是人类所有的认识能力；“观”，看法，相当于“反思判断力”所连接的自由意志；“察”，当下的直“观”，是自由意志的当下实践。“视，其所以”，认识能力是人所凭借的；</li></ol><p>缠中说禅白话直译<br>子曰：视，其所以；观，其所由；察，其所安。人焉廋哉？人焉廋哉？<br>孔子说：认识能力，人的凭借；自由意志，人的遵从；当下直 “观”，即自由意志的当下实践，人的归依。人，哪里有隈曲啊？人，哪里有隈曲啊？</p><h1 id="第三十三句"><a href="#第三十三句" class="headerlink" title="第三十三句"></a>第三十三句</h1><ol start="33"><li>子曰：不知，命无以为君子也；不知，礼无以立也；不知，言无以知人也。</li></ol><p>缠中说禅白话直译<br>子曰：不知，命无以为君子也。不知，礼无以立也。不知，言无以知人也。<br>孔子说：没有智慧，不可能承担君子的使命；没有智慧，不可能建立社会正常的秩序；没有智慧，不可能产生使人智慧的言论。</p><h1 id="第三十四句"><a href="#第三十四句" class="headerlink" title="第三十四句"></a>第三十四句</h1><ol start="34"><li>子曰：由知、德者，鲜矣！<br>圣人之道”，就是将“人不知”的世界变为“人不愠”世界的道路，这里没有任何固定的模式和先验的走法，路是人走出来的，是人所“由”而来，是人所“蹈行，践履”而来。没有人的“蹈行，践履”，何来路？除了“知、德”，行“圣人之道”的君子无所“蹈行，践履”也无须“蹈行，践履”。</li></ol><p>缠中说禅白话直译<br>子曰：由知、德者，鲜矣！<br>孔子说：蹈行、践履“闻、见、学、行”“圣人之道”智慧、所得的君子，永远处在创新、创造之中啊。</p><h1 id="第三十五句"><a href="#第三十五句" class="headerlink" title="第三十五句"></a>第三十五句</h1><ol start="35"><li>子曰：民可，使由之；不可，使知之。</li></ol><p>缠中说禅白话直译<br>子曰：民可，使由之；不可，使知之。<br>孔子说：民众当下适合的，放任民众去蹈行、践履；民众当下不适合的，放任民众运用智慧去创造、创新。</p><h1 id="第三十六句"><a href="#第三十六句" class="headerlink" title="第三十六句"></a>第三十六句</h1><ol start="36"><li>子曰：由诲女，知之乎！知之为，知之；不知为，不知；是知也！</li></ol><p>缠中说禅白话直译<br>子曰：由诲女，知之乎！知之为，知之；不知为，不知；是知也。<br>孔子说：实践教导你，以此而有智慧啊。依智慧而进一步实践，以此而有新的智慧；不依以实践而有的智慧进一步实践，就不会有新的智慧。这，就是最根本的智慧。</p><p>自注： 认识来源于实践，从无知到有一定的感性认识，再从感性认识到一定的理性认识。认识是一个过程，不可以说是直接没有感性认识就产生了理性认识。在认识的过程中当然时时刻刻伴随着实践，正式有了实践才让认识不断深化。</p><h1 id="第三十七句"><a href="#第三十七句" class="headerlink" title="第三十七句"></a>第三十七句</h1><ol start="37"><li>子曰：我非生而知之者，好古，敏以求之者也。<br>本章，孔子提出了学习前人知识、智慧的三个步骤：好、敏、求。 首先，对前人知识、智慧所凝结成的遗典、典章等必须尊重、善待进而学习、研究，才谈得上“好”。尊重、善待进而学习、研究，真正把握以后，还需要在实践中继续印证，这才是“敏”。“敏”，有两层的含义：其一，前人知识、智慧都来源于其当下的实践，而时代变化了，条件变化了，其应用可能要失效，可能有所改变，这必须在实践中才能印证、发现；其二，对前人知识、智慧的把握，特别对于那些洞穿时间的智慧的把握，必须在实践中慢慢体会、摸索，才能发现前人的真义，决不能像某些人对待孔子、马克思那样，根本没弄明白就扮代表，这样是谈不上“好”，更谈不上“敏”了。有了印证，自然就有了选择的基础，选择不是机械地挑选，不是用对错等简单标准来划分，而是根据当下的实践有机地发展、延伸，这样才不辜负古人，也不辜负自己，这才算得上是“求”。</li></ol><p>缠中说禅白话直译<br>子曰：我非生而知之者，好古，敏以求之者也。<br>孔子说：我不是天生、先验地依赖天生、先验而有智慧的人，只是爱好学习、研究先哲遗典、古代典章，并在实践中对此印证、选择的人。</p><p>自注： 这句话在讲前人的经验可以学习，但是不能直接使用。为什么这么说，因为条件改变了，当时这个方法能解决这个问题，那是因为由实践这个方法的条件，而现在没有满足这个方法的条件，所以这个问题使用这个方法就解决不了。一句话，实事求是。那么有没有普世的方法？答案是没有的，但是有普世的法则，内心有内心的心法，社会有社会的法则。熟悉和了解这些法则的过程中必然要经历大量的实践，才能认识到。但是即使认识到这些法则，能不能熟练的用于也是一件困难的事情，知之为知之，不知为不知。迎接挑战，这样的人生才有意义。征服一个又一个巅峰，达到内圣外王的境界。</p><h1 id="第三十八句"><a href="#第三十八句" class="headerlink" title="第三十八句"></a>第三十八句</h1><ol start="38"><li>孔子曰∶生而知之者，上也；学而知之者，次也；困而学之，又其次也。困而不学，民斯为下矣！<br>缠中说禅白话直译<br>孔子曰∶生而知之者，上也；学而知之者，次也；困而学之，又其次也。困而不学，民斯为下矣！<br>孔子说：所有人，天生地依赖天生而有智慧，是最好的；所有人，都能自由地学习且通过学习而有智慧，是稍差的；所有人，被分为不同类别而得到不同类别的学习，是更差的。所有人，被分为不同类别而某类人得不到学习的机会，这就是民众被当成卑下的原因啊。</li></ol><h1 id="第三十九句"><a href="#第三十九句" class="headerlink" title="第三十九句"></a>第三十九句</h1><ol start="39"><li>子曰：盖有不知而作之者，我无是也。多闻，择其善者而从之；多见而识之；知之次也。</li></ol><p>缠中说禅白话直译<br>子曰∶盖有不知而作之者，我无是也。多闻，择其善者而从之；多见而识之；知之次也。<br>孔子说：大概存在没有智慧却凭没有智慧而有所作为的人，我不是这样的。在一个能让每个人都能自由见闻的社会里，尽可能地扩展自己的见闻，选择超过自己的见解，依据其见解而不是依据有此见解的人或群体，深入探讨、吸收学习；进而让自己的见识逐步深厚，才能更清楚地去辨别、辩正各种知识的真伪、深浅。但这些都是智慧的临时落脚处，不是智慧的真正所在。</p><h1 id="第四十句"><a href="#第四十句" class="headerlink" title="第四十句"></a>第四十句</h1><ol start="40"><li>子张学干禄。子曰：多闻阙疑，慎言其余，则寡尤。多见阙殆，慎行其余，则寡悔。言寡尤，行寡悔，禄在其中矣。</li></ol><p>缠中说禅白话直译<br>子张学干禄。子曰：多闻阙疑，慎言其余，则寡尤。多见阙殆，慎行其余，则寡悔。言寡尤，行寡悔，禄在其中矣。<br>孔子说：子张求问获取福运的方法。孔子说：见闻广泛而去除疑惑，见识深厚而去除危险，遵循如此“闻见”而如此“言行”，那么言行都会少过失。言行少过失，福运在其中啊。</p><h1 id="第四十一句"><a href="#第四十一句" class="headerlink" title="第四十一句"></a>第四十一句</h1><ol start="41"><li>子曰∶君子谋道不谋食。耕也，馁在其中矣；学也，禄在其中矣。君子忧道不忧贫。</li></ol><p>缠中说禅白话直译<br>子曰∶君子谋道不谋食。耕也，馁在其中矣；学也，禄在其中矣。君子忧道不忧贫。<br>孔子说：“闻、见、学、行”“圣人之道”的君子，按“道之谋”谋划而不按“食之谋”谋划。以人的欲望饥饿为基础的生产，新的欲望饥饿就在其中啊；以人与天地关系中对照、校对确定人之所需，福运、真正的幸福就在其中啊。君子只担忧如何“闻、见、学、行”“圣人之道”的“道之谋”，而不担忧“馁、耕、食” “食之谋”的恶性循环必然导致的人在物质与精神上的贫穷。</p><p>自注： 经常的被不知道来源的焦虑所裹挟，思考一下来源有嘈杂的网络环境，以贩卖焦虑为谋利的文章。君子谋道不谋食，这个时代，饿死还是很难的，解决食之谋的方法还是有很多的。减掉一些愿望，不做一些没有实现可能的幻想，仔细想想我有的，然后再想我这么使用我有的来实现我想要的。</p><h1 id="第四十二句"><a href="#第四十二句" class="headerlink" title="第四十二句"></a>第四十二句</h1><ol start="42"><li>子曰：君子不器。</li></ol><p>缠中说禅白话直译<br>子曰∶君子不器。<br>孔子说：君子不相。</p><h1 id="第四十三句"><a href="#第四十三句" class="headerlink" title="第四十三句"></a>第四十三句</h1><ol start="43"><li>子曰：古之学者为己；今之学者为人。</li></ol><p>缠中说禅白话直译<br>子曰：古之学者为己；今之学者为人。<br>孔子说：无论古今，真正的学问与学人，都不离“内圣外王”、“为己为人”的一体之学。</p><p>自注：“是故内圣外王之道，暗而不明，郁而不发，天下之人，各为其所欲焉，以自为方。”<br>在王阳明看来，“内圣”的基础，是人之为人必要有的独立人格，恰如孔子所说的“古之学者为己”。在王阳明看来，唯有找到“天理”的指引，人才有内在驱动力，才能找到人生方向，而获得“天理”途径，便是通过“格物”进而“正心”，通过“知行合一”。通过书本获得知识算不得“理”，只有在生活中对其进行实践、验证进而获得的个人独特的理解，才算是“真知”。博学、审问、慎思、明辨、笃行者，皆所以为惟精而求惟一也。他如博文者，即约礼之功；格物致知者，即诚意之功；道问学即尊德性之功；明善即诚身之功：无二说也。”观诸圣之一学：基督教曰树一、恒一；伊斯兰曰独一无二；印度教曰不二； 佛教曰三昧(一境)；道教曰贞一；黄帝曰守一； 管子曰专一； 老子曰执一； 孔子曰精一； 山人说的就是一个一，故人戏称山人为一先生、不二先生。人要有所作为就必须独善其一，国家民族之强盛势必用一，有统一的思想，有惟一的民族哲学理念。</p><h1 id="第四十四句"><a href="#第四十四句" class="headerlink" title="第四十四句"></a>第四十四句</h1><ol start="44"><li>子曰：三年学不至，於榖不易，得也。</li></ol><p>缠中说禅白话直译<br>子曰：三年学不至，於榖不易，得也。<br>孔子说：多年闻“圣人之道”、见“圣人之道”、“对照”“圣人”、在现实社会中不断地“校对”，虽然不能达到尽善尽美，但能对“圣人之道”的“学”达到一生不退转的位次，这才算是“学”有所得啊。</p><p>自注： 闻，见，学，行圣人之道，就要坚定的走下去。</p><h1 id="第四十五句"><a href="#第四十五句" class="headerlink" title="第四十五句"></a>第四十五句</h1><ol start="45"><li>子曰：学如不及，犹恐失之。</li></ol><p>缠中说禅白话直译<br>子曰：学如不及，犹恐失之。<br>孔子说：闻“圣人之道”、见“圣人之道”、“对照”“圣人”、在现实社会中不断地“校对”而不能达到尽善尽美，是因为踌躇、恐惧、疑虑使它迷失而不能直下承担。</p><h1 id="第四十六句"><a href="#第四十六句" class="headerlink" title="第四十六句"></a>第四十六句</h1><ol start="46"><li>子曰：学而不思则罔，思而不学则殆。<br>学和思本来就是一体的。</li></ol><p>缠中说禅白话直译<br>子曰：学而不思则罔，思而不学则殆。<br>孔子说：将差异性的“学”与同一性的“思”分开，都只能迷惘、疲怠而无所得。</p><h1 id="第四十七句"><a href="#第四十七句" class="headerlink" title="第四十七句"></a>第四十七句</h1><ol start="47"><li>子曰：唯！女子与小人为难、养也。近之则不孙，远之则怨。</li></ol><p>缠中说禅白话直译<br>子曰：唯！女子与小人为难、养也。近之则不孙，远之则怨。<br>孔子说：是的！你的儿女跟随小人而“闻、见、学、行”，就产生灾难、痒疾。依附小人，就失去子嗣；违背小人，就埋下仇恨。</p><h1 id="第四十八句"><a href="#第四十八句" class="headerlink" title="第四十八句"></a>第四十八句</h1><ol start="48"><li>子曰：唯上知与下愚不移。</li></ol><p>缠中说禅白话直译<br>子曰：唯上知与下愚不移。<br>孔子说：愿真正“见、闻、学、行”“圣人之道”的君子，结交、亲附没有智慧、充满贪婪、恐惧的小人而成就“见、闻、学、行”“圣人之道”的不退转。</p><h1 id="第四十九句"><a href="#第四十九句" class="headerlink" title="第四十九句"></a>第四十九句</h1><ol start="49"><li>子曰：温故而知新，可以为师矣。</li></ol><p>缠中说禅白话直译<br>子曰：温故而知新，可以为师矣。<br>孔子说：应当把“积聚、蕴藏故有的、经过时间沉淀、检验的智慧而保持智慧当下鲜活的创造与呈现”作为君子“见、闻、学、行”“圣人之道”所师法的目标啊。</p><p>自注： 温故而知新”有四解。</p><p>1、温故才知新，温习已学的知识，并且由其中获得新的领悟；</p><p>2、温故及知新，一方面要温习典章故事，另一方面又努力撷取新的知识；</p><p>3、温故，知新。随着自己阅历的丰富和理解能力的提高，回头再看以前看过的知识，总能从中体会到更多的东西；</p><p>4、是指通过回味历史，而可以预见，以及解决未来的问题。这才是一个真正的大师应该具有的能力。</p><h1 id="第五十句"><a href="#第五十句" class="headerlink" title="第五十句"></a>第五十句</h1><ol start="50"><li>子曰：吾十有五而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲不逾矩。</li></ol><p>缠中说禅白话直译<br>子曰：吾十有五而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲不逾矩。<br>孔子说：我十五岁的境界、所为用“从此闻见学行圣人之道”来标记，三十岁的境界、所为用“穷尽闻见学行圣人之道的现实可能位次”来标记，四十岁的境界、所为用“透彻闻见学行圣人之道现实可能位次的不患”来标记，五十岁的境界、所为用“闻见学行圣人之道让智慧依当下生存鲜活地呈现”来标记，六十岁的境界、所为用“遵循当下生存鲜活呈现的智慧而闻见学行圣人之道以成就内圣”来标记，七十岁的境界、所为用“依从民心期望但不超越闻见学行圣人之道在当下现实中可能实现位次而成就外王”来标记。</p><h1 id="第五十一句"><a href="#第五十一句" class="headerlink" title="第五十一句"></a>第五十一句</h1><ol start="51"><li>子曰：君子，食无求饱，居无求安；敏於事而慎於言；就有，道而正焉；可谓好学也已。</li></ol><p>缠中说禅白话直译<br>子曰：君子，食无求饱，居无求安；敏於事而慎於言；就有，道而正焉；可谓好学也已。<br>孔子说：“闻见学行”“圣人之道”的人，对欲望不贪求从而满足，对生存的环境不贪求从而安身；通过当下的事情去印证，使得理论、言论顺应当下的实际；对现实究底穷源，使现实行“圣人之道”而在现实中成就之，称之为“好学”，是适当的啊。</p><p>自注： 走上圣人之道的人，把对欲望的不贪求看作满足。</p><h1 id="第五十二句"><a href="#第五十二句" class="headerlink" title="第五十二句"></a>第五十二句</h1><ol start="52"><li>子曰：十室之邑，必有忠信如丘者焉，不如丘之好学也。</li></ol><p>缠中说禅白话直译<br>子曰：十室之邑，必有忠信如丘者焉，不如丘之好学也。<br>孔子说：所有国家，倘若有遵从我的“忠信”标准的在其中，不若有遵从我的“好学”标准的在其中。</p><h1 id="第五十三句"><a href="#第五十三句" class="headerlink" title="第五十三句"></a>第五十三句</h1><ol start="53"><li>子曰：三人行，必有我师焉：择其善者而从之，其不善者而改之。</li></ol><p>缠中说禅白话直译<br>子曰：三人行，必有我师焉：择其善者而从之，其不善者而改之。<br>孔子说：与“君、父、师”同行，倘若有让我师法的在此：选取他们完善的并在当下现实更广泛的范围应用、检验，选取他们不完善的并在当下现实中不断修改、完善。</p><h1 id="第五十四句"><a href="#第五十四句" class="headerlink" title="第五十四句"></a>第五十四句</h1><ol start="54"><li>子夏曰：日知其所亡，月无忘其所能，可谓好学也已矣！<br>钱穆：子夏说：“每天能知道所不知道的，每月能不忘了所已能的，可说是好学了。”<br>知识是知识，技术高于知识，心法高于技术，法则高于心法。</li></ol><h1 id="第五十五句"><a href="#第五十五句" class="headerlink" title="第五十五句"></a>第五十五句</h1><ol start="55"><li>子夏曰：仕而优则学；学而优则仕。<br>钱穆：子夏说：“仕者有余力宜从学。学者有余力宜从仕。”</li></ol><h1 id="第五十六句"><a href="#第五十六句" class="headerlink" title="第五十六句"></a>第五十六句</h1><ol start="56"><li>子夏曰：百工居肆以成其事；君子学以致其道。<br>钱穆：子夏说：“百工长日居住肆中以成其器物，君子终身在学之中以求致此道。”<br>直译大致就是：就像各种工匠在手工业作坊里为完成他们的制作，君子在学中为完成他们的事业。</li></ol><h1 id="第五十七句"><a href="#第五十七句" class="headerlink" title="第五十七句"></a>第五十七句</h1><ol start="57"><li>子谓子夏曰：女为君子儒！无为小人儒！<br>钱穆：先生对子夏道：“你该为一君子儒，莫为一小人儒。”</li></ol><p>自注： 这句话更像老人对孩子的劝诫，重点就是看怎么解释这君子儒和小人儒了。</p><h1 id="第五十八句"><a href="#第五十八句" class="headerlink" title="第五十八句"></a>第五十八句</h1><ol start="58"><li>哀公问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不迁怒，不贰过。不幸短命死矣，今也则亡，未闻好学者也。”<br>钱穆：鲁哀公问孔子道：“你的学生们，哪个是好学的呀？”孔子对道：“有颜回是好学的，他有怒能不迁向别处，有过失能不再犯。可惜短寿死了，目下则没有听到好学的了。”</li></ol><h1 id="第五十九句"><a href="#第五十九句" class="headerlink" title="第五十九句"></a>第五十九句</h1><ol start="59"><li>季康子问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不幸短命死矣！今也则亡。”<br>钱穆：季康子问孔子：“你的弟子哪个是好学的呀？”孔子对道：“有颜回是好学的，不幸短命死了，现在是没有了。”</li></ol><h1 id="第六十句"><a href="#第六十句" class="headerlink" title="第六十句"></a>第六十句</h1><ol start="60"><li>子曰：语之而不惰者，其回也与？</li></ol><p>缠中说禅白话直译<br>子曰：语之而不惰者，其回也与？<br>孔子说：任何人与他辩论而他都能语不衰败的所谓能辩之士，难道只有颜回吗？</p><h1 id="第六十一句"><a href="#第六十一句" class="headerlink" title="第六十一句"></a>第六十一句</h1><ol start="61"><li>子贡问君子。子曰：先行其言而后从之。</li></ol><p>缠中说禅白话直译<br>子贡问君子。子曰：先行其言而后从之。<br>子贡问君子，孔子说：“先使自己的言论、思想以及相应的行为一以贯之，然后再使之广泛。”</p><h1 id="第六十二句"><a href="#第六十二句" class="headerlink" title="第六十二句"></a>第六十二句</h1><ol start="62"><li>子贡问曰：“赐也何如？”子曰：“女，器也。”曰：“何器也？”曰：“瑚琏也。”</li></ol><p>缠中说禅白话直译<br>子贡问曰：“赐也何如？”子曰：“女，器也。”曰：“何器也？”曰：“瑚琏也。”<br>子贡问：“我，怎么样？”孔子说：“你，“器”呀。”问：“什么器皿？”答：“宗庙里盛黍稷的瑚琏那样的名贵器皿”</p><h1 id="第六十三句"><a href="#第六十三句" class="headerlink" title="第六十三句"></a>第六十三句</h1><ol start="63"><li>子贡问曰：“有一言而可以终身行之者乎？”子曰：“其恕乎？己所不欲，勿施於人。”</li></ol><p>缠中说禅白话直译<br>子贡问曰：“有一言而可以终身行之者乎？”子曰：“其恕乎？己所不欲，勿施於人。”<br>子贡问：“有可以终身一而贯之的言论吗？”孔子说：“自己不想要的就不施加给别人，难道就是“恕”吗？”</p><h1 id="第六十四句"><a href="#第六十四句" class="headerlink" title="第六十四句"></a>第六十四句</h1><ol start="64"><li>子贡曰：“我不欲人之加诸我也，吾亦欲无加诸人。”子曰：“赐也，非尔所及也。”</li></ol><p>缠中说禅白话直译<br>子贡曰：“我不欲人之加诸我也，吾亦欲无加诸人。”子曰：“赐也，非尔所及也。”<br>子贡问：“我不想别人诬枉我，我也不想诬枉别人。”孔子说：“子贡啊，这不是你所能达到的。”</p><h1 id="第六十五句"><a href="#第六十五句" class="headerlink" title="第六十五句"></a>第六十五句</h1><ol start="65"><li>子曰：“赐也，女以予为多学而识之者与？”对曰：“然，非与？”曰：“非也！予一以贯之。”</li></ol><p>缠中说禅白话直译<br>子曰：“赐也，女以予为多学而识之者与？”对曰：“然，非与？”曰：“非也！予一以贯之。”<br>孔子问：“子贡啊，你把我当成不断学习从而了解现实当下的人吗？”子贡回答：“对，不是这样吗？”孔子说：“不是啊，我只是直下承担当下现实而贯通它。”</p><h1 id="第六十六句"><a href="#第六十六句" class="headerlink" title="第六十六句"></a>第六十六句</h1><ol start="66"><li>子曰：“参乎！吾道一以贯之。”曾子曰：“唯。”子出。门人问曰：“何谓也？”曾子曰：“夫子之道，忠恕而已矣。”</li></ol><p>缠中说禅白话直译<br>子曰：“参乎！吾道一以贯之。”曾子曰：“唯。”子出。门人问曰：“何谓也？”曾子曰：“夫子之道，忠恕而已矣。”<br>孔子说：“曾参啊！我“闻见学行”圣人之道一以贯之。”曾参说：“是。”孔子出去。孔子的其他弟子问：““一以贯之”是什么意思？”曾参回答：“老师的道理，只是“尽已之心以待人，推己之心以及人”罢了。”</p><h1 id="第六十七句"><a href="#第六十七句" class="headerlink" title="第六十七句"></a>第六十七句</h1><ol start="67"><li>有子曰：其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道生。孝弟也者，其为仁之本与！<br>钱穆：有子说：“若其人是一个孝弟之人，而会存心喜好犯上的，那必很少了。若其人不喜好犯上，而好作乱的，就更不会有了。君子专力在事情的根本处，根本建立起，道就由此而生了。孝弟该是仁道的根本吧？”</li></ol><h1 id="六十八句"><a href="#六十八句" class="headerlink" title="六十八句"></a>六十八句</h1><ol start="68"><li>孟懿子问孝。子曰：“无违”。樊迟御，子告之曰：“孟孙问孝於我，我对曰，”无违。””樊迟曰：“何谓也？”子曰：“生，事之以礼；死，葬之以礼，祭之以礼。”</li></ol><p>缠中说禅白话直译<br>孟懿子问孝。子曰：“无违”。樊迟御，子告之曰：“孟孙问孝於我，我对曰，”无违。””樊迟曰：“何谓也？”子曰：“生，事之以礼；死，葬之以礼，祭之以礼。”<br>孟懿子问孝。孔子说：“不要离开。”樊迟替孔子赶车，孔子对他说：“孟孙向我问孝，我回答说：“不要离开”。”樊迟说：“什么意思？”孔子道：“父母在世，用社会当下约定俗成的规范去侍奉他们；父母去世，用社会当下约定俗成的规范去安葬、祭祀他们。”</p><h1 id="第六十九句"><a href="#第六十九句" class="headerlink" title="第六十九句"></a>第六十九句</h1><ol start="69"><li>子游问孝。子曰：“今之孝者，是谓能养。至於犬马，皆能有养；不敬，何以别乎。”</li></ol><p>缠中说禅白话直译<br>子游问孝。子曰：“今之孝者，是谓能养。至於犬马，皆能有养；不敬，何以别乎。”<br>子游问孝。孔子说：“能养父母就被认为是现在的孝了。甚至狗和马，都会有人养；如果内心不敬，又用什么来区别这两者？”</p><h1 id="第七十句"><a href="#第七十句" class="headerlink" title="第七十句"></a>第七十句</h1><ol start="70"><li>孟武伯问孝。子曰：“父母唯其疾之忧。”</li></ol><p>缠中说禅白话直译<br>孟武伯问孝。子曰：“父母唯其疾之忧。”<br>孟武伯问孝，孔子说：“（孝就是）纵使自己生病也担忧父母的那种当下产生的感情。”</p><h1 id="第七十一句"><a href="#第七十一句" class="headerlink" title="第七十一句"></a>第七十一句</h1><ol start="71"><li>子夏问孝。子曰：色难。有事，弟子服其劳；有酒食，先生馔，曾是以为孝乎？<br>缠中说禅白话直译<br>子夏问孝。子曰：色难。有事，弟子服其劳；有酒食，先生馔，曾是以为孝乎？<br>子夏问孝，孔子说：“有事故，让年轻人负担其中的烦劳；有酒食，让年长者吃喝；但如果这些行为不是发自当下的情感，只是由于一种道德规范的力量，内心不情愿甚至在外显露出脸色为难，那么，难道就能把这种行为当成孝吗？</li></ol><h1 id="第七十二句"><a href="#第七十二句" class="headerlink" title="第七十二句"></a>第七十二句</h1><ol start="72"><li>子曰：父母在，不远游，游必有方。</li></ol><p>缠中说禅白话直译<br>子曰：父母在，不远游，游必有方。<br>孔子说：“当父母健在时，即使是游学也不能到偏远险恶之地，否则一定招致旁人或命运的诅咒。”</p><h1 id="第七十三句"><a href="#第七十三句" class="headerlink" title="第七十三句"></a>第七十三句</h1><ol start="73"><li>子曰：父母之年，不可不知也。一则以喜，一则以惧。</li></ol><p>缠中说禅白话直译<br>子曰：父母之年，不可不知也。一则以喜，一则以惧。<br>孔子说：“父母的年龄、生日等，不能不常常挂念以至能脱口而出。这种当下的情感，一方面带着欢喜，一方面带着忧惧，悲欣交集。</p><h1 id="第七十四句"><a href="#第七十四句" class="headerlink" title="第七十四句"></a>第七十四句</h1><ol start="74"><li>子曰：君子喻於义，小人喻於利。</li></ol><p>缠中说禅白话直译<br>子曰：君子喻於义，小人喻於利。<br>孔子说：“君子被各种现实社会结构以及对应的各种道德、法度等规范的关系之网中蕴藏的力量所开导，小人被利益、利害关系所组成的现实社会结构以及其对应的一套现实运行机制的关系之网中蕴藏的力量所开导。”</p><h1 id="第七十五句"><a href="#第七十五句" class="headerlink" title="第七十五句"></a>第七十五句</h1><ol start="75"><li>子曰：君子周而不比，小人比而不周。</li></ol><p>缠中说禅白话直译<br>子曰：君子周而不比，小人比而不周。<br>孔子说：君子，见闻学行周遍而没有疏漏，却不会让别人和自己步调一致、比肩而行；小人，让别人和自己步调一致、比肩而行，见闻学行却不能周遍而没有疏漏。</p><h1 id="第七十六句"><a href="#第七十六句" class="headerlink" title="第七十六句"></a>第七十六句</h1><ol start="76"><li>子曰：君子和而不同；小人同而不和。</li></ol><p>缠中说禅白话直译<br>子曰：君子和而不同；小人同而不和。<br>孔子说：君子相应而不聚集，小人聚集而不相应。</p><h1 id="第七十七句"><a href="#第七十七句" class="headerlink" title="第七十七句"></a>第七十七句</h1><ol start="77"><li>子曰：君子成，人之美；不成，人之恶。小人反是。</li></ol><p>缠中说禅白话直译<br>子曰：君子成，人之美；不成，人之恶。小人反是。<br>孔子说：人不断滋生美德，君子成就；人不断滋生恶行，君子不成。小人的成就与此相反。</p><h1 id="第七十八句"><a href="#第七十八句" class="headerlink" title="第七十八句"></a>第七十八句</h1><ol start="78"><li>子曰：君子之於天下也，无适也，无莫也，义之於比。</li></ol><p>缠中说禅白话直译<br>子曰：君子之於天下也，无适也，无莫也，义之於比。<br>孔子说：君子对于天下的一切，没有行为的归向，也没有思想的向往，甚至可以让自己的容貌呈现出小人的“比“相。</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔6</title>
    <link href="/2024/04/10/ganwu6/"/>
    <url>/2024/04/10/ganwu6/</url>
    
    <content type="html"><![CDATA[<h1 id="自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。"><a href="#自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。" class="headerlink" title="自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。"></a>自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。</h1><h1 id="“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。"><a href="#“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。" class="headerlink" title="“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。"></a>“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。</h1><p>君子慎独，自己一个人独处时也要保持着德行。控制自己的贪，嗔、痴、慢、疑。活着挺难的，做人是难的。</p><h1 id="情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。"><a href="#情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。" class="headerlink" title="情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。"></a>情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。</h1>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔5</title>
    <link href="/2024/04/08/ganwu5/"/>
    <url>/2024/04/08/ganwu5/</url>
    
    <content type="html"><![CDATA[<h1 id="活着需要的能力"><a href="#活着需要的能力" class="headerlink" title="活着需要的能力"></a>活着需要的能力</h1><h2 id="基本生存能力"><a href="#基本生存能力" class="headerlink" title="基本生存能力"></a>基本生存能力</h2><p>1.自我安全保障能力<br>2.基础急救<br>3.基本生存<br>4.方位感</p><h2 id="基础工作能力"><a href="#基础工作能力" class="headerlink" title="基础工作能力"></a>基础工作能力</h2><p>1.制作个人简历<br>2.时间管理能力-如何使用日历和计划清单<br>3.基础写作<br>4.公众讲话<br>5.有效沟通<br>6.基础电脑操作技术<br>7.基础的媒体和文档管理能力<br>8.OFFICE的应用能力<br>9.研究和探索能力</p><h2 id="家务处理能力"><a href="#家务处理能力" class="headerlink" title="家务处理能力"></a>家务处理能力</h2><p>1.如何打扫卫生<br>2.基础烹饪能力<br>3.基础家装修理能力</p><h2 id="财务管理能力"><a href="#财务管理能力" class="headerlink" title="财务管理能力"></a>财务管理能力</h2><p>1.制作家庭预算<br>2.制作家庭账本<br>3.基本投资能力<br>4.基本谈判能力</p><h2 id="自我认知能力"><a href="#自我认知能力" class="headerlink" title="自我认知能力"></a>自我认知能力</h2><p>1.搞清楚自己的使命、方向和人生目标的能力<br>2.平衡生活的能力<br>3.探索并清晰自己的价值观系统<br>4.管理自己情绪的能力</p><h2 id="人际沟通能力"><a href="#人际沟通能力" class="headerlink" title="人际沟通能力"></a>人际沟通能力</h2><p>1.基本礼节常识<br>2.幽默感<br>3.亲密关系的沟通和爱的能力<br>4.表达和赞赏<br>5.接受批评的能力</p><h2 id="思维认知能力"><a href="#思维认知能力" class="headerlink" title="思维认知能力"></a>思维认知能力</h2><p>1.批判性思维（本质思考）<br>2.整合思维能力（迁移思考）<br>3.解决问题能力<br>4.自我控制能力（安排规律的生活作息）<br>5.养生的能力</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔4</title>
    <link href="/2024/04/07/ganwu3/"/>
    <url>/2024/04/07/ganwu3/</url>
    
    <content type="html"><![CDATA[<h1 id="王国维先生曾在《人间词话》中写到对人生三重境界的感悟："><a href="#王国维先生曾在《人间词话》中写到对人生三重境界的感悟：" class="headerlink" title="王国维先生曾在《人间词话》中写到对人生三重境界的感悟："></a>王国维先生曾在《人间词话》中写到对人生三重境界的感悟：</h1><p>“古今成大事业、大学问者，必经过三种境界。‘昨夜西风凋碧树，独上高楼，望尽天涯路’，此第一境界；‘衣带渐宽终不悔，为伊消得人憔悴’，此第二境界；‘众里寻他千百度，蓦然回首，那人却在，灯火阑珊处’，此第三境界。”</p><h2 id="第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》"><a href="#第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》" class="headerlink" title="第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》"></a>第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》</h2><p>在西风的狂吹下，枝繁叶茂的绿树也开始凋谢了，表示形式非常危急，环境十分恶劣，在这种状态下作者夜不成眠，辗转反侧，为自己前途命运无比担忧，但他并没有丧失信心，因此颓废，而是想要努力克服困难，力求上进，争取找到自己的前进方向。于是，作者愤然起身，独上高楼，高瞻远瞩，想要望尽天涯海角，找到前进的路。在这一境界中，可以看做人涉世不久，对人生的无比迷茫，正如现在刚毕业的大学生。但是在迷茫中有多少人因此而坠入歧途，自暴自弃，人生路漫漫，我们也应该上下而求索。正如鲁迅先生所说的一句话：“世界上本没有路，走的人多了，也便成了路”</p><p>注：走出自己的路，见路不走，实事求是。（2024&#x2F;5&#x2F;3）</p><h2 id="第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》："><a href="#第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》：" class="headerlink" title="第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》："></a>第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》：</h2><p>柳永如此艳丽之词，也被王国维拿来说明学问之事。诗人所忧之事，是“相思”，但相思到如此地步，我只有柳永能做到了，可见柳永真是一个重情之人。联系到人生，做一件事能专一到这种地步，不成功都难。继第一阶段的迷茫之后，在这一阶段中便有了目标了，在追逐目标的过程中，必然会遇到许多的困难，而我们要做的就是像柳永想念女子一样，即使被折磨得瘦骨伶仃，形容憔悴，我始终不放弃自己的目标，一往直前。</p><h2 id="第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。”-出自南宋辛弃疾《青玉案·元夕》"><a href="#第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。”-出自南宋辛弃疾《青玉案·元夕》" class="headerlink" title="第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。” 出自南宋辛弃疾《青玉案·元夕》"></a>第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。” 出自南宋辛弃疾《青玉案·元夕》</h2><p>寻觅了千百次，却在无意间看到那人在灯火阑珊处。在苦苦追寻，历经磨难之后，总算看到惊喜了，之前的付出都有了回报。在人生的旅途中，在追求成功的路上，我们时常会陷入迷茫、困惑、苦恼中，甚至怀疑自己有没有选错目标，还该不该坚持下去，这些都很正常的，毕竟成功哪有那么容易呢？在这场旅途中，必定有很多人中途放弃，马云有句话说得好：“今天很残酷，明天更残酷，后天很美好，但大多数人都死在明天晚上，真正的英雄才能见到后天的太阳。”成功总是不经意间到来，这是一个人历尽千帆之后上天赐予的惊喜。这个惊喜有时来的很晚，但只要我们一直坚持，他迟早是要来的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔3</title>
    <link href="/2024/04/06/ganwu2/"/>
    <url>/2024/04/06/ganwu2/</url>
    
    <content type="html"><![CDATA[<h1 id="学习最重要的是什么？"><a href="#学习最重要的是什么？" class="headerlink" title="学习最重要的是什么？"></a>学习最重要的是什么？</h1><p>清醒，知道自己在做什么？ 即明白此时此刻到底在做什么。这一刻我在干什么？正在解决什么问题？这一天我学到了什么东西？取得了什么进步？不断回答这些问题的过程就是不断给予大脑正反馈的过程。时时刻刻用收获刺激大脑是学习上瘾和长时间专注学习的秘诀。</p><h1 id="什么在阻挡我进步？"><a href="#什么在阻挡我进步？" class="headerlink" title="什么在阻挡我进步？"></a>什么在阻挡我进步？</h1><p>我自己，我自己的思维在阻挡我的进步。为什么我在刷手机，看视频时不会觉得无聊，在放下手机后，反而觉得我聊了。这样当再次拿起手机时才会解决。我不喜欢这样没有掌控感的感觉，这样的感觉是矛盾的。这样的矛盾怎么来的勒？大脑中的思维和自我的觉知的意识之间的矛盾，怎么化解这种矛盾，我认为的解决方法是清楚自己在干嘛，清楚自己此时此刻在干嘛。建立一个观察者来观察自己。</p><h1 id="我为什么会刷手机？"><a href="#我为什么会刷手机？" class="headerlink" title="我为什么会刷手机？"></a>我为什么会刷手机？</h1><p>我为什么会刷手机，是因为我觉得无聊。我为什么觉得无聊？是因为没有事情可以做，或者是有些事情做了看不到效果，没有刷手机那样即时的奖励感。但是刷手机的过程中的信息是我不喜欢的，这些信息太单一，太主观（酒色财气，好色、贪财、逞气，为人生四戒），很不幸这些都有，当然也有好的，但是需要我去花时间去寻找。这也是一个麻烦事。回到怎么解决刷手机这件事，拿回对做事情的掌控感，明白自己现在在干嘛。就这样简单。</p><h1 id="阐述一下上瘾机制"><a href="#阐述一下上瘾机制" class="headerlink" title="阐述一下上瘾机制"></a>阐述一下上瘾机制</h1><p>上瘾的机理与多巴胺有关，并且随着上瘾行为的次数增多，大脑中的对与多巴胺的神经受体的敏感程度会下降，进而导致要想获取与之前相同的快感，就需要付出更多或更具有刺激性的行为来刺激多巴胺的生成。理论依据，行为和感觉是一体的，感觉会影响行为，行为会影响感觉，但是使用行为去影响感觉是更有理智的。第二点，失乐园理论，按照上述的描述，随着大脑中多巴胺快感产生的受体的敏感性下降，要想获得与之前相同的快感，需要付出的行为会更多，但是这样总会有个上限，导致人在做不出要产生多巴胺的行为时，就会陷入失乐园状态。第三点，痛苦和快感需要平衡，当我感觉痛苦时我需要快乐（多巴胺）来平衡，当我感到快乐时，快乐完成后会产生空虚（痛苦）来平衡快乐。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔2</title>
    <link href="/2024/04/05/ganwu0/"/>
    <url>/2024/04/05/ganwu0/</url>
    
    <content type="html"><![CDATA[<h1 id="怎么能做好一件事？"><a href="#怎么能做好一件事？" class="headerlink" title="怎么能做好一件事？"></a>怎么能做好一件事？</h1><p>做事，我将其解释为使用自己的方式去实现自己的想法。一个足以支撑去做这件事的动机，加上没有复杂的情绪影响，持之以恒的勤奋，那我觉得这件事的成功概率会大大的加强。一点点自己给自己的意义加上不要脑子的勤奋就可以成事了。为什么说不要脑子，因为带上脑子就会产生情绪，有了情绪就会产生内耗，当然对待情绪要辩证的看待，但是在做事初期一定不要带有情绪，在后面做这件事有一定积累后，就可以带上脑子了。<br>这里需要说明一点的是，成功是一个随机事件，不是必然事件。任何人做任何事情，都不能百分之百的把握成功。但是所有人都可以做一件事情，那就是持之以恒的勤奋，去提升成功的概率。知易行难，从小事做起，不断改变，积少成多。这是一个很重要的思维方法。</p><h1 id="心法、法则、技术之间的关系。"><a href="#心法、法则、技术之间的关系。" class="headerlink" title="心法、法则、技术之间的关系。"></a>心法、法则、技术之间的关系。</h1><p>心法胜于法则，法则胜于技术（方法）。<br>所谓技术，比如说使用进步本的技术来操作管理各个学科的知识，知识会遗忘，技术则能熟能生巧，历久弥新。<br>所谓法则，例如学习过程中唯一不变的目的就是进步，认识到这个法则之后所开发的技术都是为了这个法则服务的。世界上没有普世的方法，却有普世的法则。比如要把一件事做好，就要不断学习，不断学习的目的是不断进步，进而能把事情做的更好。<br>所谓心法，是指人对生命、对世界的基本态度和根本认知。例如，“自胜者强”，给强者下了定义，不胜别人，只胜自己的人是强者，战胜自己内心的情绪，内心的恐惧，控制自己不再内耗的人是强者。以自胜者强的定义去看待别人和自己，自然形成心态，进而形成心法。自胜者强包含了学习进步的法则，但是学习进步的法则取不涵盖自胜者强。</p><h1 id="我该怎么才能做好一件事。"><a href="#我该怎么才能做好一件事。" class="headerlink" title="我该怎么才能做好一件事。"></a>我该怎么才能做好一件事。</h1><p>注意这里的主语是我，我该怎么才能做好一件事？ 那就是不要带着情绪去做一件事，带着脑子去做这件事就行了，凡事想多了，就做不成了。<br>不要急躁，慢慢来才最快。不要傲慢，不要觉得这个太简单就不去做，认识是要不断重复的。<br>要早睡早起，11点之前睡觉，在早上7点起床就是一件很容易的一件事。但是要是在11点之后睡觉，要在早上7点起床就不是那么一件容易的事情了。<br>不要给自己增加烦劳，世上本无事，庸人自扰之。<br>要给自己制定一个计划，在本子上写上自己要做的事情。（这点我确实没有做好，目前来讲，我都是随性而为。）<br>不要在做的过程中去看距离结果还有多远，不要和别人分享自己的喜悦，万一没有成功岂不是很尴尬。生命是一修行。<br>勤奋，不要让事情来裹挟着你，而是让我去驱赶着事情。生命是场旅行，我可以当过过客，也可以成为风景中的一部分。<br>体验是最重要的，体验自己的喜悦，体验自己的悲伤，体验自己的失落，体验自己的愉悦。活在当下，不要考虑过去和未来。<br>不要在乎结果，做好当下的事情，现在的事情做好了，结果不会太差。听天命，尽人事。<br>内心的法则和社会的法则不同，甚至相反，但是我可以使用心法来统领。内圣外王，或许先要内圣，而后外王。<br>决心来自一个明确的、具体的理由。记住是一个理由，唯一的理由，不是许多理由。</p><p>2024&#x2F;5&#x2F;3<br>注：条件，前提，因由，不同条件产生不同的表象，但是背后却有相同的本质。冰、雪、雾、霜、露。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一点小确幸</title>
    <link href="/2024/04/04/tupian/"/>
    <url>/2024/04/04/tupian/</url>
    
    <content type="html"><![CDATA[<h1 id="这里是图片"><a href="#这里是图片" class="headerlink" title="这里是图片"></a>这里是图片</h1><h2 id="清晨的树"><a href="#清晨的树" class="headerlink" title="清晨的树"></a>清晨的树</h2><p><img src="/pic/79f388cc7d53668d55773baf30bb10c.jpg"></p><h2 id="早晨的红日"><a href="#早晨的红日" class="headerlink" title="早晨的红日"></a>早晨的红日</h2><p><img src="/pic/2.jpg"></p><h2 id="一个不知名的地方"><a href="#一个不知名的地方" class="headerlink" title="一个不知名的地方"></a>一个不知名的地方</h2><p><img src="/pic/l2.jpg"></p><h2 id="图书馆"><a href="#图书馆" class="headerlink" title="图书馆"></a>图书馆</h2><p><img src="/pic/jc1.jpg"></p><h2 id="好看的壁纸"><a href="#好看的壁纸" class="headerlink" title="好看的壁纸"></a>好看的壁纸</h2><p><img src="/pic/3.jpg"><br><img src="/pic/3.png"><br><img src="/pic/v2-3a1fb23e19b2448033a9b7333941f465_r.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>图片</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔</title>
    <link href="/2024/04/04/ganwu1/"/>
    <url>/2024/04/04/ganwu1/</url>
    
    <content type="html"><![CDATA[<h1 id="随笔"><a href="#随笔" class="headerlink" title="随笔"></a>随笔</h1><h2 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h2><p>为什么要写下这篇随笔，最近经常上网，突然觉得什么都不真实，所以我决定写下一些我觉得真实的想法。并且决定这段时间先不上网看他人的对于这个世界的解释，太嘈杂了。先看看书，集百家之言，成一家之说。先把自己的世界观，人生观，价值观建立完成，才能更好的和人交流，辩论。</p><p>  见相非相，即见如来</p><p>很多时候，我们这个世界的认识是荒谬到极点而不自知的。人是立场的，好恶的，我们总是只相信自己愿意相信的东西。我们习惯于先形成观点，然后再寻找既有立场的正面证据，在不知不觉中偏离真实。改变总是很难的，一点点的进步总是让我感到欢喜的。每个人的生命中都隐含着一颗觉知的种子，一旦唤醒，便势不可挡，在追求真知、真实的路上一去不反。</p><p>世界是真实的，世界就是那样，但是世界的解释权却不再世界本身，而在于我们。解释权归我们所有，对世界的解释的方法造就了不同的人，并且我们对世界的解释的方式会受到各种各样的影响，正确的，错误的，客观的，主观的。不同的解释方式产生不同的行为，不同的行为会造成不同的结果，造成的结果无论好坏都要受着，这是因果，不可能逃脱。</p><p>最近觉得网络的东西太过于无聊，各种各样的信息，不同立场的人，不同角度的人。热爱生活的，厌世嫉俗的。不同境遇的，充满选择的人和没有选择的人。世界就是这么的参差不齐。回到自我本身，我该怎么去解释这个世界。最近读马克思，实事求是是必要的，实践是检验真理的唯一标准是必要的，与其去网络上看别人任何解释这个世界，不如自己拿起笔写下自己对世界的解释。别人的经验是只能参照的，</p><p>要有自我主动权，解放思想，我们要有敢于摸着石头过河的勇气。成功了，增长能力也有了经验；失败了，就有了一次知道错误出在哪儿的认识。不要高估自己的能力，不要傲慢。人都有一个通病：说到别人的时候，就是这也不行，那也不行；而说到自己的时候，就有一种世外高人的感觉。平庸来自傲慢，如果人人生而伟大，如果每个人本来都是生命的奇迹。那么，傲慢毁掉了多少人？<br>成功的经验可以借鉴但是不可以复制，“见路不走”是不唯经验教条。成功的经验我们一般不能复制，因为那个经验发生的条件已成过去式，现在的条件只符合现在时。众生总是看到事物的各种表相而偏离本质，要见，见自本性，向内求解，以自己的觉性来看待事物。大家都是人，别人能做到的我也能做到。事实不是这样的，别人能做到的我不一定能做到，‘都是人’只是其中的一个条件，只有我具备了别人能做到的全部条件，我才可能做到，而事实上我很难悉数复制别人的条件，只有根据我的条件去做我能做到的，才是不脱离实际的。<br>对人必须有理有节，对己则可自由豁达。小到个人，家庭，大到国家，社会，自由都是有前提的；不同的人有不同的自由。</p><p>认识是慢慢的认识的，不可能突然从小学生到大学生的，这需要个过程。如果想要从小学生到大学生，这是不可能的。想可想之想，能可能之能。<br>关于认识我觉得实践论中的阐述是最好的，这里直接粘贴了，实践过程中，开始只是看到过程中各个事物的现象方面，看到各个事物的片面，看到各个事物之间的外部联系。例如有些外面的人们到延安来考察，头一二天，他们看到了延安的地形、街道、屋宇，接触了许多的人，参加了宴会、晚会和群众大会，听到了各种说话，看到了各种文件，这些就是事物的现象，事物的各个片面以及这些事物的外部联系。这叫做认识的感性阶段，就是感觉和印象的阶段。（认识的第一阶段）社会实践的继续，使人们在实践中引起感觉和印象的东西反复了多次，于是在人们的脑子里生起了一个认识过程中的突变（即飞跃），产生了概念。概念这种东西已经不是事物的现象，不是事物的各个片面，不是它们的外部联系，而是抓着了事物的本质，事物的全体，事物的内部联系了。《三国演义》上所谓“眉头一皱计上心来”，我们普通说话所谓“让我想一想”，就是人在脑子中运用概念以作判断和推理的工夫。这是认识的第二个阶段。<br>外来的考察团先生们在他们集合了各种材料，加上他们“想了一想”之后，他们就能够作出“共产党的抗日民族统一战线的政策是彻底的、诚恳的和真实的”这样一个判断了。在他们作出这个判断之后，如果他们对于团结救国也是真实的的话，那末他们就能够进一步作出这样的结论：“抗日民族统一战线是能够成功的。”这个概念、判断和推理的阶段，在人们对于一个事物的整个认识过程中是更重要的阶段，也就是理性认识的阶段。  认识的真正任务在于经过感觉而到达于思维，到达于逐步了解客观事物的内部矛盾，了解它的规律性，了解这一过程和那一过程间的内部联系，即到达于论理的认识。<br>在低级阶段，认识表现为感性的，在高级阶段，认识表现为论理的，但任何阶段，都是统一的认识过程中的阶段。感性和理性二者的性质不同，但又不是互相分离的，它们在实践的基础上统一起来了。</p><p>关于做事（做事也是认识的范畴），也是这样，慢慢来才最快。这里同样copy实践论中的内容。<br>常常听到一些同志在不能勇敢接受工作任务时说出来的一句话：没有把握。为什么没有把握呢？<br>因为他对于这项工作的内容和环境没有规律性的了解，或者他从来就没有接触过这类工作，或者接触得不多，因而无从谈到这类工作的规律性。及至把工作的情况和环境给以详细分析之后，他就觉得比较地有了把握，愿意去做这项工作。如果这个人在这项工作中经过了一个时期，他有了这项工作的经验了，而他又是一个肯虚心体察情况的人，不是一个主观地、片面地、表面地看问题的人，他就能够自己做出应该怎样进行工作的结论，他的工作勇气也就可以大大地提高了。只有那些主观地、片面地和表面地看问题的人，跑到一个地方，不问环境的情况，不看事情的全体（事情的历史和全部现状），也不触到事情的本质（事情的性质及此一事情和其他事情的内部联系），就自以为是地发号施令起来，这样的人是没有不跌交子的。<br>由此看来，认识的过程，第一步，是开始接触外界事情，属于感觉的阶段。第二步，是综合感觉的材料加以整理和改造，属于概念、判断和推理的阶段。只有感觉的材料十分丰富（不是零碎不全）和合于实际（不是错觉），才能根据这样的材料造出正确的概念和论理来。</p><p>实践是认识的来源，认识又可以变革实践。实践和认识的关系如同美和丑的关系，如同硬币的一体两面，是辩证的。通过实践而发现真理，又通过实践而证实真理和发展真理。从感性认识而能动地发展到理性认识，又从理性认识而能动地指导革命实践，改造主观世界和客观世界。实践、认识、再实践、再认识，这种形式，循环往复以至无穷，而实践和认识之每一循环的内容，都比较地进到了高一级的程度。这就是辩证唯物论的全部认识论，这就是辩证唯物论的知行统一观。</p><p>所以我的想法是，自己去实践，去认识，而不是去把他人带有主观的解释当作自己的解释，我不愿意吃别人的口水，关于这个世界我要亲自去尝一尝，去变革一下。</p><h2 id="注文"><a href="#注文" class="headerlink" title="注文"></a>注文</h2><p>注： 2024&#x2F;4&#x2F;30,又一次修改，不上网是真的难（哈哈哈哈）。<br>我看书的最终目的是集成百家之言，成一家之说，以自己的见解和实践去解释这个世界，改造这个世界。</p><p>认识论指出，成功秘诀就是坚持不懈。成功有时不需要精明，更依赖坚持，稳重坚守就是成功。不断的进步，从感性认识到理性认识。<br>2024&#x2F;5&#x2F;2日修改<br>解释一下什么是世界观，价值观，人生观。</p><ol><li>世界观(亦称“宇宙观”)，通常是指人们对整个世界(即对自然界、社会和人的思维)的根本看法。世界观不同，表现为人们在认识和改造世界时的立场、观点和方法的不同。一个人世界观、人生观的改变是一种根本的改变。世界观的基本问题是精神与物质、思维与存在、主观与客观的关系问题。在实践的基础上，实事求是，解放思想，尊重客观规律，注重调查研究，不断总结经验，开动脑筋想问题、办事情。</li><li>人生观，是人们对人生问题的根本看法。主要内容是对人生目的、意义的认识和对人生的态度，具体包括公私观、义利观、苦乐观、荣辱观、幸福观和生死观等。人生观是人们在人生实践和生活环境中逐步形成的。</li><li>价值观，是人们对价值问题的根本看法，是人们在一定历史条件下经过反复实践逐渐形成的一种与人的主观需要相连的判断好坏、是非、利弊、善恶的观念。包括对价值的实质、构成、标准的认识，这些认识的不同，形成了人们不同的价值观。每个人都是在各自的价值观的引导下，形成不同的价值取向，追求着各自认为最有价值的东西。</li></ol><p>阐述一下三者关系<br>世界观是人生观和价值观的基础，世界观决定着人生观和价值观。作为对人生意义和目的的特定理解的人生观，以及作为主体设定其价值目标和行为取向的价值观，都要以一定世界观作为思想基础，并支配其人生思考和选择的表现形式。世界观、人生观、价值观虽然各有自己的特定内容，但三者是统一的，不可分割的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>句子</title>
    <link href="/2024/04/02/juzhi0/"/>
    <url>/2024/04/02/juzhi0/</url>
    
    <content type="html"><![CDATA[<BR><h2 id="1"><a href="#1" class="headerlink" title="1"></a>1</h2><p>图难于其易，为大于其细。天下难事必作于易，天下大事必作于细。是以圣人终不为大，故能成其大。</p><h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><p>天有道，则无常道。事于道，则天有道看与事则无常，无常则明，明则通，则世事洞明。世事洞明则世事可治愈渐达佳境。</p><p>解释： 天如果真的有直指究竟的道，也肯定不是一直不变的道。做事依照着前人经验，则“道”看似有规律可循，其实世事无常没有相同一件事，懂得无常的道理就能把事情看清楚，把事情看清楚了就能做到通达。明白世事无常，才能看清一切事物“无常”的本质，看清本质就能做到通达，通达则洞明世事。世事洞明，才可能治愈一切事。看透世事无常来去皆好的本质，人就能做到通达，通达则洞明世事。世事洞明，自然可以治愈一切事。人能够活成这样的通透，当然就没有任何挂碍，也就步入了人生最妙的佳境!</p><h2 id="3"><a href="#3" class="headerlink" title="3"></a>3</h2><p>夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。</p><h2 id="4"><a href="#4" class="headerlink" title="4"></a>4</h2><p>为天地立心，为生民立命，为往圣继绝学，为万世开太平</p><p>（注：横渠四句教：第一句，为天地立心，这句话的前提是天地是没有心的，那什么有心，人有心？人有心之后，天地才有了心。人为天地所立的心称为道心，在认识论的层面去为天地所立心。人不知，人不相，人不愠。第二句，为生民立命，说人心，要帮助人构建价值观，要让人心基于正道，构建价值观。只有当一个人构建其了完备自洽的价值观，价值观得出了一个关于人生的终极价值判断，这个价值判断才叫做命，古时候叫“天命”，现在叫“使命”。有了使命我们才能用尽一生去追求，才不会犹豫不决，才不会彷徨不前，才能够“虽千万人吾往矣”。第三局，“为往圣继绝学”，说的是要学习之前的知识，要不断打磨之间的价值观，不断迭代这个时代的价值观。“苟日新，又日新，日日新”。第四句，为万世开太平，怎么才能为万世开太平？哲学家们只是用不用的方式解释世界,问题在于改变世界，实践，理论联系实践，中国的经典从来都是强调实践的，允执厥中”，也就是“中庸”，什么叫“中庸”？“中”的标准就是“仁”，“庸”则是“用”，是时时刻刻、千秋万世的“中用”，不实践怎么能中用呢？所以孔子说“游于艺”，不但要具备付诸实践的才能，还要在领域内追求极致，做到“游”，也就是“游刃有余”。唯有如此，才是“君子不器（而无不器）”，才能做到“无为而无不为”，才能“为万世开太平”。探讨自然与社会的基本规律，为民众摸索出一条共同遵行的大道，继承优良的传统文化，为后世开辟永久太平的基业。概括而言，就是探索精神，担当精神，奉献精神，使命精神。很高大上吗？不，这只是读书人本分而已。）</p><h2 id="5"><a href="#5" class="headerlink" title="5"></a>5</h2><p>真正的理性从来都是当下的，从来都是实践的，而实践，从来都是当下的理性。</p><h2 id="6"><a href="#6" class="headerlink" title="6"></a>6</h2><p>这是一个纷纷扰扰的滚滚红尘，众生沉迷而不求解脱，驱使着尘世中的我们有时也不得不加入其中滚一下；这是一个没有方向的名利场，大家每个人都有自己的执念，且价值观单一，在单一的价值观之下，为了同一个东西，大家便如丛林法则中的生物一样，让名利场成为一个绞肉机。</p><p>注：人总是要死亡的，生命的起点到终点，生命又有生命意义。这个过程就是意义，死亡是告诉我生命的意义就是感受过程，不用太多的去忧愁什么，天下本无事，庸人自扰之。 </p><p>焦虑，欲望，犹豫，焦虑对未来事件的过度担忧和恐慌，对未来的迷茫和没信心。怎么解决这个问题，感受过程，活在当下，专注于眼前的事，不必胡思乱想。静坐，或许是一种回归当下，摆脱头脑中的忧虑的方法。<br>欲望是有好坏之分的，好的欲望能帮我们正向成长，但是不好的欲望却是阻碍成长的绊脚石。要想破除欲望之贼，就要学会舍弃。舍弃不是失去，而是另一种获得。不要害怕舍弃，舍弃一件事物的终点和获得另一件事物的起点重合于一点。<br>犹豫，面对事情犹豫不决是一种很常见的现象。在决心做事时；就立刻去做，不要考虑事情的结果。做事的过程中不断磨砺自己，在事情中做到不以物喜，不以己悲。感受过程中的快乐，这样自然不会害怕因为选择错误而带来的失败。<br>“ 坐中静，破焦虑之贼；舍中得，破欲望之贼；事上练，破犹豫之贼，三贼皆破，则万事可成。”</p><p>长寿秘诀就是生活规律；成功秘诀就是坚持不懈。成功有时不需要精明，更依赖坚持，稳重坚守就是成功。）</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/04/02/hello-world/"/>
    <url>/2024/04/02/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span> <br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
