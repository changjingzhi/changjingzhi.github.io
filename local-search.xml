<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>随笔14</title>
    <link href="/2024/05/01/ganwu14/"/>
    <url>/2024/05/01/ganwu14/</url>
    
    <content type="html"><![CDATA[<h2 id="内心的想法"><a href="#内心的想法" class="headerlink" title="内心的想法"></a>内心的想法</h2><p>清空掉头脑中的思想，没有欲望，感受着时间的流动，像是生命在流动。没有感情的波动，一切都刚刚好。发自内心中的喜悦，获得了片刻的自由。<br>人啊人啊，什么禁锢了我，心中的牢笼束缚了自己内心的想法。欲望来源于什么，物质禁锢了身体，外在便不再自由。<br>我该怎么突破这内心和外在的牢笼，去感受那片刻、短暂的喜悦。<br>心啊心啊，为何要自我束缚，物啊物啊，将我带入了沉沦。<br>外在的法则和心中的法则并不相通，我连接了内心和外在。<br>时间啊，你在不断的消逝，也在不断的增加。<br>新生和死亡，未来和过去，或许只有现在最重要。<br>牢笼将我们禁锢，死亡为我指引方向，感受提出了方法。<br>去感受悲伤，感受快乐，感受情绪，感受内心。去感受风，感受雨，感受这个世界。<br>累了就休息，困了就睡觉。生活就是如此简单。<br>活出自己想要的人生，只有自己才知道自己的价值。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑——强化学习——使用智能体来玩游戏</title>
    <link href="/2024/04/30/tiankeng6/"/>
    <url>/2024/04/30/tiankeng6/</url>
    
    <content type="html"><![CDATA[<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://github.com/luozhiyun993/FlappyBird-PPO-pytorch">github上的代码</a><br><a href="https://hrl.boyuai.com/chapter/">动手强化学习</a><br><a href="https://github.com/alexzhuustc/gym-flappybird">tensorflow实现的强化学习</a><br><a href="https://github.com/alexzhuustc/gym-flappybird">pytorch+gym实现的flyappybird</a></p><h2 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h2><p>（广义的讲）强化学习是机器通过与环境交互来实现目标的一种计算方法。机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，这个环境发生相应的改变并且将相应的奖励反馈和下一轮状态传回机器。这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的累积奖励的期望。强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。<br>在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移。智能体则在下一轮交互中感知到新的环境状态，依次类推。<br>在这个过程中，智能体有3种控制要素、即感知、决策和奖励<br>感知：智能体在某种程度上感知环境的状态，从而知道自己所处的现状。<br>决策： 智能体根据感知的现状计算出达到目标需要采取的动作的过程叫做决策。比如，针对当前棋盘决定下一颗落子的位置。<br>奖励： 环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈。这个标量信号衡量智能体的好坏。（类似于在深度学习中的损失函数，）</p><h2 id="游戏选择-FlappyBird"><a href="#游戏选择-FlappyBird" class="headerlink" title="游戏选择 FlappyBird"></a>游戏选择 FlappyBird</h2><p>flappy bird》是一款由来自越南的独立游戏开发者Dong Nguyen所开发的作品，游戏于2013年5月24日上线，并在2014年2月突然暴红。2014年2月，《Flappy Bird》被开发者本人从苹果及谷歌应用商店（Google Play）撤下。2014年8月份正式回归App Store，正式加入Flappy迷们期待已久的多人对战模式。游戏中玩家必须控制一只小鸟，跨越由各种不同长度水管所组成的障碍。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构-shuffleNet</title>
    <link href="/2024/04/30/deeplearnpaper5/"/>
    <url>/2024/04/30/deeplearnpaper5/</url>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/32304419">ShuffleNet论文参考</a><br><a href="https://arxiv.org/pdf/1707.01083">ShuffleNet的论文原文</a></p><h2 id="实践使用shuffleNet来实现垃圾的40分类"><a href="#实践使用shuffleNet来实现垃圾的40分类" class="headerlink" title="实践使用shuffleNet来实现垃圾的40分类"></a>实践使用shuffleNet来实现垃圾的40分类</h2><h3 id="划分固定数据集"><a href="#划分固定数据集" class="headerlink" title="划分固定数据集"></a>划分固定数据集</h3><p>在这里划分固定数据集，生成两个csv表，一个是训练集，一个是验证集</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs lua">import <span class="hljs-built_in">os</span><br>import csv<br>import numpy as np<br>train_path = <span class="hljs-string">&quot;train_data.csv&quot;</span><br>val_path = <span class="hljs-string">&quot;val_data.csv&quot;</span><br><br>train_percent = <span class="hljs-number">0.9</span><br><br>def create_data_txt(<span class="hljs-built_in">path</span>):<br>    f_train = <span class="hljs-built_in">open</span>(train_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&quot;&quot;</span>)<br>    f_val = <span class="hljs-built_in">open</span>(val_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&quot;&quot;</span>)<br>    train_writer = csv.writer(f_train)<br>    val_writer = csv.writer(f_val)<br><br>    <span class="hljs-keyword">for</span> cls,dirname <span class="hljs-keyword">in</span> enumerate(<span class="hljs-built_in">os</span>.listdir(<span class="hljs-built_in">path</span>)):<br>        flist = <span class="hljs-built_in">os</span>.listdir(<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>,dirname))<br>        np.<span class="hljs-built_in">random</span>.shuffle(flist)<br>        fnum = <span class="hljs-built_in">len</span>(flist)<br>        <span class="hljs-keyword">for</span> i,filename <span class="hljs-keyword">in</span> enumerate(flist):<br>            <span class="hljs-keyword">if</span> i &lt; fnum*train_percent:<br>                train_writer.writerow([<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>,dirname,filename),str(cls)])<br>            <span class="hljs-keyword">else</span>:<br>                val_writer.writerow([<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>, dirname, filename), str(cls)])<br><br>    f_train.<span class="hljs-built_in">close</span>()<br>    f_val.<span class="hljs-built_in">close</span>()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    create_data_txt(<span class="hljs-string">&quot;data_garbage&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="dataset-设置。"><a href="#dataset-设置。" class="headerlink" title="dataset 设置。"></a>dataset 设置。</h3><p>在这里设置数据预处理的操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms,utils<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset,DataLoader<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>train_tf = transforms.Compose([<br>    <span class="hljs-comment"># transforms.RandomResizedCrop(size=(224,224), scale=(0.9,1.1)),</span><br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    transforms.RandomRotation(<span class="hljs-number">10</span>),<br>    transforms.ColorJitter(brightness=(<span class="hljs-number">0.9</span>,<span class="hljs-number">1.1</span>),contrast=(<span class="hljs-number">0.9</span>,<span class="hljs-number">1.1</span>)),<br>    <span class="hljs-comment"># transforms.Resize((50,50)),</span><br>    transforms.ToTensor(),<br>])<br><br>val_tf = transforms.Compose([<br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>    <span class="hljs-comment"># transforms.Grayscale(1),</span><br>    transforms.ToTensor(),<br>])<br><br><span class="hljs-comment">#自定义数据集</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Animals_dataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,istrain=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-keyword">if</span> istrain:<br>            f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;train_data.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;val_data.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>)<br>        self.dataset = f.readlines()<br>        f.close()<br>        self.istrain = istrain<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.dataset)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        data = self.dataset[index]<br>        img_path = data.split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">0</span>]<br>        cls = <span class="hljs-built_in">int</span>(data.split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">1</span>])<br><br>        img_data = Image.<span class="hljs-built_in">open</span>(img_path).convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>        <span class="hljs-keyword">if</span> self.istrain:<br>            dst = train_tf(img_data)<br>        <span class="hljs-keyword">else</span>:<br>            dst =val_tf(img_data)<br><br>        <span class="hljs-keyword">return</span> dst,torch.tensor(cls)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">visulization</span>():<br>    train_dataset = Animals_dataset(<span class="hljs-literal">True</span>)<br>    train_dataloader = DataLoader(train_dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>    examples = <span class="hljs-built_in">enumerate</span>(train_dataloader)<br>    batch_index,(data, lable) = <span class="hljs-built_in">next</span>(examples)<br>    <span class="hljs-built_in">print</span>(data.shape)<br><br>    grid = utils.make_grid(data)<br>    plt.imshow(grid.numpy().transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>    plt.show()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    visulization()<br></code></pre></td></tr></table></figure><h3 id="训练代码"><a href="#训练代码" class="headerlink" title="训练代码"></a>训练代码</h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim,nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> dataset <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>m = nn.Softmax(dim=<span class="hljs-number">1</span>)<br>def train(<span class="hljs-keyword">method</span>=&quot;normal&quot;,ckpt_path=&quot;&quot;):<br>    # 数据集和数据加载器<br>    train_dataset = Animals_dataset(<span class="hljs-keyword">True</span>)<br>    train_dataloader = DataLoader(train_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-keyword">True</span>)<br>    val_dataset = Animals_dataset(<span class="hljs-keyword">False</span>)<br>    val_dataloader = DataLoader(val_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-keyword">False</span>)<br><br>    #模型<br>    device = torch.device(&quot;cuda&quot; <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> &quot;cpu&quot;)#系统自己决定有啥训练<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">method</span>==&quot;normal&quot;:<br>       # 创建ShuffleNet模型<br>        model = models.shufflenet_v2_x1_0(pretrained=<span class="hljs-keyword">True</span>)  # 使用预训练的ShuffleNetV2模型<br><br>        # 修改最后的全连接层以适应您的数据集<br>        num_ftrs = model.fc.in_features<br>        model.fc = nn.Linear(num_ftrs,<span class="hljs-number">40</span>)  # 将全连接层输出维度修改为您数据集的类别数<br>        model.<span class="hljs-keyword">to</span>(device)<br>    print(&quot;train on &quot;,device)<br>    #损失函数（二分类交叉熵）<br>    loss_fn = nn.CrossEntropyLoss()<br><br>    #优化器<br>    optimizer = optim.RMSprop(model.parameters(),lr=<span class="hljs-number">0.0001</span>)<br><br>    #断点恢复<br>    start_epoch = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">if</span> ckpt_path != &quot;&quot;:<br>        <span class="hljs-keyword">checkpoint</span> = torch.<span class="hljs-keyword">load</span>(ckpt_path)<br>        model.load_state_dict(<span class="hljs-keyword">checkpoint</span>[&quot;net&quot;])<br>        optimizer.load_state_dict(<span class="hljs-keyword">checkpoint</span>[&quot;optimizer&quot;])<br>        start_epoch = <span class="hljs-keyword">checkpoint</span>[&quot;epoch&quot;] + <span class="hljs-number">1</span><br><br>    #训练<br>    train_loss_arr = []<br>    train_acc_arr = []<br>    val_loss_arr = []<br>    val_acc_arr = []<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">30</span>):<br>        train_loss_total = <span class="hljs-number">0</span><br>        train_acc_total = <span class="hljs-number">0</span><br>        val_loss_total = <span class="hljs-number">0</span><br>        val_acc_total = <span class="hljs-number">0</span><br>        model.train()<br>        <span class="hljs-keyword">for</span> i,(train_x,train_y) <span class="hljs-keyword">in</span> enumerate(train_dataloader):<br>            train_x = train_x.<span class="hljs-keyword">to</span>(device)<br>            train_y = train_y.<span class="hljs-keyword">to</span>(device)<br><br>            train_y_pred = model(train_x)<br>            train_loss = loss_fn(train_y_pred.squeeze(),train_y)<br>            train_acc = (m(train_y_pred).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] == train_y).sum()/train_y.shape[<span class="hljs-number">0</span>]<br>            train_loss_total += train_loss.data.item()<br>            train_acc_total += train_acc.data.item()<br><br>            train_loss.backward()<br>            optimizer.step()<br>            optimizer.zero_grad()<br><br>            print(&quot;epoch:&#123;&#125; train_loss: &#123;&#125; train_acc: &#123;&#125;&quot;.format(epoch,train_loss.data.item(),train_acc.data.item()))<br>        <br>        train_loss_arr.append(train_loss_total / len(train_dataloader))<br>        train_acc_arr.append(train_acc_total / len(train_dataloader))<br><br>        model.eval()<br><br>        <span class="hljs-keyword">for</span> j, (val_x,val_y) <span class="hljs-keyword">in</span> enumerate(val_dataloader):<br>            val_x = val_x.<span class="hljs-keyword">to</span>(device)<br>            val_y = val_y.<span class="hljs-keyword">to</span>(device)<br><br>            val_y_pred = model(val_x)<br>            val_loss = loss_fn(val_y_pred.squeeze(),val_y)<br>            val_acc = (m(val_y_pred).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]==val_y).sum()/val_y.shape[<span class="hljs-number">0</span>]<br>            val_loss_total += val_loss.data.item()<br>            val_acc_total += val_acc.data.item()<br><br>        val_loss_arr.append(val_loss_total / len(val_dataloader))  # 平均值<br>        val_acc_arr.append(val_acc_total / len(val_dataloader))<br>        print(&quot;epoch:&#123;&#125; val_loss:&#123;&#125; val_acc:&#123;&#125;&quot;.format(epoch, val_loss_arr[<span class="hljs-number">-1</span>], val_acc_arr[<span class="hljs-number">-1</span>]))<br><br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)   # 画布一分为二,<span class="hljs-number">1</span>行<span class="hljs-number">2</span>列，用第一个<br>    plt.title(&quot;loss&quot;)<br>    plt.plot(train_loss_arr, &quot;r&quot;, label=&quot;train&quot;)<br>    plt.plot(val_loss_arr, &quot;b&quot;, label=&quot;val&quot;)<br>    plt.legend()<br><br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  # 画布一分为二,<span class="hljs-number">1</span>行<span class="hljs-number">2</span>列，用第一个<br>    plt.title(&quot;acc&quot;)<br>    plt.plot(train_acc_arr, &quot;r&quot;, label=&quot;train&quot;)<br>    plt.plot(val_acc_arr, &quot;b&quot;, label=&quot;val&quot;)<br>    plt.legend()<br>    plt.savefig(&quot;loss_acc-1.png&quot;)<br><br>    plt.<span class="hljs-keyword">show</span>()<br><br>    # 保存模型<br>    # <span class="hljs-number">1.</span>torch.save()<br>    # <span class="hljs-number">2.</span>文件的后缀名：.pt、.pth、.pkl<br>    torch.save(model.state_dict(), r&quot;shuffeNet.pth&quot;)<br>    print(&quot;保存模型成功!&quot;)<br><br><br><br><span class="hljs-keyword">if</span> __name__ == &quot;__main__&quot;:<br>    train()<br><br><br>    train()<br><br><br></code></pre></td></tr></table></figure><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="什么是断点训练"><a href="#什么是断点训练" class="headerlink" title="什么是断点训练"></a>什么是断点训练</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文思路——论文阅读文献</title>
    <link href="/2024/04/30/paper-idear3/"/>
    <url>/2024/04/30/paper-idear3/</url>
    
    <content type="html"><![CDATA[<h2 id="论文阅读记录"><a href="#论文阅读记录" class="headerlink" title="论文阅读记录"></a>论文阅读记录</h2><ol><li><a href="https://ieeexplore.ieee.org/abstract/document/10179900">DICE-Net</a> 发表在IEEE Access 2022-2023年实时影响因子为3.9，中科院分区3区到4区。论文为OA论文，开源。<br>启发是对数据预处理的方法（比如RBP，SCC），一种未验证的双输入数据模型结构（注：我看来就是数据的通道加一）。这篇论文在公开 <a href="https://www.mdpi.com/2306-5729/8/6/95">数据集</a>  做的分类为CN&#x2F;AD,FTD&#x2F;CN,这么划分，是因为他要做对比实验，横向对比。实验结果是CN&#x2F;AD 的ACC为83.28%，SENS 79.81% ，SPEC 为87.94%, PREC为88.94%，F1为84.12% 。在FTD&#x2F;CN上ACC为74.96%，SENS 60.62% ，SPEC 为78.63%, PREC为64.01%，F1为62.27%</li></ol><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="公开数据集介绍"><a href="#公开数据集介绍" class="headerlink" title="公开数据集介绍"></a>公开数据集介绍</h3><p>This article provides a detailed description of a resting-state EEG dataset of individuals with Alzheimer’s disease and frontotemporal dementia, and healthy controls. The dataset was collected using a clinical EEG system with 19 scalp electrodes while participants were in a resting state with their eyes closed. The data collection process included rigorous quality control measures to ensure data accuracy and consistency. The dataset contains recordings of 36 Alzheimer’s patients, 23 frontotemporal dementia patients, and 29 healthy age-matched subjects.<br>提供了19通道的 (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2) 的脑电记录数据，有36个AD患者，23个FDT（前额叶痴呆），29个CN对照。数据包含了未经过伪迹处理的和已经经过伪迹处理的信号（伪迹处理的信号过程请参考论文）</p><h3 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h3><p>鉴于论文大修，所以我打算重新构建一下思路。</p><ol><li><p>标题</p></li><li><p>摘要</p></li><li><p>关键词</p></li><li><p>介绍</p></li><li><p>数据和方法</p></li><li><p>结果</p></li><li><p>讨论</p></li><li><p>参考文献</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数1</title>
    <link href="/2024/04/29/xianxindaishu/"/>
    <url>/2024/04/29/xianxindaishu/</url>
    
    <content type="html"><![CDATA[<p>神经元的工程实现使用了矩阵来作为工程学上的实现。线性代数研究向量空间和线性映射的理论。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ol><li>行列式 ，行列式的概念和基本性质 行列式按行（列）展开定理 。</li><li>矩阵 ， 矩阵的概念， 矩阵的线性运算， 矩阵的乘法， 方阵的幂， 方阵乘积的行列式， 矩阵的转置，逆矩阵的概念和性质， 矩阵可逆的充分必要条件， 伴随矩阵，矩阵的初等变换，初等矩阵，矩阵的秩，矩阵的等价 分块矩阵及其运算。 </li><li>向量 ，向量的概念， 向量的线性组合和线性表示， 向量组的线性相关与线性无关， 向量组的极， 大线性无关组， 等价向量组， 向量组的秩， 向量组的秩与矩阵的秩之间的关系， 向量的内积 ，线性无关向量组的的正交规范化方法 。</li><li>线性方程组 ，线性方程组的克拉默（Cramer）法则， 齐次线性方程组有非零解的充分必要条件， 非齐次线性方程组有解的充分必要条件， 线性方程组解的性质和解的结构， 齐次线性方程组的基础解系和通解， 非齐次线性方程组的通解 。</li><li>矩阵的特征值和特征向量， 矩阵的特征值和特征向量的概念、性质， 相似矩阵的概念及性质， 矩阵可相似对角化的充分必要条件及相似对角矩阵， 实对称矩阵的特征值、特征向量及其相似对角矩阵 。</li><li>二次型 ，二次型及其矩阵表示，合同变换与合同矩阵，二次型的秩，惯性定理，二次型的标准形和规范形， 用正交变换和配方法化二次型为标准形，二次型及其矩阵的正定性。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>有意思的句子</title>
    <link href="/2024/04/29/juzhi4/"/>
    <url>/2024/04/29/juzhi4/</url>
    
    <content type="html"><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>腰有十文，必振衣作响；</p><p>每遇美人，必急色登床；</p><p>相逢故交，必装逼摆阔；</p><p>每与人言，必谈及贵戚；</p><p>施人小惠，必广布于众；</p><p>与人言谈，必刁言逞才；</p><p>当人前称兄道弟，背人后揭人隐私；</p><p>借钱时觍颜如乞，拖款时蛮横如王。</p><p>—— 改述自林语堂</p><h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><p>小有姿色，则矫揉造作；</p><p>每逢故友，必揭人情史；</p><p>屡踩前任，且喋喋不休；</p><p>偶购名牌，必通告全网；</p><p>严人宽己，常自怜自艾；</p><p>妒人姻缘，必暗中破坏；</p><p>用你时姐妹情深，不用你音讯全无；</p><p>当人前惺惺作态，背人后口吐芬芳。</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高等数学</title>
    <link href="/2024/04/29/gaodengshuxue1/"/>
    <url>/2024/04/29/gaodengshuxue1/</url>
    
    <content type="html"><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>最近需要解析梯度下降算法，对于矩阵搞不清楚怎么进行多矩阵的梯度下降，算起来是补充基础了。</p><p><img src="/pic/gaodengshuxue1.jpg" alt="数学的图"></p><h2 id="高等数学研究了什么问题？"><a href="#高等数学研究了什么问题？" class="headerlink" title="高等数学研究了什么问题？"></a>高等数学研究了什么问题？</h2><p>高等数学是由微积分学，较深入的代数学、几何学以及它们之间的交叉内容所形成的一门基础学科。主要内容包括：数列、极限、微积分、空间解析几何与线性代数、级数、常微分方程。初等数学研究的是常量与匀变量，高等数学研究的是非匀变量。</p><ol><li><p>函数、极限、连续。<br>函数的概念是什么？函数的有界性、单调性、周期性和奇偶性是什么？复合函数、反函数、分段函数和隐函数。基本初等函数的性质。函数关系的建立。<br>数列极限与函数极限的定义及其性质，函数的左极限与右极限，无穷小量和无穷大量的概念及其关系，无穷小量的性质及无穷小量的比较。极限的存在准则：单调有界性准则和夹逼准则，两个重要极限。<br>函数连续的概念，函数间断点的类型，初等函数的连续性，闭区间上连续函数的性质。</p></li><li><p>一元函数的微分的问题。什么是微分？导数、微分。<br>导数和微分的概念，导数的几何意义和物理意义，函数的可导性与连续性之间的关系。<br>平面曲线的切线和法线，导数和微分的四则运算，基本初等函数的导数，复合函数、反函数、隐函数以及参数方程所确定的函数的微分法，高阶段导数，一阶段微分形式的不变性，微分中值定理（罗尔中值定理，拉格朗日中值定理，泰勒定理，柯西中值定理），洛必达法则，函数单调性的判别，函数的极值，函数的凹凸性，拐点以及渐近线，函数图形的描绘，函数的最大小值。<br>弧微分，曲率的概念，曲率圆与曲率半径。</p></li><li><p>一元函数积分学。<br>原函数和不定积分的概念，不定积分的基本性质，基本积分公式，定积分的概念和基本性质，定积分中值定理，积分上限的函数及其导数，牛顿-莱布尼茨（Newton-Leibniz）公式，不定积分和定积分的换元积分法与分部积分法。<br>有理函数、三角函数的有理式和简单无理函数的积分，反常（广义）积分，定积分的应用（平面图形的面积，平面曲线的弧长，旋转体的体积及侧面积，平行截面积为已知的立体体积、功、引力、压力、质心、形心等）及函数平均值。</p></li><li><p>多元函数微分学。<br>多元函数的概念，二元函数的几何意义，二元函数的极限与连续的概念，有界闭区间上二元连续函数的性质，多元函数的偏导数和全微分，多元复合函数、隐函数的求导法，二阶偏导数，多元函数的极值和条件极值、最大值和最小值，二重积分的概念、基本性子和计算。</p></li><li><p>常微分方程。<br>常微分方程的基本概念，变量可分离的微分方程，齐次微分方程，一阶线性微分方程 ，可降阶的高阶微分方程，线性微分方程解的性质及解的结构定理，二阶常系数齐次线性微分方程， 高于二阶的某些常系数齐次线性微分方程，简单的二阶常系数非齐次线性微分方程 ，微分方程的简单应用 </p></li><li><p>级数</p></li></ol><h2 id="填坑"><a href="#填坑" class="headerlink" title="填坑"></a>填坑</h2><h3 id="函数的概念"><a href="#函数的概念" class="headerlink" title="函数的概念"></a>函数的概念</h3><p>在数学中，函数是一种对应关系，它将一个集合中的每个元素（称为自变量）映射到另一个集合中的唯一元素（称为因变量）。函数通常用一个符号表示，例如 f(x)，表示自变量为 x 时对应的因变量值。<br>函数的概念包括以下几个要点：</p><p>定义域（Domain）： 函数的定义域是指所有可能作为自变量的值的集合。函数在定义域内有定义。<br>值域（Range）： 函数的值域是指所有可能作为因变量的值的集合。对于实函数（实数到实数的映射），值域是函数的所有可能取值的集合。<br>图像（Graph）： 函数的图像是在坐标系中表示函数的一种方式，横轴表示自变量，纵轴表示因变量。函数的图像可以帮助我们直观地理解函数的性质。<br>对应关系（Correspondence）： 函数是一种对应关系，它确保了每个自变量都有唯一的因变量与之对应。<br>函数的表示方式： 函数可以通过公式、图表、数据表等方式来表示。常见的函数包括多项式函数、指数函数、对数函数、三角函数等。</p><p>注： 补充概念，数学中的“元”指未知数，常见的一元一次，二元一次。数学中的“项”代表由数与未知数还有运算符号组成的基本算术单元。“次”就是方程中未知数的乘方数。<br><img src="/pic/gaodengshuxue2.png" alt="一元二次方程组"></p><h3 id="函数的性质。"><a href="#函数的性质。" class="headerlink" title="函数的性质。"></a>函数的性质。</h3><p>函数的有界性、单调性、周期性和奇偶性是什么？</p><p>有界性（Boundedness）：一个函数在某个区间内是有界的，意味着存在一个常数M，使得函数的取值始终在一个特定的范围内，即|f(x)| ≤ M，对于所有的x在给定的区间内成立。如果一个函数既有上界又有下界，则称该函数在该区间内是有界的。<br>函数的有界性判断方法？</p><p>单调性（Monotonicity）：一个函数在某个区间内是单调的，意味着函数的值随着自变量的增加而增加或者随着自变量的减少而减少。如果函数在区间上满足f(x1) ≤ f(x2)对于任意x1 &lt; x2或者f(x1) ≥ f(x2)对于任意x1 &lt; x2，则称函数在该区间内是单调递增或单调递减的。<br>函数单调性判断方法？</p><ol><li>定义法</li><li>导数法</li></ol><p>周期性（Periodicity）：一个函数在某个区间内是周期的，意味着存在一个正数T，使得对于所有的x在该区间内，有f(x+T) &#x3D; f(x)。即函数在一个周期内重复。</p><p>奇偶性（Odd and Even）：一个函数的奇偶性指的是函数的对称性。如果对于所有的x在定义域内，有f(-x) &#x3D; -f(x)，则称函数是奇函数。如果对于所有的x在定义域内，有f(-x) &#x3D; f(x)，则称函数是偶函数。</p><h3 id="复合函数"><a href="#复合函数" class="headerlink" title="复合函数"></a>复合函数</h3><p>设函数y&#x3D;f(u)的定义域为Du，函数u&#x3D;g(x)的定义域为Dg，值域Rg。若Du和Rg的交集不为空集，则称函数y&#x3D;f[g(x) ]为函数y&#x3D;f(u)与函数u&#x3D;g(x )的复合函数，它的定义域为{x|x属于Dg，g(x)属于Du}。<br>例如：如果有函数f(x) &#x3D; x^2 和g(x) &#x3D; 2x +1,那么他们的复合函数f(g(x))就是先计算g(x)，得到2x+1，然后将2x+1作为f(x)的输入，最终得到f(g(x)) &#x3D; (2x+1)^2<br>复合函数的性质：</p><ol><li>结合律：即(f。g)。h&#x3D;f。(g。h)，其中。表示复合。</li><li>交换律：一般情况下，复合函数不满足交换律，即一般情况下f。g&#x3D;&#x2F;&#x3D;g。f</li><li>单位元：对于每个函数f(x)，都存在一个单位元函数e(x) &#x3D; x ，使得f。e &#x3D; f和e。f &#x3D; f</li></ol><h3 id="反函数"><a href="#反函数" class="headerlink" title="反函数"></a>反函数</h3><p>反函数是指一个函数的逆运算，如果函数f将集合A中的元素映射到集合B中的元素，那么f的反函数f^-1将集合B中的元素映射回集合A中的元素，使得f^-1(f(x))&#x3D;x对于所有x成立。<br>例如：如果有函数f(x) &#x3D; 2x ,它将实数集合中的每一个数映射到其自身的两倍，那么它的反函数就是f^-1(x) &#x3D; x&#x2F;2，将任意实数x映射回其的一半。反函数在数学中可以用来解决函数的逆运算问题。</p>]]></content>
    
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文思路——数据预处理</title>
    <link href="/2024/04/28/paper-idear2/"/>
    <url>/2024/04/28/paper-idear2/</url>
    
    <content type="html"><![CDATA[<h2 id="目前的数据预处理有"><a href="#目前的数据预处理有" class="headerlink" title="目前的数据预处理有"></a>目前的数据预处理有</h2><p>This may be time-domain features(时域特征)<a href="dx.doi.org/10.3390/diagnostics11081437">45——2021年</a></p><ol><li>absolute band power(绝对功率谱)</li><li>Discrete Wavelet Transform (离散小波变换)<a href="https://ieeexplore.ieee.org/document/9857825">37-2022-IEEE</a></li><li>permutation entropy (排列熵) or spectral entropy (熵谱)[21-2021-Complexity of EEG dynamics for early diagnosis of Alzheimer’s disease using permutation entropy neuromarker,’’ C]</li><li>coherence anaylysis features (相干性分析) such as spectrall coherece(光谱相干性)</li><li>RBP (注：按照下面的频率划分，the shape of data is [T , B ,C]&#x3D;[T, 5, C]，Delta: 0.5 – 4 Hz Theta: 4 – 8 Hz Alpha: 8 – 13 Hz Beta: 13-25 Hz Gamma: 25-45 Hz,或许这个数据预处理的方法会生效 )</li><li>spectral coherence connectivity (光谱相干性，光谱相干性建立在PSD，PSD是功率谱密度)</li><li>FFT (Fast Fourier Transform) 快速傅里叶变换</li></ol><h2 id="数据预处理组合思路"><a href="#数据预处理组合思路" class="headerlink" title="数据预处理组合思路"></a>数据预处理组合思路</h2><ol><li>Morlet Wavelet Transform ——&gt; RBP</li><li>Welch PSD ——&gt; SSC</li></ol><h2 id="需要注意的前提知识。"><a href="#需要注意的前提知识。" class="headerlink" title="需要注意的前提知识。"></a>需要注意的前提知识。</h2><ol><li>AD patients may exhaibit changes in the EEG signal, such as reduced(减少) alpha power and increased (增加) theta power.<a href="https://www.sciencedirect.com/science/article/abs/pii/S1388245721005976">39-2021-Clinical Neurophysiology-3区 </a><br>It can be visually observed that AD group has lower delta connectivity than CN group in multiple brain locations.This finding is supported by the literature  [53-2016]<a href="https://www.sciencedirect.com/science/article/pii/S1388245715009839()">https://www.sciencedirect.com/science/article/pii/S1388245715009839()</a></li><li>Train, validation and test sets are created.</li><li>The time frequency transforms and the feature extraction steps were implemented in Python 3.10 using the MNE library.</li><li>GFlops (计算量)</li><li>hyperparameters (超参数)</li></ol><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="怎么计算模型的计算量。"><a href="#怎么计算模型的计算量。" class="headerlink" title="怎么计算模型的计算量。"></a>怎么计算模型的计算量。</h3><h2 id="看论文的心态"><a href="#看论文的心态" class="headerlink" title="看论文的心态"></a>看论文的心态</h2><p>Because it is an English paper, there is a kind of resistance. Take your time.<br>由于是英文论文，有种抗拒的心态。慢慢看吧。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><p>FFT快速傅里叶变换<a href="https://zhuanlan.zhihu.com/p/347091298">参考博客</a></p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import os<br>import numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-built_in">from</span> tqdm import tqdm<br><br><span class="hljs-comment"># 定义输入和输出文件夹路径</span><br>input_folder = <span class="hljs-string">&#x27;data_cut_npy/AD&#x27;</span>  <span class="hljs-comment"># 输入文件夹路径，包含要进行 FFT 变换的 .npy 文件</span><br>output_folder = <span class="hljs-string">&#x27;data_FFT_npy/AD&#x27;</span>  <span class="hljs-comment"># 输出文件夹路径，用于保存变换后的数据</span><br><br><span class="hljs-comment"># 确保输出文件夹存在</span><br>os.makedirs(output_folder, exist_ok=True)<br><br><span class="hljs-comment"># 获取输入文件夹中的所有 .npy 文件</span><br>file_list = os.listdir(input_folder)<br>npy_files = [<span class="hljs-built_in">file</span> <span class="hljs-keyword">for</span> <span class="hljs-built_in">file</span> <span class="hljs-keyword">in</span> file_list <span class="hljs-keyword">if</span> <span class="hljs-built_in">file</span>.endswith(<span class="hljs-string">&#x27;.npy&#x27;</span>)]<br><br><span class="hljs-comment"># 遍历每个 .npy 文件进行 FFT 变换并保存</span><br><span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> tqdm(npy_files, desc=<span class="hljs-string">&#x27;Processing&#x27;</span>, unit=<span class="hljs-string">&#x27;file&#x27;</span>):<br>    <span class="hljs-comment"># 读取 .npy 文件</span><br>    file_path = os.path.join(input_folder, file_name)<br>    data = np.<span class="hljs-built_in">load</span>(file_path)<br><br>    <span class="hljs-comment"># 对数据中的每一行进行 FFT 变换</span><br>    fft_data = np.apply_along_axis(np.fft.fft, axis=<span class="hljs-number">0</span>, arr=data)<br><br>    <span class="hljs-comment"># 获取 FFT 结果的幅值</span><br>    fft_magnitude = np.<span class="hljs-built_in">abs</span>(fft_data)<br><br>    <span class="hljs-comment"># 构造输出文件路径</span><br>    output_file_name = file_name.<span class="hljs-built_in">replace</span>(<span class="hljs-string">&#x27;.npy&#x27;</span>, <span class="hljs-string">&#x27;_fft.npy&#x27;</span>)<br>    output_file_path = os.path.join(output_folder, output_file_name)<br><br>    <span class="hljs-comment"># 保存 FFT 结果</span><br>    np.save(output_file_path, fft_magnitude)<br><br></code></pre></td></tr></table></figure></li><li><p>RBP 按频率划分[T, C, B]</p></li></ol><h2 id="什么是时域和频域"><a href="#什么是时域和频域" class="headerlink" title="什么是时域和频域"></a>什么是时域和频域</h2><p><a href="https://zhuanlan.zhihu.com/p/401681076">参考资料</a><br>原始的EEG数据是由很多个样本点数所构成的一个有限的离散的时间序列数据。至于样本点数的多少，则由采样率所决定，比如采样率为1000Hz，那么每秒就有1000个数据样本点。其中，每个样本点数据代表的是脑电波幅的大小，物理学上称为电压值，单位为伏特（V），由于脑电信号通常较弱，所以更常使用的单位为微伏（uV）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> mne<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_and_save_set_files</span>(<span class="hljs-params">input_folder, output_folder</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_data</span>(<span class="hljs-params">raw</span>):<br>        <span class="hljs-comment"># 获取信号数据</span><br>        data = raw.get_data() <span class="hljs-comment"># shape: (n_channels, n_samples)</span><br><br>        <span class="hljs-comment"># 定义频率范围</span><br>        freq_ranges = &#123;<br>            <span class="hljs-string">&#x27;Delta&#x27;</span>: (<span class="hljs-number">0.5</span>, <span class="hljs-number">4</span>),<br>            <span class="hljs-string">&#x27;Theta&#x27;</span>: (<span class="hljs-number">4</span>, <span class="hljs-number">8</span>),<br>            <span class="hljs-string">&#x27;Alpha&#x27;</span>: (<span class="hljs-number">8</span>, <span class="hljs-number">13</span>),<br>            <span class="hljs-string">&#x27;Beta&#x27;</span>: (<span class="hljs-number">13</span>, <span class="hljs-number">25</span>),<br>            <span class="hljs-string">&#x27;Gamma&#x27;</span>: (<span class="hljs-number">25</span>, <span class="hljs-number">45</span>)<br>        &#125;<br><br>        <span class="hljs-comment"># 初始化频带划分后的数据</span><br>        data_bands = []<br><br>        <span class="hljs-comment"># 对每个频带进行处理</span><br>        <span class="hljs-keyword">for</span> _, (fmin, fmax) <span class="hljs-keyword">in</span> freq_ranges.items():<br>            <span class="hljs-comment"># 使用 mne.filter 函数进行滤波</span><br>            filtered_data = mne.<span class="hljs-built_in">filter</span>.filter_data(data, raw.info[<span class="hljs-string">&#x27;sfreq&#x27;</span>], fmin, fmax)<br><br>            <span class="hljs-comment"># 将滤波后的数据存储到列表中</span><br>            data_bands.append(filtered_data)<br><br>        <span class="hljs-comment"># 将列表转换为numpy数组</span><br>        data_bands = np.array(data_bands)<br><br>        <span class="hljs-keyword">return</span> data_bands<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">find_set_files</span>(<span class="hljs-params">root_folder</span>):<br>        set_files = []<br>        <span class="hljs-keyword">for</span> root, dirs, files <span class="hljs-keyword">in</span> os.walk(root_folder):<br>            <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>                <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&#x27;.set&#x27;</span>):<br>                    set_files.append(os.path.join(root, file))<br>        <span class="hljs-keyword">return</span> set_files<br><br>    <span class="hljs-comment"># 找到所有的 .set 文件</span><br>    set_files = find_set_files(input_folder)<br><br>    <span class="hljs-keyword">for</span> file_path <span class="hljs-keyword">in</span> tqdm(set_files, desc=<span class="hljs-string">&#x27;Processing files&#x27;</span>):<br>        <span class="hljs-comment"># 读取 .set 文件</span><br>        raw = mne.io.read_raw_eeglab(file_path, preload=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># 处理数据</span><br>        processed_data = process_data(raw)<br><br>        <span class="hljs-comment"># 构造新的文件路径</span><br>        npy_file_name = os.path.basename(file_path).replace(<span class="hljs-string">&#x27;.set&#x27;</span>, <span class="hljs-string">&#x27;.npy&#x27;</span>)<br>        npy_file_path = os.path.join(output_folder, npy_file_name)<br><br>        <span class="hljs-comment"># 保存为 .npy 文件</span><br>        np.save(npy_file_path, processed_data)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 指定您的 AD 文件夹路径和保存 .npy 文件的新文件夹路径</span><br>    ad_folder = <span class="hljs-string">r&#x27;C:/Users/Administrator/Desktop/RBP/data/FDT&#x27;</span><br>    output_folder = <span class="hljs-string">r&#x27;C:/Users/Administrator/Desktop/RBP/data_npy/FDT&#x27;</span><br><br>    <span class="hljs-comment"># 处理并保存数据</span><br>    process_and_save_set_files(ad_folder, output_folder)<br><br></code></pre></td></tr></table></figure><h2 id="赫兹-HZ-的定义是什么？"><a href="#赫兹-HZ-的定义是什么？" class="headerlink" title="赫兹(HZ)的定义是什么？"></a>赫兹(HZ)的定义是什么？</h2><p>Hz 是频率的单位。频率是指电脉冲，交流电波形，电磁波，声波和机械的振动周期循环时，1秒钟重复的次数。1Hz代表每秒钟周期震动1次，60Hz代表每秒周期震动60次。<br>对于声音，人类的听觉范围为20Hz～20000Hz，低于这个范围叫做次声波，高于这个范围的叫做超声波。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>兰亭集序</title>
    <link href="/2024/04/28/juzhi3/"/>
    <url>/2024/04/28/juzhi3/</url>
    
    <content type="html"><![CDATA[<center><font size = 6> 兰亭集序 / 兰亭序<font><center><center >魏晋：王羲之<center><font size = 5>永和九年，岁在癸丑，暮春之初，会于会稽山阴之兰亭，修禊事也。群贤毕至，少长咸集。此地有崇山峻岭，茂林修竹；又有清流激湍，映带左右，引以为流觞曲水，列坐其次。虽无丝竹管弦之盛，一觞一咏，亦足以畅叙幽情。<p>是日也，天朗气清，惠风和畅，仰观宇宙之大，俯察品类之盛，所以游目骋怀，足以极视听之娱，信可乐也。</p><p>夫人之相与，俯仰一世，或取诸怀抱，悟言一室之内；或因寄所托，放浪形骸之外。虽趣舍万殊，静躁不同，当其欣于所遇，暂得于己，快然自足，不知老之将至。及其所之既倦，情随事迁，感慨系之矣。向之所欣，俯仰之间，已为陈迹，犹不能不以之兴怀。况修短随化，终期于尽。古人云：“死生亦大矣。”岂不痛哉！(不知老之将至 一作：曾不知老之将至)</p><p>每览昔人兴感之由，若合一契，未尝不临文嗟悼，不能喻之于怀。固知一死生为虚诞，齐彭殇为妄作。后之视今，亦犹今之视昔。悲夫！故列叙时人，录其所述，虽世殊事异，所以兴怀，其致一也。后之览者，亦将有感于斯文。</p><font><div style="text-align: left;"><font size = 3>译文：永和九年，时在癸丑之年，三月上旬，我们会集在会稽郡山阴城的兰亭，为了做禊礼这件事。诸多贤士能人都汇聚到这里，年长、年少者都聚集在这里。兰亭这个地方有高峻的山峰，茂盛高密的树林和竹丛；又有清澈激荡的水流，在亭子的左右辉映环绕，我们把水引来作为飘传酒杯的环形渠水，排列坐在曲水旁边，虽然没有管弦齐奏的盛况，但喝着酒作着诗，也足够来畅快表达幽深内藏的感情了。<p>这一天，天气晴朗，和风习习，抬头纵观广阔的天空，俯看观察大地上繁多的万物，用来舒展眼力，开阔胸怀，足够来极尽视听的欢娱，实在很快乐。</p><p>人与人相互交往，很快便度过一生。有的人在室内畅谈自己的胸怀抱负；就着自己所爱好的事物，寄托自己的情怀，不受约束，放纵无羁的生活。虽然各有各的爱好，安静与躁动各不相同，但当他们对所接触的事物感到高兴时，一时感到自得，感到高兴和满足，竟然不知道衰老将要到来。等到对于自己所喜爱的事物感到厌倦，心情随着当前的境况而变化，感慨随之产生了。过去所喜欢的东西，转瞬间，已经成为旧迹，尚且不能不因为它引发心中的感触，况且寿命长短，听凭造化，最后归结于消灭。古人说：“死生毕竟是件大事啊。”怎么能不让人悲痛呢？</p><p>每当我看到前人兴怀感慨的原因，与我所感叹的好像符契一样相合，没有不面对着他们的文章而嗟叹感伤的，在心里又不能清楚地说明。本来知道把生死等同的说法是不真实的，把长寿和短命等同起来的说法是妄造的。后人看待今人，也就像今人看待前人。可悲呀！所以一个一个记下当时与会的人，录下他们所作的诗篇。纵使时代变了，事情不同了，但触发人们情怀的原因，他们的思想情趣是一样的。后世的读者，也将对这次集会的诗文有所感慨。</p></div size = 3><font><div style="text-align: left;"><font size = 3></div size = 3><font>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔13</title>
    <link href="/2024/04/28/ganwu13/"/>
    <url>/2024/04/28/ganwu13/</url>
    
    <content type="html"><![CDATA[<h2 id="实事求是"><a href="#实事求是" class="headerlink" title="实事求是"></a>实事求是</h2><p>重新写一下实事求是，做事最重要的是什么？清醒，明白自己在干嘛，明白自己做事是为了进步的，没有进步这件事就不需要去做。以进步为导向，以完成这个目标为目的，不畏惧目标的远大，以实事求是的态度，对一个不懂的东西，要去研究它，了解它，等研究透了，知其然，知其所以然。等了解这项工作后才能评价它，在这之后能不能做我能清楚的明白，在没有做之前不需要情绪上的波动，不需要什么自卑，崇拜。<br>实事求是的条件是分析现实的条件，分析条件的组成，但这只是一个思路，具体到做又有实践中需要注意的，但是知和行是一体两面的（感觉对这个理论还是很混沌，没有很好的阐述出来）。实践是检验真理的唯一标准，明白这句话需要这句清晰的知道实践，真理，标准的范畴。<br>今天看到一篇博客，觉得是实事求举得例子是举得很好的，这里放个链接(<a href="https://www.zhihu.com/question/365148040/answer/968807103">https://www.zhihu.com/question/365148040/answer/968807103</a>)</p><p>对于我来讲以现实为角度，如同子曰：为政以德譬，如北辰居其所而众星共之。当人把北极星的位置确定后，执持这位置相应就可以定出其他星星位置；当人从现实出发分析把握了现实关系的逻辑结构后，还不能把握大道吗？一切都从也只能从现实出发，现实在什么阶段，什么位次，是必须首要分析的问题。<br>哲学家们只是用不用的方式解释世界,问题在于改变世界</p><h2 id="精神的独立"><a href="#精神的独立" class="headerlink" title="精神的独立"></a>精神的独立</h2><p>人不仅要在物质上独立，更要在精神上独立。畏难情绪，不验证的验证，对于不验证验证了什么？验证了自己能不能做这件事，验证了自己不能做这件事。这样的精神气质就不对了，很难提升境界了。人云亦云，亦步亦趋，这样的精神是不行的，精神上畏难，即使再努力也是没有用的。不要认为任何人是不可超越的，始终相信不断的积累是可以达到他们的境界的。无论是智力不如别人，还是学的不够扎实。精神上站起来，认识到自己，实践是认识的来源，认识是改变的第一步，是成长的第一步。</p><h2 id="教育的作用"><a href="#教育的作用" class="headerlink" title="教育的作用"></a>教育的作用</h2><p>教育，读书，明理，解决问题。中国的大学教育被大众大众赋予了太多的神话，认为读了大学什么问题就解决了，上了大学一切就都解决了。我想这样的想法或多或少是不恰当的，把自己的未来放置于虚幻的未来，不立足于当下，立足于现实，想来是不恰当的。我认为教育的作用是去启发自我的本性，培养自我的能力，让自己能够去迎接挑战，而不是去等，去守株待兔，等待那虚幻的未来。知识付出一点时间和精力就能够获取，但是对于心理和人格的成长教育是欠缺的，这欠缺的反而需要学生自己去寻找，自己去开启心智的成长。<br>谈精神之独立，鲁迅是谈的好的，为此我在这挖一个坑（读鲁迅的杂文集）</p><h2 id="对死亡抱有希望真的会有解脱吗？"><a href="#对死亡抱有希望真的会有解脱吗？" class="headerlink" title="对死亡抱有希望真的会有解脱吗？"></a>对死亡抱有希望真的会有解脱吗？</h2><p>时常的，在我闲下来的时候，从高处往下看，总会幻想跳下去后，一切问题就都解决了。将解决问题的最终方式诉诸于死亡是我最后解决问题的办法，因为生命最终都会消逝，或早或晚没有什么不好，有的只有快慢。人的这一生，如昙花一现。如草木春绿秋枯，如曦月东升西落。死亡好像没有什么不好，但是我认为没有考虑到的是，人出生下来就应该背负一定的责任，对自己负责，对家人负责。自己亲手了结自己的生命是对自己不负责的表现，对家人不负责的表现。烦恼总是有的，但是为什么不换个想法，这件事最好会达到什么程度，做不好我会遭受什么，大部分时候我感到困惑，感到不解的来源是我的情绪，然而情绪在这种时候是最没有用处的东西，被情绪所裹挟是没有任何益处的，控制自己的情绪又是困难的，诉诸于死亡是无法之法。或许可以在感受情绪的同时，去想想最坏的结果，最坏的结果会导致我死亡，会影响我的生活，亦或者会失去什么，预演之后我能接受最坏的结果，再回来看最坏的结果也不过如此，更重要的是最坏的结果还没有发生，还有去改变的机会，这时就付诸一切的努力去改变。接受最坏的结果，尽力改变我接受的最坏的结果，即使最后失败了，我也欣然接受，而不是陷入不证之证，把希望诉诸于死亡。</p><h2 id="做事闲谈"><a href="#做事闲谈" class="headerlink" title="做事闲谈"></a>做事闲谈</h2><p>最开始开始写随笔是4月5日，现在已经积累了13篇随笔了，回头看看好像也没有什么难的，好像慢慢的就有了，我完成网站的构建，一开始我对于建网站只是有一个很模糊的想法，只是想了一下，慢慢的我就开始接触域名的购买，框架的使用，跳转，其中虽然有困难，也好像困惑了很久，但是也都解决了，所以请不要担心你行动的结果——仅仅关注行动本身就好了。行动的结果会自然而然地产生。问题不是用来忧虑的，而是用来解决的，对待问题不应该感到忧虑，而应该感到快乐，快乐的是又有进步的空间，又有一道难题需要攻破。或许当时很忧虑，后来回头看看，回首向来萧瑟处，归去，也无风雨也无晴。每个人都真正会从事一生的事业，那就是：学习和成长！</p><h2 id="意义的来源"><a href="#意义的来源" class="headerlink" title="意义的来源"></a>意义的来源</h2><p>在随笔11中我写道“人生的意义是什么？”回答：“每个人都有不同的答案。”，我的答案是什么？每天进步一点点。<br>认识到死亡给我的意义。明天和意外哪个先来，谁也不知道。死亡不不可以控制的，但是我想心由境转，境由心生。认识到死亡是存在的一段时间，我认为一切是无意义的，死亡到来之时所有的一切都将消散，与我而言没有意义。于浩歌狂热之际中寒，于天上看见深渊。一天无意义，一生无意义。生命呀，你到底是为了什么？活着你到底是为了什么，活着就是为了活着，活着就是为了感受活着，活着就是每一天，每一刻去感受活着，在面对一个个挑战时有勇气去战胜自己，死亡赋予我战胜困难的勇气，在过程中感到意义，感到快乐，活着就是有意义，有意义就是好好活。不以物喜，不以自悲。</p><p>注： 今天重新看了之前的随笔，发现了许多错别字，这里挖个坑（后面慢慢修改，因为看和改完全是两种条件。改的条件是需要电脑和我发现错别字。）</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch基础</title>
    <link href="/2024/04/27/deeplearnbook3-1/"/>
    <url>/2024/04/27/deeplearnbook3-1/</url>
    
    <content type="html"><![CDATA[<p>参考书目：《Python深度学习基于PyTorch》</p><h2 id="Pytorch-基础"><a href="#Pytorch-基础" class="headerlink" title="Pytorch 基础"></a>Pytorch 基础</h2><p>Pytorch采用python语言接口实现编程，非常容易上手。它就像带GPU的Numpy，与Python一样都属于动态框架。PyTorch继承了Torch灵活、动态的编程环境和用户友好的界面，支持以快速灵活的方式构建动态神经网络，还允许在训练过程中快速更改代码而不妨碍其性能，支持动态图形等尖端AI模型的能力，是快速实验的理想选择。</p><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>Pytorch是建立在Torch库上的python包，目的在于加速深度学习的应用。它包含了多维张量的数据结构以及基于其上的多种数学操作。动态计算图<br>PyTorch 主要由4个包组成：<br>torch：类似于Numpy的通用数组库，可将张量类型转换为torch.cuda.Tensor.Float，并在GPU上进行计算。<br>torch.autograd: 用于构建计算图形并自动获取梯度的包<br>torch.nn: 具有共享层和损失函数的神经网络库。<br>torch.ptim： 具有通用优化算法（如SGD，Adam等）的优化包</p><h2 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h2><p>pytorch的GPU版本 的安装配置有一点繁琐，这里阐述一下需要的装备</p><ol><li>电脑（装有显卡，本台电脑是GTX-3080），安装GPU的驱动（如英伟达的NVDIA）以及CUDA，cuDNN计算框架。安装GPU驱动的时候就会安装CUDA，cuDNN的安装要去官网查找对应版本。</li><li>软件miniconda ，python的环境包管理</li><li>VS_code</li></ol><h2 id="Numpy和Tensor"><a href="#Numpy和Tensor" class="headerlink" title="Numpy和Tensor"></a>Numpy和Tensor</h2><p>前面说到深度学习的最主要的东西是矩阵，深度学习就是一个大的函数。Tensor是numpy的Pytorch中的实现(这么说不知道行不行，但我是这么认为的)，pytorch中的Tensor可以是零维（又称为一个标量或一个数）、一维、二维以及多维数组。Tensor可以把产生的Tensor放置在GPU中进行加速计算。<br>对Tensor的操作很多，从接口的角度可以划分为两类，<br>torch.function 如 torch.sum、torch.add<br>tensor.function ，如tensor.view、tensor.add等<br>这些操作对于大部分Tensor都是等价的，比如torch.add(x)与x.add(y)等价（注：前提是x的dtype是tensor）</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import torch<br>x = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[1 , 2]</span>)<br>y =torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[3,4]</span>)<br>z = x<span class="hljs-selector-class">.add</span>(y)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(z)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x)</span></span><br>x<span class="hljs-selector-class">.add_</span>(y)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x)</span></span><br></code></pre></td></tr></table></figure><p>Tensor创建的方式</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"># <span class="hljs-built_in">Tensor</span>(*size) 直接从参数构建一个张量<br></code></pre></td></tr></table></figure><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="仍然没有搞明白使用梯度下降算法来怎么对矩阵中的参数进行更新的方法？"><a href="#仍然没有搞明白使用梯度下降算法来怎么对矩阵中的参数进行更新的方法？" class="headerlink" title="仍然没有搞明白使用梯度下降算法来怎么对矩阵中的参数进行更新的方法？"></a>仍然没有搞明白使用梯度下降算法来怎么对矩阵中的参数进行更新的方法？</h3><p>目前网络上的梯度下降多使用线性函数来进行距离，没有推导过程，而我又不会推导，死循环了。所以我想知道怎么使用矩阵来实现梯度下降，进而实现SGD，Adam，Rsomp等梯度下降优化函数。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——ResNet</title>
    <link href="/2024/04/27/deeplearnpaper4/"/>
    <url>/2024/04/27/deeplearnpaper4/</url>
    
    <content type="html"><![CDATA[<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/101332297">信念网络</a><br><a href="https://zhuanlan.zhihu.com/p/56961832">论文分析</a><br><a href="https://zhuanlan.zhihu.com/p/159162779">论文中文翻译</a><br><a href="https://arxiv.org/pdf/1512.03385">论文原文</a><br><a href="https://www.bilibili.com/video/BV1P3411y7nn/">李沐读论文</a></p><h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h2 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;resnet in pytorch</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Deep Residual Learning for Image Recognition</span><br><span class="hljs-string">    https://arxiv.org/abs/1512.03385v1</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicBlock</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Basic Block for resnet 18 and resnet 34</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment">#BasicBlock and BottleNeck block</span><br>    <span class="hljs-comment">#have different output size</span><br>    <span class="hljs-comment">#we use class attribute expansion</span><br>    <span class="hljs-comment">#to distinct</span><br>    expansion = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, stride=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-comment">#residual function</span><br>        self.residual_function = nn.Sequential(<br>            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels * BasicBlock.expansion)<br>        )<br><br>        <span class="hljs-comment">#shortcut</span><br>        self.shortcut = nn.Sequential()<br><br>        <span class="hljs-comment">#the shortcut output dimension is not the same with residual function</span><br>        <span class="hljs-comment">#use 1*1 convolution to match the dimension</span><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> in_channels != BasicBlock.expansion * out_channels:<br>            self.shortcut = nn.Sequential(<br>                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(out_channels * BasicBlock.expansion)<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> nn.ReLU(inplace=<span class="hljs-literal">True</span>)(self.residual_function(x) + self.shortcut(x))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BottleNeck</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Residual block for resnet over 50 layers</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    expansion = <span class="hljs-number">4</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, stride=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.residual_function = nn.Sequential(<br>            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels * BottleNeck.expansion),<br>        )<br><br>        self.shortcut = nn.Sequential()<br><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> in_channels != out_channels * BottleNeck.expansion:<br>            self.shortcut = nn.Sequential(<br>                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(out_channels * BottleNeck.expansion)<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> nn.ReLU(inplace=<span class="hljs-literal">True</span>)(self.residual_function(x) + self.shortcut(x))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNet</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, block, num_block, num_classes=<span class="hljs-number">3</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        self.in_channels = <span class="hljs-number">64</span><br><br>        self.conv1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>))<br>        <span class="hljs-comment">#we use a different inputsize than the original paper</span><br>        <span class="hljs-comment">#so conv2_x&#x27;s stride is 1</span><br>        self.conv2_x = self._make_layer(block, <span class="hljs-number">64</span>, num_block[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)<br>        self.conv3_x = self._make_layer(block, <span class="hljs-number">128</span>, num_block[<span class="hljs-number">1</span>], <span class="hljs-number">2</span>)<br>        self.conv4_x = self._make_layer(block, <span class="hljs-number">256</span>, num_block[<span class="hljs-number">2</span>], <span class="hljs-number">2</span>)<br>        self.conv5_x = self._make_layer(block, <span class="hljs-number">512</span>, num_block[<span class="hljs-number">3</span>], <span class="hljs-number">2</span>)<br>        self.avg_pool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.fc = nn.Linear(<span class="hljs-number">512</span> * block.expansion, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, block, out_channels, num_blocks, stride</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;make resnet layers(by layer i didnt mean this &#x27;layer&#x27; was the</span><br><span class="hljs-string">        same as a neuron netowork layer, ex. conv layer), one layer may</span><br><span class="hljs-string">        contain more than one residual block</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            block: block type, basic block or bottle neck block</span><br><span class="hljs-string">            out_channels: output depth channel number of this layer</span><br><span class="hljs-string">            num_blocks: how many blocks per layer</span><br><span class="hljs-string">            stride: the stride of the first block of this layer</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            return a resnet layer</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># we have num_block blocks per layer, the first block</span><br>        <span class="hljs-comment"># could be 1 or 2, other blocks would always be 1</span><br>        strides = [stride] + [<span class="hljs-number">1</span>] * (num_blocks - <span class="hljs-number">1</span>)<br>        layers = []<br>        <span class="hljs-keyword">for</span> stride <span class="hljs-keyword">in</span> strides:<br>            layers.append(block(self.in_channels, out_channels, stride))<br>            self.in_channels = out_channels * block.expansion<br><br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output = self.conv1(x)<br>        output = self.conv2_x(output)<br>        output = self.conv3_x(output)<br>        output = self.conv4_x(output)<br>        output = self.conv5_x(output)<br>        output = self.avg_pool(output)<br>        output = output.view(output.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        output = self.fc(output)<br><br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet18</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 18 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BasicBlock, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet34</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 34 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BasicBlock, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet50</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 50 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BottleNeck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet101</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 101 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BottleNeck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">23</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet152</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; return a ResNet 152 object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> ResNet(BottleNeck, [<span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">36</span>, <span class="hljs-number">3</span>])<br><br><br><br><span class="hljs-comment"># import torch</span><br><span class="hljs-comment"># from torchsummary import summary</span><br><br><span class="hljs-comment"># # Instantiate the ResNet model (choose the variant you want, e.g., resnet18())</span><br><span class="hljs-comment"># model = resnet18()</span><br><br><span class="hljs-comment"># # Move the model to the device (e.g., GPU if available)</span><br><span class="hljs-comment"># device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="hljs-comment"># model.to(device)</span><br><br><span class="hljs-comment"># # Print the model summary</span><br><span class="hljs-comment"># summary(model, (1, 33, 1025))  # Adjust the input size (channels, height, width) as needed</span><br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——GoogleNet</title>
    <link href="/2024/04/27/deeplearnpaper3/"/>
    <url>/2024/04/27/deeplearnpaper3/</url>
    
    <content type="html"><![CDATA[<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>2014年<br><a href="https://zhuanlan.zhihu.com/p/482776152">GoogleNet解读</a><br><a href="https://blog.csdn.net/Jwenxue/article/details/107788765">论文翻译中文</a><br><a href="https://arxiv.org/pdf/1409.4842v1">论文原文</a><br><a href="https://www.jianshu.com/p/6a9e33e34571">论文中文翻译</a></p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;google net in pytorch</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">[1] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,</span><br><span class="hljs-string">    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Going Deeper with Convolutions</span><br><span class="hljs-string">    https://arxiv.org/abs/1409.4842v1</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-comment">#1x1conv branch</span><br>        self.b1 = nn.Sequential(<br>            nn.Conv2d(input_channels, n1x1, kernel_size=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n1x1),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>        <span class="hljs-comment">#1x1conv -&gt; 3x3conv branch</span><br>        self.b2 = nn.Sequential(<br>            nn.Conv2d(input_channels, n3x3_reduce, kernel_size=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n3x3_reduce),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(n3x3_reduce, n3x3, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n3x3),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>        <span class="hljs-comment">#1x1conv -&gt; 5x5conv branch</span><br>        <span class="hljs-comment">#we use 2 3x3 conv filters stacked instead</span><br>        <span class="hljs-comment">#of 1 5x5 filters to obtain the same receptive</span><br>        <span class="hljs-comment">#field with fewer parameters</span><br>        self.b3 = nn.Sequential(<br>            nn.Conv2d(input_channels, n5x5_reduce, kernel_size=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n5x5_reduce),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(n5x5_reduce, n5x5, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n5x5, n5x5),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(n5x5, n5x5, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(n5x5),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>        <span class="hljs-comment">#3x3pooling -&gt; 1x1conv</span><br>        <span class="hljs-comment">#same conv</span><br>        self.b4 = nn.Sequential(<br>            nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>            nn.Conv2d(input_channels, pool_proj, kernel_size=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(pool_proj),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GoogleNet</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_class=<span class="hljs-number">3</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.prelayer = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">192</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>        )<br><br>        <span class="hljs-comment">#although we only use 1 conv layer as prelayer,</span><br>        <span class="hljs-comment">#we still use name a3, b3.......</span><br>        self.a3 = Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, <span class="hljs-number">96</span>, <span class="hljs-number">128</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)<br>        self.b3 = Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">192</span>, <span class="hljs-number">32</span>, <span class="hljs-number">96</span>, <span class="hljs-number">64</span>)<br><br>        <span class="hljs-comment">##&quot;&quot;&quot;In general, an Inception network is a network consisting of</span><br>        <span class="hljs-comment">##modules of the above type stacked upon each other, with occasional</span><br>        <span class="hljs-comment">##max-pooling layers with stride 2 to halve the resolution of the</span><br>        <span class="hljs-comment">##grid&quot;&quot;&quot;</span><br>        self.maxpool = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br><br>        self.a4 = Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, <span class="hljs-number">96</span>, <span class="hljs-number">208</span>, <span class="hljs-number">16</span>, <span class="hljs-number">48</span>, <span class="hljs-number">64</span>)<br>        self.b4 = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, <span class="hljs-number">112</span>, <span class="hljs-number">224</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        self.c4 = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        self.d4 = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, <span class="hljs-number">144</span>, <span class="hljs-number">288</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        self.e4 = Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br><br>        self.a5 = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br>        self.b5 = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">48</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br><br>        <span class="hljs-comment">#input feature size: 8*8*1024</span><br>        self.avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.dropout = nn.Dropout2d(p=<span class="hljs-number">0.4</span>)<br>        self.linear = nn.Linear(<span class="hljs-number">1024</span>, num_class)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.prelayer(x)<br>        x = self.maxpool(x)<br>        x = self.a3(x)<br>        x = self.b3(x)<br><br>        x = self.maxpool(x)<br><br>        x = self.a4(x)<br>        x = self.b4(x)<br>        x = self.c4(x)<br>        x = self.d4(x)<br>        x = self.e4(x)<br><br>        x = self.maxpool(x)<br><br>        x = self.a5(x)<br>        x = self.b5(x)<br><br>        <span class="hljs-comment">#&quot;&quot;&quot;It was found that a move from fully connected layers to</span><br>        <span class="hljs-comment">#average pooling improved the top-1 accuracy by about 0.6%,</span><br>        <span class="hljs-comment">#however the use of dropout remained essential even after</span><br>        <span class="hljs-comment">#removing the fully connected layers.&quot;&quot;&quot;</span><br>        x = self.avgpool(x)<br>        x = self.dropout(x)<br>        x = x.view(x.size()[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)<br>        x = self.linear(x)<br><br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">googlenet</span>():<br>    <span class="hljs-keyword">return</span> GoogleNet()<br><br><br><span class="hljs-comment"># import torch</span><br><span class="hljs-comment"># from torchsummary import summary</span><br><br><span class="hljs-comment"># # Instantiate the GoogleNet model</span><br><span class="hljs-comment"># model = googlenet()</span><br><br><span class="hljs-comment"># # Move the model to the device (e.g., GPU if available)</span><br><span class="hljs-comment"># device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="hljs-comment"># model.to(device)</span><br><br><span class="hljs-comment"># # Print the model summary</span><br><span class="hljs-comment"># summary(model, (1, 33, 1025))  # Adjust the input size (channels, height, width) as needed</span><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python——flask后端代码开发</title>
    <link href="/2024/04/26/tiankeng5/"/>
    <url>/2024/04/26/tiankeng5/</url>
    
    <content type="html"><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>最近我们在研究如何将直接的模型部署在网站上面，所以我们打算写一个python工程来放置对模型部署，使用的flask来进行模型和前端的交互。<br>大概思路是这样的，前端传入一个文件，经过flask传输到服务器，触发处理求取，模型处理完成后返回模型处理结果。<br>增加功能是用户可以自己选择模型种类和通道种类。（用户可以自由选取自己想处理的通道对应的模型）</p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="跨域"><a href="#跨域" class="headerlink" title="跨域"></a>跨域</h3><p>http:&#x2F;&#x2F; (协议)<br><a href="http://hostname(主机名)：port（端口号）">http://hostname(主机名)：port（端口号）</a> 这三个东西有一个不同就叫做跨域，跨域的本质是浏览器的安全保护。<br>怎么解决，后端允许跨域。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔12</title>
    <link href="/2024/04/26/ganwu12/"/>
    <url>/2024/04/26/ganwu12/</url>
    
    <content type="html"><![CDATA[<h2 id="当我感觉烦躁时我该怎么办？"><a href="#当我感觉烦躁时我该怎么办？" class="headerlink" title="当我感觉烦躁时我该怎么办？"></a>当我感觉烦躁时我该怎么办？</h2><p>感到烦躁是自己告诉自己应该休息了，大脑到达负荷了。出去走走，小睡一会，放松放松。<br>精力越是消极、情绪越低落，表现就越糟糕；相反，精力越积极、情绪越高涨，表现也会越高效。精力主要来自体能、思维、意志、情感</p><ol><li>锻炼，适当的锻炼会对精力有很大的帮助，间歇性训练，每一次哪怕只是维持1分钟左右，都会产生出乎意料的积极影响。俯卧撑微行为计划</li><li>呼吸，恢复精力时有技巧的，三次一组吸气，三次一组吸气、也就是，把一次吸气分为三次，把呼气分成六次，这样通过深度、平静、有节奏地呼吸会激发精力，带来放松、精力不够的时候可以尝试做一下这个动作。</li><li>食物，早餐非常重要，它不仅能提高血糖水平，还能强力推动机体新陈代谢。</li><li>充足的睡眠，充足的睡眠是最重要的精力恢复来源。中午小睡一会，能快速恢复精力。随时准备20分钟的小睡。</li><li>情绪，满足和安全感的活动都能够激发正面情感、能够恢复精力，但是很多人会觉得看电视，拿着Ipad追剧，看综艺也能带来满足感，看电视带来了只是暂时的恢复，时间长了、反而让人消耗精力。要去多做一些能够有社交性的活动，比如骑自行车、参加读书会、听音乐会等等，因为可以和其它人交流，这样满足感会时间会持续的长一些。</li><li>思维精力恢复的关键呢，就是让大脑能有间歇地休息。创造性需要投入和抽离、思考和放松、活跃与休息之间有节奏的交替进行。</li></ol><h2 id="我的精力恢复方式"><a href="#我的精力恢复方式" class="headerlink" title="我的精力恢复方式"></a>我的精力恢复方式</h2><ol><li>抖腿</li><li>小睡</li><li>做一个俯卧撑。</li><li>吃点坚果</li><li>多喝水</li><li>在进步本上记录时间点</li></ol><p>你对自己从事一生的事业了解之匮乏是难以想象的。”每个人都真正会从事一生的事业，那就是：学习和成长！</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑——软编码</title>
    <link href="/2024/04/26/tiankeng4/"/>
    <url>/2024/04/26/tiankeng4/</url>
    
    <content type="html"><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>今天想重新复写一下模型结构，让模型能够自适应的去适应在不同的数据集，实现软编码。<br>首先介绍一下硬编码<br><a href="https://blog.csdn.net/weixin_44943389/article/details/134928228">参考博客</a><br>硬编码是指将具体的数值、路径、参数等直接写入程序代码中，而不通过变量或配置文件来表示。这样的做法使得程序中的这些数值和参数变得固定，不容易修改，且缺乏灵活性。硬编码的值通常被称为”魔法数”（Magic Numbers）或”魔法字符串”，因为它们没有直观的含义，只能通过查看代码来了解。<br>例如，以下是一个硬编码的示例，其中数值 10 直接出现在代码中：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss">for <span class="hljs-selector-tag">i</span> in <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Iteration&quot;</span>, i)<br></code></pre></td></tr></table></figure><p>软编码（Softcoding）：</p><p>软编码是指通过变量、配置文件、参数等方式将具体数值或参数抽象出来，而不是直接写入代码。通过软编码，程序变得更加灵活，可以更容易地进行修改和维护，且适应性更强。</p><p>使用软编码的例子：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs maxima"><span class="hljs-built_in">iterations</span> = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">iterations</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Iteration&quot;</span>, i)<br></code></pre></td></tr></table></figure><p>硬编码：将具体数值、参数等直接写入程序代码中，缺乏灵活性，不易修改和维护。</p><p>软编码：通过变量、配置文件等方式将数值或参数抽象出来，使得程序更具灵活性，易于修改和维护。</p><h2 id="我的解决办法"><a href="#我的解决办法" class="headerlink" title="我的解决办法"></a>我的解决办法</h2><ol><li><p>在foward中进行重新赋值（有问题），问题就是没有前向训练过程中都重新创建了一个linear，这个linear层的参数没有训练，相当于随机（注：只是我猜的，没有验证，挖个坑在这）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">IntegratedNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(IntegratedNet, self).__init__()<br>        self.linear = <span class="hljs-literal">None</span> <span class="hljs-comment"># 在初始类中先第一一个self.linear=None ，而后在forward中重新定义</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">if</span> self.linear <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            self.linear = nn.Linear(in_features=x.size(<span class="hljs-number">1</span>), out_features=<span class="hljs-number">1</span>)<br>        <br>        x = self.linear(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 创建模型实例</span><br>model = IntegratedNet()<br><br><span class="hljs-comment"># 创建输入数据</span><br>x = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">512</span>, <span class="hljs-number">64</span>)<br><br><span class="hljs-comment"># 前向传播</span><br>output = model(x)<br><br><span class="hljs-comment"># 打印输出的形状</span><br><span class="hljs-built_in">print</span>(output.size())<br><br></code></pre></td></tr></table></figure></li><li><p>第二种解决方法在__init__中留下一个接口，在调用这个模型时直接重新赋值。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">class</span> IntegratedNet(nn.Module):<br>    <span class="hljs-attribute">def</span> __init__(self, input_size=<span class="hljs-number">3</span>, mlp_dim=<span class="hljs-number">512</span>, mlp_ratio=<span class="hljs-number">4</span>,<br>                 <span class="hljs-attribute">dims</span>=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">320</span>, <span class="hljs-number">512</span>],in_feature=<span class="hljs-number">64</span>):<br>        <span class="hljs-attribute">super</span>(IntegratedNet, self).__init__()<br><br><span class="hljs-attribute">model</span> = IntegratedNet(input_size=<span class="hljs-number">2</span>,in_feature=<span class="hljs-number">209</span>)  # 全部重新赋值，实现<br><span class="hljs-attribute">device</span> = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> if torch.cuda.is_available() else <span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-attribute">model</span> = model.to(device)<br><br><span class="hljs-comment"># 打印模型结构摘要</span><br><span class="hljs-attribute">summary</span>(model, (<span class="hljs-number">2</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3333</span>))<br><br></code></pre></td></tr></table></figure></li><li><p>重新构建网络结构，加入自适应池化层。</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《定风波》</title>
    <link href="/2024/04/25/juzhi2/"/>
    <url>/2024/04/25/juzhi2/</url>
    
    <content type="html"><![CDATA[<center> <font size = 6>《定风波》 <font></center>  三月七日，沙湖道中遇雨。雨具先去，同行皆狼狈，余独不觉。已而遂晴，故作此词。  莫听穿林打叶声，何妨吟啸且徐行。竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生。  料峭春风吹酒醒，微冷，山头斜照却相迎。回首向来萧瑟处，归去，也无风雨也无晴。<p><font size = 3><font>译文：三月七日，在沙湖道上赶上了下雨。雨具先前被带走了，同行的人都觉得很狼狈，只有我不这么觉得。过了一会儿天晴了，就创作了这首词。不用注意那穿林打叶的雨声，不妨一边吟咏长啸着，一边悠然地行走。竹杖和草鞋轻捷得胜过骑马，有什么可怕的？一身蓑衣任凭风吹雨打，照样过我的一生。春风微凉，将我的酒意吹醒，寒意初上，山头初晴的斜阳却应时相迎。回头望一眼走过来遇到风雨的地方，回去吧，对我来说，既无所谓风雨，也无所谓天晴。</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>喜欢的一些句子</title>
    <link href="/2024/04/25/juzhi1/"/>
    <url>/2024/04/25/juzhi1/</url>
    
    <content type="html"><![CDATA[<ol><li><p>于浩歌狂热之际中寒，于天上看见深渊，于一切眼中看见无所有，于无所希望中得救 —— 鲁迅《墓碣文》</p></li><li><p>但是太阳，他每时每刻都是夕阳也都是旭日。 当他熄灭着走下山去收尽苍凉残照之际，正是他在另一面，燃烧着爬上山巅布散烈烈朝辉之时。那一天，我也将沉静着走下山去，扶着我的拐杖。有一天，在某一处山洼里，势必会跑上来一个欢蹦的孩子，抱着他的玩具。——史铁生</p></li><li><p>生命就是这样一个过程，一个不断超越自身局限的过程，这就是命运，任何人都是一样。在这过程中，我们遭遇痛苦、超越局限、从而感受幸福。所以一切人都是平等的，我们毫不特殊。——史铁生 《病隙碎笔》</p></li><li><p>只要你不停的向上走，一级级楼梯就没有尽头，在你向上走的脚下，它们也在向上长。——卡夫卡《律师》</p></li><li><p>找到属于自己的意义，赋予生命目的，每一天都像向日葵朝向太阳一样，充满方向和意义的活，是人类能活出的最好样子，它治愈我们的根本恐惧。</p></li><li><p>成功就是用自己喜欢的方式过一生。这句话分三部分。首先要知道自己喜欢什么，其次要有追逐它的勇气，追到了，还需要一生不渝的毅力。</p></li><li><p>“我来到这个世界，不是为了繁衍后代，而是来看花怎么开，水怎么流，太阳怎么升起，夕阳如何落下。我活在世上，无非是想要明白些道理，遇见有趣的事。生命是一场偶然，我在其中寻找因果。”生命对于每个人，它的意义是不一样的，每个人都是宇宙中一个独特的存在。</p></li><li><p>世界上只有一种英雄主义，那就是认清生活的真相后依旧热爱生活。</p></li><li><p>一切的一切都是个人的选择，自己支付代价，自己承担后果，旁人没什么评价的资格。世界上很多事是你不能细想也不能过分纠结的，太纷杂的想法会如同重重的枷锁，束缚住你的脚步，等你回头看的时候，你发现其实走错路没什么可怕的，反而是踟蹰不决让自己停留在原地，丧失了人生很多重要的体验。</p></li><li><p>愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光，就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。此后如竟没有炬火：我便是唯一的光。倘若有了炬火，出了太阳，我们自然心悦诚服的消失。不但毫无不平，而且还要随喜赞美这炬火或太阳；因为他照了人类，连我都在内。——鲁迅《热风·随感录四十一》</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔11</title>
    <link href="/2024/04/25/ganwu11/"/>
    <url>/2024/04/25/ganwu11/</url>
    
    <content type="html"><![CDATA[<p>“人生的意义是什么？”<br>回答：“每个人都有不同的答案。”</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文思路</title>
    <link href="/2024/04/25/paper_idear/"/>
    <url>/2024/04/25/paper_idear/</url>
    
    <content type="html"><![CDATA[<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><ol><li>傅里叶变换 （时间域变换为频域）</li><li>归一化（零归一化，批归一化，层归一化）</li></ol><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ol><li>startRule</li><li>ReLU</li></ol><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><ol><li>编码器——解码器</li><li>残差连接（基于ResNet）</li><li>MLP</li><li>双输入卷积神经网络。</li></ol><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ol><li>交叉熵损失函数</li></ol><h2 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h2><ol><li>SGD</li><li>RMSprop</li></ol><h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><ol><li>ACC</li><li>pression</li><li>Recall</li><li>F1-score</li><li>混淆矩阵</li></ol><h2 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h2><ol><li>双输入卷积神经网络 （傅里叶信号+归一化信号）</li></ol><h2 id="实验对比设置"><a href="#实验对比设置" class="headerlink" title="实验对比设置"></a>实验对比设置</h2><ol><li>横向实验： 3个数据集，其中包含为1个私有数据集，2个公开数据集。（注：私有数据集中的数据使用不同仪器采集，可以分为很多类。公开数据集本身为一个数据集，但是有两个阶段数据，一个为包含了伪迹信号的数据，一个为未包含伪迹信号的数据集。）</li><li>纵向实验: 模型对比，AlexNet，CNN，googleNet，shuffleNet，ResNet（注：对比模型不足。）</li></ol><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><ol><li>双数据输入，对数据采用不同的数据预处理，比如FFT(傅里叶变换)，小波变换，零归一化.</li><li>在公开数据集上验证处理不同的数据预处理方法对实验结果的好坏。</li><li>对比私有数据集，验证自己模型的稳健性（鲁棒性）。</li><li>使用图像处理的模型结构。</li><li>公开数据集的二分类结果，对比论文。</li><li>验证选取合适的数据预处理方法是合适的。(注：数据预处理+深度学习模型架构。)</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python自定义包的层级引用</title>
    <link href="/2024/04/24/tiankeng3/"/>
    <url>/2024/04/24/tiankeng3/</url>
    
    <content type="html"><![CDATA[<h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><p>今天debug的时候自定义了一个函数，使用了start主函数来引用processing函数，processing函数引用了同级文件夹中的python文件中的dataset函数，在运行processing的时候，test是通过的，但是在使用start函数来调用processing函数，processing函数函数调用dataset函数时就出现了报错，提示找不到这个包。（注：这里需要指明的是start函数放置在根文件夹中，processing函数放置在processing文件夹中）问题就在于python文件的文件运行路径的出错。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs stylus">│  start<span class="hljs-selector-class">.py</span><br>│<br>├─static<br>│  │  __init__<span class="hljs-selector-class">.py</span><br>│  │<br>│  ├─model<br>│  │      MLPForMer<span class="hljs-selector-class">.pth</span><br>│  │<br>│  ├─processing<br>│  │  │  dataset<span class="hljs-selector-class">.py</span><br>│  │  │  net<span class="hljs-selector-class">.py</span><br>│  │  │  processing<span class="hljs-selector-class">.py</span><br>│  │  │  __init__<span class="hljs-selector-class">.py</span><br>│  │  │<br>│  │  └─__pycache__<br>│  │          dataset<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │          net<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │          processing<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │          __init__<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│  │<br>│  ├─result<br>│  │      average_probabilities<span class="hljs-selector-class">.csv</span><br>│  │      average_probabilities<span class="hljs-selector-class">.png</span><br>│  │<br>│  ├─tmp<br>│  │      <span class="hljs-number">1</span><span class="hljs-selector-class">.edf</span><br>│  │<br>│  └─__pycache__<br>│          __init__<span class="hljs-selector-class">.cpython-38</span><span class="hljs-selector-class">.pyc</span><br>│<br>└─templates<br>        upload.html<br></code></pre></td></tr></table></figure><h2 id="填坑"><a href="#填坑" class="headerlink" title="填坑"></a>填坑</h2><p>对待这种问题目前我知道的有两种方法</p><ol><li>第一种方法在processing文件中明确的所以绝对引用的方法,因为问题是出现在processing中的。</li></ol><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> <span class="hljs-keyword">static</span>.processing.net <span class="hljs-keyword">import</span> * <span class="hljs-comment"># 这里引用是相对于start函数的位置</span><br><br></code></pre></td></tr></table></figure><ol start="2"><li>第二种方法，在__init__文件中给出直接引用<br>1.相对引用package需要采用from 相对位置 import package_name的方式。因为相对位置只能写在from和import中间。<br>2.from . import * 只会检索当前目录下的module，而不会导入package。</li></ol><h3 id="挖坑-1"><a href="#挖坑-1" class="headerlink" title="挖坑"></a>挖坑</h3><h3 id="windown怎么打印树状图？"><a href="#windown怎么打印树状图？" class="headerlink" title="windown怎么打印树状图？"></a>windown怎么打印树状图？</h3><p>使用<code>tree</code>来打印文件夹<br>使用<code>tree /f</code>来打印文件目录，如上面的文件目录结构。</p><h3 id="init-文件的作用是什么？"><a href="#init-文件的作用是什么？" class="headerlink" title="__init__文件的作用是什么？"></a>__init__文件的作用是什么？</h3><p>作为包的标识：</p><ol><li>当一个目录包含__init__.py文件时，Python会将该目录视为一个包，而不仅仅是一个普通的目录。这使得包内的模块可以被正确导入和使用。</li><li><strong>init</strong>.py文件可以是一个空文件，也可以包含初始化包的代码，比如设置包的属性、导入子模块等。</li></ol><p>初始化包：</p><ol><li>在包被导入时，<strong>init</strong>.py文件会在包内的其他模块之前被执行。这使得可以在__init__.py中执行一些初始化操作，比如设置包级别的变量、执行必要的初始化代码等。</li><li>这也可以用于在导入包时自动执行一些操作，比如注册插件、加载配置等。·</li></ol><h2 id="填坑-1"><a href="#填坑-1" class="headerlink" title="填坑"></a>填坑</h2><h3 id="居中显示"><a href="#居中显示" class="headerlink" title="居中显示"></a>居中显示</h3><p>可以使用center标签，或者使用div标签，或者使用p标签，或者h标签都是可以的</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">center</span>&gt;</span> <span class="hljs-tag">&lt;&gt;</span>数据结构和算法是居中展示，使用center标签<span class="hljs-tag">&lt;/<span class="hljs-name">center</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">center</span>&gt;</span>数据结构和算法是居中展示，使用div标签<span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">align</span>=<span class="hljs-string">&quot;center&quot;</span>&gt;</span>数据结构和算法是居中展示，使用p标签<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">h5</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;text-align:center&quot;</span>&gt;</span>数据结构和算法是居中展示，使用h标签<span class="hljs-tag">&lt;/<span class="hljs-name">h5</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="给改文字大小"><a href="#给改文字大小" class="headerlink" title="给改文字大小"></a>给改文字大小</h3><p>使用font标签，字体使用face，颜色使用color，尺寸使用size。<br>颜色可以使用字母比如red，black，blue，yellow等，也可以是十六进制表示比如#0000ff或者#F025AB等等<br>size 是从1到7，数字越小字体越小，浏览器默认是3<br>这几个属性可以都设置，也可以只设置其中的1到2个</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">&lt;font <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;黑体&quot;</span>&gt;我是黑体字体&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;微软雅黑&quot;</span>&gt;我是微软雅黑字体&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;STCAIYUN&quot;</span>&gt;我是华文彩字体云&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">color</span>=red <span class="hljs-attribute">size</span>=3 <span class="hljs-attribute">face</span>=<span class="hljs-string">&quot;黑体&quot;</span>&gt;我是红色，黑色字体，大小是3&lt;/font&gt;<br>&lt;font <span class="hljs-attribute">color</span>=#F025AB <span class="hljs-attribute">size</span>=5&gt;我的颜色是#F025AB，大小是5&lt;/font&gt;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔10</title>
    <link href="/2024/04/24/ganwu10/"/>
    <url>/2024/04/24/ganwu10/</url>
    
    <content type="html"><![CDATA[<h3 id="怎么才能快乐？"><a href="#怎么才能快乐？" class="headerlink" title="怎么才能快乐？"></a>怎么才能快乐？</h3><p>不以物喜，不以己悲。因为外物而带来的快乐是会失去的，喜悦要来源于自己的内心。对我而言，战胜一个又一个困难的过程是有意思的，可能这件事我不是很感兴趣，但是令我开心的是解决问题的过程。我知道自己现在无法解决这个问题，但是慢慢的去做，在做的过程中我发现自己爱上了这个感觉，爱上了解决问题的过程，就像米哈里所说的心流状态，即使是一点点进步我就会产生一点点发自内心的喜悦。</p><p>一呼一吸，一言一行。花开花落，云卷云舒。感受过程，提升自己。一句我从小听到的话，一句很普通的话，隐含着巨大的道理——“每天进步一点点”,这句话在我曾经就读的小学校园的门口就能看到。进步是令人快乐的，这种快乐不是来源于外物，而是来源于自己的内心。胡适先生为“中国科学社”写社歌，最后几句歌词就是:我们唱天行有常，我们唱致知穷理。怕什么真理无穷，进一寸有一寸的欢喜。1934年，他写《“九·一八”的第三周年纪念告全国的青年》。其中说:“努力一分，就有一分的效果。努力百分，就有百分的效果。”</p><p>以勇气来迎接人生的每一个挑战。这个挑战不一定很宏大，可能它就是今天我要8点起床，晚上11点睡觉，可能就是我今天要做一个俯卧撑，跑一圈操场，一个普普通通的挑战。改变总是开始于微小的，即使是写一个project也是从新建文件开始的。积极向上，把每一次挑战看作一次进步的机会，即使失败了又有什么问题，我想这个过程中一定是快乐的。苦难不值得被歌颂，认清苦难的现实和战胜苦难的勇气才值得被歌颂。</p><p>踏上自我成长的道路，每一个过程都是令人快乐的。</p><h3 id="今天冲浪的感受。"><a href="#今天冲浪的感受。" class="headerlink" title="今天冲浪的感受。"></a>今天冲浪的感受。</h3>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像处理-数据预处理</title>
    <link href="/2024/04/23/deeplearnbook3/"/>
    <url>/2024/04/23/deeplearnbook3/</url>
    
    <content type="html"><![CDATA[<h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><p>在深度学习中，图像数据通常以多维数组（在Python中通常使用Numpy数组）的形式表示，这个数组的形状（shape）取决于图像的维度和颜色通道数。<br>灰度图像：对于灰度图像（也就是黑白图像），shape通常是两维的，表示图像的高度和宽度。例如，一个256x256像素的灰度图像的shape将是(256, 256)。灰度图像的像素值通常在0到255之间，其中0表示黑色，255表示白色，中间的值表示不同的灰度级别。这是因为每个像素通常由8位（一个字节）表示，所以可以有256（即$2^8$）个不同的可能值。然而，这并不是唯一的表示方式。有时，为了方便计算，我们可能会将像素值归一化到0到1之间。在这种情况下，0仍然表示黑色，1表示白色，中间的值表示不同的灰度级别。<br>彩色图像：对于彩色图像，通常使用RGB（红，绿，蓝）三个颜色通道，所以shape是三维的。例如，一个256x256像素的RGB彩色图像的shape将是(256, 256, 3)。这里的3代表三个颜色通道。彩色图像通常由三个颜色通道组成：红色（R），绿色（G）和蓝色（B）。每个通道的像素值通常在0到255之间，其中0表示该颜色的完全缺失，255表示该颜色的最大强度。所以，一个RGB颜色图像的像素值范围在理论上是0到255的三维空间，即(0,0,0)到(255,255,255)。同样，有时我们也会将每个颜色通道的像素值归一化到0到1之间。在这种情况下，(0,0,0)表示黑色，(1,1,1)表示白色，其他值表示不同的颜色。需要注意的是，虽然RGB是最常用的颜色空间，但也有其他的颜色空间，如HSV（色相，饱和度，亮度）或者CMYK（青色，品红，黄色，黑色），它们的取值范围可能会有所不同。<br>图像批量：在深度学习中，我们通常会一次处理多个图像，这就是所谓的批量（batch）。在这种情况下，图像数据的shape将是四维的：(批量大小, 高度, 宽度, 颜色通道数)。例如，如果我们有32个256x256像素的RGB图像，那么这个批量的shape将是(32, 256, 256, 3)。</p><h2 id="显示彩色图像"><a href="#显示彩色图像" class="headerlink" title="显示彩色图像"></a>显示彩色图像</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> cv2 # opencv中按BGR排布，蓝绿红<br><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">img</span> = np.zeros((<span class="hljs-number">5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>),dtype=np.uint8)<br><br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>) <br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>) #中间的白色区块。 <br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<span class="hljs-number">255</span>)<br><span class="hljs-attribute">img</span>.itemset((<span class="hljs-number">4</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">255</span>)<br><br><span class="hljs-attribute">cv2</span>.namedWindow(&#x27;img&#x27;,cv2.WINDOW_NORMAL)<br><span class="hljs-attribute">cv2</span>.resizeWindow(&#x27;img&#x27;,<span class="hljs-number">500</span>,<span class="hljs-number">500</span>)<br><span class="hljs-attribute">cv2</span>.imshow(&#x27;img&#x27;,img)<br><span class="hljs-attribute">cv2</span>.waitKey()<br><span class="hljs-attribute">cv2</span>.destroyAllWindows()<br></code></pre></td></tr></table></figure><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><ol><li>二值化处理：这是最基本的阈值处理方法。对于每个像素，我们选择一个阈值。如果像素值大于阈值，我们将其设置为一个值（通常是白色），如果像素值小于或等于阈值，我们将其设置为另一个值（通常是黑色）。这样我们就得到了一个二值图像。</li><li>反二值化处理：这是二值化处理的反向操作。如果像素值大于阈值，我们将其设置为一个值（通常是黑色），如果像素值小于或等于阈值，我们将其设置为另一个值（通常是白色）。</li><li>截断阈值处理：对于每个像素，如果其值大于阈值，我们将其设置为阈值。如果像素值小于或等于阈值，我们保持其原值不变。</li><li>超阈值零处理：对于每个像素，如果其值大于阈值，我们保持其原值不变。如果像素值小于或等于阈值，我们将其设置为零。</li><li>低阈值零处理：这是超阈值零处理的反向操作。如果像素值大于阈值，我们将其设置为零。如果像素值小于或等于阈值，我们保持其原值不变。</li><li>自适应阈值处理：这是一种更复杂的方法，它不使用固定的阈值。相反，它根据像素周围的小区域计算阈值。因此，对于同一张图片上的不同区域，我们可以有不同的阈值。这对于当图像的光照条件变化很大时，例如，一半是明亮的，一半是暗淡的图像，非常有用。</li></ol><h2 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h2><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs smali">import cv2<br><br><span class="hljs-comment"># 读取图像</span><br>image = cv2.imread(&#x27;e1.jpg&#x27;, cv2.IMREAD_GRAYSCALE)<br><br><span class="hljs-comment"># 二值化处理</span><br>_, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)<br><br><span class="hljs-comment"># 反二值化处理</span><br>_, binary_inv_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)<br><br><span class="hljs-comment"># 截断阈值处理</span><br>_, trunc_image = cv2.threshold(image, 127, 255, cv2.THRESH_TRUNC)<br><br><span class="hljs-comment"># 超阈值零处理</span><br>_, tozero_inv_image = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO_INV)<br><br><span class="hljs-comment"># 低阈值零处理</span><br>_, tozero_image = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO)<br><br><span class="hljs-comment"># 自适应阈值处理</span><br>adaptive_image = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)<br><br><span class="hljs-comment"># 保存处理后的图像</span><br>cv2.imwrite(&#x27;binary_image.jpg&#x27;, binary_image)<br>cv2.imwrite(&#x27;binary_inv_image.jpg&#x27;, binary_inv_image)<br>cv2.imwrite(&#x27;trunc_image.jpg&#x27;, trunc_image)<br>cv2.imwrite(&#x27;tozero_inv_image.jpg&#x27;, tozero_inv_image)<br>cv2.imwrite(&#x27;tozero_image.jpg&#x27;, tozero_image)<br>cv2.imwrite(&#x27;adaptive_image.jpg&#x27;, adaptive_image)<br><br></code></pre></td></tr></table></figure><p><img src="/pic/e1.jpg" alt="原图"><br><img src="/pic/binary_image.jpg" alt="二值化处理图像"><br><img src="/pic/binary_inv_image.jpg" alt="反二值化处理图像"><br><img src="/pic/trunc_image.jpg" alt="截断阈值处理图像"><br><img src="/pic/tozero_inv_image.jpg" alt="超阈值处理图像"><br><img src="/pic/tozero_image.jpg" alt="低阈值零处理图像"><br><img src="/pic/adaptive_image.jpg" alt="自适应阈值处理图像"></p><h3 id="挖更大的坑，opencv库。"><a href="#挖更大的坑，opencv库。" class="headerlink" title="挖更大的坑，opencv库。"></a>挖更大的坑，opencv库。</h3><h3 id="彩色图像怎么转换为二维图像的？"><a href="#彩色图像怎么转换为二维图像的？" class="headerlink" title="彩色图像怎么转换为二维图像的？"></a>彩色图像怎么转换为二维图像的？</h3><p>首先灰度图像中的一个像素点的范围为0-255，彩色图像可以理解为3个灰度图重合。</p><h3 id="需要深度解析代码中的含义，比如一个参数有什么用处。"><a href="#需要深度解析代码中的含义，比如一个参数有什么用处。" class="headerlink" title="需要深度解析代码中的含义，比如一个参数有什么用处。"></a>需要深度解析代码中的含义，比如一个参数有什么用处。</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑——经典网络结构——AlexNet</title>
    <link href="/2024/04/23/tiankeng2/"/>
    <url>/2024/04/23/tiankeng2/</url>
    
    <content type="html"><![CDATA[<h2 id="问题1，卷积是什么？作用什么？"><a href="#问题1，卷积是什么？作用什么？" class="headerlink" title="问题1，卷积是什么？作用什么？"></a>问题1，卷积是什么？作用什么？</h2><p>卷积（Convolution）是一种数学运算，常用于信号处理和图像处理领域。在信号处理中，卷积用于将输入信号与卷积核（也称为滤波器）进行运算，产生输出信号。<br>卷积的作用有以下几个方面：</p><ol><li>信号滤波：卷积可以用于信号滤波，通过将输入信号与合适的卷积核进行卷积运算，可以实现对信号的滤波操作。滤波可以用于去除信号中的噪声、平滑信号、强调信号中的某些频率成分等。</li><li>特征提取：在图像处理中，卷积可以用于特征提取。通过将图像与不同的卷积核进行卷积运算，可以提取出图像中的不同特征，例如边缘、纹理、角点等。这些特征可以用于图像识别、目标检测和图像处理中的其他任务。</li><li>信号压缩：卷积可以用于信号压缩。通过将输入信号与适当的卷积核进行卷积运算，可以将信号表示转换为另一种表示形式，通常具有更紧凑的表示。这种表示形式可以用于信号压缩和数据压缩。</li><li>卷积神经网络：卷积神经网络（Convolutional Neural Network，CNN）是一种基于卷积运算的深度学习模型，广泛应用于图像识别、计算机视觉和自然语言处理等领域。卷积在 CNN 中用于提取图像或文本的特征，并通过多层卷积和池化操作来实现对输入数据的高级表示和分类。如果输入数据为图片，那么卷积层的作用就是提取图片中的信息，这些信息被称为图像特征，这些特征是由图像中的每个像素通过组合或者独立的方式所体现，比如图片的纹理特征、颜色特征、空间特征。</li></ol><p>关于卷积其实还有很多问题，比如说输入一张（3x255x255）的图片，输入后经过卷积后输出的特征图大小为什么shape。1x1卷积为什么可以实现升维和降维。）</p><h2 id="问题2，池化是什么？作用是什么？"><a href="#问题2，池化是什么？作用是什么？" class="headerlink" title="问题2，池化是什么？作用是什么？"></a>问题2，池化是什么？作用是什么？</h2><p>池化（Pooling）是一种常用的操作，通常与卷积神经网络（CNN）结合使用。池化操作通过对输入数据的局部区域进行聚合或采样来减小数据的空间尺寸，从而减少参数数量、降低计算量，并提取出输入数据的重要特征。</p><p>池化的作用有以下几个方面</p><ol><li>降采样：池化操作可以减小输入数据的空间尺寸，从而降低后续层的计算复杂度。通过降低数据的维度，池化可以在保留重要特征的同时减少冗余信息，提高计算效率。</li><li>平移不变性：池化操作具有一定的平移不变性。在图像处理中，通过对局部区域进行池化操作，可以使得输入图像在平移、旋转和缩放等变换下具有一定的不变性。这对于图像识别和目标检测等任务是有益的。</li><li>特征提取：池化操作可以提取输入数据的重要特征。通过对局部区域进行池化，池化操作会选择区域中的最大值（最大池化）或平均值（平均池化）作为输出值，从而提取出输入数据的显著特征。这有助于减少数据的维度，并保留重要的特征信息。</li><li>减少过拟合：池化操作可以在一定程度上减少过拟合。通过减小数据的空间尺寸，池化操作可以降低模型的参数数量，从而减少过拟合的风险。此外，池化操作还可以通过丢弃一些冗余信息来提高模型的泛化能力。</li></ol><p>池化的种类</p><ol><li>最大池化（Max Pooling）：最大池化是一种常见的池化操作。在最大池化中，输入数据的局部区域被分割成不重叠的块，然后在每个块中选择最大值作为输出。最大池化可以提取出输入数据的显著特征，同时减小数据的空间尺寸。</li><li>平均池化（Average Pooling）：平均池化是另一种常见的池化操作。在平均池化中，输入数据的局部区域被分割成不重叠的块，然后计算每个块中元素的平均值作为输出。平均池化可以平滑输入数据并减小数据的空间尺寸。</li><li>自适应池化（Adaptive Pooling）：自适应池化是一种具有灵活性的池化操作。与最大池化和平均池化不同，自适应池化不需要指定池化窗口的大小，而是根据输入数据的尺寸自动调整池化窗口的大小。这使得自适应池化可以适应不同尺寸的输入数据。</li><li>全局池化（Global Pooling）：全局池化是一种特殊的池化操作，它将整个输入数据的空间尺寸缩减为一个单一的值或向量。全局池化可以通过对输入数据的所有位置进行池化操作，从而提取出输入数据的全局特征。常见的全局池化有全局平均池化（Global Average Pooling）和全局最大池化（Global Max Pooling）。</li></ol><h2 id="问题3，全连接是什么？作用是什么？"><a href="#问题3，全连接是什么？作用是什么？" class="headerlink" title="问题3，全连接是什么？作用是什么？"></a>问题3，全连接是什么？作用是什么？</h2><h2 id="问题4，AlexNet论文使用的loss函数是什么？"><a href="#问题4，AlexNet论文使用的loss函数是什么？" class="headerlink" title="问题4，AlexNet论文使用的loss函数是什么？"></a>问题4，AlexNet论文使用的loss函数是什么？</h2><h2 id="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"><a href="#问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？" class="headerlink" title="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"></a>问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？</h2><h2 id="问题6，AlexNet论文中使用的评价指标是什么？"><a href="#问题6，AlexNet论文中使用的评价指标是什么？" class="headerlink" title="问题6，AlexNet论文中使用的评价指标是什么？"></a>问题6，AlexNet论文中使用的评价指标是什么？</h2><h2 id="问题7，AlexNet中的创新点是什么？"><a href="#问题7，AlexNet中的创新点是什么？" class="headerlink" title="问题7，AlexNet中的创新点是什么？"></a>问题7，AlexNet中的创新点是什么？</h2><ol><li>ReLU激活函数的引入，采样非线性单元（ReLU）的深度卷积神经网络训练时间要比tanh单元要快几倍。而时间开销是进行模型训练过程中的很重要的因数。同时ReLU有效的防止了过拟合的现象。</li><li>层叠池化操作，以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize&#x3D;2×2，stride&#x3D;2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize &gt; stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。</li><li>Dropout操作， Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。</li><li>网络层数更深，与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。当然，这种思考的重要节点出现在VGG网络（下一篇博文VGG论文中将会讲到）。</li></ol><h2 id="问题8，优化函数的具体实现是什么？"><a href="#问题8，优化函数的具体实现是什么？" class="headerlink" title="问题8，优化函数的具体实现是什么？"></a>问题8，优化函数的具体实现是什么？</h2><h2 id="问题9，关于卷积后特征图应该怎么计算？"><a href="#问题9，关于卷积后特征图应该怎么计算？" class="headerlink" title="问题9，关于卷积后特征图应该怎么计算？"></a>问题9，关于卷积后特征图应该怎么计算？</h2><h2 id="问题10，什么是过拟合合和欠拟合？"><a href="#问题10，什么是过拟合合和欠拟合？" class="headerlink" title="问题10，什么是过拟合合和欠拟合？"></a>问题10，什么是过拟合合和欠拟合？</h2><h2 id="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"><a href="#问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？" class="headerlink" title="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"></a>问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？</h2><h2 id="问题12，-神经元数量和参数量的计算方法是什么？"><a href="#问题12，-神经元数量和参数量的计算方法是什么？" class="headerlink" title="问题12， 神经元数量和参数量的计算方法是什么？"></a>问题12， 神经元数量和参数量的计算方法是什么？</h2><h2 id="问题13，-softMax的机制是怎么样的？"><a href="#问题13，-softMax的机制是怎么样的？" class="headerlink" title="问题13， softMax的机制是怎么样的？"></a>问题13， softMax的机制是怎么样的？</h2><h2 id="问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？"><a href="#问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？" class="headerlink" title="问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？"></a>问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？</h2><h2 id="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？"><a href="#问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？" class="headerlink" title="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？"></a>问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？</h2><h2 id="问题16，-什么是超参数？"><a href="#问题16，-什么是超参数？" class="headerlink" title="问题16， 什么是超参数？"></a>问题16， 什么是超参数？</h2><h2 id="问题17，-什么是监督学习和无监督学习，半监督学习？"><a href="#问题17，-什么是监督学习和无监督学习，半监督学习？" class="headerlink" title="问题17， 什么是监督学习和无监督学习，半监督学习？"></a>问题17， 什么是监督学习和无监督学习，半监督学习？</h2>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑-PyTorch基础——Numpy</title>
    <link href="/2024/04/23/tiankeng1/"/>
    <url>/2024/04/23/tiankeng1/</url>
    
    <content type="html"><![CDATA[<h3 id="向量和数组之间的关系是什么？向量的定义是什么？"><a href="#向量和数组之间的关系是什么？向量的定义是什么？" class="headerlink" title="向量和数组之间的关系是什么？向量的定义是什么？"></a>向量和数组之间的关系是什么？向量的定义是什么？</h3><p>在数学科物理中，向量被定义为具有大小和方向量。例如速度是一个向量，因为它不仅有大小（数独），还有方向（行进的方向）。<br>数组是编程中的一种基本数据结构，用于存储一组有序的元素。这些元素可以是任何类型，如整形、浮点数、字符串等。<br>标量（scalar）是零维只有大小，没有方向的量，如1，2，3<br>向量（Vector）是一维只有大小和方向的量，如（1，2）。（计算方向的公式为：）<br>矩阵（Matrix）是二维的向量，[[1, 2], [2, 3]]<br>张量（Tensor） 按照任意维排列的一堆数字的推广。矩阵不过是三维张量下的一个二维切面。要在三维张量下找到零维张量需要三个维度的坐标来定位。（注：张量可以是多维的）</p><h3 id="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"><a href="#矩阵是什么，作用是什么？如何实现矩阵的加减乘除" class="headerlink" title="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"></a>矩阵是什么，作用是什么？如何实现矩阵的加减乘除</h3><ol><li>矩阵是一个二维数组，由行和列的元素组成。在数学中，矩阵通常用大写字母表示，如 A，B 等，矩阵中的元素通常用小写字母表示，如aij​，表示矩阵 A 的第 i 行第 j 列的元素。</li><li>矩阵可以用来表示线性变换，解决线性方程组，或者表示图形的变换。在数据科学和机器学习中，矩阵通常用于存储和操作大量的数据。</li></ol><h4 id="实现矩阵的加减乘除。"><a href="#实现矩阵的加减乘除。" class="headerlink" title="实现矩阵的加减乘除。"></a>实现矩阵的加减乘除。</h4><p>加法：两个矩阵相加，只有在它们的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相加的结果。例如，如果A &#x3D; aij 和B &#x3D; bij 是同样大小的矩阵，那么它们的和C &#x3D; [ cij ]是矩阵 ,其中cij &#x3D; aij + bij。对应相加<br>减法：矩阵的减法与加法类似，只有在两个矩阵的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相减的结果。<br>乘法：矩阵的乘法比较复杂。如果A 是一个 m×n 的矩阵，B 是一个n×p 的矩阵，那么它们的乘积 AB 是一个 m×p 的矩阵，其元素由A 的行和 B 的列的对应元素的乘积之和给出。<br>除法：在矩阵中，通常不直接定义除法。但是，我们可以通过乘以逆矩阵来实现类似的效果。如果A是一个可逆的（也就是说，存在一个矩阵 （A-1）使得，A（A-1） &#x3D; （A-1）A &#x3D; I其中 𝐼I 是单位矩阵），那么我们可以定义B&#x2F;A为（BA-1），即是B矩阵除以A矩阵等于B乘以A矩阵的转置。但是，请注意，不是所有的矩阵都是可逆的。 </p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs makefile">import numpy as np<br><br><span class="hljs-comment"># 创建两个矩阵</span><br>A = np.array([[1, 2], [3, 4]])<br>B = np.array([[5, 6], [7, 8]])<br><br><span class="hljs-comment"># 矩阵加法</span><br>C = A + B<br><br><span class="hljs-comment"># 矩阵减法</span><br>D = A - B<br><br><span class="hljs-comment"># 矩阵乘法</span><br>E = np.dot(A, B)<br><br><span class="hljs-comment"># 矩阵除法（通过乘以逆矩阵）</span><br>F = np.dot(A, np.linalg.inv(B)) <br><br></code></pre></td></tr></table></figure><h3 id="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"><a href="#傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）" class="headerlink" title="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"></a>傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）</h3><h4 id="基本介绍。"><a href="#基本介绍。" class="headerlink" title="基本介绍。"></a>基本介绍。</h4><p>傅里叶变换是一种在数学、物理和工程中广泛使用的数学变换，它可以将一个函数或信号从其原始的时间或空间表示转换为频率表示。这对于许多应用都非常有用，因为它可以揭示信号的频率成分，这在原始的时间或空间表示中可能不明显。<br>傅里叶变换的基本思想是，任何函数都可以表示为一系列正弦波和余弦波的叠加。换句话说，我们可以将一个复杂的信号分解为一系列更简单的正弦波和余弦波。</p><h4 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h4><p>傅里叶变换的基本原理是将一个函数或信号从其原始的时间或空间表示转换为频率表示。这是通过将函数表示为一系列正弦波和余弦波的叠加来实现的。<br><img src="/pic/fly1.jpg" alt="傅里叶变换示意图"></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># 创建一个简单的信号</span><br><span class="hljs-attribute">t</span> = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">500</span>)<br><span class="hljs-attribute">f</span> = np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">50</span> * t) + <span class="hljs-number">0</span>.<span class="hljs-number">5</span> * np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">120</span> * t)<br><br><span class="hljs-comment"># 绘制原始信号</span><br><span class="hljs-attribute">plt</span>.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-attribute">plt</span>.plot(t, f)<br><span class="hljs-attribute">plt</span>.title(&#x27;Original Signal&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Time&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Amplitude&#x27;)<br><br><span class="hljs-comment"># 计算傅里叶变换</span><br><span class="hljs-attribute">F</span> = np.fft.fft(f)<br><br><span class="hljs-comment"># 计算频率</span><br><span class="hljs-attribute">freq</span> = np.fft.fftfreq(t.shape[-<span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 绘制频谱</span><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-attribute">plt</span>.plot(freq, np.abs(F))<br><span class="hljs-attribute">plt</span>.title(&#x27;Frequency Spectrum&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Frequency&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Magnitude&#x27;)<br><br><span class="hljs-attribute">plt</span>.tight_layout()<br><span class="hljs-attribute">plt</span>.show()<br><br></code></pre></td></tr></table></figure><h3 id="什么是对象？-封装，继承，多态是什么？"><a href="#什么是对象？-封装，继承，多态是什么？" class="headerlink" title="什么是对象？ 封装，继承，多态是什么？"></a>什么是对象？ 封装，继承，多态是什么？</h3><p>什么是对象？<br>在面向对象编程（Object-Oriented Programming，OOP）中，对象是类的实例。类是一种抽象的概念，用于描述具有相似属性和行为的对象的集合。对象是类的具体实现，它具有类定义的属性和方法。<br>对象可以看作是现实世界中的实体或概念在程序中的表示。每个对象都有自己的状态（属性）和行为（方法），并且可以与其他对象进行交互。</p><p>封装<br>封装是面向对象编程的一种重要概念，它将数据和操作数据的方法捆绑在一起，形成一个称为类的单个实体。封装隐藏了数据的内部实现细节，只暴露对外部可见的接口。这样可以保护数据的完整性，并提供更好的代码组织和维护性。<br>通过封装，对象的内部状态可以被保护起来，只能通过公共接口进行访问和修改。这样可以防止对数据的不合理访问和修改，增加了代码的安全性和可靠性。</p><p>继承<br>继承是面向对象编程中的另一个重要概念，它允许一个类继承另一个类的属性和方法。继承创建了一个类的层次结构，其中一个类（称为子类或派生类）可以从另一个类（称为父类或基类）继承属性和方法。<br>通过继承，子类可以继承父类的特性，并且可以添加自己的特定特性。这样可以实现代码的重用和扩展，减少了重复编写代码的工作量。</p><p>多态<br>多态是面向对象编程中的另一个重要概念，它允许使用统一的接口来处理不同的对象类型。多态性允许同一个方法在不同的对象上产生不同的行为。<br>通过多态，可以编写通用的代码，可以处理多个不同类型的对象，而无需针对每种类型编写特定的代码。这提高了代码的灵活性和可扩展性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 封装示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model</span>):<br>        self.brand = brand<br>        self.model = model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 Car 对象并访问其信息</span><br>my_car = Car(<span class="hljs-string">&quot;Toyota&quot;</span>, <span class="hljs-string">&quot;Corolla&quot;</span>)<br>my_car.display_info()<br><br><span class="hljs-comment"># 继承示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ElectricCar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model, battery_capacity</span>):<br>        <span class="hljs-built_in">super</span>().__init__(brand, model)<br>        self.battery_capacity = battery_capacity<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Electric Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>, Battery Capacity: <span class="hljs-subst">&#123;self.battery_capacity&#125;</span> kWh&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 ElectricCar 对象并访问其信息</span><br>my_electric_car = ElectricCar(<span class="hljs-string">&quot;Tesla&quot;</span>, <span class="hljs-string">&quot;Model S&quot;</span>, <span class="hljs-number">100</span>)<br>my_electric_car.display_info()<br><br><span class="hljs-comment"># 多态示例</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_car_info</span>(<span class="hljs-params">car</span>):<br>    car.display_info()<br><br><span class="hljs-comment"># 使用 show_car_info 函数展示不同类型的车辆信息</span><br>show_car_info(my_car)<br>show_car_info(my_electric_car)<br><br></code></pre></td></tr></table></figure><h3 id="python中的不同代码高亮表示什么？"><a href="#python中的不同代码高亮表示什么？" class="headerlink" title="python中的不同代码高亮表示什么？"></a>python中的不同代码高亮表示什么？</h3><p>在Python的IDLE编程环境中，不同颜色的文本表示不同的含义。以下是IDLE中常见的颜色及其含义：<br>黑色：普通的代码文本。<br>蓝色：关键字，例如if、else、for、while等。<br>绿色：字符串文本。<br>红色：语法错误或代码中的错误。<br>紫色：函数和方法的名称。<br>棕色：数字。<br>橙色：内置函数和模块的名称。<br>灰色：注释。</p>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——VGG</title>
    <link href="/2024/04/22/deeplearnpaper2/"/>
    <url>/2024/04/22/deeplearnpaper2/</url>
    
    <content type="html"><![CDATA[<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>2014年<br><a href="https://blog.csdn.net/C_chuxin/article/details/82833070">中英文对照翻译</a><br><a href="https://zhuanlan.zhihu.com/p/460777014">VGG论文解读</a><br><a href="https://arxiv.org/pdf/1409.1556">原文</a><br><a href="https://zhuanlan.zhihu.com/p/107884876">VGG论文解读</a></p><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>VGG是牛津大学的Visual Geometry Group的团队在ILSVRC 2014上的相关工作。在这项工作中，主要研究卷积网络深度对大规模图像识别准确率的影响。其主要的贡献是对使用非常小的卷积滤波器（3 X 3）的体系架构来增加网络深度进行彻底的评估。实验结果表明将网络的深度提升至16-19个权重层可以实现对现有技术的显著改进。其在2014年的 imageNet 大规模视觉挑战赛（ILSVRC - 2014）中取得亚军。（冠军是 GoogleNet，预告下一篇是GoogleNet）</p><h2 id="VGG原理"><a href="#VGG原理" class="headerlink" title="VGG原理"></a>VGG原理</h2><p>VGG原理<br>相比于 LeNet 网络，VGG 网络的一个改进点是将 大尺寸的卷积核 用 多个小尺寸的卷积核 代替。</p><p>比如：VGG使用 2个3X3的卷积核 来代替 5X5的卷积核，3个3X3的卷积核 代替7X7的卷积核。</p><p>这样做的好处是：</p><ol><li>在保证相同感受野的情况下，多个小卷积层堆积可以提升网络深度，增加特征提取能力（非线性层增加）。</li><li>参数更少。比如 1个大小为5的感受野 等价于 2个步长为1，3X3大小的卷积核堆叠。（即1个5X5的卷积核等于2个3X3的卷积核）。而1个5X5卷积核的参数量为 5<em>5</em>C^2。而2个3X3卷积核的参数量为 2<em>3</em>3*C^2。很显然，18C^2 &lt; 25C^2。</li><li>3X3卷积核更有利于保持图像性质。</li></ol><p>VGG缺点：</p><p>VGG耗费更多计算资源，并且使用了更多的参数（这里不是3x3卷积的锅），导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。<br>注：这里参数量的计算，忽略了偏置。并且假设 输入和输出通道数都为C。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>VGGNet以下6种不同结构，我们以通常所说的VGG-16(即下图D列)为例，展示其结构示意图<br><img src="/pic/VGG.png" alt="VGG_6种模型结构"><br><img src="/pic/VGG_16.png" alt="VGG_16模型结构图"><br><img src="/pic/VGG_16_chanshu.png" alt="VGG参数图"><br><img src="/pic/VGG_chanshu.png" alt="VGG16参数图"></p><h2 id="摘要-Abstract"><a href="#摘要-Abstract" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><p>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Ourmain contribution is a thorough evaluation of networks of increasing depth usingan architecture withvery small (3×3) convolution filters, which shows that a significant improvementon the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. Wehave made our two best-performing ConvNet models publicly available to facili-tate further research on the use of deep visual representations in computer vision.</p><p>在这项工作中，我们研究了卷积网络深度对其在大规模图像识别设置中的准确性的影响。我们的主要贡献是使用一个非常小的(3×3)卷积filter的架构对增加深度的网络进行了彻底的评估，这表明通过将深度提升到16 - 19个weight层，可以显著改善先前的配置。这些发现是我们提交ImageNet挑战赛2014的基础，我们的团队分别获得了本地化和分类的第一名和第二名。我们还展示了我们的成果可以很好地推广到其他数据集，在这些数据集上他们可以得到最优结果。我们已经公开了两个性能最好的卷积神经网络模型，以促进在计算机视觉中使用深度视觉表示的进一步研究。</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>训练和之前的AlexNet整体类似，使用小批量梯度下降，参数方面：batch设为256，动量设为0.9，除最后一层外的全连接层也都使用了丢弃率0.5的dropout。learning rate最初设为0.01,权重衰减系数为5×10^-4。对于权重层采用了随机初始化，初始化为均值0，方差0.01的正态分布。 训练的图像数据方面，为了增加数据集，和AlexNet一样，这里也采用了随机水平翻转和随机RGB色差进行数据扩增。对经过重新缩放的图片随机排序并进行随机剪裁得到固定尺寸大小为224×224的训练图像。</p><h2 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h2><p><img src="/pic/VGG_train_result.webp" alt="对比结果"><br>通过表格间各个网络的对比发现如下结论：</p><p>总体来说卷积网络越深，损失越小，效果越好。<br>C优于B，表明多增加的非线性relu有效<br>D优于C，表明了卷积层filter对于捕捉空间特征有帮助。<br>E深度达到19层后达到了损失的最低点，但是对于其他更大型的数据集来说，可能更深的模型效果更好。<br>B和同类型filter size为5×5的网络进行了对比，发现其top-1错误率比B高7%，表明小尺寸filter效果更好。<br>在训练中，采用浮动尺度效果更好，因为这有助于学习分类目标在不同尺寸下的特征。</p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="使用VGG来实现垃圾的40分类"><a href="#使用VGG来实现垃圾的40分类" class="headerlink" title="使用VGG来实现垃圾的40分类"></a>使用VGG来实现垃圾的40分类</h3><ol><li>第一步准备训练集，固定数据集（当然也可以不固定数据集，但是在对比实验中一定要固定数据集划分）<br>utils.py文件,这个文件的作业是产生2个csv文件，固定训练集和测试集<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs lua">import <span class="hljs-built_in">os</span><br>import csv<br>import numpy as np<br>train_path = <span class="hljs-string">&quot;train_data.csv&quot;</span><br>val_path = <span class="hljs-string">&quot;val_data.csv&quot;</span><br><br>train_percent = <span class="hljs-number">0.9</span><br><br>def create_data_txt(<span class="hljs-built_in">path</span>):<br>    f_train = <span class="hljs-built_in">open</span>(train_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&quot;&quot;</span>)<br>    f_val = <span class="hljs-built_in">open</span>(val_path,<span class="hljs-string">&quot;w&quot;</span>,newline=<span class="hljs-string">&quot;&quot;</span>)<br>    train_writer = csv.writer(f_train)<br>    val_writer = csv.writer(f_val)<br><br>    <span class="hljs-keyword">for</span> cls,dirname <span class="hljs-keyword">in</span> enumerate(<span class="hljs-built_in">os</span>.listdir(<span class="hljs-built_in">path</span>)):<br>        flist = <span class="hljs-built_in">os</span>.listdir(<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>,dirname))<br>        np.<span class="hljs-built_in">random</span>.shuffle(flist)<br>        fnum = <span class="hljs-built_in">len</span>(flist)<br>        <span class="hljs-keyword">for</span> i,filename <span class="hljs-keyword">in</span> enumerate(flist):<br>            <span class="hljs-keyword">if</span> i &lt; fnum*train_percent:<br>                train_writer.writerow([<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>,dirname,filename),str(cls)])<br>            <span class="hljs-keyword">else</span>:<br>                val_writer.writerow([<span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">path</span>, dirname, filename), str(cls)])<br><br>    f_train.<span class="hljs-built_in">close</span>()<br>    f_val.<span class="hljs-built_in">close</span>()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    create_data_txt(<span class="hljs-string">&quot;data_garbage&quot;</span>)<br><br></code></pre></td></tr></table></figure></li></ol><p>dataset.py 文件根据utisl.py文件来对数据进行数据预处理操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms,utils<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset,DataLoader<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>train_tf = transforms.Compose([<br>    <span class="hljs-comment"># transforms.RandomResizedCrop(size=(224,224), scale=(0.9,1.1)),</span><br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    transforms.RandomRotation(<span class="hljs-number">10</span>),<br>    transforms.ColorJitter(brightness=(<span class="hljs-number">0.9</span>,<span class="hljs-number">1.1</span>),contrast=(<span class="hljs-number">0.9</span>,<span class="hljs-number">1.1</span>)),<br>    <span class="hljs-comment"># transforms.Resize((50,50)),</span><br>    transforms.ToTensor(),<br>])<br><br>val_tf = transforms.Compose([<br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>    <span class="hljs-comment"># transforms.Grayscale(1),</span><br>    transforms.ToTensor(),<br>])<br><br><span class="hljs-comment">#自定义数据集</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Animals_dataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,istrain=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-keyword">if</span> istrain:<br>            f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;train_data.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;val_data.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>)<br>        self.dataset = f.readlines()<br>        f.close()<br>        self.istrain = istrain<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.dataset)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        data = self.dataset[index]<br>        img_path = data.split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">0</span>]<br>        cls = <span class="hljs-built_in">int</span>(data.split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">1</span>])<br><br>        img_data = Image.<span class="hljs-built_in">open</span>(img_path).convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>        <span class="hljs-keyword">if</span> self.istrain:<br>            dst = train_tf(img_data)<br>        <span class="hljs-keyword">else</span>:<br>            dst =val_tf(img_data)<br><br>        <span class="hljs-keyword">return</span> dst,torch.tensor(cls)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">visulization</span>():<br>    train_dataset = Animals_dataset(<span class="hljs-literal">True</span>)<br>    train_dataloader = DataLoader(train_dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>    examples = <span class="hljs-built_in">enumerate</span>(train_dataloader)<br>    batch_index,(data, lable) = <span class="hljs-built_in">next</span>(examples)<br>    <span class="hljs-built_in">print</span>(data.shape)<br><br>    grid = utils.make_grid(data)<br>    plt.imshow(grid.numpy().transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>    plt.show()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    visulization()<br></code></pre></td></tr></table></figure><p>train.py 训练模型的代码</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim,nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> dataset <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>m = nn.Softmax(dim=<span class="hljs-number">1</span>)<br>def train(<span class="hljs-keyword">method</span>=&quot;normal&quot;,ckpt_path=&quot;&quot;):<br>    # 数据集和数据加载器<br>    train_dataset = Animals_dataset(<span class="hljs-keyword">True</span>)<br>    train_dataloader = DataLoader(train_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-keyword">True</span>)<br>    val_dataset = Animals_dataset(<span class="hljs-keyword">False</span>)<br>    val_dataloader = DataLoader(val_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-keyword">False</span>)<br><br>    #模型<br>    device = torch.device(&quot;cuda&quot; <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> &quot;cpu&quot;)#系统自己决定有啥训练<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">method</span>==&quot;normal&quot;:<br>        model = models.vgg16(num_classes=<span class="hljs-number">40</span>,dropout=<span class="hljs-number">0.45</span>).<span class="hljs-keyword">to</span>(device)<br>    elif <span class="hljs-keyword">method</span>==&quot;step1&quot;:<br>        model=models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> model.parameters():<br>            i.requires_grad=<span class="hljs-keyword">False</span><br>        model.classifier=nn.Sequential(<br>            nn.Linear(<span class="hljs-number">512</span>*<span class="hljs-number">7</span>*<span class="hljs-number">7</span>,<span class="hljs-number">2048</span>),<br>            nn.ReLU(<span class="hljs-keyword">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.35</span>),<br>            nn.Linear(<span class="hljs-number">2048</span>,<span class="hljs-number">1024</span>),<br>            nn.ReLU(<span class="hljs-keyword">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.35</span>),<br>            nn.Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">40</span>)<br>        )<br>        model.<span class="hljs-keyword">to</span>(device)<br>    elif <span class="hljs-keyword">method</span>==&quot;step2&quot;:<br>        model=models.vgg16()<br>        model.classifier=nn.Sequential(<br>            nn.Linear(<span class="hljs-number">512</span> * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">2048</span>),<br>            nn.ReLU(<span class="hljs-keyword">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.35</span>),<br>            nn.Linear(<span class="hljs-number">2048</span>, <span class="hljs-number">1024</span>),<br>            nn.ReLU(<span class="hljs-keyword">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.35</span>),<br>            nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">40</span>)<br>        )<br>        model.load_state_dict(torch.<span class="hljs-keyword">load</span>(&quot;model/vgg16_step1_trush.pth&quot;))<br>        model.<span class="hljs-keyword">to</span>(device)<br>    print(&quot;train on &quot;,device)<br>    #损失函数（二分类交叉熵）<br>    loss_fn = nn.CrossEntropyLoss()<br><br>    #优化器<br>    optimizer = optim.SGD(model.parameters(),lr=<span class="hljs-number">0.01</span>,momentum=<span class="hljs-number">0.9</span>)<br><br>    #断点恢复<br>    start_epoch = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">if</span> ckpt_path != &quot;&quot;:<br>        <span class="hljs-keyword">checkpoint</span> = torch.<span class="hljs-keyword">load</span>(ckpt_path)<br>        model.load_state_dict(<span class="hljs-keyword">checkpoint</span>[&quot;net&quot;])<br>        optimizer.load_state_dict(<span class="hljs-keyword">checkpoint</span>[&quot;optimizer&quot;])<br>        start_epoch = <span class="hljs-keyword">checkpoint</span>[&quot;epoch&quot;] + <span class="hljs-number">1</span><br><br>    #训练<br>    train_loss_arr = []<br>    train_acc_arr = []<br>    val_loss_arr = []<br>    val_acc_arr = []<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):<br>        train_loss_total = <span class="hljs-number">0</span> #所有batch的loss累加值<br>        train_acc_total = <span class="hljs-number">0</span> #所有batch的acc累加值<br>        val_loss_total = <span class="hljs-number">0</span><br>        val_acc_total = <span class="hljs-number">0</span><br><br>        model.train()#标志此时为训练状态，启用dropout随机失活，否则不启用<br>        <span class="hljs-keyword">for</span> i,(train_x,train_y) <span class="hljs-keyword">in</span> enumerate(train_dataloader):<br>            train_x = train_x.<span class="hljs-keyword">to</span>(device)<br>            train_y = train_y.<span class="hljs-keyword">to</span>(device)<br><br>            #前向传播<br>            train_y_pred = model(train_x)<br>            train_loss = loss_fn(train_y_pred,train_y)<br>            train_acc = (m(train_y_pred).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]==train_y).sum()/train_y.shape[<span class="hljs-number">0</span>]<br>            train_loss_total += train_loss.data.item()<br>            train_acc_total += train_acc.data.item()<br>            #反向传播<br>            train_loss.backward()<br>             #梯度下降<br>            optimizer.step()<br>            optimizer.zero_grad()<br><br>            print(&quot;epoch:&#123;&#125; train_loss:&#123;&#125; train_acc:&#123;&#125;&quot;.format(epoch, train_loss.data.item(), train_acc.data.item()))<br><br>        train_loss_arr.append(train_loss_total / len(train_dataloader)) #平均值<br>        train_acc_arr.append(train_acc_total / len(train_dataloader))<br><br>        #测试集<br>        <span class="hljs-keyword">for</span> j, (val_x, val_y) <span class="hljs-keyword">in</span> enumerate(val_dataloader):<br>            val_x = val_x.<span class="hljs-keyword">to</span>(device)<br>            val_y = val_y.<span class="hljs-keyword">to</span>(device)<br>            #前向传播<br>            val_y_pred,_,_ = model(val_x)<br>            val_loss = loss_fn(val_y_pred,val_y)<br>            val_acc = (m(val_y_pred).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]==val_y).sum()/val_y.shape[<span class="hljs-number">0</span>]<br>            val_loss_total += val_loss.data.item()<br>            val_acc_total += val_acc.data.item()<br><br>        val_loss_arr.append(val_loss_total / len(val_dataloader))  # 平均值<br>        val_acc_arr.append(val_acc_total / len(val_dataloader))<br>        print(&quot;epoch:&#123;&#125; val_loss:&#123;&#125; val_acc:&#123;&#125;&quot;.format(epoch, val_loss_arr[<span class="hljs-number">-1</span>], val_acc_arr[<span class="hljs-number">-1</span>]))<br>        #保存模型（断点连续）<br>        <span class="hljs-keyword">checkpoint</span>=&#123;<br>            &quot;net&quot;:model.state_dict(),<br>            &quot;optimizer&quot;:optimizer.state_dict(),<br>            &quot;epoch&quot;:epoch<br>        &#125;<br>        torch.save(<span class="hljs-keyword">checkpoint</span>,&quot;checkpoint/ckpt.pth&quot;)<br><br><br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>) #画布一分为二,<span class="hljs-number">1</span>行<span class="hljs-number">2</span>列，用第一个<br>    plt.title(&quot;loss&quot;)<br>    plt.plot(train_loss_arr,&quot;r&quot;,label = &quot;train&quot;)<br>    plt.plot(val_loss_arr,&quot;b&quot;,label = &quot;val&quot;)<br>    plt.legend()<br><br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  # 画布一分为二,<span class="hljs-number">1</span>行<span class="hljs-number">2</span>列，用第一个<br>    plt.title(&quot;acc&quot;)<br>    plt.plot(train_acc_arr, &quot;r&quot;, label=&quot;train&quot;)<br>    plt.plot(val_acc_arr, &quot;b&quot;, label=&quot;val&quot;)<br>    plt.legend()<br>    plt.savefig(&quot;loss/loss_acc_vgg.png&quot;)<br><br>    plt.<span class="hljs-keyword">show</span>()<br><br>    #保存模型<br>    #<span class="hljs-number">1.</span>torch.save()<br>    #<span class="hljs-number">2.</span>文件的后缀名：.pt、.pth、.pkl<br>    torch.save(model.state_dict(),&quot;model/vgg_trush.pth&quot;)<br>    print(&quot;保存模型成功!&quot;)<br><br><br><span class="hljs-keyword">if</span> __name__ == &quot;__main__&quot;:<br>    train()<br><br><br></code></pre></td></tr></table></figure><p>test.py测试模型的代码</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch.cuda<br><br><span class="hljs-keyword">from</span> torchvision import models<br>import os<br><span class="hljs-keyword">from</span> torch import nn<br><span class="hljs-keyword">from</span> dataset import *<br><span class="hljs-keyword">from</span> PIL import Image<br><span class="hljs-keyword">from</span> torch.utils.data import DataLoader<br><span class="hljs-keyword">from</span> dataset import *<br><span class="hljs-keyword">from</span> sklearn.metrics import recall_score, f1_score, precision_score, confusion_matrix<br><span class="hljs-keyword">from</span> matplotlib import rcParams<br>rcParams[<span class="hljs-string">&#x27;font.family&#x27;</span>] = <span class="hljs-string">&#x27;SimHei&#x27;</span><br><br>m = nn.Softmax(<span class="hljs-attribute">dim</span>=1)<br>labels = os.listdir(<span class="hljs-string">&quot;data_garbage&quot;</span>)<br><br>def evaluate():<br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    model = models.googlenet(<span class="hljs-attribute">num_classes</span>=40).to(device)<br>    model.load_state_dict(torch.load(<span class="hljs-string">&quot;model/vgg_trush.pth&quot;</span>))<br>    model.eval()<br><br>    img = Image.open(<span class="hljs-string">&quot;tests/5.jpg&quot;</span>)<br>    dst = val_tf(img).<span class="hljs-keyword">to</span>(device)<br>    dst = torch.unsqueeze(dst, <span class="hljs-attribute">dim</span>=0)   # (1, 3, 224, 224)<br>    y_hat = model(dst)<br><br>    values = m(y_hat).sort(<span class="hljs-attribute">dim</span>=1, <span class="hljs-attribute">descending</span>=<span class="hljs-literal">True</span>)[0][0]<br>    index = m(y_hat).sort(<span class="hljs-attribute">dim</span>=1, <span class="hljs-attribute">descending</span>=<span class="hljs-literal">True</span>)[1][0]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(5):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;:&#125; - &#123;:.5f&#125;&quot;</span>.format(labels[index[i]], values[i]))<br><br>    plt.imshow(img)<br>    plt.show()<br><br>def val():<br>    # 数据集和数据加载器<br>    val_dataset = Animals_dataset(<span class="hljs-literal">False</span>)<br>    val_data_loader = DataLoader(val_dataset, <span class="hljs-attribute">batch_size</span>=128, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">drop_last</span>=<span class="hljs-literal">True</span>)<br><br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    model = models.googlenet(<span class="hljs-attribute">num_classes</span>=40).to(device)<br>    model.load_state_dict(torch.load(<span class="hljs-string">&quot;model/vgg_trush.pth&quot;</span>))<br>    model.eval()<br><br>    val_y_total = []<br>    val_y_pred_total = []<br>    <span class="hljs-keyword">for</span> val_x, val_y <span class="hljs-keyword">in</span> val_data_loader:<br>        val_x = val_x.<span class="hljs-keyword">to</span>(device)<br>        val_y_pred = model(val_x).cpu()<br><br>        val_y_total.extend(val_y.cpu().numpy())    # 将列表中的数据取出来追加<br>        val_y_pred_total.extend(m(val_y_pred).max(<span class="hljs-attribute">dim</span>=1)[1].cpu().numpy())<br><br>    p = precision_score(val_y_total, val_y_pred_total, <span class="hljs-attribute">average</span>=<span class="hljs-string">&quot;weighted&quot;</span>)<br>    recall = recall_score(val_y_total, val_y_pred_total, <span class="hljs-attribute">average</span>=<span class="hljs-string">&quot;weighted&quot;</span>)<br>    f1 = f1_score(val_y_total, val_y_pred_total, <span class="hljs-attribute">average</span>=<span class="hljs-string">&quot;weighted&quot;</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;precision: &#123;:.5f&#125;, recall=&#123;:.5f&#125;, f1=&#123;:.5f&#125;&quot;</span>.format(p, recall, f1))<br><br>    cm = confusion_matrix(val_y_total, val_y_pred_total)<br><br>    plt.imshow(cm, <span class="hljs-attribute">cmap</span>=plt.cm.Blues)<br>    plt.xticks(range(40), <span class="hljs-attribute">labels</span>=labels)<br>    plt.yticks(range(40), <span class="hljs-attribute">labels</span>=labels)<br><br>    plt.colorbar()<br>    plt.xlabel(<span class="hljs-string">&quot;预测值&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;真实值&quot;</span>)<br>    thresh = cm.mean()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(40):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(40):<br>            <span class="hljs-built_in">info</span> = cm[j, i]<br>            plt.text(i, j, info, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;white&quot;</span> <span class="hljs-keyword">if</span> info&gt;thresh <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;black&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;confusion_matrix.jpg&quot;</span>)<br>    plt.show()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    evaluate()<br><br><br></code></pre></td></tr></table></figure><h3 id="什么是感受野？"><a href="#什么是感受野？" class="headerlink" title="什么是感受野？"></a>什么是感受野？</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>填坑博客总目录</title>
    <link href="/2024/04/22/tiankeng/"/>
    <url>/2024/04/22/tiankeng/</url>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="PyTorch基础——Numpy"><a href="#PyTorch基础——Numpy" class="headerlink" title="PyTorch基础——Numpy"></a>PyTorch基础——Numpy</h2><h2 id="经典网络结构——AlexNet"><a href="#经典网络结构——AlexNet" class="headerlink" title="经典网络结构——AlexNet"></a>经典网络结构——AlexNet</h2>]]></content>
    
    
    
    <tags>
      
      <tag>填坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔9</title>
    <link href="/2024/04/22/ganwu9/"/>
    <url>/2024/04/22/ganwu9/</url>
    
    <content type="html"><![CDATA[<h1 id="夜晚思考"><a href="#夜晚思考" class="headerlink" title="夜晚思考"></a>夜晚思考</h1><p>作为一个大学生，住在寝室是很正常的。2024&#x2F;4&#x2F;21的夜晚不是很寻常，床下的键盘声和电脑的光亮让我难以入睡。思绪浮想联翩，世界毁灭了，我要毁灭了。情绪在波动，心脏在疼痛。我该怎么去停止这键盘声和光亮从而让我安静的入眠。<br>人总是以为自己是站在道德的高点，很不幸的告诉我自己，当自以为在道德高点时，我其实已经没有了道德。以自己最大的恶意去揣测他人，已经不道德了。键盘声和光亮真的不能让我入睡吗？还是自己不让自己睡觉，自己的情绪，自己禁锢自己。意识和情绪不是一体的，控制自己的情绪。<br>我总是有两个自己，一个以恶意揣测别人，一个则想怎么去解决这个问题。逃离，争吵，苦恼，毁灭世界。思想斗争吧，预演所有情况吧，一个小时后，问题不能被解决，反而越来越难受。沟通一下吧。起身，正坐。问道：“兄弟，你有什么重要的事情需要去完成吗？” 答曰：“作业没有写完，正在写作业。”听之，甚觉羞愧，焕然冰释。<br>自己禁锢自己，被情绪裹挟，人啊人啊。<br><img src="/pic/ganwu9.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔8</title>
    <link href="/2024/04/21/ganwu8/"/>
    <url>/2024/04/21/ganwu8/</url>
    
    <content type="html"><![CDATA[<h1 id="做事中什么最重要？"><a href="#做事中什么最重要？" class="headerlink" title="做事中什么最重要？"></a>做事中什么最重要？</h1><p>明白自己在干什么，明白此时此刻我在干什么。为什么说这是最重要的?拿做数学题来讲吧，要做一道数学题，必须先看题，了解题目的内容，获取前提条件。获取了前提条件之后，有两种可能，会做和不会做，会做的过程中，最好的感觉是，看了一眼题目之后就胸有成竹了，虽然不知道这道题的答案是什么，但是清楚这道题应该怎么去解答，应该在什么部分注意。这是最好的状态，很清晰的明白自己在做什么的状态。对于第二种，不知道该怎么解决的，首先得清楚自己不会做，然后去分析自己哪里不会，而不是自己不会就不会，这样的心态是大忌，不要傲慢，没有人生下来就会所有事情，都是从不会到会的，对待自己会别人不会的题，不要傲慢，不要鄙视他人，因为我也是从不会到会的。了解自己哪里不会了之后，去找到解决方法，补足不会的点，然后就能解决这道数学题了。这是第二个情况的解决方法，在整个流程中，我都清晰自己在干什么，而不是迷迷糊糊的不知道自己在干什么。<br>再举一个例子，拿科研来讲，一篇论文需要注意什么？第一，创新点。第二，实验设计。第三，文章表述。这三点的重要性不分先后。我想的，我做的，我写出来的是三种东西。在这个过程中需要清晰的认识自己在做什么，如果很清晰的知道，胸有成竹的，就可以去做，如果是第二种情况，那就慢慢来补充自己欠缺的知识。需要说明的是，胸有成竹和存在不足的情况可能会周期交互，一段时间的胸有成竹和一段时间的不足。但是在解决不足的过程却是胸有成竹的，清楚的明白自己需要干什么。</p><h2 id="《胸有成竹》-苏轼"><a href="#《胸有成竹》-苏轼" class="headerlink" title="《胸有成竹》-苏轼"></a>《胸有成竹》-苏轼</h2><p>竹之始生，一寸之萌耳，而节叶具焉；自蜩蝮蛇蚹，以至于剑拔十寻者，生而有之也。<br>今画者乃节节而为之，叶叶而累之，岂复有竹乎？故画竹必先得成竹于胸中，执笔熟视，乃见其所欲画者，急起从之，振笔直遂，以追其所见，如兔起鹘落，少纵则逝矣。与可之教予如此。予不能然也，而心识其所以然。夫既心识其所以然，而不能然者，内外不一，心手不相应，不学之过也。故凡有见于中，而操之不熟者，平居自视了然，而临时忽焉丧之，岂独竹乎？子由为《墨竹赋》以遗与可曰：“庖丁，解牛者也，而养生者取之；轮扁，斫轮者也，而读书者与之。今夫夫子之托于斯竹也，而予以为有道者则非耶？”子由未尝画也，故得其意而已。若予者，岂独得其意，并得其法。</p><p>译文：竹子开始生出时，只是一寸高的萌芽而已，但节、叶都具备了。从蝉破壳而出、蛇长出鳞一样的状态，直至像剑拔出鞘一样长到八丈高，都是一生长出来就有的。如今画竹的人都是一节节地画它，一叶叶地堆积它，这样哪里还会有完整的、活生生的竹子呢？所以画竹必定要心里先有完整的竹子形象，拿起笔来仔细看去，就看到了自己所想画的竹子，急速起身跟住它，动手作画，一气呵成，以追上自己所见到的，如兔子跃起奔跑、隼俯冲下搏，稍一放松就消失了。与可告诉我的是如此。我不能做到这样，但心里明白这样做的道理。既然心里明白这样做的道理，但不能做到这样，是由于内外不一，心与手不相适应，没有学习的过错。所以凡是在心中有了构思，但是做起来不熟练的，平常自己认为很清楚，可事到临头忽然又忘记了，这种现象难道仅仅是画竹有吗？ 　子由写了篇《墨竹赋》，把它送给与可，说：“丁厨子，是杀牛的，但讲求养生的人从他的行动中悟出了道理；轮匠扁，是造车轮的，但读书的人赞成他讲的道理。如今您寄托意蕴在这幅竹画上，我认为您是深知道理的人，难道不是吗？”子由没有作过画，所以只得到了他的意蕴。象我这样的人，哪里仅仅是得到与可的意蕴，并且也得到了与可的方法。</p><p>自注： 我觉得苏轼这篇说说很好，但是需要补充的是，这里以做事的态度讨论，其他角度碍于自己的层次有限暂时不讨论，胸有成竹是最好的做事状态。竹子一开始是具备了节和叶，但是只是一寸高的萌芽的，要从一寸长的萌芽成长到高耸，需要很多条件都满足，竹子的成长也是一点点，一节一节来长得。很多时候做事都不是胸有成竹的状态， 一开始都是懵懂不知道的，只有在做的得心应手时才有胸有成竹的状态，在做事之前已作好充分准备，对事情的成功已有了十分的把握；最开始的竹子都是矮小的，高耸的竹子都是一点点成长的。苏轼说的没错的是故画竹必先得成竹于胸中，执笔熟视，乃见其所欲画者，急起从之，振笔直遂，以追其所见，如兔起鹘落，少纵则逝矣。但是他没有说的是这是大佬做事的境界，小白不都是从今画者乃节节而为之，叶叶而累之吗？<br>故凡有见于中，而操之不熟者，平居自视了然，而临时忽焉丧之，岂独竹乎？这句叫人不要傲慢，知行合一。<br>子由为《墨竹赋》以遗与可曰：“庖丁，解牛者也，而养生者取之；轮扁，斫轮者也，而读书者与之。今夫夫子之托于斯竹也，而予以为有道者则非耶？”子由未尝画也，故得其意而已。若予者，岂独得其意，并得其法。这句法则都是相同的，方法是法则的具体体现。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典网络结构——AlexNet</title>
    <link href="/2024/04/21/deeplearnpaper/"/>
    <url>/2024/04/21/deeplearnpaper/</url>
    
    <content type="html"><![CDATA[<p>我给自己挖了很多坑没有去填，只能慢慢填了，今天先填第一个坑。<br><a href="https://blog.csdn.net/guzhao9901/article/details/118552085">本人参考博客1-</a><br><a href="https://zhuanlan.zhihu.com/p/618545757">本人参考博客2-</a><br><a href="https://blog.csdn.net/hongbin_xu/article/details/80271291">本人参考博客3-AlexNet的翻译</a><br><a href="https://blog.csdn.net/ARYAD/article/details/107687362">本人参考的博客-模型结构发展简史</a></p><h1 id="AlexNet-介绍"><a href="#AlexNet-介绍" class="headerlink" title="AlexNet 介绍"></a>AlexNet 介绍</h1><p><a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">论文原文链接</a><br>AlexNet是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年ImageNet图像分类竞赛中提出的一种经典的卷积神经网络。AlexNet在 ImageNet 大规模视觉识别竞赛中取得了优异的成绩，把深度学习模型在比赛中的正确率提升到一个前所未有的高度。因此，它的出现对深度学习发展具有里程碑式的意义。<br><a href="https://github.com/aaron-xichen/pytorch-playground">可以参考的github仓库</a></p><ol><li>AlexNet的输入为RGB三通道大小的图像，图像的shape可以表述为（227x227x3）。AlexNet共包含5个卷积层（包含3个池化）和3个全连接层。其中每个卷积层都包含卷积核、偏置项、ReLU激活函数和局部响应归一化（LRN）模块。第1，2，5个卷积层后面都跟着一个最大池化层，后三个层为全连接层。最终的输出层为softmax（这里有一个很有意思的知识，softmax怎么将网络输出转化为概率值，后面再说。）</li></ol><p><img src="/pic/paper_Alex_1.png" alt="AlexNet模型结构图"><br>这里需要指出的是，在网络设计上并非上图所示，上图包含了GPU通信的部分。这是因为当时的GPU内存的限制引起的，作者使用了两块GPU进行计算<br>废话不多说，直接上代码。代码来源为《动手深度学习》</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn, optim<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-meta"># 上面的部分为引入包操作，介绍一下上面引入的包的作用。</span><br><span class="hljs-meta"># time：这是python中的内置的模块，用于处理时间相关的操作。可以用来获取当前的时间，或者在程序中添加延迟。</span><br><span class="hljs-meta"># torch：这是pytorch库的主要部分，一个用于机器学习和深度学习的开源库。提高高效的张量（多维数组）计算（类似于Numpy）的方式，同时支持GPU计算（基于CUDA和CUDNN）</span><br><span class="hljs-meta"># torch.nn 是pytorch中的一个子模块，提供构建神经网络所需要的各种工具和组件。</span><br><span class="hljs-meta"># torch.optim也是pytorch中的一个子模块，提供各种优化算法，比如SGD，Adam和RMSProp等（这里给自己挖个坑）</span><br><span class="hljs-meta"># torch.torchvision，一个与PyTorch关联的库，专门用于处理图像和视频的计算机视觉任务。它提供许多预训练的模型，如ResNet，VGG和AlexNet等，同时还有常见的数据集，如ImageNet，CIFAR10/100，MNIST等。</span><br><br><span class="hljs-title">device</span> = torch.device(&#x27;cuda&#x27; <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> &#x27;cpu&#x27;)<br><span class="hljs-meta"># 这一句的作用是选取GPU训练还是CPU训练。</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">AlexNet</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">AlexNet</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.conv = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding（输出通道数，输出通道数，卷积核大小，步长，填充，这里又有坑，关于卷积后特征图应该怎么计算？）</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(), # <span class="hljs-type">ReLU</span>激活函数</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2), # kernel_size, stride 最大池化，3x3的池化层，步长为2.意思是一个3x3的二维矩阵，按照最大值来输出最大特征。</span><br><span class="hljs-class">            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(96, 256, 5, 1, 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2),</span><br><span class="hljs-class">            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span><br><span class="hljs-class">            # 前两个卷积层后不使用池化层来减小输入的高和宽</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(256, 384, 3, 1, 1), # 第三个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(384, 384, 3, 1, 1), # 第四个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(384, 256, 3, 1, 1), # 第五个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2)</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">         # 这里全连接层的输出个数比<span class="hljs-type">LeNet</span>中的大数倍。使用丢弃层来缓解过拟合</span><br><span class="hljs-class">        self.fc = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Linear</span>(256*5*5, 4096), # 线性层，256*5*5为输入大小，4096为输出大小。</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout</span>(0.5), # 随机失活，<span class="hljs-type">AlexNet</span>的主要创新点之一。这里失活率为0.5</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(4096, 4096),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout</span>(0.5),</span><br><span class="hljs-class">            # 输出层。由于这里使用<span class="hljs-type">Fashion</span>-<span class="hljs-type">MNIST</span>，所以用类别数为10，而非论文中的1000</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(4096, 10), # 输出类为10.</span><br><span class="hljs-class">        )</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-title">img</span>): # 前向传播，forward在代码中需要自定义。在这里可以加入残差等操作。</span><br><span class="hljs-class">        feature = self.conv(<span class="hljs-title">img</span>)</span><br><span class="hljs-class">        output = self.fc(<span class="hljs-title">feature</span>.<span class="hljs-title">view</span>(<span class="hljs-title">img</span>.<span class="hljs-title">shape</span>[0], -1))</span><br><span class="hljs-class">        return output</span><br></code></pre></td></tr></table></figure><ol start="2"><li>背景介绍，在AlexNet网络问世之前，大量的学者在进行图像分类、分割、识别的操作时，主要是通过对图像提取特征或特征+机器学习的方法，手工提取特征是非常难的事情，即特征工程。为了提升准确率或减少人工复杂度等种种原因。因此，学界一直认为，特征是不是可以进行学习？如果可以学习，特征之间的表示方法是什么？例如第一层为线或是点特征，第二层为线与点组成的初步特征，第三层为局部特征）？从这一思想出发，特征可学习且自动组合并给出结果，这是典型的“end-to-end” 。</li></ol><h1 id="论文阅读："><a href="#论文阅读：" class="headerlink" title="论文阅读："></a>论文阅读：</h1><p>先阐述一下论文结构</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">0</span>. 标题（title）<br><span class="hljs-attribute">0</span>.<span class="hljs-number">5</span>. 摘要（Abstract）<br><span class="hljs-attribute">1</span>. 介绍（Introduction）<br><span class="hljs-attribute">2</span>. 数据集（The Dataset）<br><span class="hljs-attribute">3</span>. 网络结构（The Architecture）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">1</span> ReLU非线性单元（ReLU Nonlinearity）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">2</span> 多GPU训练（Training <span class="hljs-literal">on</span> Multiple GPU）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">3</span> 局部响应和归一化（Local Response Normalization）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">4</span> 层叠池化（Overlapping Pooling）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">5</span> 整体结构（Overall Architecture）<br><span class="hljs-attribute">4</span>. 减少过拟合（Reducing Overfitting） <br><span class="hljs-attribute">4</span>.<span class="hljs-number">1</span> 数据增强 （Data Augmentation）<br><span class="hljs-attribute">4</span>.<span class="hljs-number">2</span> 随机失活 （Dropout）<br><span class="hljs-attribute">5</span>. 学习细节 （Details of learning）<br><span class="hljs-attribute">6</span>. 结果 （Results）<br><span class="hljs-attribute">6</span>.<span class="hljs-number">1</span> 定性评估（Qualitative Evacuation）<br><span class="hljs-attribute">6</span>.<span class="hljs-number">2</span> 讨论（Discussion） <br></code></pre></td></tr></table></figure><p>这里需要说明的是由于markdown的限制和本人技术能力的欠缺。在这篇博文中不放公式，如果想看公式，请去看原论文，数学的公式才是最简洁的表达方式，前提是能够看懂，看懂了之后就像打开新世界的大门。感觉就像我有一双滑板鞋，我走到那就穿到哪。<br>0. 标题论文标题为<br>ImageNet Classification with Deep Convolutional Neural Networks<br>摘要： 我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。（开门见山，直接说干了什么。）在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率（这里使用的是错误率的评估指标，和我目前使用的Acc，Presion，Recall，召回率评估指标不一样。），这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成（这里挖一个坑，参数量和神经元数量的评估指标不一样。），其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。（这里挖个坑softMax的机制是怎么样的？）为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法（dropout，随机丢弃的机制是什么？），该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模式的一个变种，取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。</p><ol><li><p>介绍：目前，机器学习方法对物体识别非常重要。为了改善他们的表现（前提条件就是之前的表现不是很好），我们可以收集更大的数据集，训练更强大的模型，并使用更好的技术来防止过拟合。（这里指出减少过拟合的方法有增大数据集，更改模型结构，使用更好的优化技术。）直到最近，标记好图像的数据集相对还较小——大约上万的数量级（例如，NORB，Caltech-101&#x2F;256和CIFAR-10&#x2F;100）。使用这种规模的数据集可以很好地解决简单的识别任务，特别是如果他们增加了保留标签转换（label-preserving transformations）。例如，目前MNIST数字识别任务的最低错误率（&lt;0.3％）基本达到了人类的识别水平。但是物体在现实环境中可能表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。事实上，小图像数据集的缺点已是众所周知（例如，Pinto），但直到最近才可以收集到数百万的标记数据集。新的大型数据集包括LabelMe ，其中包含数十万个完全分割的图像，以及ImageNet ，其中包含超过15,000万个超过22,000个类别的高分辨率图像。（目前的研究的对象，研究现状。）<br>要从数百万图像中学习数千个类别，我们需要一个具有强大学习能力的模型。（这里暗示这篇文章的模型大小非常大，但是现在看来入门级把，毕竟是12年前的文章了，开山鼻祖了。）然而，物体识别任务的巨大复杂性意味着即使是像ImageNet这样大的数据集也不能完美地解决这个问题，所以我们的模型也需要使用很多先验知识来弥补我们数据集不足的问题。卷积神经网络（CNN）就构成了一类这样的模型（卷积神经网络可以获取先验的知识，来弥补数据集不足的问题，后面是卷积神经网络为什么能够实现获取先验知识。）。它们的容量可以通过改变它们的深度和宽度来控制，并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。<br>尽管CNN具有很好的质量，并且尽管其局部结构的效率相对较高，但将它们大规模应用于高分辨率图像时仍然显得非常昂贵。幸运的是，当前的GPU可以用于高度优化的二维卷积，能够加速许多大型CNN的训练，并且最近的数据集（如ImageNet）包含足够多的标记样本来训练此类模型，而不会出现严重的过度拟合。<br>本文的具体贡献如下：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。（codeing能力还是有的，我的目标就是能够实现自己的想法，Talk is cheap. Show me the code.这句话真的是令人兴奋）我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3章节。我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。（这点可能验证了特征是层级表示的，）最后，网络的规模主要受限于目前GPU上可用的内存量以及我们可接受的训练时间。我们的网络需要在两块GTX 580 3GB GPU上花费五到六天的时间来训练。我们所有的实验都表明，通过等待更快的GPU和更大的数据集出现，我们的结果可以进一步完善。</p></li><li><p>数据集：ImageNet是一个拥有超过1500万个已标记高分辨率图像的数据集，大概有22,000个类别。（对数据集有一个基本介绍，保证权威性，说明没有造假）图像都是从网上收集，并使用Amazon-Mechanical Turk群智工具人工标记（人工智能，人工越多越智能，找不到工作就去打标签，打标签的特点就是不费脑子，一坐坐一天。ImageNet是李飞飞<a href="https://baike.baidu.com/item/%E6%9D%8E%E9%A3%9E%E9%A3%9E/7448630">放个连接</a>。从2010年起，作为Pascal视觉对象挑战赛的一部分，这是每年举办一次的名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。 ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。<br>ILSVRC-2010是ILSVRC中的唯一可以使用测试集标签的版本，因此这也正是我们进行大部分实验的版本。由于我们也在ILSVRC-2012比赛中引入了我们的模型，因此在第6部分中，我们也会给出此版本数据集的结果，尽管这个版本的测试集标签不可用。在ImageNet上，习惯上使用两种错误率：top-1和top-5，其中top-5错误率是正确标签不在被模型认为最可能的五个标签之中的测试图像的百分率。（原来错误率来源于这个比赛）<br>ImageNet由可变分辨率的图像组成，而我们的系统需要固定的输入尺寸。因此，我们将图像下采样到256×256的固定分辨率。给定一个矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心的256×256的图片。除了将每个像素中减去训练集的像素均值之外，我们没有以任何其他方式对图像进行预处理。所以我们在像素的（中心）原始RGB值上训练了我们的网络。</p></li><li><p>图（前文放的模型结构图）概括了我们所提出网络的结构。它包含八个学习层——五个卷积层和三个全连接层。下面，我们将描述一些所提出网络框架中新颖或不寻常的地方。 3.1-3.4节按照我们对它们重要性的估计进行排序，其中最重要的是第一个。<br>RelU：对一个神经元模型的输出的常规套路是，给他接上一个激活函数：（tanh（x）的公式，）就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数（ReLU的公式f(x)&#x3D;max(0,x)注：因为ReLU的公式比较简单所以这里放一下)如慢得多。根据Nair和Hinton的说法[20]（这篇论文相当于为ReLU背书了，就相当于我的理论依据），我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。这个结果从图1中可以看出来（实验证明来了，填坑，使用了实验证明），该图展示了对于一个特定的四层CNN，CIFAR-10数据集训练中的误差率达到25%所需要的迭代次数。从这张图的结果可以看出，如果我们使用传统的饱和神经元模型来训练CNN，那么我们将无法为这项工作训练如此大型的神经网络。我们并不是第一个考虑在CNN中替换掉传统神经元模型的(继续理论证明，巨大的论文阅读量，)。例如，Jarrett等人[11]声称，非线性函数在他们的对比度归一化问题上，再接上局部均值池化单元，在Caltech-101数据集上表现的非常好。然而，在这个数据集中，主要担心的还是防止过拟合，所以他们观察到的效果与我们在使用ReLU时观察到的训练集的加速能力还是不一样。加快训练速度对大型数据集上训练的大型模型的性能有很大的影响。<br>在多个GPU上训练：单个GTX 580 GPU只有3GB内存（当时GPU的内存确实小，不过也挺厉害了。有时候真觉得自己跟不上时代了，对时间没有一点感觉，12年24年对我有什么区别？），这限制了可以在其上训练的网络的最大尺寸。事实证明，120万个训练样本足以训练那些因规模太大而不适合使用一个GPU训练的网络。因此，我们将网络分布在两个GPU上。目前的GPU很适合于跨GPU并行化操作，因为它们能够直接读写对方的内存，而无需通过主机内存。我们采用的并行化方案基本上将半个内核（或神经元）放在各个GPU上，（有种左右脑的感觉）——另外还有一个技巧：GPU只在某些层间进行通信。这意味着，例如，第3层的内核从第2层的所有内核映射（kernel maps）中获取输入。然而，第4层中的内核又仅从位于同一GPU上的第3层中的那些内核映射获取输入。选择连接模式对于交叉验证是一个不小的问题，但这使得我们能够精确调整通信量，直到它的计算量的达到可接受的程度。由此产生的架构有点类似于Cire¸san等人使用的“柱状”CNN[5]，除了我们的每列不是独立的之外（见图2）。与一个GPU上训练的每个卷积层只有一半的内核数量的网络相比，该方案分别将我们的top-1和top-5错误率分别降低了1.7％和1.2％。双GPU网络的训练时间比单GPU网络更少。<br>局部响应归一化：ReLU具有理想的属性，它们不需要对输入进行归一化来防止它们饱和。如果至少有一些训练实例为ReLU产生了正的输入，那么这个神经元就会学习。然而，我们还是发现下面的这种归一化方法有助于泛化。设aix,y表示第i个内核计算(x,y)位置的ReLU非线性单元的输出，而响应归一化（Local Response Normalization）的输出值定义为bix,y其中，（公式）求和部分公式中的 n表示同一个位置下与该位置相邻的内核映射的数量，而N表示这一层所有的内核数（即通道数）。内核映射的顺序当然是任意的，并且在训练之前就已经定好了。这种响应归一化实现了一种模仿真实神经元的横向抑制，从而在使用不同内核计算的神经元输出之间产生较大的竞争。常数k都是超参数（hyper-parameters），它们的值都由验证集决定。我们取 k&#x3D;2。我们在某些层的应用ReLU后再使用这种归一化方法（参见第3.5节）。这个方案与Jarrett等人[11]的局部对比归一化方案有些相似之处，但我们的被更准确地称为“亮度归一化”，因为我们没有减去均值。响应归一化将我们的top-1和top-5的错误率分别降低了1.4％和1.2％。我们还验证了这种方案在CIFAR-10数据集上的有效性：没有进行归一化的四层CNN实现了13％的测试错误率，而进行了归一化的则为11％。<br>层叠池化：CNN中的池化层汇集了相同内核映射中相邻神经元组的输出。在传统方法中，相邻池化单元之间互不重叠（例如[17,11,4]）。更准确地说，一个池化层可以被认为是由一些间隔为s个像素的池化单元组成的网格，每个都表示了一个以池化单元的位置为中心的大小为z×z的邻域。如果我们令s &#x3D; z，我们就可以得到CNN中常用的传统的局部池化。<br>整体结构：现在我们已经准备好描述CNN的整体架构了。如图2所示，这个网络包含了八层权重;前五个是卷积层，其余三个为全连接层。最后的全连接层的输出被送到1000维的softmax函数，其产生1000个类的预测。我们的网络最大化多项逻辑回归目标，这相当于在预测的分布下最大化训练样本中正确标签对数概率的平均值。第二，第四和第五个卷积层的内核仅与上一层存放在同一GPU上的内核映射相连（见图2）。第三个卷积层的内核连接到第二层中的所有内核映射。全连接层中的神经元连接到前一层中的所有神经元。响应归一化层紧接着第一个和第二个卷积层。 在3.4节中介绍的最大池化层，后面连接响应归一化层以及第五个卷积层。将ReLU应用于每个卷积层和全连接层的输出。第一个卷积层的输入为224×224×3的图像，对其使用96个大小为11×11×3、步长为4（步长表示内核映射中相邻神经元感受野中心之间的距离）的内核来处理输入图像。第二个卷积层将第一个卷积层的输出（响应归一化以及池化）作为输入，并使用256个内核处理图像，每个内核大小为5×5×48。第三个、第四个和第五个卷积层彼此连接而中间没有任何池化或归一化层。第三个卷积层有384个内核，每个的大小为3×3×256，其输入为第二个卷积层的输出。第四个卷积层有384个内核，每个内核大小为3×3×192。第五个卷积层有256个内核，每个内核大小为3×3×192。全连接层各有4096个神经元。</p></li><li><p>减少过拟合。我们的神经网络架构拥有6000万个参数。尽管ILSVRC的1000个类别使得每个训练样本从图像到标签的映射被限制在了10 bit之内，但这不足以保证训练这么多参数而不出现过拟合。下面，我们将介绍对付过度拟合的两个方法。<br>数据增强： 减小过拟合的最简单且最常用的方法就是，使用标签保留转换（label-preserving transformations，例如[25,4,5]），人为地放大数据集。我们采用两种不同形式的数据增强方法，它们都允许通过很少的计算就能从原始图像中生成转换图像，所以转换后的图像不需要存储在硬盘上。在我们实现过程中，转换后的图像是使用CPU上的Python代码生成的，在生成这些转换图像的同时，GPU还在训练上一批图像数据。所以这些数据增强方案实际上是很高效的。<br>数据增强的第一种形式包括平移图像和水平映射。我们通过从256×256图像中随机提取224×224的图像块（及其水平映射）并在这些提取的图像块上训练我们的网络来做到这一点。这使我们的训练集的规模增加了2048倍，尽管由此产生的训练样本当然还是高度相互依赖的。如果没有这个方案，我们的网络就可能会遭受大量的的过拟合，可能会迫使我们不得不使用更小的网络。在测试时，网络通过提取5个224×224的图像块（四个角块和中心块）以及它们的水平映射（因此总共包括10个块）来进行预测，并求网络的softmax层的上的十个预测结果的均值。第二种形式的数据增强包括改变训练图像中RGB通道的灰度。具体而言，我们在整个ImageNet训练集的图像的RGB像素值上使用PCA。对于每个训练图像，我们添加多个通过PCA找到的主成分，大小与相应的特征值成比例，乘以一个随机值，该随机值属于均值为0、标准差为0.1的高斯分布。因此，对于每个图像的RGB像素有：Ixy&#x3D;[IRxy IGxy IBxy]T（自己去看论文中的公式），我们加入如下的值：[p1 p2 p3] [α1λ1 α2λ2 α3λ3]T其中， pi和 λi分别是3x3的RGB协方差矩阵的第 i个特征向量和第i个的特征值，而 αi是前面所说的随机值。对于一张特定图像中的所有像素，每个 αi只会被抽取一次，知道这张图片再次用于训练时，才会重新提取随机变量。这个方案近似地捕捉原始图像的一些重要属性，对象的身份不受光照的强度和颜色变化影响。这个方案将top-1错误率降低了1％以上。<br>Dropout： 结合许多不同模型的预测结果是减少测试错误率的一种非常成功的方法[1,3]，但对于已经花费数天时间训练的大型神经网络来说，它似乎成本太高了。然而，有一种非常有效的模型组合方法，在训练期间，只需要消耗1&#x2F;2的参数。这个新发现的技术叫做“Dropout”[10]，它会以50%的概率将隐含层的神经元输出置为0。以这种方法被置0的神经元不参与网络的前馈和反向传播。因此，每次给网络提供了输入后，神经网络都会采用一个不同的结构，但是这些结构都共享权重。这种技术减少了神经元的复杂适应性，因为神经元无法依赖于其他特定的神经元而存在。因此，它被迫学习更强大更鲁棒的功能，使得这些神经元可以与其他神经元的许多不同的随机子集结合使用。在测试时，我们试着使用了所有的神经元，并将它们的输出乘以0.5。这与采用大量dropout的网络产生的预测结果分布的几何均值近似。我们在图2中的前两个全连接层上使用了dropout。没有dropout，我们的网络会出现严重的过拟合。Dropout大概会使达到收敛的迭代次数翻倍。</p></li><li><p>训练细节。我们使用随机梯度下降法来训练我们的模型，每个batch有128个样本，动量（momentum）为0.9，权重衰减（weight decay）为0.0005。我们发现这种较小的权重衰减对于模型的训练很重要。换句话说，权重衰减在这里不仅仅是一个正则化方法：它减少了模型的训练误差。权重ω的更新法则是：（自己看公式去）<br>我们使用标准差为0.01、均值为0的高斯分布来初始化各层的权重。我们使用常数1来初始化了网络中的第二个、第四个和第五个卷积层以及全连接层中的隐含层中的所有偏置参数。这种初始化权重的方法通过向ReLU提供了正的输入，来加速前期的训练。我们使用常数0来初始化剩余层中的偏置参数。<br>我们对所有层都使用相同的学习率，在训练过程中又手动进行了调整。我们遵循的启发式方法是：以当前的学习速率训练，验证集上的错误率停止降低时，将学习速率除以10.学习率初始时设为0.01，并且在终止前减少3次。我们使用120万张图像的训练集对网络进行了大约90次迭代的训练，这在两块NVIDIA GTX 580 3GB GPU上花费了大约5到6天的时间。（这里说明了优化函数，超参数设置。这里挖个坑，什么是超参数？）</p></li><li><p>结果：我们在ILSVRC-2010上取得的结果如表1所示。我们的网络的top-1和top-5测试集错误率分别为37.5％和17.0％。在ILSVRC-2010比赛期间取得的最佳成绩是47.1％和28.2％，其方法是对六种不同的稀疏编码模型所产生的预测结果求平均[2]。此后公布的最佳结果为45.7％、25.7％，其方法是对两种经过密集采样的特征[24]计算出来的Fisher向量（FV）训练的两个分类器取平均值。我们的网络实现了37.5％和17.0％的前1和前5个测试集错误率5。在ILSVRC-2010比赛期间取得的最佳成绩是47.1％和28.2％，其中一种方法是对六种针对不同特征进行训练的稀疏编码模型所产生的预测进行平均[2]，此后最佳公布结果为45.7％， 25.7％，其中一种方法是：对两个在不同取样密度的Fisher向量上训练的分类器取平均。（纵向对比了，相同数据集，不同模型。）<br>我们还在ILSVRC-2012竞赛中使用了我们的模型，并在表2中给出了我们的结果。由于ILSVRC-2012测试集标签未公开，因此我们无法给出我们测试过的所有模型在测试集上的错误率。在本节的其余部分中，我们将验证集和测试集的错误率互换，因为根据我们的经验，它们之间的差值不超过0.1％（见表2）。本文描述的CNN的top-5错误率达到了18.2％。对五个相似CNN的预测结果计算均值，得到的错误率为16.4％。单独一个CNN，在最后一个池化层之后，额外添加第六个卷积层，对整个ImageNet Fall 2011 release(15M images, 22K categories)进行分类，然后在ILSVRC-2012上“微调”（fine-tuning）网络，得到的错误率为16.6％。对整个ImageNet Fall 2011版本的数据集下预训练的两个CNN，求他们输出的预测值与前面提到的5个不同的CNN输出的预测值的均值，得到的错误率为15.3％。比赛的第二名达到了26.2％的top-5错误率，他们的方法是：对几个在特征取样密度不同的Fisher向量上训练的分类器的预测结果取平均的方法[7]。<br>最后，我们还在ImageNet Fall 2009版本的数据集上提交了错误率，总共有10,184个类别和890万张图像。在这个数据集中，我们遵循文献中的使用一半图像用于训练，一半图像用于测试的惯例。由于没有建立测试集，所以我们的拆分方法有必要与先前作者使用的拆分方法不同，但这并不会对结果产生显著的影响。我们在这个数据集上的top-1和top-5错误率分别是67.4％和40.9％，是通过前面描述的网络获得的，但是在最后的池化层上还有额外的第6个卷积层。该数据集此前公布的最佳结果是78.1％和60.9％[19]。<br>定性评估：图3（自己看论文去）显示了由网络的两个数据连接层学习得到的卷积内核。（网络结构还可以画出来，也是挺有意思的。）该网络已经学习到许多频率和方向提取的内核，以及各种色块。请注意两个GPU所展现的不同特性，这也是3.5节中介绍的限制互连的结果。GPU1上的内核在很大程度上与颜色无关，然而GPU2上的内核在很大程度上都于颜色有关。这种特异性在每次迭代期间都会发生，并且独立于任何特定的随机权重初始化过程（以GPU的重新编号为模）。<br>在图4（自己看论文去，图4展示了一堆实验结果）的左边，我们通过计算8张测试图像的top-5预测来定性评估网络的训练结果。请注意，即使是偏离中心的物体，如左上角的螨虫，也可以被网络识别出来。大多数top-5的标签都显得比较合理。例如，只有其他类型的猫才被认为是豹子的可能标签。在某些情况下（栅栏、樱桃），照片的关注点存在模糊性，不知道到底该关注哪个。另一个研究可视化的网络的方法是，考虑由最后一个4096维隐含层中的图像的特征的激活函数输出值。如果两幅图像产生有的欧氏距离，我们可以认为高层次的神经网络认为它们是相似的。图4显示了测试集中的5个图像和来袭训练集的6个图像，这些图像根据这种度量方法来比较它们中的哪一个与其最相似。请注意，在像素层次上，待检测的训练图像通常不会与第一列中的查询图像有较小的L2距离。例如，检索到的狗和大象有各种不同的姿势。我们在补充材料中提供了更多测试图像的结果。通过使用欧式距离来计算两个4096维实值向量的相似性，效率不高，但是通过训练自编码器可以将这些向量压缩为较短的二进制码，能够使其更高效。与应用自编码器到原始像素[14]相比，这应该是更好的图像检索方法。它不使用图像标签，因此更秦翔宇检索具有相似图案边缘的图像，不管它们的图像语义是否相似。</p></li><li><p>讨论：我们的研究结果表明，一个大的深层卷积神经网络能够在纯粹使用监督学习（这里有个概念，监督学习和无监督学习，半监督学习。挖个坑）的情况下，在极具挑战性的数据集上实现破纪录的结果。值得注意的是，如果移除任何一个卷积层，网络的性能就会下降。例如，删除任何中间层的结果会导致网络性能的top-1错误率下降2%。因此网络的深度对于实现我们的结果真的很重要。（基本上后面的深度学习的思路就是堆网络结构）<br>为了简化我们的实验，我们没有使用任何无监督的预训练方法，尽管这样可能会有所帮助，特别是如果我们获得了足够的计算能力来显著地增加网络的大小而不会相应地增加已标记数据的数量。到目前为止，我们的结果已经获得了足够的进步，因为我们已经使网络更大，并且训练了更长时间。但我们仍然有很大的空间去优化网络，使之能够像人类的视觉系统一样感知。最后，我们希望对视频序列使用非常大的深度卷积神经网路，其中时间结构提供了非常有用的信息，这些信息往往在静态图像中丢失了，或者说不太明显。</p></li></ol><h1 id="个人感觉"><a href="#个人感觉" class="headerlink" title="个人感觉"></a>个人感觉</h1><p>论文很短，内容很多。展现在论文中的，没有展现在论文中的。学习的过程中既有鲜花也有荆棘，这是客观的条件，我承认有人会有论语中的天生的智慧，看待世界的方式就不一样，这是现实，但是那又有什么？不管怎么样先把下面的坑填了。还有就是博客中难免有错别字，记得更改。+</p><h2 id="问题1，卷积是什么？作用什么？"><a href="#问题1，卷积是什么？作用什么？" class="headerlink" title="问题1，卷积是什么？作用什么？"></a>问题1，卷积是什么？作用什么？</h2><h2 id="问题2，池化是什么？作用是什么？"><a href="#问题2，池化是什么？作用是什么？" class="headerlink" title="问题2，池化是什么？作用是什么？"></a>问题2，池化是什么？作用是什么？</h2><h2 id="问题3，全连接是什么？作用是什么？"><a href="#问题3，全连接是什么？作用是什么？" class="headerlink" title="问题3，全连接是什么？作用是什么？"></a>问题3，全连接是什么？作用是什么？</h2><h2 id="问题4，AlexNet论文使用的loss函数是什么？"><a href="#问题4，AlexNet论文使用的loss函数是什么？" class="headerlink" title="问题4，AlexNet论文使用的loss函数是什么？"></a>问题4，AlexNet论文使用的loss函数是什么？</h2><h2 id="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"><a href="#问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？" class="headerlink" title="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"></a>问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？</h2><h2 id="问题6，AlexNet论文中使用的评价指标是什么？"><a href="#问题6，AlexNet论文中使用的评价指标是什么？" class="headerlink" title="问题6，AlexNet论文中使用的评价指标是什么？"></a>问题6，AlexNet论文中使用的评价指标是什么？</h2><h2 id="问题7，AlexNet中的创新点是什么？"><a href="#问题7，AlexNet中的创新点是什么？" class="headerlink" title="问题7，AlexNet中的创新点是什么？"></a>问题7，AlexNet中的创新点是什么？</h2><ol><li>ReLU激活函数的引入，采样非线性单元（ReLU）的深度卷积神经网络训练时间要比tanh单元要快几倍。而时间开销是进行模型训练过程中的很重要的因数。同时ReLU有效的防止了过拟合的现象。</li><li>层叠池化操作，以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize&#x3D;2×2，stride&#x3D;2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize &gt; stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。</li><li>Dropout操作， Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。</li><li>网络层数更深，与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。当然，这种思考的重要节点出现在VGG网络（下一篇博文VGG论文中将会讲到）。</li></ol><h2 id="问题8，优化函数的具体实现是什么？"><a href="#问题8，优化函数的具体实现是什么？" class="headerlink" title="问题8，优化函数的具体实现是什么？"></a>问题8，优化函数的具体实现是什么？</h2><h2 id="问题9，关于卷积后特征图应该怎么计算？"><a href="#问题9，关于卷积后特征图应该怎么计算？" class="headerlink" title="问题9，关于卷积后特征图应该怎么计算？"></a>问题9，关于卷积后特征图应该怎么计算？</h2><h2 id="问题10，什么是过拟合合和欠拟合？"><a href="#问题10，什么是过拟合合和欠拟合？" class="headerlink" title="问题10，什么是过拟合合和欠拟合？"></a>问题10，什么是过拟合合和欠拟合？</h2><h2 id="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"><a href="#问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？" class="headerlink" title="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"></a>问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？</h2><h2 id="问题12，-神经元数量和参数量的计算方法是什么？"><a href="#问题12，-神经元数量和参数量的计算方法是什么？" class="headerlink" title="问题12， 神经元数量和参数量的计算方法是什么？"></a>问题12， 神经元数量和参数量的计算方法是什么？</h2><h2 id="问题13，-softMax的机制是怎么样的？"><a href="#问题13，-softMax的机制是怎么样的？" class="headerlink" title="问题13， softMax的机制是怎么样的？"></a>问题13， softMax的机制是怎么样的？</h2><h2 id="问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？"><a href="#问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？" class="headerlink" title="问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？"></a>问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？</h2><h2 id="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？"><a href="#问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？" class="headerlink" title="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？"></a>问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？</h2><h2 id="问题16，-什么是超参数？"><a href="#问题16，-什么是超参数？" class="headerlink" title="问题16， 什么是超参数？"></a>问题16， 什么是超参数？</h2><h2 id="问题17，-什么是监督学习和无监督学习，半监督学习？"><a href="#问题17，-什么是监督学习和无监督学习，半监督学习？" class="headerlink" title="问题17， 什么是监督学习和无监督学习，半监督学习？"></a>问题17， 什么是监督学习和无监督学习，半监督学习？</h2><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。"><a href="#关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。" class="headerlink" title="关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。"></a>关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch基础——Numpy</title>
    <link href="/2024/04/21/deeplearnbook2/"/>
    <url>/2024/04/21/deeplearnbook2/</url>
    
    <content type="html"><![CDATA[<p>第一部分</p><h1 id="Numpy基础"><a href="#Numpy基础" class="headerlink" title="Numpy基础"></a>Numpy基础</h1><h2 id="什么是Numpy？"><a href="#什么是Numpy？" class="headerlink" title="什么是Numpy？"></a>什么是Numpy？</h2><p>NumPy（Numerical Python的简称）是一个开源的Python库，用于进行科学计算。它提供了一个强大的N维数组对象，以及大量的函数用于处理这些数组。NumPy的主要功能包括：</p><ol><li>多维数组对象：NumPy的核心功能是其多维数组对象（ndarray）。这是一个快速、灵活的容器，可以容纳大量同类型数据，使你能够对这些数据进行数学运算。</li><li>广播功能：NumPy提供了广播功能，这是一种强大的机制，允许NumPy在执行算术运算时处理不同形状的数组。</li><li>数学函数：NumPy提供了大量的数学函数，可以对数组中的元素进行各种数学运算，如加、减、乘、除、平方根等。</li><li>线性代数：NumPy包含了线性代数函数库，可以进行矩阵乘法、求逆、解线性方程，傅里叶变换等操作。</li><li>随机数生成：NumPy提供了生成各种随机数的功能，如均匀分布、正态分布等。</li><li>更方便的读取\写入磁盘上的阵列数据和操作存储映像文件的工具。<br>NumPy是许多科学计算库（如Pandas、Matplotlib、SciPy等）的基础库，也是机器学习和数据科学中常用的库。在深度学习中图像、声音、文本等输入数据最终都要转换为数组或矩阵，NumPy的多维数组可以用来表示向量、矩阵和张量，这些都是深度学习算法的基本构成元素。</li></ol><h2 id="为什么要使用Numpy"><a href="#为什么要使用Numpy" class="headerlink" title="为什么要使用Numpy"></a>为什么要使用Numpy</h2><p>实际上python包含多个数据类型，数值类型（int，float，complex）、布尔类型（bool）、字符串（str）、列表（list）、元组（tuple）、字典（dict，{‘name’:’chenli’,’age’: 114514}）、集合（set{}）。<br>python包含这么多的数据类型，其中的列表（list）和数组（array）为什么不能用？原因是对于大数据来说，这些结构有很多不足。比如由于列表的元素可以是任何对象，因此列表中所保存的是对象的指针。例如为了存储[1,2,3],就需要3个指针和3个整数对象，这样对于数值运算来讲，严重浪费了计算机中的内存和CPU或GPU的算力。对于array，它可以直接保存数值，但是它不支持多维，并且对应操作的函数也不多，因此也不适合。</p><h2 id="怎么使用Numpy？"><a href="#怎么使用Numpy？" class="headerlink" title="怎么使用Numpy？"></a>怎么使用Numpy？</h2><p>第一步使用 <code>pip install numpy </code>来下载numpy库。</p><ol><li><p>生成Numpy数组。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>lst1= <span class="hljs-string">[[3.14, 2.17,0,1,2],[1,2,3,4,5]]</span> # 一个二维列表<br>nd1 = np.array(lst1)# 使用np.array()将列表数据类型转换成np数据类型<br><span class="hljs-built_in">print</span>(nd1) # 显示的结果为一个二维列表<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(nd1)) # 显示结果为&lt;class <span class="hljs-string">&#x27;numpy.ndarray&#x27;</span>&gt;    <br></code></pre></td></tr></table></figure></li><li><p>numpy中的random模块生成数组</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">nd1</span> = np.random.random([<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]) # 产生一个[<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]ndarray，范围为<span class="hljs-number">0</span>-<span class="hljs-number">1</span>之间的随机数。<br><span class="hljs-attribute">np</span>.random.seed(<span class="hljs-number">123</span>) # 为了每次生成同一份数据，可以指定一个随机种子，而后生成的随机数据是固定的。<br><span class="hljs-attribute">nd2</span> = np.random.random(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 产生一个[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]的ndarray<br><span class="hljs-attribute">np</span>.random.shuffle(nd2) # 随机打乱nd2中的数据。<br><span class="hljs-attribute">nd3</span> = np.random.uniform(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 生成均匀分布的随机数<br><span class="hljs-attribute">nd4</span> = np.random.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 生成标准正态分布的随机数 <br><span class="hljs-attribute">nd5</span> = np.random.randint(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 生成随机的整数<br></code></pre></td></tr></table></figure></li><li><p>numpy中创建特定形状的多维数组</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">nd1</span> = np.zeros([<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]) # 生成全是<span class="hljs-number">0</span>的<span class="hljs-number">3</span>x3的矩阵，np.zeros()生成全是<span class="hljs-number">0</span>的ndarray<br><span class="hljs-attribute">nd2</span> = np.zeros_like(nd1) # 以ndrr相同维度创建元素为<span class="hljs-number">0</span>的数组<br><span class="hljs-attribute">nd3</span> = np.ones_like(nd1) # 以nd1维度创建元素全是<span class="hljs-number">1</span>的数组<br><span class="hljs-attribute">nd4</span> = np.empty_like(nd1) # 以nd1维度创建一个空数组<br><span class="hljs-attribute">nd5</span> = np.eye(<span class="hljs-number">5</span>) # 该函数用于创建一个<span class="hljs-number">5</span>x5的矩阵，对角线为<span class="hljs-number">1</span>，其余为<span class="hljs-number">0</span><br><span class="hljs-attribute">nd6</span> = np.full((<span class="hljs-number">3</span>,<span class="hljs-number">5</span>),<span class="hljs-number">666</span>) # 创建<span class="hljs-number">3</span>x5的元素全为<span class="hljs-number">666</span>的数组，<span class="hljs-number">666</span>为指定值<br></code></pre></td></tr></table></figure></li><li><p>保存使用numpy生成的数据</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import numpy <span class="hljs-keyword">as</span> np<br>nd = np.<span class="hljs-built_in">random</span>.<span class="hljs-built_in">random</span>([<span class="hljs-number">5</span>,<span class="hljs-number">5</span>])<br>np.savetxt(X=nd,fname=<span class="hljs-string">&#x27;test.txt&#x27;</span>) <span class="hljs-comment"># 保存nd，文件名称为test.txt</span><br>nd2 = np.loadtxt(<span class="hljs-string">&#x27;test.txt&#x27;</span>) <span class="hljs-comment"># 从test.txt中加载数据</span><br></code></pre></td></tr></table></figure></li><li><p>利用arange、linspace函数来生成数组<br>arange是numpy模块中的函数，格式为：<code>arange([start],stop[,step],dtype=None)</code><br>其中start与stop用来限定范围，step是步长默认为1，step可以为小数。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">nd</span> = np.arange(<span class="hljs-number">10</span>) # np中的内容为[<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span> <span class="hljs-number">8</span> <span class="hljs-number">9</span>]<br><span class="hljs-attribute">nd1</span> = np.arange(<span class="hljs-number">1</span>，<span class="hljs-number">4</span>，<span class="hljs-number">0</span>.<span class="hljs-number">5</span>) # np中的内容为[<span class="hljs-number">1</span>.<span class="hljs-number">0</span> <span class="hljs-number">1</span>.<span class="hljs-number">5</span> <span class="hljs-number">2</span>.<span class="hljs-number">0</span> <span class="hljs-number">2</span>.<span class="hljs-number">5</span> <span class="hljs-number">3</span>.<span class="hljs-number">0</span> <span class="hljs-number">3</span>.<span class="hljs-number">5</span> <span class="hljs-number">4</span>.<span class="hljs-number">0</span>]<br><span class="hljs-attribute">nd2</span> = np.aramge(<span class="hljs-number">9</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>) # nd2中的内容为[<span class="hljs-number">9</span> <span class="hljs-number">8</span> <span class="hljs-number">7</span> <span class="hljs-number">6</span> <span class="hljs-number">5</span> <span class="hljs-number">4</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>linspace也是numpy中常用的函数，格式为：<code>np.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None)</code><br>linspace可以根据输入数据的指定范围以及等份数量，自动生成线性分量。endpoint（包含终点）默认为True。等分量num默认为50，如果将retstep设置为True，则会返回一个带步长的ndarray</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">print</span>(np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>)) # 产生<span class="hljs-number">10</span>个数，间隔为<span class="hljs-number">0</span>.<span class="hljs-number">111111</span> <br></code></pre></td></tr></table></figure></li><li><p>获取数据。<br>数据生成后，如何读取数据，常用数据的的方法。（类似于python数据切片的方法）</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">np</span>.random.seed(<span class="hljs-number">2024</span>)<br><span class="hljs-attribute">nd</span> = np.random.random([<span class="hljs-number">10</span>]) # 产生一维的<span class="hljs-number">10</span>个数据点<br><span class="hljs-attribute">nd</span>[<span class="hljs-number">3</span>]  # 获取指定位置的数据，获取第<span class="hljs-number">4</span>个元素<br><span class="hljs-attribute">nd</span>[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>] # 截取一段数据<br><span class="hljs-attribute">nd</span>[<span class="hljs-number">1</span>:<span class="hljs-number">6</span>:<span class="hljs-number">2</span>] # 获取固定间隔的数据<br><span class="hljs-attribute">nd</span>[::-<span class="hljs-number">2</span>] # 倒序取数<br><span class="hljs-attribute">nd2</span> = np.arange(<span class="hljs-number">25</span>).reshape([<span class="hljs-number">5</span>,<span class="hljs-number">5</span>]) # 产生一个<span class="hljs-number">25</span>个数据点的一维数据，而后通过reshape进行形状重整为[<span class="hljs-number">5</span>,<span class="hljs-number">5</span>]<br><span class="hljs-attribute">nd2</span>[:,<span class="hljs-number">1</span>:<span class="hljs-number">3</span>] # 截取多维数组中，指定的列，读取第<span class="hljs-number">2</span>，<span class="hljs-number">3</span>列。<br></code></pre></td></tr></table></figure></li><li><p>Numpy的算术运算<br>在机器学习和深度学习中，涉及大量的数组或矩阵运算，这里介绍两种常用的运算。一种是对应元素相乘，又称逐元乘法（Element-Wisr Product）运算符为np.multiply()或*。一种是点积或内积元素，运算符为np.dot()</p></li></ol><p>逐元乘法</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">a</span> = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[-<span class="hljs-number">1</span>,<span class="hljs-number">4</span>]])<br><span class="hljs-attribute">b</span> = np.array([[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])<br><span class="hljs-attribute">a</span>*b<br><br><span class="hljs-comment"># 结果为array([2,0],[-3,16]) , 运算过程为 1*2 = 2， 2*0 = 0 ， -1*3 = -3， 4*4 =16</span><br><span class="hljs-comment"># 另外一种写法，np.multiply(a,b) 结果和上面一样</span><br><span class="hljs-comment"># Numpy数组不仅可以和数组进行对应元素相乘，还可以和单一数值（或称为标量）来进行运算</span><br></code></pre></td></tr></table></figure><p>点积运算（Dot Product）又可以称为内积，在Numpy用np.dot表示</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs lua">X1 = np.array(<span class="hljs-string">[[1,2],[3,4]]</span>)<br>X2 = np.array(<span class="hljs-string">[[5,6,7],[8,9,10]]</span>)<br>X3 = np.dot(X1,X2)<br><span class="hljs-built_in">print</span>(X3)<br># 结果为<span class="hljs-string">[[21,24,27],[47,54,61]]</span><br># 运算方法，<span class="hljs-number">1</span>*<span class="hljs-number">5</span>+<span class="hljs-number">2</span>*<span class="hljs-number">8</span> = <span class="hljs-number">21</span>, <span class="hljs-number">1</span>*<span class="hljs-number">6</span> + <span class="hljs-number">2</span>*<span class="hljs-number">9</span>=<span class="hljs-number">24</span>, <span class="hljs-number">1</span>*<span class="hljs-number">7</span>+<span class="hljs-number">2</span>*<span class="hljs-number">7</span>=<span class="hljs-number">27</span> <br># <span class="hljs-number">3</span>*<span class="hljs-number">5</span>+<span class="hljs-number">4</span>*<span class="hljs-number">8</span> = <span class="hljs-number">47</span>,<span class="hljs-number">3</span>*<span class="hljs-number">6</span>+<span class="hljs-number">4</span>*<span class="hljs-number">9</span>=<span class="hljs-number">54</span>, <span class="hljs-number">3</span>*<span class="hljs-number">7</span>+<span class="hljs-number">4</span>*<span class="hljs-number">10</span>=<span class="hljs-number">61</span><br></code></pre></td></tr></table></figure><ol start="8"><li>数组变形<br>在机器学习和深度学习任务中，通常需要将处理好的数据以模型能够接收的格式进行输入，然后进行一系列运算，最终返回一个处理结果。然而由于不同模型所接受的输入格式不一样，往往需要先对其进行一系列变形和运算，从而将数据处理成符合模型要求的格式。<br>更改数组的形状。方法有<figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs tap"><span class="hljs-comment"># arr.reshape(),将向量arr维度进行改变，不修改向量本身</span><br><span class="hljs-comment"># arr.resize(), 重新将向量arr维度进行改变，修改向量本身</span><br><span class="hljs-comment"># arr.T ,对向量进行转置</span><br><span class="hljs-comment"># arr.ravel ，对向量arr进行展平，即将多维数组变成1维数组，不会产生原数组的副本</span><br><span class="hljs-comment"># arr.flatten，对向量arr进行展平，即将多维数值变成1维数组，返回原数组的副本</span><br><span class="hljs-comment"># arr.squeeze(), 只能对维度为1的进行降维。对多维数组使用时不会进行任何报错，但是不会产生任何影响</span><br><span class="hljs-comment"># arr.transpose，对高维矩阵进行轴转换</span><br>import numpy as np<br><br>arr = np.arange(10)<br>print(arr.reshape(2,5))<br><span class="hljs-comment"># 指定维度时可以只指定行数和列数，其他用-1代替</span><br>print(arr.reshape([5,-1]))<br>print(arr)<br>print(arr.reshape([-1,5]))<br><br><span class="hljs-comment">#结果[[0 1 2 3 4]</span><br> [5<span class="hljs-number"> 6 </span>7<span class="hljs-number"> 8 </span>9]]<br>[[0 1]<br> [2 3]<br> [4 5]<br> [6 7]<br> [8 9]]<br>arr [0<span class="hljs-number"> 1 </span>2<span class="hljs-number"> 3 </span>4<span class="hljs-number"> 5 </span>6<span class="hljs-number"> 7 </span>8 9] <span class="hljs-comment"># 没有对arr本身进行修改</span><br>[[0<span class="hljs-number"> 1 </span>2<span class="hljs-number"> 3 </span>4]<br> [5<span class="hljs-number"> 6 </span>7<span class="hljs-number"> 8 </span>9]]<br></code></pre></td></tr></table></figure></li></ol><p>使用resize来修改向量维度，</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>arr = np.arange(<span class="hljs-number">10</span>)<br><span class="hljs-built_in">print</span>(arr)<br>arr.resize(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>) <span class="hljs-comment"># 直接对向量修改，不像arr.reshape没有对向量进行修改。</span><br><span class="hljs-built_in">print</span>(arr)<br></code></pre></td></tr></table></figure><p>向量转置 ,T</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><br>arr = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">12</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)#注这里重新进行赋值了，reshape保存了下来<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr.T)</span></span><br></code></pre></td></tr></table></figure><p>向量展平.ravel</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br>arr = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">6</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">2</span>,-<span class="hljs-number">1</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;按照列优先，展平&#x27;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr.ravel(<span class="hljs-string">&#x27;F&#x27;</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;按照行优先，展平&#x27;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(arr.ravel()</span></span>)<br></code></pre></td></tr></table></figure><p>矩阵转换为向量，这种需求经常出现在卷积神经网络与全连接之间，用于数据的展平,flatten</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs maxima">import numpy as <span class="hljs-built_in">np</span><br>a = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">floor</span>(<span class="hljs-number">10</span>*<span class="hljs-built_in">np</span>.<span class="hljs-built_in">random</span>.<span class="hljs-built_in">random</span>(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(a.<span class="hljs-built_in">flatten</span>()) # 需要注意的是一些方法是直接对向量进行修改，一些没有对向量进行修改，这点需要注意。<br></code></pre></td></tr></table></figure><p>降维操作，我也是经常使用的一个方法，squeeze 主要用来降维，把矩阵中含有1的维度去掉，在pytorch中还有一种与之相反的一种操作，torch.unsqueeze()</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">arr</span> =np.arange(<span class="hljs-number">3</span>).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)<br><span class="hljs-attribute">print</span>(arr.shape) # <span class="hljs-number">3</span>,<span class="hljs-number">1</span><br><span class="hljs-attribute">print</span>(arr.squeeze().shape) # (<span class="hljs-number">3</span>,)<br><span class="hljs-attribute">arr1</span> = np.arange(<span class="hljs-number">6</span>).reshpe(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br><span class="hljs-attribute">print</span>(arr1.shape) # (<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br><span class="hljs-attribute">print</span>(arr1.squeeze().shape) #(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>高维转换，transpose，这个在深度学习中经常使用，比如把RGB转换为GBR</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">arr</span> = np.arange(<span class="hljs-number">24</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">print</span>(arr.shape) # (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>) 注意这里为什么shape不用加括号，因为shape是一个np的属性，而不是方法。在构建一个class时，有属性和方法的区别，这里挖个坑<br><span class="hljs-attribute">print</span>(arr.transpose().shape) # (<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><ol start="9"><li>合并数组<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># <span class="hljs-built_in">np</span>.<span class="hljs-built_in">append</span> ,内存占用大<br># <span class="hljs-built_in">np</span>.concatenate 没有内存占用问题<br># <span class="hljs-built_in">np</span>.stack, 沿着新的轴加入一系列数组<br># <span class="hljs-built_in">np</span>.hstack ，堆栈数组垂直顺序（行）<br># <span class="hljs-built_in">np</span>.vstack ,堆栈数组垂直顺序（列）<br># 对于<span class="hljs-built_in">append</span> 和concatenate ，待合并的数组必须有相同的行数或列数（满足一个即可）<br></code></pre></td></tr></table></figure></li></ol><p>append合并一维数组</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[1,2,3]</span>)<br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[4,5,6]</span>)<br>c = np<span class="hljs-selector-class">.append</span>(<span class="hljs-selector-tag">a</span>,b)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(c)</span></span><br></code></pre></td></tr></table></figure><p>合并多维数组</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>a = np.arange(4).reshape(2,2)<br>b = np.arange(4).reshape(2,2)<br>c = np.append(a,b,<span class="hljs-attribute">axis</span>=0)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;按行合并后的结果&#x27;</span>)<br><span class="hljs-built_in">print</span>(c)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;合并后数据维度&#x27;</span>，c.shape) <br><span class="hljs-comment"># 按例合并</span><br>d = np.append(a,b,<span class="hljs-attribute">axis</span>=1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;按列进行合并&#x27;</span>)<br><span class="hljs-built_in">print</span>(d)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;合并后的数据维度&#x27;</span>,d.shape)<br></code></pre></td></tr></table></figure><p>concatenate沿指定轴连接数组或矩阵</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[1,2]</span>,<span class="hljs-selector-attr">[3,4]</span>])<br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[5,6]</span>)<br>c = np<span class="hljs-selector-class">.concatenate</span>((<span class="hljs-selector-tag">a</span>,b),axis=<span class="hljs-number">0</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(c)</span></span><br>d = np<span class="hljs-selector-class">.concatenate</span>((<span class="hljs-selector-tag">a</span>,<span class="hljs-selector-tag">b</span>.T),axis=<span class="hljs-number">1</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(d)</span></span><br></code></pre></td></tr></table></figure><p>stack 沿着固定轴堆积或矩阵</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.array(<span class="hljs-string">[[1,2],[3,4]]</span>)<br>b = np.array(<span class="hljs-string">[[5,6],[7,8]]</span>)<br><span class="hljs-built_in">print</span>(np.stack((a,b),axis=<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><ol start="11"><li><p>通用函数<br>sqrt ,计算序列化数据的平方根<br>sin,cos 三角函数<br>abs ， 计算序列化数据的绝对值<br>dot， 矩阵运算<br>log,log10,log2， 对数函数<br>exp, 指数函数<br>cumsum,cumproduct , 累计求和，求积<br>sum ,对一个序列化数据进行求和<br>mean ， 计算均值<br>median ，计算中位数<br>std , 计算标准差<br>var ， 计算方差</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import <span class="hljs-selector-tag">time</span> <br>import math<br>import numpy as np<br><br>x = <span class="hljs-selector-attr">[i * 0.001 for i in np.arange(1000000)]</span><br>start = <span class="hljs-selector-tag">time</span><span class="hljs-selector-class">.clock</span>()<br><span class="hljs-keyword">for</span> <span class="hljs-selector-tag">i</span> , t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(x):<br>    x<span class="hljs-selector-attr">[i]</span> = math<span class="hljs-selector-class">.sin</span>(t)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;math.sin:&#x27;</span>,time.clock()</span></span>-start)<br><br>x = <span class="hljs-selector-attr">[i*0.001 for i in np.arange(100000)]</span><br>x = np<span class="hljs-selector-class">.array</span>(x)<br>start =<span class="hljs-selector-tag">time</span><span class="hljs-selector-class">.clock</span>()<br>np<span class="hljs-selector-class">.sin</span>(x)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;numpy.sin:&#x27;</span>,time.clock()</span></span>-start)<br></code></pre></td></tr></table></figure></li><li><p>广播机制<br>Numpy 中的Universal functional 中要求输入的数组shape是一致的，当数组的shape不相等时，则会使用广播机制。但是使用广播机制需要满足一定的规则，否则将出错。<br>1） 让所有输入数组都向其中的shape最长的数组看齐，不足的部分则通过在前面加1补齐。<br>2） 输出数组的shape是输出数组shape的各个轴上的最大值</p></li></ol><ol start="3"><li>如果输入数组的某个轴和输出数组的对于长度相同或者某个轴的长度为1时，这个数组能被用来计算，否则出错<br>4） 当输入数组的某个轴的长度为1时，沿着此轴运算时都用（或复制）此轴上的第一组值。</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br>A  = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">0</span>,<span class="hljs-number">40</span>,<span class="hljs-number">10</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>)<br>B = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">0</span>,<span class="hljs-number">3</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;A矩阵的形状:&#123;&#125;,B矩阵的形状：&#123;&#125;&#x27;</span>.format(A.shape,B.shape)</span></span>)<br>C = A + B<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;C矩阵的形状：&#123;&#125;&#x27;</span>.format(C.shape)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(C)</span></span><br></code></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Numpy是深度学习的入门库，学习是很重要的。这里只是列举了一些主要内容，如果想要了解更多的内容，可以登录Numpy官网（<a href="http://www.numpy.org/%EF%BC%89%E6%9F%A5%E7%9C%8B%E6%9B%B4%E5%A4%9A%E7%9A%84%E5%86%85%E5%AE%B9%E3%80%82">http://www.Numpy.org/）查看更多的内容。</a></p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="向量和数组之间的关系是什么？向量的定义是什么？"><a href="#向量和数组之间的关系是什么？向量的定义是什么？" class="headerlink" title="向量和数组之间的关系是什么？向量的定义是什么？"></a>向量和数组之间的关系是什么？向量的定义是什么？</h3><p>在数学科物理中，向量被定义为具有大小和方向量。例如速度是一个向量，因为它不仅有大小（数独），还有方向（行进的方向）。<br>数组是编程中的一种基本数据结构，用于存储一组有序的元素。这些元素可以是任何类型，如整形、浮点数、字符串等。<br>标量（scalar）是零维只有大小，没有方向的量，如1，2，3<br>向量（Vector）是一维只有大小和方向的量，如（1，2）。（计算方向的公式为：）<br>矩阵（Matrix）是二维的向量，[[1, 2], [2, 3]]<br>张量（Tensor） 按照任意维排列的一堆数字的推广。矩阵不过是三维张量下的一个二维切面。要在三维张量下找到零维张量需要三个维度的坐标来定位。（注：张量可以是多维的）</p><h3 id="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"><a href="#矩阵是什么，作用是什么？如何实现矩阵的加减乘除" class="headerlink" title="矩阵是什么，作用是什么？如何实现矩阵的加减乘除"></a>矩阵是什么，作用是什么？如何实现矩阵的加减乘除</h3><ol><li>矩阵是一个二维数组，由行和列的元素组成。在数学中，矩阵通常用大写字母表示，如 A，B 等，矩阵中的元素通常用小写字母表示，如aij​，表示矩阵 A 的第 i 行第 j 列的元素。</li><li>矩阵可以用来表示线性变换，解决线性方程组，或者表示图形的变换。在数据科学和机器学习中，矩阵通常用于存储和操作大量的数据。</li></ol><h4 id="实现矩阵的加减乘除。"><a href="#实现矩阵的加减乘除。" class="headerlink" title="实现矩阵的加减乘除。"></a>实现矩阵的加减乘除。</h4><p>加法：两个矩阵相加，只有在它们的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相加的结果。例如，如果A &#x3D; aij 和B &#x3D; bij 是同样大小的矩阵，那么它们的和C &#x3D; [ cij ]是矩阵 ,其中cij &#x3D; aij + bij。对应相加<br>减法：矩阵的减法与加法类似，只有在两个矩阵的行数和列数都相等时才是定义的。结果矩阵的每个元素是相应的元素相减的结果。<br>乘法：矩阵的乘法比较复杂。如果A 是一个 m×n 的矩阵，B 是一个n×p 的矩阵，那么它们的乘积 AB 是一个 m×p 的矩阵，其元素由A 的行和 B 的列的对应元素的乘积之和给出。<br>除法：在矩阵中，通常不直接定义除法。但是，我们可以通过乘以逆矩阵来实现类似的效果。如果A是一个可逆的（也就是说，存在一个矩阵 （A-1）使得，A（A-1） &#x3D; （A-1）A &#x3D; I其中 𝐼I 是单位矩阵），那么我们可以定义B&#x2F;A为（BA-1），即是B矩阵除以A矩阵等于B乘以A矩阵的转置。但是，请注意，不是所有的矩阵都是可逆的。 </p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs makefile">import numpy as np<br><br><span class="hljs-comment"># 创建两个矩阵</span><br>A = np.array([[1, 2], [3, 4]])<br>B = np.array([[5, 6], [7, 8]])<br><br><span class="hljs-comment"># 矩阵加法</span><br>C = A + B<br><br><span class="hljs-comment"># 矩阵减法</span><br>D = A - B<br><br><span class="hljs-comment"># 矩阵乘法</span><br>E = np.dot(A, B)<br><br><span class="hljs-comment"># 矩阵除法（通过乘以逆矩阵）</span><br>F = np.dot(A, np.linalg.inv(B)) <br><br></code></pre></td></tr></table></figure><h3 id="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"><a href="#傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）" class="headerlink" title="傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）"></a>傅里叶变换是什么？原理是这样的，怎么实现？（这里开一个新坑，数字信号处理）</h3><h4 id="基本介绍。"><a href="#基本介绍。" class="headerlink" title="基本介绍。"></a>基本介绍。</h4><p>傅里叶变换是一种在数学、物理和工程中广泛使用的数学变换，它可以将一个函数或信号从其原始的时间或空间表示转换为频率表示。这对于许多应用都非常有用，因为它可以揭示信号的频率成分，这在原始的时间或空间表示中可能不明显。<br>傅里叶变换的基本思想是，任何函数都可以表示为一系列正弦波和余弦波的叠加。换句话说，我们可以将一个复杂的信号分解为一系列更简单的正弦波和余弦波。</p><h4 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h4><p>傅里叶变换的基本原理是将一个函数或信号从其原始的时间或空间表示转换为频率表示。这是通过将函数表示为一系列正弦波和余弦波的叠加来实现的。<br><img src="/pic/fly1.jpg" alt="傅里叶变换示意图"></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># 创建一个简单的信号</span><br><span class="hljs-attribute">t</span> = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">500</span>)<br><span class="hljs-attribute">f</span> = np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">50</span> * t) + <span class="hljs-number">0</span>.<span class="hljs-number">5</span> * np.sin(<span class="hljs-number">2</span> * np.pi * <span class="hljs-number">120</span> * t)<br><br><span class="hljs-comment"># 绘制原始信号</span><br><span class="hljs-attribute">plt</span>.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-attribute">plt</span>.plot(t, f)<br><span class="hljs-attribute">plt</span>.title(&#x27;Original Signal&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Time&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Amplitude&#x27;)<br><br><span class="hljs-comment"># 计算傅里叶变换</span><br><span class="hljs-attribute">F</span> = np.fft.fft(f)<br><br><span class="hljs-comment"># 计算频率</span><br><span class="hljs-attribute">freq</span> = np.fft.fftfreq(t.shape[-<span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 绘制频谱</span><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-attribute">plt</span>.plot(freq, np.abs(F))<br><span class="hljs-attribute">plt</span>.title(&#x27;Frequency Spectrum&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(&#x27;Frequency&#x27;)<br><span class="hljs-attribute">plt</span>.ylabel(&#x27;Magnitude&#x27;)<br><br><span class="hljs-attribute">plt</span>.tight_layout()<br><span class="hljs-attribute">plt</span>.show()<br><br></code></pre></td></tr></table></figure><h3 id="什么是对象？-封装，继承，多态是什么？"><a href="#什么是对象？-封装，继承，多态是什么？" class="headerlink" title="什么是对象？ 封装，继承，多态是什么？"></a>什么是对象？ 封装，继承，多态是什么？</h3><p>什么是对象？<br>在面向对象编程（Object-Oriented Programming，OOP）中，对象是类的实例。类是一种抽象的概念，用于描述具有相似属性和行为的对象的集合。对象是类的具体实现，它具有类定义的属性和方法。<br>对象可以看作是现实世界中的实体或概念在程序中的表示。每个对象都有自己的状态（属性）和行为（方法），并且可以与其他对象进行交互。</p><p>封装<br>封装是面向对象编程的一种重要概念，它将数据和操作数据的方法捆绑在一起，形成一个称为类的单个实体。封装隐藏了数据的内部实现细节，只暴露对外部可见的接口。这样可以保护数据的完整性，并提供更好的代码组织和维护性。<br>通过封装，对象的内部状态可以被保护起来，只能通过公共接口进行访问和修改。这样可以防止对数据的不合理访问和修改，增加了代码的安全性和可靠性。</p><p>继承<br>继承是面向对象编程中的另一个重要概念，它允许一个类继承另一个类的属性和方法。继承创建了一个类的层次结构，其中一个类（称为子类或派生类）可以从另一个类（称为父类或基类）继承属性和方法。<br>通过继承，子类可以继承父类的特性，并且可以添加自己的特定特性。这样可以实现代码的重用和扩展，减少了重复编写代码的工作量。</p><p>多态<br>多态是面向对象编程中的另一个重要概念，它允许使用统一的接口来处理不同的对象类型。多态性允许同一个方法在不同的对象上产生不同的行为。<br>通过多态，可以编写通用的代码，可以处理多个不同类型的对象，而无需针对每种类型编写特定的代码。这提高了代码的灵活性和可扩展性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 封装示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model</span>):<br>        self.brand = brand<br>        self.model = model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 Car 对象并访问其信息</span><br>my_car = Car(<span class="hljs-string">&quot;Toyota&quot;</span>, <span class="hljs-string">&quot;Corolla&quot;</span>)<br>my_car.display_info()<br><br><span class="hljs-comment"># 继承示例</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ElectricCar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, brand, model, battery_capacity</span>):<br>        <span class="hljs-built_in">super</span>().__init__(brand, model)<br>        self.battery_capacity = battery_capacity<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">display_info</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Electric Car: <span class="hljs-subst">&#123;self.brand&#125;</span> <span class="hljs-subst">&#123;self.model&#125;</span>, Battery Capacity: <span class="hljs-subst">&#123;self.battery_capacity&#125;</span> kWh&quot;</span>)<br><br><span class="hljs-comment"># 创建一个 ElectricCar 对象并访问其信息</span><br>my_electric_car = ElectricCar(<span class="hljs-string">&quot;Tesla&quot;</span>, <span class="hljs-string">&quot;Model S&quot;</span>, <span class="hljs-number">100</span>)<br>my_electric_car.display_info()<br><br><span class="hljs-comment"># 多态示例</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_car_info</span>(<span class="hljs-params">car</span>):<br>    car.display_info()<br><br><span class="hljs-comment"># 使用 show_car_info 函数展示不同类型的车辆信息</span><br>show_car_info(my_car)<br>show_car_info(my_electric_car)<br><br></code></pre></td></tr></table></figure><h3 id="python中的不同代码高亮表示什么？"><a href="#python中的不同代码高亮表示什么？" class="headerlink" title="python中的不同代码高亮表示什么？"></a>python中的不同代码高亮表示什么？</h3><p>在Python的IDLE编程环境中，不同颜色的文本表示不同的含义。以下是IDLE中常见的颜色及其含义：<br>黑色：普通的代码文本。<br>蓝色：关键字，例如if、else、for、while等。<br>绿色：字符串文本。<br>红色：语法错误或代码中的错误。<br>紫色：函数和方法的名称。<br>棕色：数字。<br>橙色：内置函数和模块的名称。<br>灰色：注释。</p><h3 id="怎么对通道数位置的npy数据进行叠加"><a href="#怎么对通道数位置的npy数据进行叠加" class="headerlink" title="怎么对通道数位置的npy数据进行叠加"></a>怎么对通道数位置的npy数据进行叠加</h3><p>思路分析，<br>x &#x3D; np.array([1 ,  2]) # shape为(1,2)<br>y &#x3D; np.array([3 ,  4]) # shape为(1,2)<br>怎么叠加np为[[1 , 2],[3 ,4]] #shape为(2,2)</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">x</span> = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])  # shape为(<span class="hljs-number">2</span>,)<br><span class="hljs-attribute">y</span> = np.array([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])  # shape为(<span class="hljs-number">2</span>,)<br><br><span class="hljs-attribute">z</span> = np.vstack((x, y))  # 叠加x和y，得到z<br><span class="hljs-attribute">print</span>(z)  # 输出结果为[[<span class="hljs-number">1</span> <span class="hljs-number">2</span>]<br>          <span class="hljs-comment">#           [3 4]]</span><br><span class="hljs-attribute">print</span>(z.shape)  # 输出结果为(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br></code></pre></td></tr></table></figure><p>堆栈数组垂直顺序（行）</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-attribute">x</span> = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])  # shape为(<span class="hljs-number">2</span>,)<br><span class="hljs-attribute">y</span> = np.array([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])  # shape为(<span class="hljs-number">2</span>,)<br><br><span class="hljs-attribute">z</span> = np.hstack((x, y))  # 堆叠x和y，得到z<br><span class="hljs-attribute">print</span>(z)  # 输出结果为[<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span>]<br><span class="hljs-attribute">print</span>(z.shape)  # 输出结果为(<span class="hljs-number">4</span>,)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch介绍</title>
    <link href="/2024/04/19/deeplearnbook1/"/>
    <url>/2024/04/19/deeplearnbook1/</url>
    
    <content type="html"><![CDATA[<p>在深度学习，要永远抱着学徒的心。<br>本人参考书目为《Python深度学习基于PyTorch》 <a href="http://www.feiguyunai.com/">下载链接</a> <a href="https://github.com/Wumg3000/feiguyunai">使用下载链接——github</a></p><h1 id="目前深度学习的框架有什么？"><a href="#目前深度学习的框架有什么？" class="headerlink" title="目前深度学习的框架有什么？"></a>目前深度学习的框架有什么？</h1><ol><li>TensorFlow ：由Google开发的开源深度学习框架，提供了灵活性和高性能计算能力。TensorFlow 2.x版本引入了更加易用的Keras API作为主要接口。<a href="https://github.com/tensorflow/tensorflow">TensorFlow的github链接</a></li><li>PyTorch ：由Facebook开发的开源深度学习框架，以动态计算图的方式进行建模，易于调试和学习。PyTorch在研究领域广泛应用。<a href="https://github.com/pytorch/pytorch">PyTorch的github链接</a></li><li>Keras：最初作为独立的深度学习框架，现在已经成为TensorFlow的高级API。Keras提供了简单易用的接口，适合快速搭建深度学习模型。 <a href="https://github.com/keras-team/keras">Keras的github链接</a></li><li>MXNet：由Apache软件基金会支持的深度学习框架，具有高度可扩展性和灵活性。MXNet支持动态和静态计算图。[MXNet的github链接]（<a href="https://github.com/apache/mxnet%EF%BC%89">https://github.com/apache/mxnet）</a></li><li>CNTK (Microsoft Cognitive Toolkit)：由微软开发的深度学习框架，提供了高效的性能和多GPU支持。 <a href="https://github.com/microsoft/CNTK">CNTK的github链接</a></li><li>PaddlePaddle（百度飞桨）。这是一个由百度开发的开源深度学习平台，它为深度学习研究人员和开发者提供了丰富的API，支持多种模型结构，可以用来创建各种深度学习模型。[百度飞浆的链接]（<a href="https://www.paddlepaddle.org.cn/%EF%BC%89">https://www.paddlepaddle.org.cn/）</a></li></ol><h1 id="为什么要学习PyTorch？"><a href="#为什么要学习PyTorch？" class="headerlink" title="为什么要学习PyTorch？"></a>为什么要学习PyTorch？</h1><ol><li>pytorch是动态计算图，用法更接近python，并且pytoch与python共同使用了numpy的命令，降低了学习的门槛，比TensorFlow更容易上手</li><li>pytorch需要定义网络层、参数更新等关键步骤，有助于学习深度学习的核心（根据梯度更新参数。）</li><li>pytorch的流行仅次于TensorFlow。在github上的stareed为77.7K （此数据截止到2024&#x2F;4&#x2F;19日）</li><li>pytorch的动态图机制在调试方面非常简单，如果计算图运行出错，马上可以跟踪到问题。pytorch的调试和python一样，可以通过断点检查来解决问题。</li></ol><h1 id="解释一下这本书的结构"><a href="#解释一下这本书的结构" class="headerlink" title="解释一下这本书的结构"></a>解释一下这本书的结构</h1><ol><li>第一部分：介绍深度学习的基石Numpy，介绍PyTorch基础于pytorch构建神经网络的工具箱和数据处理工具。</li><li>第二部分：这本书的核心内容，包括机器学习的流程，常用算法和技巧等内容。实现了基于卷积神经网络的多个视觉处理实例，实现了多个自然语言处理、时间序列方面的实例。介绍了编码器——解码器模型、带注意力的编码器——解码器模型、对抗式生成器以及多种衍生生成器。（注：这里阐述一下关于深度学习、机器学习、人工智能之间的关系。人工智能包含机器学习，机器学习包含深度学习）</li><li>第三部分：实战部分，这部分在介绍相关原理、架构的基础上，使用了pytoch实现了多个深度学习典型实例，比如人脸识别、迁移学习、数据增强、中英文互译、生成式网络实例、模型迁移、强化学习、深度强化学习等实例。</li></ol><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="什么是python的断点检查？"><a href="#什么是python的断点检查？" class="headerlink" title="什么是python的断点检查？"></a>什么是python的断点检查？</h3><h3 id="什么是动态计算图？"><a href="#什么是动态计算图？" class="headerlink" title="什么是动态计算图？"></a>什么是动态计算图？</h3>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔7</title>
    <link href="/2024/04/19/ganwu7/"/>
    <url>/2024/04/19/ganwu7/</url>
    
    <content type="html"><![CDATA[<h1 id="what-is-important-in-life？（生命中什么最重要？）"><a href="#what-is-important-in-life？（生命中什么最重要？）" class="headerlink" title="what is important in life？（生命中什么最重要？）"></a>what is important in life？（生命中什么最重要？）</h1><h3 id="Today-I-can’t-help-but-ask-myself-what-is-most-important-in-life-I-don’t-think-I-can-answer-the-question-because-I-don’t-know-how-to-answer-the-question-But-I-think-I-may-have-a-clue-to-this-problem-thinking-from-my-20-years-of-experience-I-often-worry-about-what-I-don’t-have-but-the-result-of-worrying-is-only-worrying-itself-which-does-nothing-to-change-the-status-quo-or-makes-the-result-worse-I-thought-I-needed-to-get-rid-of-this-worry-so-I-saw-death-I-saw-history-the-flow-of-the-past-the-men-I-have-learned-that-nothing-is-permanent-in-the-face-of-death-and-that-time-diminishes-everything-So-I-saw-the-past-the-future-in-the-past-regret-and-anxiety-about-the-future-saw-the-present-the-past-is-gone-the-future-future-instead-of-falling-into-the-past-regret-and-anxiety-about-the-future-why-not-change-a-mentality-to-spend-the-present-Life-has-no-meaning-instead-of-blindly-looking-for-the-meaning-of-the-castle-in-the-air-it-is-better-to-spend-the-present-with-gratitude-for-the-past-and-hope-for-the-future-Look-to-the-future-based-on-the-present-not-to-worry-about-the-future-the-future-is-not-to-worry-but-to-create-For-the-present-attitude-not-happy-with-things-not-sad-Lief-is-short-kiss-slowly-laugh-insanely-love-truly-and-forgive-quickly"><a href="#Today-I-can’t-help-but-ask-myself-what-is-most-important-in-life-I-don’t-think-I-can-answer-the-question-because-I-don’t-know-how-to-answer-the-question-But-I-think-I-may-have-a-clue-to-this-problem-thinking-from-my-20-years-of-experience-I-often-worry-about-what-I-don’t-have-but-the-result-of-worrying-is-only-worrying-itself-which-does-nothing-to-change-the-status-quo-or-makes-the-result-worse-I-thought-I-needed-to-get-rid-of-this-worry-so-I-saw-death-I-saw-history-the-flow-of-the-past-the-men-I-have-learned-that-nothing-is-permanent-in-the-face-of-death-and-that-time-diminishes-everything-So-I-saw-the-past-the-future-in-the-past-regret-and-anxiety-about-the-future-saw-the-present-the-past-is-gone-the-future-future-instead-of-falling-into-the-past-regret-and-anxiety-about-the-future-why-not-change-a-mentality-to-spend-the-present-Life-has-no-meaning-instead-of-blindly-looking-for-the-meaning-of-the-castle-in-the-air-it-is-better-to-spend-the-present-with-gratitude-for-the-past-and-hope-for-the-future-Look-to-the-future-based-on-the-present-not-to-worry-about-the-future-the-future-is-not-to-worry-but-to-create-For-the-present-attitude-not-happy-with-things-not-sad-Lief-is-short-kiss-slowly-laugh-insanely-love-truly-and-forgive-quickly" class="headerlink" title="Today, I can’t help but ask myself what is most important in life. I don’t think I can answer the question because I don’t know how to answer the question. But I think I may have a clue to this problem, thinking from my 20 years of experience, I often worry about what I don’t have, but the result of worrying is only worrying itself, which does nothing to change the status quo, or makes the result worse. I thought I needed to get rid of this worry, so I saw death, I saw history, the flow of the past, the men. I have learned that nothing is permanent in the face of death, and that time diminishes everything. So I saw the past, the future, in the past regret and anxiety about the future saw the present, the past is gone, the future future, instead of falling into the past regret and anxiety about the future, why not change a mentality to spend the present. Life has no meaning, instead of blindly looking for the meaning of the castle in the air, it is better to spend the present with gratitude for the past and hope for the future. Look to the future based on the present, not to worry about the future, the future is not to worry, but to create. For the present attitude, not happy with things, not sad. Lief is short, kiss slowly, laugh insanely, love truly and forgive quickly."></a>Today, I can’t help but ask myself what is most important in life. I don’t think I can answer the question because I don’t know how to answer the question. But I think I may have a clue to this problem, thinking from my 20 years of experience, I often worry about what I don’t have, but the result of worrying is only worrying itself, which does nothing to change the status quo, or makes the result worse. I thought I needed to get rid of this worry, so I saw death, I saw history, the flow of the past, the men. I have learned that nothing is permanent in the face of death, and that time diminishes everything. So I saw the past, the future, in the past regret and anxiety about the future saw the present, the past is gone, the future future, instead of falling into the past regret and anxiety about the future, why not change a mentality to spend the present. Life has no meaning, instead of blindly looking for the meaning of the castle in the air, it is better to spend the present with gratitude for the past and hope for the future. Look to the future based on the present, not to worry about the future, the future is not to worry, but to create. For the present attitude, not happy with things, not sad. Lief is short, kiss slowly, laugh insanely, love truly and forgive quickly.</h3><h3 id="今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！"><a href="#今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！" class="headerlink" title="今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！"></a>今天，我情不自禁的问自己生命中什么最重要。我想我不能回答这个问题的答案，因为我不知道怎么去回答这个问题的答案。但是我想我或许有一点关于这个问题线索，从我20年中的经历来思考，我经常为自己所未拥有的事物所忧虑，但是忧虑的结果只能是忧虑本身，对现状没有一点改变，或者导致结果更坏。我想我需要摆脱这种忧虑，于是我看到了死亡，看到了历史，往事流转，风流人物。又有多少存在于世上，我学到了在死亡面前没有什么是永恒的，时间会冲淡一切。于是我看到了过去、未来，在对过去的悔恨和对未来的焦虑中看到了当下，过去已逝，未来未来，与其陷入对过去的悔恨中和对未来的焦虑中，为什么不换一个心态去度过当下。人生本来就没有意义，与其去一味的寻找那空中楼阁的意义，不如怀着对过去的感恩于对未来的期盼去度过当下。基于现状去展望明天，而不是去忧虑未来，未来不是是用来忧虑的，而是用来创造的。对于当下的态度，不以物喜，不以及悲，要发自内心的开心，发自内心的悲伤。既然生命如此短暂，那么慢慢地亲吻，尽情地欢笑，真心去爱，快快的原谅吧！</h3>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《诫子书》</title>
    <link href="/2024/04/18/jzs/"/>
    <url>/2024/04/18/jzs/</url>
    
    <content type="html"><![CDATA[<h1 id="诫子书"><a href="#诫子书" class="headerlink" title="诫子书"></a>诫子书</h1><h2 id="夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。"><a href="#夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。" class="headerlink" title="夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。"></a>夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。</h2><p>自注：从论语中我们可以得到，圣人的任务是将人不知的世界改造成人不愠的世界。而君子最开始是人不知的，从人不知的群体中诞生的，那么怎么才能从人不知的群体中成长为君子。我想戒子书中给出了一个思路。<br>第一要有志向，但是志向不是说凭空随便想一个就行了，它必须要基于自身条件实现的可能性，不然随便设立的志向即无实现的可能，也把自己的精力给浪费了。那么怎么才能有一个好的志向？在我面前看来，不要期许有什么好的志向，先把眼前要做的做好，基于现在，现实，充分认识到现实存在条件，然后一天天的打算，一天天的实现自己的打算，在实践的过程中不断深化自己做事的观点，想法，我想慢慢的根植于自己内心的想法就能变成志向，而且这个志向会更有实现的可能性。（认知来源于实践，理论指导实践。）<br>第二要对事情的实现减少期望感，为什么要这样说？首先我们讨论事情的实现。怎么才能实现一件事情？构成这件事的基本条件满足时，那么这件事情就已经实现了。比如吃饭，吃饭有人，要有饭，构成吃饭的动作，那么吃饭这件事就可以开始了。那么怎么中断吃饭这件事勒？抽取吃饭的条件就行了，将吃饭的人给抽取，吃饭这件事就不能进行了。所以完成一件事很难，因为要满足各种条件，前提条件满足了，那么这件事就可能完成，为什么是可能完成，因为在事情发展的过程中还会存在各种各样的影响因素，使构成事情的基本条件被抽取，这样事情就不能够发展了。站在这个角度上，不对事情抱有期待感是有道理的。<br>第三要认识到积累的重要性，千里之行，始于足下。可能我看到这个人有很多奖项，很多我没有的条件。但是我想我没有看到的是，人家背后的付出，只看到人家怎么怎么样，没有看到人家付出时的艰辛。经济学上讲，要想收获什么，就必须付出什么。时间，金钱，必须要拿自己拥有的去换取自己想要的。为什么这里要谈积累的重要性？举个例子，拿这个博客来说把，最开始肯定是从第一篇开始的，不可能从第n篇开始吧。而这么多博文的内容，其背后必然是时间，知识的积累才能诞生的。</p><p><img src="/pic/jzs1.jpg" alt="路是靠走出来的，不是靠想出来的"></p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习之开篇</title>
    <link href="/2024/04/16/deeplearn1/"/>
    <url>/2024/04/16/deeplearn1/</url>
    
    <content type="html"><![CDATA[<h1 id="为什么要开这个坑"><a href="#为什么要开这个坑" class="headerlink" title="为什么要开这个坑"></a>为什么要开这个坑</h1><ol><li>因为我是人工智能专业的学生</li><li>在学习的过程中起到记录和反思的作用</li><li>单纯想开</li></ol><h1 id="开这个坑，打算怎么填坑"><a href="#开这个坑，打算怎么填坑" class="headerlink" title="开这个坑，打算怎么填坑"></a>开这个坑，打算怎么填坑</h1><p>慢慢填呗，图难于其易，为大于其细。天下难事必作于易，天下大事必作于细。是以圣人终不为大，故能成其大。</p><h1 id="这个坑的流程是什么？"><a href="#这个坑的流程是什么？" class="headerlink" title="这个坑的流程是什么？"></a>这个坑的流程是什么？</h1><ol><li>先讲pytorch</li><li>基于pytorch实现一些经典网络，完成一些案例</li><li>看论文，介绍一些经典的论文，比如AlexNet，VGG，CNN，GoolgeNet，Unet，shuffleNet</li><li>开始跑yolo，更改网络模块。</li></ol><p>注： 这里只是一个流程，在开始后会有更多的坑需要去填的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>峨眉山</title>
    <link href="/2024/04/14/tupian2/"/>
    <url>/2024/04/14/tupian2/</url>
    
    <content type="html"><![CDATA[<h1 id="清晨的树"><a href="#清晨的树" class="headerlink" title="清晨的树"></a>清晨的树</h1><p><img src="/pic/e1.jpg"><br><img src="/pic/e2.jpg"><br><img src="/pic/e3.jpg"><br><img src="/pic/e4.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>图片</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论语</title>
    <link href="/2024/04/14/luwnyu/"/>
    <url>/2024/04/14/luwnyu/</url>
    
    <content type="html"><![CDATA[<p>注： 本博客参考的论语注解来源于缠中说禅，仅作为学习使用。<br>论语不仅是一本修身的学说，更是一本治世的学说。<br>修身，齐家，平天下。<br>修身（定、静、安、虑、得）</p><h1 id="第一句"><a href="#第一句" class="headerlink" title="第一句"></a>第一句</h1><ol><li>子曰：学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？</li></ol><p>问：什么是学？<br>答：闻“圣人之道”、见“圣人之道”、“对照”“圣人”、在现实社会中不断地“校对”。<br>问：谁学？<br>答：君子。<br>问：学什么？<br>答：成“圣人”之道。<br>问：学了能成什么？<br>答：“圣人”。</p><p>闻，见，学，行<br>闻圣人之道，见圣人之道，学圣人之道，行圣人之道<br>行“圣人之道”的人就是要使得“不知之人”变得“不愠”，使得“不知之世界”变得“不愠”。<br>人不知，人不相，人不愠。</p><p>2024&#x2F;5&#x2F;2注： 人不知：有一种人，不愿意付出努力，希望坐享其成，或者通过捷径来取得成果，即使知道努力是通往外界的必经因素，人的天性是懒惰的，通过自律或者其他外界因素才能让自己足够努力。在这种情况下看到他人付出努力取得成功了，为了让自己不显得那么失败，于是就寻找虚幻的优越感，例如努力的人是因为智商不够才需要努力（或者因为别的不足才需要努力），虚构出别人不如自己的假象，人并不关心真相，只关心如何让自己获得优越感，如何维持自己的傲慢。</p><p>注:人不相，见相非相，即见如来。见性，见智，凡所有相，皆是虚妄，若见诸相非相，既见如来。世界呈现在我们面前的是表象，非相才是本质。一切的现象都是由非相这个条件构成的，非相指一切事物形成的条件，肉眼是看不到的，什么条件产生什么结果，这个叫因果，这个因果本身的来源无从探究。要达到看清事物的本质的境界，首先向内求解，看到自己的本性，清楚自己的条件，这就是觉性，又可称为自我觉醒。之后再通过自己的觉性的智慧看待事物，做符合条件的事物及发展规律的事情，才可能达到与之相应的结果。若执着于事物的表象，不断在这个世界里沉迷见色（色：包括但不限于，酒、色、财、气），一旦执着于这些表现，当然会着像、着礼（人情世俗中的世俗），开始执着于礼的形式，在这种形式下产生各种各样的观点，立场，分别，执着。源于各种各样的概念，这些概念又将事物划分成了各种各样的等级，人的观念自然出现了各种各样鉴别，亲疏，好恶，从而相，而忽视了本质，一直处于人不知，达不到人不相的境界了，。</p><h1 id="第二句"><a href="#第二句" class="headerlink" title="第二句"></a>第二句</h1><ol start="2"><li>子曰：朝闻道夕死，可矣！<br>子曰： 朝闻，夕死。可矣！</li></ol><p>君子慎独，一旦开始走上圣人的道路，就要严格的要求自己。无论什么情况，与大家在一起时与自己一个人在一起时所表现出来的状态是一样的。<br>解释：君子从“闻其道”开始，无论任何地方，无论条件恶劣还是优越，甚至出生入死，都要不断地“固守”，“承担”“圣人之道”之行直到最终成就“不愠的世界”而不退转，只有这样，才可以行“圣人之道”呀。</p><p>自注： 立志走上成为圣人的这条道路上，就要时时刻刻牢记自己内心的要求，以现实为标准。时时刻刻的固守，达到内圣外王的境界。</p><h1 id="第三句"><a href="#第三句" class="headerlink" title="第三句"></a>第三句</h1><ol start="3"><li>子在川上曰：逝者如斯夫，不舍昼夜。</li></ol><p>解释：孔子在河流的源头，抚今追昔、满怀感慨，自告且忠告所有决心开始“见、学、行”“圣人之道”的君子：“立志“见、学、行”“圣人之道”的君子，就要像这江水一样，从“闻其道”的源头开始，后浪推前浪，生生不息、前赴后继，无论任何时候、任何地方，无论条件恶劣还是优越，甚至出生入死，都要不断地“固守”，“承担”“圣人之道”之行直到最终成就“不愠的世界”而不退转。”这里必须明确，这话既是孔子自己的感慨，也是对所有有志于圣人之道的人的忠告和勉励。</p><h1 id="第四句"><a href="#第四句" class="headerlink" title="第四句"></a>第四句</h1><ol start="4"><li>子曰：人能弘道，非道弘人。<br>就是“道”不是目的，只有“人”才是目的，只有现实中的“人”才是目的，一切以打着虚无飘渺的所谓“道”为目的，以现实的“人”为手段的所谓“闻、见、学、行”“圣人之道”，都是《论语》背道而驰的。</li></ol><p>注：孔子说：“人能够把道发扬光大，不是道能把人发扬光大。”</p><h1 id="第五句"><a href="#第五句" class="headerlink" title="第五句"></a>第五句</h1><ol start="5"><li>子曰：攻乎异端，斯害也己。</li></ol><p>对于行“圣人之道”的君子，“异端”只不过是“别为一端行非圣人之道”的“不知”者，如果没有这种人，“圣人之道”之行就成了无源之水。“不知”，如同米；“不愠”，如同饭；<br>在我看来，这句话更像是说要允许矛盾的存在，不能只允许一种情况的存在。毕竟按照矛盾的观点来看，正是矛盾才使事物能够不断地发展。圣人也是从不知者中诞生的，如果不允许不知者的话那么就不可能诞生知者了。<br>那么问题来了，圣人的作用是什么？人不知，人不愠。天下大同。对于“别为一端行非圣人之道”的“不知”者，行“圣人之道”的君子不是要攻打他们、消灭他们，而是要如把“米”煮成“饭”般把他们从“不知”者变成“不愠”者，变成行“圣人之道”的君子，把“不知”的世界变成“不愠”的世界，只有这样，才算是真行“圣人之道”。</p><h1 id="第六句"><a href="#第六句" class="headerlink" title="第六句"></a>第六句</h1><ol start="6"><li>子曰：道，不同、不相为谋。<br>子曰： 道，不同，不相为谋。<br>不相： 不以外在为判断条件，要看其本质。类似于见相非相，即见如来（此中有真意，欲辩以忘言）<br>解释：<br>道，圣人之道，就如同大河，大河是不会去“选择”的、也不会去强迫“一致”，是“不相”、“不同”的。 “圣人之道”之“谋”，就是“不同”、“不相”。最常见的以“相”相之就是所谓的“以貌取人”，延伸下去，根据思想、观点、意识形态、经济水平等等，都是以“相”相之，都不是“不相”，是和“圣人之道”相违的。</li></ol><p>注：“你得允许一部分人先高雅起来，一部分人后高雅起来，一部分人怎么也高雅不起来。”</p><h1 id="第七句"><a href="#第七句" class="headerlink" title="第七句"></a>第七句</h1><ol start="7"><li>子曰：有教无类。<br>自注：学问就一定有好坏之分吗？什么是检验的标准，实践是检验真理的唯一标准。现实是检验真理的唯一标准。</li></ol><h1 id="第八句"><a href="#第八句" class="headerlink" title="第八句"></a>第八句</h1><ol start="8"><li>子曰：士志於道，而耻恶衣恶食者，未足与议也！<br>注：“耻恶衣恶食者”，就是“相”如果一个人，立志要行“圣人之道”，却把人分为“好衣好食”、“恶衣恶食”两类人，也就是以贫富划分人，而选择以“恶衣恶食”也就是穷人为耻，远离他们，那这种人谈论的“圣人之道”只是羊头狗肉的勾当。为什么？因为他不能“不相”。</li></ol><h1 id="第九句"><a href="#第九句" class="headerlink" title="第九句"></a>第九句</h1><ol start="9"><li>子曰：贤哉，回也！一箪食，一瓢饮，在陋巷，人不堪其忧，回也不改其乐。贤哉，回也！<br>自注：真评和加贫。真有本事和假有本事。<br>颜回这个“安贫乐道”的典型，并不是故意去“贫”，并不是故意要“恶衣恶食”，也不是如某些宗教所教唆的故意去苦行，这些都是严重地“相”了，这些都是和君子谋“圣人之道”所必须坚持的“不相”原则背道而驰的。</li></ol><h1 id="第十句"><a href="#第十句" class="headerlink" title="第十句"></a>第十句</h1><ol start="10"><li>子曰：贫而无怨难；富而无骄易。</li></ol><p>这句有多种解释：第一种，从人不知的社会到人不相的社会，过程中，贫穷而不怨恨是困难的，富贵而不骄横是容易的。<br>问题是贫穷真的会使人怨恨吗？富贵真的会使人不骄横吗？在人不知的社会中（最初始的人不知），现实却又是穷人经常乐呵呵，富人却骄横无理。（穷乐呵，为富不仁。）<br>第二种解释，在任何“人不知”的社会中，都体现为“贫而怨难；富而骄易。</p><h1 id="第十一句"><a href="#第十一句" class="headerlink" title="第十一句"></a>第十一句</h1><ol start="11"><li>子贡曰：“贫而无谄，富而无骄，何如？”子曰：“可也；未若贫而乐，富而好礼者也。”</li></ol><p>“贫而谄”不得，最终就会“贫而怨难”，因“怨”有“仇”而“敌对”甚至“造反”，但“造反”成功的马上又成为“富而骄”，又有新的“贫而谄”，结果不断循环，都逃不出这个“贫而谄，富而骄”的“人不知”社会。<br>儒家看穿了这个“贫而谄，富而骄”的恶性循环，知道在这里打圈圈是没用的，而要打破这个恶性循环的办法，只有通过“人不相”而达到“人不愠”，最终摆脱“贫而谄，富而骄”的“人不知”的恶性循环。要实现这个打破，首先就要实现“贫而无谄，富而无骄”的“人不相”，为此，就必须要实现对“贫富”之相的“不相”，达到“人不相”。为什么实现对“贫富”之相的“不相”，就能实现“人不相”？是因为只要存在人与人的地方，就必然会出现各种方面的“贫富”之相，消灭这种“贫富”之相、将之抹平是不可能的，唯一办法就是使之“不相”，使得各种“贫富”之相能平等地存在，实现其“不同”，容纳各种“不同”而成其大，最终成就其“大同”。儒家、《论语》认为，这种“大同社会”的实现是当下的，是可以现世实现的，这种看法是由儒家的入世以及现世精神所决定的。<br>就像天幕红尘中说的，你要允许你得允许一部分人先高雅起来，一部分人后高雅起来，一部分人怎么也高雅不起来。” </p><h1 id="第十二句"><a href="#第十二句" class="headerlink" title="第十二句"></a>第十二句</h1><ol start="12"><li>子曰：齐一变，至於鲁；鲁一变，至於道。<br>在孔子时代是打着以“仁”以“德”治国的典型，号称传承着被孔子当成典范的周公之仁德。以“仁”以“德”治国，强调善的力量，对于一个习惯于以恶为前提的“人不知”世界是不可想象的，相比“齐式”国家模式，“鲁式”国家模式的出现是一种进步，所以才有“齐一变，至於鲁”的说法。</li></ol><h1 id="第十三句"><a href="#第十三句" class="headerlink" title="第十三句"></a>第十三句</h1><ol start="13"><li>子曰：放于利而行，多怨。<br>“放于利而行，多怨。”就是无论放弃还是放纵“利”而行，都会产生“多怨”的结果。 其实，现在的人对于这句话，肯定会更容易理解。计划经济年代，都是放弃“利”而行，结果是“多怨”；而市场经济年代，放纵“利”而行，结果还是“多怨”。这“人不知”社会的总规律，绝对不能放弃或放纵“利”而行，要充分把握其“利”，所谓用其刃而不被其刃所伤。行“圣人之道”的君子，首先要是“知人”，如果自己都还“不知”，又如何去让“人不知”之相“不相”？一事不知，儒者之耻，不尽量用这世界上的知识武装自己，是没资格当儒者的。</li></ol><h1 id="第十四句"><a href="#第十四句" class="headerlink" title="第十四句"></a>第十四句</h1><ol start="14"><li>子曰：好勇疾贫，乱也。人而不仁，疾之已甚，乱也。<br>“人不知”社会中同时存在的两种乱相：“贫者”，好勇斗狠；“富者”，为富不仁，被过分享乐之病急速传染，所谓纸醉金迷、醉生梦死。在“人不知”的社会，单纯的道德说教是没意义的，在“利”面前，所有的道德说教都苍白无力。这种“利”的“贫富”之相的严重对立，使得“富者”因得其“利”而放纵无度，而“贫者”因不得其“利”而不平。就算是一个懦夫，当“利”的“贫富”之相严重对立形成的落差储备到了足够大势能后，懦夫也会成为“勇夫”的。这样，自然就有了“好勇疾贫，乱也。人而不仁，疾之已甚，乱也。”。这种图景在“人不知”的社会随处可见、无处不在，《论语》早在两千多年前就已总结出来了。</li></ol><h1 id="第十五句"><a href="#第十五句" class="headerlink" title="第十五句"></a>第十五句</h1><ol start="15"><li>子曰：善人为邦百年，亦可以胜残去杀矣。诚哉是言也！<br>“胜残”、“去杀”，是两个意思相仿的词并列而成，简单说就是“战胜残暴、制止杀戮”；“善人”，就是“使人善”，“善”就是好的意思。“善人”和“胜残去杀”，其并列是一体的，如两腋之于人，双翼之于鸟，钱币的两面之于钱币。“善人、胜残去杀”，才可能“为邦百年”，让国家长治久安。“胜残去杀”，是针对“人而不仁，疾之已甚”，是针对为富不仁的“富者”，包括贼王暴君、贪官污吏、奸商恶霸等等，所谓杀一暴君而救亿万者乃真大仁矣；“善人”，是针对“好勇疾贫”的“贫者”，改善他们的生存条件、扩展他们的生存空间、提高他们的生存能力等等，都可以归之于“善人”之数。但必须强调的是，站在人和社会的整体角度，没有一个人是在任何方面都是“富”者，也没有一个人在任何方面都是“贫”者，但对于现实中的国家来说，经济、社会地位、权力等角度的“贫富”之相才最具有现实力量，这点也是不能忽视的。</li></ol><h1 id="第十六句"><a href="#第十六句" class="headerlink" title="第十六句"></a>第十六句</h1><ol start="16"><li>子曰：如有王者，必世而后仁。<br>一般来解答这一句为大致意思就成了“如果有称王的，一定要经过一世三十年，才能行其仁政。”国家长治久安的六字箴言“善人、胜残去杀”，而现实中，在“人不知”世界里，这六字箴言又有几人能办到？办不到，就必然是“城头变换大王旗”，中国历史上，这种改朝换代的事情，难道还不司空见惯？这种恶性循环中，有一个规律，就是本章的“如有王者，必世而后仁。”“王”，不一定需要有人当皇帝，如资本主义的确立也是一种王，其后到处贩卖的“民主、自由”就是“必世而后仁”了。</li></ol><h1 id="第十七句"><a href="#第十七句" class="headerlink" title="第十七句"></a>第十七句</h1><ol start="17"><li>子适卫，冉有仆。子曰：“庶矣哉！”冉有曰：“既庶矣，又何加焉？”曰：“富之。”曰：“既富矣，又何加焉？”曰：“教之。”<br>不同的社会，有不同的“庶、富、教”发展程度。而“全面发展的自由人的联合体”，就是“庶、富、教”充分发展所呈现的面貌。只有自由人，才会有多样性，才会有“不相”而“不同”，才有真正的“庶”；只有全面发展，才有真正的“富”；由“庶”而“富”，充分发展而形成“全面发展的自由人的联合体”所构成的社会结构，这才是真正的“教”。“庶、富、教”，就是不同成其大而大同。“庶、富”的发展水平，决定了“教”的发展水平，只有“庶、富”充分发展，才有“教”的充分发展，“庶、富”对应的是“人不相”，而“教”的充分发展最终对应的就是“人不愠”，只有“全面发展的自由人的联合体”构成的“教”，才是构成“人不愠”世界的社会结构基础。而只有“人不愠”，才是真正的“善人”。</li></ol><h1 id="第十八句"><a href="#第十八句" class="headerlink" title="第十八句"></a>第十八句</h1><ol start="18"><li>子曰：善人、教民七年，亦可以即戎矣。<br>通常解释为孔子说：“善人教导训练百姓七年时间，就可以叫他们去作战了”。“善人教民七年，亦可以即戎矣。”的通常断句是错的，应该是“善人、教民七年，亦可以即戎矣。”这一章是在彰显“善人”之道的力量，“教”的力量，文明的力量。“善人”之道，就是“圣人之道”一个具体过程中体现的具体形式，“圣人之道”最终要使得“人不知”的世界变成“人不愠”的世界，当然需要融合、同化那些未开化的、文明程度比较低的人、民族和国家，这是“人不知”世界一个很大的组成部分。如果说上一章更侧重于“善人”之道在国家范围的应用，那这一章就指出，“善人”之道在全世界实现的必然性，而只有在全世界的实现，才算真正的“善人”之道。孔子认为，作为“圣人之道”低级阶段的“善人”之道的实现也只能是一个全球性事件，大同，只能是大同天下，而不可能是某一国的大同，必然要“即戎”而达到天下大同。</li></ol><h1 id="第十九句"><a href="#第十九句" class="headerlink" title="第十九句"></a>第十九句</h1><ol start="19"><li>子曰：以不教民战，是谓弃之。<br>通常解释为“以不教民战”解释成“用不经教练的民众去临战阵”孔子说：“用没有经过军事训练的老百姓去打仗，这是有意让他们去送死。”。滑稽，断句应为，以不教，民战，是谓弃之。不行“善人”之道，那只能用“残、杀”，用所谓的白色恐怖来压制，企图让人民战栗、恐惧而治理国家。用“残、杀”企图使民众战栗、恐惧而治理国家的，就是遗弃、背叛民众，而最终也将被民众所遗弃。</li></ol><h1 id="第二十句"><a href="#第二十句" class="headerlink" title="第二十句"></a>第二十句</h1><ol start="20"><li>哀公问社於宰我。宰我对曰：“夏后氏以松，殷人以柏，周人以栗，曰，使民战栗。”子闻之，曰：“成事不说，遂事不谏，既往不咎。”<br>鲁哀公向孔子的弟子宰我问“土地神的祭祀”，宰我自作聪明道：“夏代用松木，殷代用柏木，而周代用栗木是为了借谐音使民战栗。”孔子听到，就告戒：“正成的事不要妄加评议，即成的事就不要徒劳劝告，已成的事就不要再生灾祸。”<br>“成事”，不是指已成的事，而是指正成的事，也就是在萌芽状态的，这时候，还需要观察，不能妄加评议，胡乱定性；“遂事”，马上就要成的事，已经无可挽回的，就不要徒费口舌去劝告了，这样只能产生怨恨；“既往”，已经过去的已成的事，要“不咎”，“咎”的本义是灾祸，已经成的事，如果错了，就不要错上加错，再生灾祸。这句话针对事物发展的三个不同阶段应该采取的态度。</li></ol><h1 id="第二十一句"><a href="#第二十一句" class="headerlink" title="第二十一句"></a>第二十一句</h1><ol start="21"><li>子曰：夷狄之有君、不如，诸夏之亡也。<br>“夷狄之有君、不如，诸夏之亡也。”的意思是：未开化的、文明程度比较低的人、民族和国家，虽然有他们自己的国体、政体，但由于没有遵从、依照文明程度比较高的人、民族和国家的政体、国体，而被后者所轻视。只要有不同的人、民族、国家同时存在，就必然有“诸夏”、“夷狄”之分，对于民族、国家来说，任何不行“圣人之道”的，无论是“齐式”的“王霸之道”还是“鲁式”的“仁德”之道，都必然会有“先进”对“落后”的轻视、压榨。一个国家、民族，如果不行“善人”之道，用“残、杀”企图让别国、别的民族战栗、恐惧而治理世界，就是遗弃、背叛各国、各民族，而最终也将被各国、各民族所遗弃。一个现成的例子，就是美国</li></ol><h1 id="第二十二句"><a href="#第二十二句" class="headerlink" title="第二十二句"></a>第二十二句</h1><ol start="22"><li>子曰：为政以德譬，如北辰居其所而众星共之。<br>这一章，其实就是上一章所说““圣人之道”、“善人之道”是大道，更是现实之道，无位可本，又何来“本位”？正因为无位可本，才可以无所位而生其本、无所本而生其位。这，才是真正的大道、现实之道。”的进一步展开。何谓“为政以德譬，如北辰居其所而众星共之。”？就是“无所位而生其本、无所本而生其位”。 只有明白了这句话，才可能真正明白马克思意义上的“具体问题具体分析”，也才可能真正明白何谓“为政以德譬，如北辰居其所而众星共之”。当人把北极星的位置确定后，执持这位置相应就可以定出其他星星位置；当人从现实出发分析把握了现实关系的逻辑结构后，“孰敢不正？</li></ol><h1 id="第二十三句"><a href="#第二十三句" class="headerlink" title="第二十三句"></a>第二十三句</h1><ol start="23"><li>季康子问政於孔子，孔子对曰：政者，正也，子帅以正，孰敢不正？<br>季康子，鲁国大夫，向孔子问政。“政者，正也”，为政，就是要立行“圣人之道”而成就之这一逻辑支点；“子帅以正，孰敢不正？”为政的人，遵循现实的逻辑，从现实出发，行“圣人之道”而成就之，其它问题就会以此为基础相应地找到解决的办法。这里必须要明确的是，现实，是最底层的支点，行“圣人之道”而成就之这个逻辑支点必须也必然在现实支点之上，离开现实，无所谓“圣人之道”。“圣人之道”，不是离开现实的乌托邦，那种把“圣人之道”装扮成某种口号、旗帜、目标，以此而驱使人，让人为此而折腾，都和“圣人之道”、《论语》、孔子毫无关系。人不是现实的奴隶，现实必须是人参与其中的，没有了人，也无所谓现实，更无所谓现实逻辑。 现实之于人，按其逻辑，有着各种不同的选择，究竟如何去选择，就构成了各色各样的政治。各种政治结构的逻辑支点，都来自现实，这逻辑支点也如同北极星，一旦确立，其它就以此为基础相应地构建。</li></ol><h1 id="第二十四句"><a href="#第二十四句" class="headerlink" title="第二十四句"></a>第二十四句</h1><ol start="24"><li>子曰：不在其位，不谋其政。<br>“不在其位，不谋其政”，就是“不谋不在其位之政”，不谋划与现实变化的位次不符的政事、政治关系、政治制度、上层建筑、生产关系等等。一切都从也只能从现实出发，现实在什么阶段，什么位次，是必须首要分析的问题。</li></ol><h1 id="第二十五句"><a href="#第二十五句" class="headerlink" title="第二十五句"></a>第二十五句</h1><ol start="25"><li>子曰：“不在其位，不谋其政。”曾子曰：“君子思不出其位。”</li></ol><h1 id="第二十六句"><a href="#第二十六句" class="headerlink" title="第二十六句"></a>第二十六句</h1><ol start="26"><li>子曰：“不患，无位；患，所以立。不患，莫己知求，为可知也。”<br>“无所位而生其本、无所本而生其位”，即所“立”、即所“止”、即所“位”。有所“立”，则“立”其“有”，其“有”必有其“位”<br>儒家，内圣、外王，“不在其位，不谋其政”的外王，是和“不患，无位；患，所以立。不患，莫己知求，为可知也。”的内圣相互相成的。这是参悟儒家之说的大关键。</li></ol><p>缠中说禅白话直译<br>子曰：“不患，无位；患，所以立。不患，莫己知求，为可知也。”<br>孔子说：“不患”，无位次；“患”，以“不患”的“无位次”而“位次”。“不患”，不以自己“所知”来选择，就是“能知”。</p><h1 id="第二十七句"><a href="#第二十七句" class="headerlink" title="第二十七句"></a>第二十七句</h1><ol start="27"><li>子曰：不患人之不己知；患其不能也。<br>通译：不要担心别人不了解自己，应该担心的是自己不了解别人。<br>正因为“不明了”的现实，所以才有了自己不断“明了”自己的可能，所以才有了“明了”的可能，不明白这一点，是不可能明白何谓“内圣”的。</li></ol><p>缠中说禅白话直译<br>子曰：“不患人之不己知；患其不能也。”<br>孔子说：不患别人或自己不明了自己，患别人或自己不能明了自己啊。</p><h1 id="第二十八句"><a href="#第二十八句" class="headerlink" title="第二十八句"></a>第二十八句</h1><ol start="28"><li>子曰：“不患人之不己知；患不知人也。”<br>通译：“不要担心别人不了解自己，应该担心的是自己不了解别人。”<br>缠中说禅白话直译<br>子曰：“不患人之不己知；患不知人也。”<br>孔子说：不患人不明了自己，患“人不知”的世界啊。</li></ol><h1 id="第二十九句"><a href="#第二十九句" class="headerlink" title="第二十九句"></a>第二十九句</h1><ol start="29"><li>子曰：“性相，近也；习相，远也。”</li></ol><p>缠中说禅白话直译<br>子曰：性相，近也；习相，远也。<br>孔子说：以性性相，缠附呀；以习习相，深奥啊。</p><h1 id="第三十句"><a href="#第三十句" class="headerlink" title="第三十句"></a>第三十句</h1><ol start="30"><li>子曰：人无远虑，必有近忧。<br>“远”，深远、深奥，同于“习相，远也”，和“习相”相关，脱离“习相”无所谓深远、深奥，不过幻想而已。“习相”，先要明其“相”，明其“相”必先明其“相”之位次，明其“相”之位次，必对其“相”的当下逻辑关系有一明确把握。<br>“虑”，审察、思虑、谋划。“虑”，不是哈姆雷特式的，而是审察、思虑、谋划的统一，三者缺一不可，而最终必须落在行动上，没有行动的“虑”也不过是幻想而已。<br>人的行为，必须从其苗头下手，不想吃恶果，最简单的方法就是不要种下其种子，忧患、祸患的种子一旦缠附，一有机会就会萌芽，就要结果。别以为可以用任何方法可以消除这种子，种子一旦种下就是无位次的，准确说，相对于现实系统来说，种子是无位次的，任何现实的把戏都消灭不了种子，种子不一定在眼前发芽，但不发芽只是机会不成熟，一旦成熟，逃都逃不掉，眼前看不到、没迹象的忧患、祸患，往往才是致命的。而这，才是真正的“近忧”。</li></ol><p>缠中说禅白话直译<br>子曰：人无远虑，必有近忧。<br>孔子说：人没有深远的审察、思虑、谋划，必然缠附祸患。</p><h1 id="第三十一句"><a href="#第三十一句" class="headerlink" title="第三十一句"></a>第三十一句</h1><ol start="31"><li>子曰：众，恶之，必察焉；众，好之，必察焉。</li></ol><p>缠中说禅白话直译<br>子曰：众，恶之，必察焉；众，好之，必察焉。<br>孔子说：一切现象，当被认为是恶的就会被厌恶，对此必须摈弃一切厌恶当下直观；一切现象，当被认为是好的就会被喜好，对此必须摈弃一切喜好当下直观。（恶并不是恶，好的并不一定是好的。）</p><h1 id="第三十二句"><a href="#第三十二句" class="headerlink" title="第三十二句"></a>第三十二句</h1><ol start="32"><li>子曰：视，其所以；观，其所由；察，其所安。人焉廋哉？人焉廋哉？<br>视”，人与认识对象之间的看，相当于感性以及康德规定性判断力所连接的知性与理性所构成的高级人类认识能力，也就是人类所有的认识能力；“观”，看法，相当于“反思判断力”所连接的自由意志；“察”，当下的直“观”，是自由意志的当下实践。“视，其所以”，认识能力是人所凭借的；</li></ol><p>缠中说禅白话直译<br>子曰：视，其所以；观，其所由；察，其所安。人焉廋哉？人焉廋哉？<br>孔子说：认识能力，人的凭借；自由意志，人的遵从；当下直 “观”，即自由意志的当下实践，人的归依。人，哪里有隈曲啊？人，哪里有隈曲啊？</p><h1 id="第三十三句"><a href="#第三十三句" class="headerlink" title="第三十三句"></a>第三十三句</h1><ol start="33"><li>子曰：不知，命无以为君子也；不知，礼无以立也；不知，言无以知人也。</li></ol><p>缠中说禅白话直译<br>子曰：不知，命无以为君子也。不知，礼无以立也。不知，言无以知人也。<br>孔子说：没有智慧，不可能承担君子的使命；没有智慧，不可能建立社会正常的秩序；没有智慧，不可能产生使人智慧的言论。</p><h1 id="第三十四句"><a href="#第三十四句" class="headerlink" title="第三十四句"></a>第三十四句</h1><ol start="34"><li>子曰：由知、德者，鲜矣！<br>圣人之道”，就是将“人不知”的世界变为“人不愠”世界的道路，这里没有任何固定的模式和先验的走法，路是人走出来的，是人所“由”而来，是人所“蹈行，践履”而来。没有人的“蹈行，践履”，何来路？除了“知、德”，行“圣人之道”的君子无所“蹈行，践履”也无须“蹈行，践履”。</li></ol><p>缠中说禅白话直译<br>子曰：由知、德者，鲜矣！<br>孔子说：蹈行、践履“闻、见、学、行”“圣人之道”智慧、所得的君子，永远处在创新、创造之中啊。</p><h1 id="第三十五句"><a href="#第三十五句" class="headerlink" title="第三十五句"></a>第三十五句</h1><ol start="35"><li>子曰：民可，使由之；不可，使知之。</li></ol><p>缠中说禅白话直译<br>子曰：民可，使由之；不可，使知之。<br>孔子说：民众当下适合的，放任民众去蹈行、践履；民众当下不适合的，放任民众运用智慧去创造、创新。</p><h1 id="第三十六句"><a href="#第三十六句" class="headerlink" title="第三十六句"></a>第三十六句</h1><ol start="36"><li>子曰：由诲女，知之乎！知之为，知之；不知为，不知；是知也！</li></ol><p>缠中说禅白话直译<br>子曰：由诲女，知之乎！知之为，知之；不知为，不知；是知也。<br>孔子说：实践教导你，以此而有智慧啊。依智慧而进一步实践，以此而有新的智慧；不依以实践而有的智慧进一步实践，就不会有新的智慧。这，就是最根本的智慧。</p><p>自注： 认识来源于实践，从无知到有一定的感性认识，再从感性认识到一定的理性认识。认识是一个过程，不可以说是直接没有感性认识就产生了理性认识。在认识的过程中当然时时刻刻伴随着实践，正式有了实践才让认识不断深化。</p><h1 id="第三十七句"><a href="#第三十七句" class="headerlink" title="第三十七句"></a>第三十七句</h1><ol start="37"><li>子曰：我非生而知之者，好古，敏以求之者也。<br>本章，孔子提出了学习前人知识、智慧的三个步骤：好、敏、求。 首先，对前人知识、智慧所凝结成的遗典、典章等必须尊重、善待进而学习、研究，才谈得上“好”。尊重、善待进而学习、研究，真正把握以后，还需要在实践中继续印证，这才是“敏”。“敏”，有两层的含义：其一，前人知识、智慧都来源于其当下的实践，而时代变化了，条件变化了，其应用可能要失效，可能有所改变，这必须在实践中才能印证、发现；其二，对前人知识、智慧的把握，特别对于那些洞穿时间的智慧的把握，必须在实践中慢慢体会、摸索，才能发现前人的真义，决不能像某些人对待孔子、马克思那样，根本没弄明白就扮代表，这样是谈不上“好”，更谈不上“敏”了。有了印证，自然就有了选择的基础，选择不是机械地挑选，不是用对错等简单标准来划分，而是根据当下的实践有机地发展、延伸，这样才不辜负古人，也不辜负自己，这才算得上是“求”。</li></ol><p>缠中说禅白话直译<br>子曰：我非生而知之者，好古，敏以求之者也。<br>孔子说：我不是天生、先验地依赖天生、先验而有智慧的人，只是爱好学习、研究先哲遗典、古代典章，并在实践中对此印证、选择的人。</p><p>自注： 这句话在讲前人的经验可以学习，但是不能直接使用。为什么这么说，因为条件改变了，当时这个方法能解决这个问题，那是因为由实践这个方法的条件，而现在没有满足这个方法的条件，所以这个问题使用这个方法就解决不了。一句话，实事求是。那么有没有普世的方法？答案是没有的，但是有普世的法则，内心有内心的心法，社会有社会的法则。熟悉和了解这些法则的过程中必然要经历大量的实践，才能认识到。但是即使认识到这些法则，能不能熟练的用于也是一件困难的事情，知之为知之，不知为不知。迎接挑战，这样的人生才有意义。征服一个又一个巅峰，达到内圣外王的境界。</p><h1 id="第三十八句"><a href="#第三十八句" class="headerlink" title="第三十八句"></a>第三十八句</h1><ol start="38"><li>孔子曰∶生而知之者，上也；学而知之者，次也；困而学之，又其次也。困而不学，民斯为下矣！<br>缠中说禅白话直译<br>孔子曰∶生而知之者，上也；学而知之者，次也；困而学之，又其次也。困而不学，民斯为下矣！<br>孔子说：所有人，天生地依赖天生而有智慧，是最好的；所有人，都能自由地学习且通过学习而有智慧，是稍差的；所有人，被分为不同类别而得到不同类别的学习，是更差的。所有人，被分为不同类别而某类人得不到学习的机会，这就是民众被当成卑下的原因啊。</li></ol><h1 id="第三十九句"><a href="#第三十九句" class="headerlink" title="第三十九句"></a>第三十九句</h1><ol start="39"><li>子曰：盖有不知而作之者，我无是也。多闻，择其善者而从之；多见而识之；知之次也。</li></ol><p>缠中说禅白话直译<br>子曰∶盖有不知而作之者，我无是也。多闻，择其善者而从之；多见而识之；知之次也。<br>孔子说：大概存在没有智慧却凭没有智慧而有所作为的人，我不是这样的。在一个能让每个人都能自由见闻的社会里，尽可能地扩展自己的见闻，选择超过自己的见解，依据其见解而不是依据有此见解的人或群体，深入探讨、吸收学习；进而让自己的见识逐步深厚，才能更清楚地去辨别、辩正各种知识的真伪、深浅。但这些都是智慧的临时落脚处，不是智慧的真正所在。</p><h1 id="第四十句"><a href="#第四十句" class="headerlink" title="第四十句"></a>第四十句</h1><ol start="40"><li>子张学干禄。子曰：多闻阙疑，慎言其余，则寡尤。多见阙殆，慎行其余，则寡悔。言寡尤，行寡悔，禄在其中矣。</li></ol><p>缠中说禅白话直译<br>子张学干禄。子曰：多闻阙疑，慎言其余，则寡尤。多见阙殆，慎行其余，则寡悔。言寡尤，行寡悔，禄在其中矣。<br>孔子说：子张求问获取福运的方法。孔子说：见闻广泛而去除疑惑，见识深厚而去除危险，遵循如此“闻见”而如此“言行”，那么言行都会少过失。言行少过失，福运在其中啊。</p><h1 id="第四十一句"><a href="#第四十一句" class="headerlink" title="第四十一句"></a>第四十一句</h1><ol start="41"><li>子曰∶君子谋道不谋食。耕也，馁在其中矣；学也，禄在其中矣。君子忧道不忧贫。</li></ol><p>缠中说禅白话直译<br>子曰∶君子谋道不谋食。耕也，馁在其中矣；学也，禄在其中矣。君子忧道不忧贫。<br>孔子说：“闻、见、学、行”“圣人之道”的君子，按“道之谋”谋划而不按“食之谋”谋划。以人的欲望饥饿为基础的生产，新的欲望饥饿就在其中啊；以人与天地关系中对照、校对确定人之所需，福运、真正的幸福就在其中啊。君子只担忧如何“闻、见、学、行”“圣人之道”的“道之谋”，而不担忧“馁、耕、食” “食之谋”的恶性循环必然导致的人在物质与精神上的贫穷。</p><p>自注： 经常的被不知道来源的焦虑所裹挟，思考一下来源有嘈杂的网络环境，以贩卖焦虑为谋利的文章。君子谋道不谋食，这个时代，饿死还是很难的，解决食之谋的方法还是有很多的。减掉一些愿望，不做一些没有实现可能的幻想，仔细想想我有的，然后再想我这么使用我有的来实现我想要的。</p><h1 id="第四十二句"><a href="#第四十二句" class="headerlink" title="第四十二句"></a>第四十二句</h1><ol start="42"><li>子曰：君子不器。</li></ol><p>缠中说禅白话直译<br>子曰∶君子不器。<br>孔子说：君子不相。</p><h1 id="第四十三句"><a href="#第四十三句" class="headerlink" title="第四十三句"></a>第四十三句</h1><ol start="43"><li>子曰：古之学者为己；今之学者为人。</li></ol><p>缠中说禅白话直译<br>子曰：古之学者为己；今之学者为人。<br>孔子说：无论古今，真正的学问与学人，都不离“内圣外王”、“为己为人”的一体之学。</p><p>自注：“是故内圣外王之道，暗而不明，郁而不发，天下之人，各为其所欲焉，以自为方。”<br>在王阳明看来，“内圣”的基础，是人之为人必要有的独立人格，恰如孔子所说的“古之学者为己”。在王阳明看来，唯有找到“天理”的指引，人才有内在驱动力，才能找到人生方向，而获得“天理”途径，便是通过“格物”进而“正心”，通过“知行合一”。通过书本获得知识算不得“理”，只有在生活中对其进行实践、验证进而获得的个人独特的理解，才算是“真知”。博学、审问、慎思、明辨、笃行者，皆所以为惟精而求惟一也。他如博文者，即约礼之功；格物致知者，即诚意之功；道问学即尊德性之功；明善即诚身之功：无二说也。”观诸圣之一学：基督教曰树一、恒一；伊斯兰曰独一无二；印度教曰不二； 佛教曰三昧(一境)；道教曰贞一；黄帝曰守一； 管子曰专一； 老子曰执一； 孔子曰精一； 山人说的就是一个一，故人戏称山人为一先生、不二先生。人要有所作为就必须独善其一，国家民族之强盛势必用一，有统一的思想，有惟一的民族哲学理念。</p><h1 id="第四十四句"><a href="#第四十四句" class="headerlink" title="第四十四句"></a>第四十四句</h1><ol start="44"><li>子曰：三年学不至，於榖不易，得也。</li></ol><p>缠中说禅白话直译<br>子曰：三年学不至，於榖不易，得也。<br>孔子说：多年闻“圣人之道”、见“圣人之道”、“对照”“圣人”、在现实社会中不断地“校对”，虽然不能达到尽善尽美，但能对“圣人之道”的“学”达到一生不退转的位次，这才算是“学”有所得啊。</p><p>自注： 闻，见，学，行圣人之道，就要坚定的走下去。</p><h1 id="第四十五句"><a href="#第四十五句" class="headerlink" title="第四十五句"></a>第四十五句</h1><ol start="45"><li>子曰：学如不及，犹恐失之。</li></ol><p>缠中说禅白话直译<br>子曰：学如不及，犹恐失之。<br>孔子说：闻“圣人之道”、见“圣人之道”、“对照”“圣人”、在现实社会中不断地“校对”而不能达到尽善尽美，是因为踌躇、恐惧、疑虑使它迷失而不能直下承担。</p><h1 id="第四十六句"><a href="#第四十六句" class="headerlink" title="第四十六句"></a>第四十六句</h1><ol start="46"><li>子曰：学而不思则罔，思而不学则殆。<br>学和思本来就是一体的。</li></ol><p>缠中说禅白话直译<br>子曰：学而不思则罔，思而不学则殆。<br>孔子说：将差异性的“学”与同一性的“思”分开，都只能迷惘、疲怠而无所得。</p><h1 id="第四十七句"><a href="#第四十七句" class="headerlink" title="第四十七句"></a>第四十七句</h1><ol start="47"><li>子曰：唯！女子与小人为难、养也。近之则不孙，远之则怨。</li></ol><p>缠中说禅白话直译<br>子曰：唯！女子与小人为难、养也。近之则不孙，远之则怨。<br>孔子说：是的！你的儿女跟随小人而“闻、见、学、行”，就产生灾难、痒疾。依附小人，就失去子嗣；违背小人，就埋下仇恨。</p><h1 id="第四十八句"><a href="#第四十八句" class="headerlink" title="第四十八句"></a>第四十八句</h1><ol start="48"><li>子曰：唯上知与下愚不移。</li></ol><p>缠中说禅白话直译<br>子曰：唯上知与下愚不移。<br>孔子说：愿真正“见、闻、学、行”“圣人之道”的君子，结交、亲附没有智慧、充满贪婪、恐惧的小人而成就“见、闻、学、行”“圣人之道”的不退转。</p><h1 id="第四十九句"><a href="#第四十九句" class="headerlink" title="第四十九句"></a>第四十九句</h1><ol start="49"><li>子曰：温故而知新，可以为师矣。</li></ol><p>缠中说禅白话直译<br>子曰：温故而知新，可以为师矣。<br>孔子说：应当把“积聚、蕴藏故有的、经过时间沉淀、检验的智慧而保持智慧当下鲜活的创造与呈现”作为君子“见、闻、学、行”“圣人之道”所师法的目标啊。</p><p>自注： 温故而知新”有四解。</p><p>1、温故才知新，温习已学的知识，并且由其中获得新的领悟；</p><p>2、温故及知新，一方面要温习典章故事，另一方面又努力撷取新的知识；</p><p>3、温故，知新。随着自己阅历的丰富和理解能力的提高，回头再看以前看过的知识，总能从中体会到更多的东西；</p><p>4、是指通过回味历史，而可以预见，以及解决未来的问题。这才是一个真正的大师应该具有的能力。</p><h1 id="第五十句"><a href="#第五十句" class="headerlink" title="第五十句"></a>第五十句</h1><ol start="50"><li>子曰：吾十有五而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲不逾矩。</li></ol><p>缠中说禅白话直译<br>子曰：吾十有五而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲不逾矩。<br>孔子说：我十五岁的境界、所为用“从此闻见学行圣人之道”来标记，三十岁的境界、所为用“穷尽闻见学行圣人之道的现实可能位次”来标记，四十岁的境界、所为用“透彻闻见学行圣人之道现实可能位次的不患”来标记，五十岁的境界、所为用“闻见学行圣人之道让智慧依当下生存鲜活地呈现”来标记，六十岁的境界、所为用“遵循当下生存鲜活呈现的智慧而闻见学行圣人之道以成就内圣”来标记，七十岁的境界、所为用“依从民心期望但不超越闻见学行圣人之道在当下现实中可能实现位次而成就外王”来标记。</p><h1 id="第五十一句"><a href="#第五十一句" class="headerlink" title="第五十一句"></a>第五十一句</h1><ol start="51"><li>子曰：君子，食无求饱，居无求安；敏於事而慎於言；就有，道而正焉；可谓好学也已。</li></ol><p>缠中说禅白话直译<br>子曰：君子，食无求饱，居无求安；敏於事而慎於言；就有，道而正焉；可谓好学也已。<br>孔子说：“闻见学行”“圣人之道”的人，对欲望不贪求从而满足，对生存的环境不贪求从而安身；通过当下的事情去印证，使得理论、言论顺应当下的实际；对现实究底穷源，使现实行“圣人之道”而在现实中成就之，称之为“好学”，是适当的啊。</p><p>自注： 走上圣人之道的人，把对欲望的不贪求看作满足。</p><h1 id="第五十二句"><a href="#第五十二句" class="headerlink" title="第五十二句"></a>第五十二句</h1><ol start="52"><li>子曰：十室之邑，必有忠信如丘者焉，不如丘之好学也。</li></ol><p>缠中说禅白话直译<br>子曰：十室之邑，必有忠信如丘者焉，不如丘之好学也。<br>孔子说：所有国家，倘若有遵从我的“忠信”标准的在其中，不若有遵从我的“好学”标准的在其中。</p><h1 id="第五十三句"><a href="#第五十三句" class="headerlink" title="第五十三句"></a>第五十三句</h1><ol start="53"><li>子曰：三人行，必有我师焉：择其善者而从之，其不善者而改之。</li></ol><p>缠中说禅白话直译<br>子曰：三人行，必有我师焉：择其善者而从之，其不善者而改之。<br>孔子说：与“君、父、师”同行，倘若有让我师法的在此：选取他们完善的并在当下现实更广泛的范围应用、检验，选取他们不完善的并在当下现实中不断修改、完善。</p><h1 id="第五十四句"><a href="#第五十四句" class="headerlink" title="第五十四句"></a>第五十四句</h1><ol start="54"><li>子夏曰：日知其所亡，月无忘其所能，可谓好学也已矣！<br>钱穆：子夏说：“每天能知道所不知道的，每月能不忘了所已能的，可说是好学了。”<br>知识是知识，技术高于知识，心法高于技术，法则高于心法。</li></ol><h1 id="第五十五句"><a href="#第五十五句" class="headerlink" title="第五十五句"></a>第五十五句</h1><ol start="55"><li>子夏曰：仕而优则学；学而优则仕。<br>钱穆：子夏说：“仕者有余力宜从学。学者有余力宜从仕。”</li></ol><h1 id="第五十六句"><a href="#第五十六句" class="headerlink" title="第五十六句"></a>第五十六句</h1><ol start="56"><li>子夏曰：百工居肆以成其事；君子学以致其道。<br>钱穆：子夏说：“百工长日居住肆中以成其器物，君子终身在学之中以求致此道。”<br>直译大致就是：就像各种工匠在手工业作坊里为完成他们的制作，君子在学中为完成他们的事业。</li></ol><h1 id="第五十七句"><a href="#第五十七句" class="headerlink" title="第五十七句"></a>第五十七句</h1><ol start="57"><li>子谓子夏曰：女为君子儒！无为小人儒！<br>钱穆：先生对子夏道：“你该为一君子儒，莫为一小人儒。”</li></ol><p>自注： 这句话更像老人对孩子的劝诫，重点就是看怎么解释这君子儒和小人儒了。</p><h1 id="第五十八句"><a href="#第五十八句" class="headerlink" title="第五十八句"></a>第五十八句</h1><ol start="58"><li>哀公问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不迁怒，不贰过。不幸短命死矣，今也则亡，未闻好学者也。”<br>钱穆：鲁哀公问孔子道：“你的学生们，哪个是好学的呀？”孔子对道：“有颜回是好学的，他有怒能不迁向别处，有过失能不再犯。可惜短寿死了，目下则没有听到好学的了。”</li></ol><h1 id="第五十九句"><a href="#第五十九句" class="headerlink" title="第五十九句"></a>第五十九句</h1><ol start="59"><li>季康子问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不幸短命死矣！今也则亡。”<br>钱穆：季康子问孔子：“你的弟子哪个是好学的呀？”孔子对道：“有颜回是好学的，不幸短命死了，现在是没有了。”</li></ol><h1 id="第六十句"><a href="#第六十句" class="headerlink" title="第六十句"></a>第六十句</h1><ol start="60"><li>子曰：语之而不惰者，其回也与？</li></ol><p>缠中说禅白话直译<br>子曰：语之而不惰者，其回也与？<br>孔子说：任何人与他辩论而他都能语不衰败的所谓能辩之士，难道只有颜回吗？</p><h1 id="第六十一句"><a href="#第六十一句" class="headerlink" title="第六十一句"></a>第六十一句</h1><ol start="61"><li>子贡问君子。子曰：先行其言而后从之。</li></ol><p>缠中说禅白话直译<br>子贡问君子。子曰：先行其言而后从之。<br>子贡问君子，孔子说：“先使自己的言论、思想以及相应的行为一以贯之，然后再使之广泛。”</p><h1 id="第六十二句"><a href="#第六十二句" class="headerlink" title="第六十二句"></a>第六十二句</h1><ol start="62"><li>子贡问曰：“赐也何如？”子曰：“女，器也。”曰：“何器也？”曰：“瑚琏也。”</li></ol><p>缠中说禅白话直译<br>子贡问曰：“赐也何如？”子曰：“女，器也。”曰：“何器也？”曰：“瑚琏也。”<br>子贡问：“我，怎么样？”孔子说：“你，“器”呀。”问：“什么器皿？”答：“宗庙里盛黍稷的瑚琏那样的名贵器皿”</p><h1 id="第六十三句"><a href="#第六十三句" class="headerlink" title="第六十三句"></a>第六十三句</h1><ol start="63"><li>子贡问曰：“有一言而可以终身行之者乎？”子曰：“其恕乎？己所不欲，勿施於人。”</li></ol><p>缠中说禅白话直译<br>子贡问曰：“有一言而可以终身行之者乎？”子曰：“其恕乎？己所不欲，勿施於人。”<br>子贡问：“有可以终身一而贯之的言论吗？”孔子说：“自己不想要的就不施加给别人，难道就是“恕”吗？”</p><h1 id="第六十四句"><a href="#第六十四句" class="headerlink" title="第六十四句"></a>第六十四句</h1><ol start="64"><li>子贡曰：“我不欲人之加诸我也，吾亦欲无加诸人。”子曰：“赐也，非尔所及也。”</li></ol><p>缠中说禅白话直译<br>子贡曰：“我不欲人之加诸我也，吾亦欲无加诸人。”子曰：“赐也，非尔所及也。”<br>子贡问：“我不想别人诬枉我，我也不想诬枉别人。”孔子说：“子贡啊，这不是你所能达到的。”</p><h1 id="第六十五句"><a href="#第六十五句" class="headerlink" title="第六十五句"></a>第六十五句</h1><ol start="65"><li>子曰：“赐也，女以予为多学而识之者与？”对曰：“然，非与？”曰：“非也！予一以贯之。”</li></ol><p>缠中说禅白话直译<br>子曰：“赐也，女以予为多学而识之者与？”对曰：“然，非与？”曰：“非也！予一以贯之。”<br>孔子问：“子贡啊，你把我当成不断学习从而了解现实当下的人吗？”子贡回答：“对，不是这样吗？”孔子说：“不是啊，我只是直下承担当下现实而贯通它。”</p><h1 id="第六十六句"><a href="#第六十六句" class="headerlink" title="第六十六句"></a>第六十六句</h1><ol start="66"><li>子曰：“参乎！吾道一以贯之。”曾子曰：“唯。”子出。门人问曰：“何谓也？”曾子曰：“夫子之道，忠恕而已矣。”</li></ol><p>缠中说禅白话直译<br>子曰：“参乎！吾道一以贯之。”曾子曰：“唯。”子出。门人问曰：“何谓也？”曾子曰：“夫子之道，忠恕而已矣。”<br>孔子说：“曾参啊！我“闻见学行”圣人之道一以贯之。”曾参说：“是。”孔子出去。孔子的其他弟子问：““一以贯之”是什么意思？”曾参回答：“老师的道理，只是“尽已之心以待人，推己之心以及人”罢了。”</p><h1 id="第六十七句"><a href="#第六十七句" class="headerlink" title="第六十七句"></a>第六十七句</h1><ol start="67"><li>有子曰：其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道生。孝弟也者，其为仁之本与！<br>钱穆：有子说：“若其人是一个孝弟之人，而会存心喜好犯上的，那必很少了。若其人不喜好犯上，而好作乱的，就更不会有了。君子专力在事情的根本处，根本建立起，道就由此而生了。孝弟该是仁道的根本吧？”</li></ol><h1 id="六十八句"><a href="#六十八句" class="headerlink" title="六十八句"></a>六十八句</h1><ol start="68"><li>孟懿子问孝。子曰：“无违”。樊迟御，子告之曰：“孟孙问孝於我，我对曰，”无违。””樊迟曰：“何谓也？”子曰：“生，事之以礼；死，葬之以礼，祭之以礼。”</li></ol><p>缠中说禅白话直译<br>孟懿子问孝。子曰：“无违”。樊迟御，子告之曰：“孟孙问孝於我，我对曰，”无违。””樊迟曰：“何谓也？”子曰：“生，事之以礼；死，葬之以礼，祭之以礼。”<br>孟懿子问孝。孔子说：“不要离开。”樊迟替孔子赶车，孔子对他说：“孟孙向我问孝，我回答说：“不要离开”。”樊迟说：“什么意思？”孔子道：“父母在世，用社会当下约定俗成的规范去侍奉他们；父母去世，用社会当下约定俗成的规范去安葬、祭祀他们。”</p><h1 id="第六十九句"><a href="#第六十九句" class="headerlink" title="第六十九句"></a>第六十九句</h1><ol start="69"><li>子游问孝。子曰：“今之孝者，是谓能养。至於犬马，皆能有养；不敬，何以别乎。”</li></ol><p>缠中说禅白话直译<br>子游问孝。子曰：“今之孝者，是谓能养。至於犬马，皆能有养；不敬，何以别乎。”<br>子游问孝。孔子说：“能养父母就被认为是现在的孝了。甚至狗和马，都会有人养；如果内心不敬，又用什么来区别这两者？”</p><h1 id="第七十句"><a href="#第七十句" class="headerlink" title="第七十句"></a>第七十句</h1><ol start="70"><li>孟武伯问孝。子曰：“父母唯其疾之忧。”</li></ol><p>缠中说禅白话直译<br>孟武伯问孝。子曰：“父母唯其疾之忧。”<br>孟武伯问孝，孔子说：“（孝就是）纵使自己生病也担忧父母的那种当下产生的感情。”</p><h1 id="第七十一句"><a href="#第七十一句" class="headerlink" title="第七十一句"></a>第七十一句</h1><ol start="71"><li>子夏问孝。子曰：色难。有事，弟子服其劳；有酒食，先生馔，曾是以为孝乎？<br>缠中说禅白话直译<br>子夏问孝。子曰：色难。有事，弟子服其劳；有酒食，先生馔，曾是以为孝乎？<br>子夏问孝，孔子说：“有事故，让年轻人负担其中的烦劳；有酒食，让年长者吃喝；但如果这些行为不是发自当下的情感，只是由于一种道德规范的力量，内心不情愿甚至在外显露出脸色为难，那么，难道就能把这种行为当成孝吗？</li></ol><h1 id="第七十二句"><a href="#第七十二句" class="headerlink" title="第七十二句"></a>第七十二句</h1><ol start="72"><li>子曰：父母在，不远游，游必有方。</li></ol><p>缠中说禅白话直译<br>子曰：父母在，不远游，游必有方。<br>孔子说：“当父母健在时，即使是游学也不能到偏远险恶之地，否则一定招致旁人或命运的诅咒。”</p><h1 id="第七十三句"><a href="#第七十三句" class="headerlink" title="第七十三句"></a>第七十三句</h1><ol start="73"><li>子曰：父母之年，不可不知也。一则以喜，一则以惧。</li></ol><p>缠中说禅白话直译<br>子曰：父母之年，不可不知也。一则以喜，一则以惧。<br>孔子说：“父母的年龄、生日等，不能不常常挂念以至能脱口而出。这种当下的情感，一方面带着欢喜，一方面带着忧惧，悲欣交集。</p><h1 id="第七十四句"><a href="#第七十四句" class="headerlink" title="第七十四句"></a>第七十四句</h1><ol start="74"><li>子曰：君子喻於义，小人喻於利。</li></ol><p>缠中说禅白话直译<br>子曰：君子喻於义，小人喻於利。<br>孔子说：“君子被各种现实社会结构以及对应的各种道德、法度等规范的关系之网中蕴藏的力量所开导，小人被利益、利害关系所组成的现实社会结构以及其对应的一套现实运行机制的关系之网中蕴藏的力量所开导。”</p><h1 id="第七十五句"><a href="#第七十五句" class="headerlink" title="第七十五句"></a>第七十五句</h1><ol start="75"><li>子曰：君子周而不比，小人比而不周。</li></ol><p>缠中说禅白话直译<br>子曰：君子周而不比，小人比而不周。<br>孔子说：君子，见闻学行周遍而没有疏漏，却不会让别人和自己步调一致、比肩而行；小人，让别人和自己步调一致、比肩而行，见闻学行却不能周遍而没有疏漏。</p><h1 id="第七十六句"><a href="#第七十六句" class="headerlink" title="第七十六句"></a>第七十六句</h1><ol start="76"><li>子曰：君子和而不同；小人同而不和。</li></ol><p>缠中说禅白话直译<br>子曰：君子和而不同；小人同而不和。<br>孔子说：君子相应而不聚集，小人聚集而不相应。</p><h1 id="第七十七句"><a href="#第七十七句" class="headerlink" title="第七十七句"></a>第七十七句</h1><ol start="77"><li>子曰：君子成，人之美；不成，人之恶。小人反是。</li></ol><p>缠中说禅白话直译<br>子曰：君子成，人之美；不成，人之恶。小人反是。<br>孔子说：人不断滋生美德，君子成就；人不断滋生恶行，君子不成。小人的成就与此相反。</p><h1 id="第七十八句"><a href="#第七十八句" class="headerlink" title="第七十八句"></a>第七十八句</h1><ol start="78"><li>子曰：君子之於天下也，无适也，无莫也，义之於比。</li></ol><p>缠中说禅白话直译<br>子曰：君子之於天下也，无适也，无莫也，义之於比。<br>孔子说：君子对于天下的一切，没有行为的归向，也没有思想的向往，甚至可以让自己的容貌呈现出小人的“比“相。</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔6</title>
    <link href="/2024/04/10/ganwu6/"/>
    <url>/2024/04/10/ganwu6/</url>
    
    <content type="html"><![CDATA[<h1 id="自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。"><a href="#自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。" class="headerlink" title="自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。"></a>自我成长就是自己塑造自己的过程，要克服自己行为习惯中的不良之处。要把自己应该做的做了，先痛苦后享受，不要先享受后痛苦。</h1><h1 id="“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。"><a href="#“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。" class="headerlink" title="“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。"></a>“道也者，不可须臾离也；可离，非道也。是故君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也”。</h1><p>君子慎独，自己一个人独处时也要保持着德行。控制自己的贪，嗔、痴、慢、疑。活着挺难的，做人是难的。</p><h1 id="情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。"><a href="#情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。" class="headerlink" title="情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。"></a>情绪的波动让我看不清事情的全貌，要尽力避免情绪的影响。</h1>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔5</title>
    <link href="/2024/04/08/ganwu5/"/>
    <url>/2024/04/08/ganwu5/</url>
    
    <content type="html"><![CDATA[<h1 id="活着需要的能力"><a href="#活着需要的能力" class="headerlink" title="活着需要的能力"></a>活着需要的能力</h1><h2 id="基本生存能力"><a href="#基本生存能力" class="headerlink" title="基本生存能力"></a>基本生存能力</h2><p>1.自我安全保障能力<br>2.基础急救<br>3.基本生存<br>4.方位感</p><h2 id="基础工作能力"><a href="#基础工作能力" class="headerlink" title="基础工作能力"></a>基础工作能力</h2><p>1.制作个人简历<br>2.时间管理能力-如何使用日历和计划清单<br>3.基础写作<br>4.公众讲话<br>5.有效沟通<br>6.基础电脑操作技术<br>7.基础的媒体和文档管理能力<br>8.OFFICE的应用能力<br>9.研究和探索能力</p><h2 id="家务处理能力"><a href="#家务处理能力" class="headerlink" title="家务处理能力"></a>家务处理能力</h2><p>1.如何打扫卫生<br>2.基础烹饪能力<br>3.基础家装修理能力</p><h2 id="财务管理能力"><a href="#财务管理能力" class="headerlink" title="财务管理能力"></a>财务管理能力</h2><p>1.制作家庭预算<br>2.制作家庭账本<br>3.基本投资能力<br>4.基本谈判能力</p><h2 id="自我认知能力"><a href="#自我认知能力" class="headerlink" title="自我认知能力"></a>自我认知能力</h2><p>1.搞清楚自己的使命、方向和人生目标的能力<br>2.平衡生活的能力<br>3.探索并清晰自己的价值观系统<br>4.管理自己情绪的能力</p><h2 id="人际沟通能力"><a href="#人际沟通能力" class="headerlink" title="人际沟通能力"></a>人际沟通能力</h2><p>1.基本礼节常识<br>2.幽默感<br>3.亲密关系的沟通和爱的能力<br>4.表达和赞赏<br>5.接受批评的能力</p><h2 id="思维认知能力"><a href="#思维认知能力" class="headerlink" title="思维认知能力"></a>思维认知能力</h2><p>1.批判性思维（本质思考）<br>2.整合思维能力（迁移思考）<br>3.解决问题能力<br>4.自我控制能力（安排规律的生活作息）<br>5.养生的能力</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔4</title>
    <link href="/2024/04/07/ganwu3/"/>
    <url>/2024/04/07/ganwu3/</url>
    
    <content type="html"><![CDATA[<h1 id="王国维先生曾在《人间词话》中写到对人生三重境界的感悟："><a href="#王国维先生曾在《人间词话》中写到对人生三重境界的感悟：" class="headerlink" title="王国维先生曾在《人间词话》中写到对人生三重境界的感悟："></a>王国维先生曾在《人间词话》中写到对人生三重境界的感悟：</h1><p>“古今成大事业、大学问者，必经过三种境界。‘昨夜西风凋碧树，独上高楼，望尽天涯路’，此第一境界；‘衣带渐宽终不悔，为伊消得人憔悴’，此第二境界；‘众里寻他千百度，蓦然回首，那人却在，灯火阑珊处’，此第三境界。”</p><h2 id="第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》"><a href="#第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》" class="headerlink" title="第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》"></a>第一境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”出自北宋晏殊《蝶恋花·槛菊愁烟兰泣露》</h2><p>在西风的狂吹下，枝繁叶茂的绿树也开始凋谢了，表示形式非常危急，环境十分恶劣，在这种状态下作者夜不成眠，辗转反侧，为自己前途命运无比担忧，但他并没有丧失信心，因此颓废，而是想要努力克服困难，力求上进，争取找到自己的前进方向。于是，作者愤然起身，独上高楼，高瞻远瞩，想要望尽天涯海角，找到前进的路。在这一境界中，可以看做人涉世不久，对人生的无比迷茫，正如现在刚毕业的大学生。但是在迷茫中有多少人因此而坠入歧途，自暴自弃，人生路漫漫，我们也应该上下而求索。正如鲁迅先生所说的一句话：“世界上本没有路，走的人多了，也便成了路”</p><p>注：走出自己的路，见路不走，实事求是。（2024&#x2F;5&#x2F;3）</p><h2 id="第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》："><a href="#第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》：" class="headerlink" title="第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》："></a>第二境界：“衣带渐宽终不悔，为伊消得人憔悴。”出自北宋柳永《蝶恋花·伫倚危楼风细细》：</h2><p>柳永如此艳丽之词，也被王国维拿来说明学问之事。诗人所忧之事，是“相思”，但相思到如此地步，我只有柳永能做到了，可见柳永真是一个重情之人。联系到人生，做一件事能专一到这种地步，不成功都难。继第一阶段的迷茫之后，在这一阶段中便有了目标了，在追逐目标的过程中，必然会遇到许多的困难，而我们要做的就是像柳永想念女子一样，即使被折磨得瘦骨伶仃，形容憔悴，我始终不放弃自己的目标，一往直前。</p><h2 id="第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。”-出自南宋辛弃疾《青玉案·元夕》"><a href="#第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。”-出自南宋辛弃疾《青玉案·元夕》" class="headerlink" title="第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。” 出自南宋辛弃疾《青玉案·元夕》"></a>第三境界：“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。” 出自南宋辛弃疾《青玉案·元夕》</h2><p>寻觅了千百次，却在无意间看到那人在灯火阑珊处。在苦苦追寻，历经磨难之后，总算看到惊喜了，之前的付出都有了回报。在人生的旅途中，在追求成功的路上，我们时常会陷入迷茫、困惑、苦恼中，甚至怀疑自己有没有选错目标，还该不该坚持下去，这些都很正常的，毕竟成功哪有那么容易呢？在这场旅途中，必定有很多人中途放弃，马云有句话说得好：“今天很残酷，明天更残酷，后天很美好，但大多数人都死在明天晚上，真正的英雄才能见到后天的太阳。”成功总是不经意间到来，这是一个人历尽千帆之后上天赐予的惊喜。这个惊喜有时来的很晚，但只要我们一直坚持，他迟早是要来的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔3</title>
    <link href="/2024/04/06/ganwu2/"/>
    <url>/2024/04/06/ganwu2/</url>
    
    <content type="html"><![CDATA[<h1 id="学习最重要的是什么？"><a href="#学习最重要的是什么？" class="headerlink" title="学习最重要的是什么？"></a>学习最重要的是什么？</h1><p>清醒，知道自己在做什么？ 即明白此时此刻到底在做什么。这一刻我在干什么？正在解决什么问题？这一天我学到了什么东西？取得了什么进步？不断回答这些问题的过程就是不断给予大脑正反馈的过程。时时刻刻用收获刺激大脑是学习上瘾和长时间专注学习的秘诀。</p><h1 id="什么在阻挡我进步？"><a href="#什么在阻挡我进步？" class="headerlink" title="什么在阻挡我进步？"></a>什么在阻挡我进步？</h1><p>我自己，我自己的思维在阻挡我的进步。为什么我在刷手机，看视频时不会觉得无聊，在放下手机后，反而觉得我聊了。这样当再次拿起手机时才会解决。我不喜欢这样没有掌控感的感觉，这样的感觉是矛盾的。这样的矛盾怎么来的勒？大脑中的思维和自我的觉知的意识之间的矛盾，怎么化解这种矛盾，我认为的解决方法是清楚自己在干嘛，清楚自己此时此刻在干嘛。建立一个观察者来观察自己。</p><h1 id="我为什么会刷手机？"><a href="#我为什么会刷手机？" class="headerlink" title="我为什么会刷手机？"></a>我为什么会刷手机？</h1><p>我为什么会刷手机，是因为我觉得无聊。我为什么觉得无聊？是因为没有事情可以做，或者是有些事情做了看不到效果，没有刷手机那样即时的奖励感。但是刷手机的过程中的信息是我不喜欢的，这些信息太单一，太主观（酒色财气，好色、贪财、逞气，为人生四戒），很不幸这些都有，当然也有好的，但是需要我去花时间去寻找。这也是一个麻烦事。回到怎么解决刷手机这件事，拿回对做事情的掌控感，明白自己现在在干嘛。就这样简单。</p><h1 id="阐述一下上瘾机制"><a href="#阐述一下上瘾机制" class="headerlink" title="阐述一下上瘾机制"></a>阐述一下上瘾机制</h1><p>上瘾的机理与多巴胺有关，并且随着上瘾行为的次数增多，大脑中的对与多巴胺的神经受体的敏感程度会下降，进而导致要想获取与之前相同的快感，就需要付出更多或更具有刺激性的行为来刺激多巴胺的生成。理论依据，行为和感觉是一体的，感觉会影响行为，行为会影响感觉，但是使用行为去影响感觉是更有理智的。第二点，失乐园理论，按照上述的描述，随着大脑中多巴胺快感产生的受体的敏感性下降，要想获得与之前相同的快感，需要付出的行为会更多，但是这样总会有个上限，导致人在做不出要产生多巴胺的行为时，就会陷入失乐园状态。第三点，痛苦和快感需要平衡，当我感觉痛苦时我需要快乐（多巴胺）来平衡，当我感到快乐时，快乐完成后会产生空虚（痛苦）来平衡快乐。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔2</title>
    <link href="/2024/04/05/ganwu0/"/>
    <url>/2024/04/05/ganwu0/</url>
    
    <content type="html"><![CDATA[<h1 id="怎么能做好一件事？"><a href="#怎么能做好一件事？" class="headerlink" title="怎么能做好一件事？"></a>怎么能做好一件事？</h1><p>做事，我将其解释为使用自己的方式去实现自己的想法。一个足以支撑去做这件事的动机，加上没有复杂的情绪影响，持之以恒的勤奋，那我觉得这件事的成功概率会大大的加强。一点点自己给自己的意义加上不要脑子的勤奋就可以成事了。为什么说不要脑子，因为带上脑子就会产生情绪，有了情绪就会产生内耗，当然对待情绪要辩证的看待，但是在做事初期一定不要带有情绪，在后面做这件事有一定积累后，就可以带上脑子了。<br>这里需要说明一点的是，成功是一个随机事件，不是必然事件。任何人做任何事情，都不能百分之百的把握成功。但是所有人都可以做一件事情，那就是持之以恒的勤奋，去提升成功的概率。知易行难，从小事做起，不断改变，积少成多。这是一个很重要的思维方法。</p><h1 id="心法、法则、技术之间的关系。"><a href="#心法、法则、技术之间的关系。" class="headerlink" title="心法、法则、技术之间的关系。"></a>心法、法则、技术之间的关系。</h1><p>心法胜于法则，法则胜于技术（方法）。<br>所谓技术，比如说使用进步本的技术来操作管理各个学科的知识，知识会遗忘，技术则能熟能生巧，历久弥新。<br>所谓法则，例如学习过程中唯一不变的目的就是进步，认识到这个法则之后所开发的技术都是为了这个法则服务的。世界上没有普世的方法，却有普世的法则。比如要把一件事做好，就要不断学习，不断学习的目的是不断进步，进而能把事情做的更好。<br>所谓心法，是指人对生命、对世界的基本态度和根本认知。例如，“自胜者强”，给强者下了定义，不胜别人，只胜自己的人是强者，战胜自己内心的情绪，内心的恐惧，控制自己不再内耗的人是强者。以自胜者强的定义去看待别人和自己，自然形成心态，进而形成心法。自胜者强包含了学习进步的法则，但是学习进步的法则取不涵盖自胜者强。</p><h1 id="我该怎么才能做好一件事。"><a href="#我该怎么才能做好一件事。" class="headerlink" title="我该怎么才能做好一件事。"></a>我该怎么才能做好一件事。</h1><p>注意这里的主语是我，我该怎么才能做好一件事？ 那就是不要带着情绪去做一件事，带着脑子去做这件事就行了，凡事想多了，就做不成了。<br>不要急躁，慢慢来才最快。不要傲慢，不要觉得这个太简单就不去做，认识是要不断重复的。<br>要早睡早起，11点之前睡觉，在早上7点起床就是一件很容易的一件事。但是要是在11点之后睡觉，要在早上7点起床就不是那么一件容易的事情了。<br>不要给自己增加烦劳，世上本无事，庸人自扰之。<br>要给自己制定一个计划，在本子上写上自己要做的事情。（这点我确实没有做好，目前来讲，我都是随性而为。）<br>不要在做的过程中去看距离结果还有多远，不要和别人分享自己的喜悦，万一没有成功岂不是很尴尬。生命是一修行。<br>勤奋，不要让事情来裹挟着你，而是让我去驱赶着事情。生命是场旅行，我可以当过过客，也可以成为风景中的一部分。<br>体验是最重要的，体验自己的喜悦，体验自己的悲伤，体验自己的失落，体验自己的愉悦。活在当下，不要考虑过去和未来。<br>不要在乎结果，做好当下的事情，现在的事情做好了，结果不会太差。听天命，尽人事。<br>内心的法则和社会的法则不同，甚至相反，但是我可以使用心法来统领。内圣外王，或许先要内圣，而后外王。<br>决心来自一个明确的、具体的理由。记住是一个理由，唯一的理由，不是许多理由。</p><p>2024&#x2F;5&#x2F;3<br>注：条件，前提，因由，不同条件产生不同的表象，但是背后却有相同的本质。冰、雪、雾、霜、露。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一点小确幸</title>
    <link href="/2024/04/04/tupian/"/>
    <url>/2024/04/04/tupian/</url>
    
    <content type="html"><![CDATA[<h1 id="这里是图片"><a href="#这里是图片" class="headerlink" title="这里是图片"></a>这里是图片</h1><h2 id="清晨的树"><a href="#清晨的树" class="headerlink" title="清晨的树"></a>清晨的树</h2><p><img src="/pic/79f388cc7d53668d55773baf30bb10c.jpg"></p><h2 id="早晨的红日"><a href="#早晨的红日" class="headerlink" title="早晨的红日"></a>早晨的红日</h2><p><img src="/pic/2.jpg"></p><h2 id="一个不知名的地方"><a href="#一个不知名的地方" class="headerlink" title="一个不知名的地方"></a>一个不知名的地方</h2><p><img src="/pic/l2.jpg"></p><h2 id="图书馆"><a href="#图书馆" class="headerlink" title="图书馆"></a>图书馆</h2><p><img src="/pic/jc1.jpg"></p><h2 id="好看的壁纸"><a href="#好看的壁纸" class="headerlink" title="好看的壁纸"></a>好看的壁纸</h2><p><img src="/pic/3.jpg"><br><img src="/pic/3.png"><br><img src="/pic/v2-3a1fb23e19b2448033a9b7333941f465_r.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>图片</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔</title>
    <link href="/2024/04/04/ganwu1/"/>
    <url>/2024/04/04/ganwu1/</url>
    
    <content type="html"><![CDATA[<h1 id="随笔"><a href="#随笔" class="headerlink" title="随笔"></a>随笔</h1><h2 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h2><p>为什么要写下这篇随笔，最近经常上网，突然觉得什么都不真实，所以我决定写下一些我觉得真实的想法。并且决定这段时间先不上网看他人的对于这个世界的解释，太嘈杂了。先看看书，集百家之言，成一家之说。先把自己的世界观，人生观，价值观建立完成，才能更好的和人交流，辩论。</p><p>  见相非相，即见如来</p><p>很多时候，我们这个世界的认识是荒谬到极点而不自知的。人是立场的，好恶的，我们总是只相信自己愿意相信的东西。我们习惯于先形成观点，然后再寻找既有立场的正面证据，在不知不觉中偏离真实。改变总是很难的，一点点的进步总是让我感到欢喜的。每个人的生命中都隐含着一颗觉知的种子，一旦唤醒，便势不可挡，在追求真知、真实的路上一去不反。</p><p>世界是真实的，世界就是那样，但是世界的解释权却不再世界本身，而在于我们。解释权归我们所有，对世界的解释的方法造就了不同的人，并且我们对世界的解释的方式会受到各种各样的影响，正确的，错误的，客观的，主观的。不同的解释方式产生不同的行为，不同的行为会造成不同的结果，造成的结果无论好坏都要受着，这是因果，不可能逃脱。</p><p>最近觉得网络的东西太过于无聊，各种各样的信息，不同立场的人，不同角度的人。热爱生活的，厌世嫉俗的。不同境遇的，充满选择的人和没有选择的人。世界就是这么的参差不齐。回到自我本身，我该怎么去解释这个世界。最近读马克思，实事求是是必要的，实践是检验真理的唯一标准是必要的，与其去网络上看别人任何解释这个世界，不如自己拿起笔写下自己对世界的解释。别人的经验是只能参照的，</p><p>要有自我主动权，解放思想，我们要有敢于摸着石头过河的勇气。成功了，增长能力也有了经验；失败了，就有了一次知道错误出在哪儿的认识。不要高估自己的能力，不要傲慢。人都有一个通病：说到别人的时候，就是这也不行，那也不行；而说到自己的时候，就有一种世外高人的感觉。平庸来自傲慢，如果人人生而伟大，如果每个人本来都是生命的奇迹。那么，傲慢毁掉了多少人？<br>成功的经验可以借鉴但是不可以复制，“见路不走”是不唯经验教条。成功的经验我们一般不能复制，因为那个经验发生的条件已成过去式，现在的条件只符合现在时。众生总是看到事物的各种表相而偏离本质，要见，见自本性，向内求解，以自己的觉性来看待事物。大家都是人，别人能做到的我也能做到。事实不是这样的，别人能做到的我不一定能做到，‘都是人’只是其中的一个条件，只有我具备了别人能做到的全部条件，我才可能做到，而事实上我很难悉数复制别人的条件，只有根据我的条件去做我能做到的，才是不脱离实际的。<br>对人必须有理有节，对己则可自由豁达。小到个人，家庭，大到国家，社会，自由都是有前提的；不同的人有不同的自由。</p><p>认识是慢慢的认识的，不可能突然从小学生到大学生的，这需要个过程。如果想要从小学生到大学生，这是不可能的。想可想之想，能可能之能。<br>关于认识我觉得实践论中的阐述是最好的，这里直接粘贴了，实践过程中，开始只是看到过程中各个事物的现象方面，看到各个事物的片面，看到各个事物之间的外部联系。例如有些外面的人们到延安来考察，头一二天，他们看到了延安的地形、街道、屋宇，接触了许多的人，参加了宴会、晚会和群众大会，听到了各种说话，看到了各种文件，这些就是事物的现象，事物的各个片面以及这些事物的外部联系。这叫做认识的感性阶段，就是感觉和印象的阶段。（认识的第一阶段）社会实践的继续，使人们在实践中引起感觉和印象的东西反复了多次，于是在人们的脑子里生起了一个认识过程中的突变（即飞跃），产生了概念。概念这种东西已经不是事物的现象，不是事物的各个片面，不是它们的外部联系，而是抓着了事物的本质，事物的全体，事物的内部联系了。《三国演义》上所谓“眉头一皱计上心来”，我们普通说话所谓“让我想一想”，就是人在脑子中运用概念以作判断和推理的工夫。这是认识的第二个阶段。<br>外来的考察团先生们在他们集合了各种材料，加上他们“想了一想”之后，他们就能够作出“共产党的抗日民族统一战线的政策是彻底的、诚恳的和真实的”这样一个判断了。在他们作出这个判断之后，如果他们对于团结救国也是真实的的话，那末他们就能够进一步作出这样的结论：“抗日民族统一战线是能够成功的。”这个概念、判断和推理的阶段，在人们对于一个事物的整个认识过程中是更重要的阶段，也就是理性认识的阶段。  认识的真正任务在于经过感觉而到达于思维，到达于逐步了解客观事物的内部矛盾，了解它的规律性，了解这一过程和那一过程间的内部联系，即到达于论理的认识。<br>在低级阶段，认识表现为感性的，在高级阶段，认识表现为论理的，但任何阶段，都是统一的认识过程中的阶段。感性和理性二者的性质不同，但又不是互相分离的，它们在实践的基础上统一起来了。</p><p>关于做事（做事也是认识的范畴），也是这样，慢慢来才最快。这里同样copy实践论中的内容。<br>常常听到一些同志在不能勇敢接受工作任务时说出来的一句话：没有把握。为什么没有把握呢？<br>因为他对于这项工作的内容和环境没有规律性的了解，或者他从来就没有接触过这类工作，或者接触得不多，因而无从谈到这类工作的规律性。及至把工作的情况和环境给以详细分析之后，他就觉得比较地有了把握，愿意去做这项工作。如果这个人在这项工作中经过了一个时期，他有了这项工作的经验了，而他又是一个肯虚心体察情况的人，不是一个主观地、片面地、表面地看问题的人，他就能够自己做出应该怎样进行工作的结论，他的工作勇气也就可以大大地提高了。只有那些主观地、片面地和表面地看问题的人，跑到一个地方，不问环境的情况，不看事情的全体（事情的历史和全部现状），也不触到事情的本质（事情的性质及此一事情和其他事情的内部联系），就自以为是地发号施令起来，这样的人是没有不跌交子的。<br>由此看来，认识的过程，第一步，是开始接触外界事情，属于感觉的阶段。第二步，是综合感觉的材料加以整理和改造，属于概念、判断和推理的阶段。只有感觉的材料十分丰富（不是零碎不全）和合于实际（不是错觉），才能根据这样的材料造出正确的概念和论理来。</p><p>实践是认识的来源，认识又可以变革实践。实践和认识的关系如同美和丑的关系，如同硬币的一体两面，是辩证的。通过实践而发现真理，又通过实践而证实真理和发展真理。从感性认识而能动地发展到理性认识，又从理性认识而能动地指导革命实践，改造主观世界和客观世界。实践、认识、再实践、再认识，这种形式，循环往复以至无穷，而实践和认识之每一循环的内容，都比较地进到了高一级的程度。这就是辩证唯物论的全部认识论，这就是辩证唯物论的知行统一观。</p><p>所以我的想法是，自己去实践，去认识，而不是去把他人带有主观的解释当作自己的解释，我不愿意吃别人的口水，关于这个世界我要亲自去尝一尝，去变革一下。</p><h2 id="注文"><a href="#注文" class="headerlink" title="注文"></a>注文</h2><p>注： 2024&#x2F;4&#x2F;30,又一次修改，不上网是真的难（哈哈哈哈）。<br>我看书的最终目的是集成百家之言，成一家之说，以自己的见解和实践去解释这个世界，改造这个世界。</p><p>认识论指出，成功秘诀就是坚持不懈。成功有时不需要精明，更依赖坚持，稳重坚守就是成功。不断的进步，从感性认识到理性认识。<br>2024&#x2F;5&#x2F;2日修改<br>解释一下什么是世界观，价值观，人生观。</p><ol><li>世界观(亦称“宇宙观”)，通常是指人们对整个世界(即对自然界、社会和人的思维)的根本看法。世界观不同，表现为人们在认识和改造世界时的立场、观点和方法的不同。一个人世界观、人生观的改变是一种根本的改变。世界观的基本问题是精神与物质、思维与存在、主观与客观的关系问题。在实践的基础上，实事求是，解放思想，尊重客观规律，注重调查研究，不断总结经验，开动脑筋想问题、办事情。</li><li>人生观，是人们对人生问题的根本看法。主要内容是对人生目的、意义的认识和对人生的态度，具体包括公私观、义利观、苦乐观、荣辱观、幸福观和生死观等。人生观是人们在人生实践和生活环境中逐步形成的。</li><li>价值观，是人们对价值问题的根本看法，是人们在一定历史条件下经过反复实践逐渐形成的一种与人的主观需要相连的判断好坏、是非、利弊、善恶的观念。包括对价值的实质、构成、标准的认识，这些认识的不同，形成了人们不同的价值观。每个人都是在各自的价值观的引导下，形成不同的价值取向，追求着各自认为最有价值的东西。</li></ol><p>阐述一下三者关系<br>世界观是人生观和价值观的基础，世界观决定着人生观和价值观。作为对人生意义和目的的特定理解的人生观，以及作为主体设定其价值目标和行为取向的价值观，都要以一定世界观作为思想基础，并支配其人生思考和选择的表现形式。世界观、人生观、价值观虽然各有自己的特定内容，但三者是统一的，不可分割的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>感悟</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>句子</title>
    <link href="/2024/04/02/juzhi0/"/>
    <url>/2024/04/02/juzhi0/</url>
    
    <content type="html"><![CDATA[<BR><h2 id="1"><a href="#1" class="headerlink" title="1"></a>1</h2><p>图难于其易，为大于其细。天下难事必作于易，天下大事必作于细。是以圣人终不为大，故能成其大。</p><h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><p>天有道，则无常道。事于道，则天有道看与事则无常，无常则明，明则通，则世事洞明。世事洞明则世事可治愈渐达佳境。</p><p>解释： 天如果真的有直指究竟的道，也肯定不是一直不变的道。做事依照着前人经验，则“道”看似有规律可循，其实世事无常没有相同一件事，懂得无常的道理就能把事情看清楚，把事情看清楚了就能做到通达。明白世事无常，才能看清一切事物“无常”的本质，看清本质就能做到通达，通达则洞明世事。世事洞明，才可能治愈一切事。看透世事无常来去皆好的本质，人就能做到通达，通达则洞明世事。世事洞明，自然可以治愈一切事。人能够活成这样的通透，当然就没有任何挂碍，也就步入了人生最妙的佳境!</p><h2 id="3"><a href="#3" class="headerlink" title="3"></a>3</h2><p>夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也，非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世，悲守穷庐，将复何及。</p><h2 id="4"><a href="#4" class="headerlink" title="4"></a>4</h2><p>为天地立心，为生民立命，为往圣继绝学，为万世开太平</p><p>（注：横渠四句教：第一句，为天地立心，这句话的前提是天地是没有心的，那什么有心，人有心？人有心之后，天地才有了心。人为天地所立的心称为道心，在认识论的层面去为天地所立心。人不知，人不相，人不愠。第二句，为生民立命，说人心，要帮助人构建价值观，要让人心基于正道，构建价值观。只有当一个人构建其了完备自洽的价值观，价值观得出了一个关于人生的终极价值判断，这个价值判断才叫做命，古时候叫“天命”，现在叫“使命”。有了使命我们才能用尽一生去追求，才不会犹豫不决，才不会彷徨不前，才能够“虽千万人吾往矣”。第三局，“为往圣继绝学”，说的是要学习之前的知识，要不断打磨之间的价值观，不断迭代这个时代的价值观。“苟日新，又日新，日日新”。第四句，为万世开太平，怎么才能为万世开太平？哲学家们只是用不用的方式解释世界,问题在于改变世界，实践，理论联系实践，中国的经典从来都是强调实践的，允执厥中”，也就是“中庸”，什么叫“中庸”？“中”的标准就是“仁”，“庸”则是“用”，是时时刻刻、千秋万世的“中用”，不实践怎么能中用呢？所以孔子说“游于艺”，不但要具备付诸实践的才能，还要在领域内追求极致，做到“游”，也就是“游刃有余”。唯有如此，才是“君子不器（而无不器）”，才能做到“无为而无不为”，才能“为万世开太平”。探讨自然与社会的基本规律，为民众摸索出一条共同遵行的大道，继承优良的传统文化，为后世开辟永久太平的基业。概括而言，就是探索精神，担当精神，奉献精神，使命精神。很高大上吗？不，这只是读书人本分而已。）</p><h2 id="5"><a href="#5" class="headerlink" title="5"></a>5</h2><p>真正的理性从来都是当下的，从来都是实践的，而实践，从来都是当下的理性。</p><h2 id="6"><a href="#6" class="headerlink" title="6"></a>6</h2><p>这是一个纷纷扰扰的滚滚红尘，众生沉迷而不求解脱，驱使着尘世中的我们有时也不得不加入其中滚一下；这是一个没有方向的名利场，大家每个人都有自己的执念，且价值观单一，在单一的价值观之下，为了同一个东西，大家便如丛林法则中的生物一样，让名利场成为一个绞肉机。</p><p>注：人总是要死亡的，生命的起点到终点，生命又有生命意义。这个过程就是意义，死亡是告诉我生命的意义就是感受过程，不用太多的去忧愁什么，天下本无事，庸人自扰之。 </p><p>焦虑，欲望，犹豫，焦虑对未来事件的过度担忧和恐慌，对未来的迷茫和没信心。怎么解决这个问题，感受过程，活在当下，专注于眼前的事，不必胡思乱想。静坐，或许是一种回归当下，摆脱头脑中的忧虑的方法。<br>欲望是有好坏之分的，好的欲望能帮我们正向成长，但是不好的欲望却是阻碍成长的绊脚石。要想破除欲望之贼，就要学会舍弃。舍弃不是失去，而是另一种获得。不要害怕舍弃，舍弃一件事物的终点和获得另一件事物的起点重合于一点。<br>犹豫，面对事情犹豫不决是一种很常见的现象。在决心做事时；就立刻去做，不要考虑事情的结果。做事的过程中不断磨砺自己，在事情中做到不以物喜，不以己悲。感受过程中的快乐，这样自然不会害怕因为选择错误而带来的失败。<br>“ 坐中静，破焦虑之贼；舍中得，破欲望之贼；事上练，破犹豫之贼，三贼皆破，则万事可成。”</p><p>长寿秘诀就是生活规律；成功秘诀就是坚持不懈。成功有时不需要精明，更依赖坚持，稳重坚守就是成功。）</p>]]></content>
    
    
    
    <tags>
      
      <tag>句子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/04/02/hello-world/"/>
    <url>/2024/04/02/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span> <br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
