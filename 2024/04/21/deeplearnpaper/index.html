

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="chenli">
  <meta name="keywords" content="">
  
    <meta name="description" content="我给自己挖了很多坑没有去填，只能慢慢填了，今天先填第一个坑。本人参考博客1-本人参考博客2-本人参考博客3-AlexNet的翻译本人参考的博客-模型结构发展简史 AlexNet 介绍论文原文链接AlexNet是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年ImageNet图像分类竞赛中提出的一种经典的卷积神经网络。AlexNet在 Ima">
<meta property="og:type" content="article">
<meta property="og:title" content="经典网络结构——AlexNet">
<meta property="og:url" content="https://chenlidbk.xyz/2024/04/21/deeplearnpaper/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="我给自己挖了很多坑没有去填，只能慢慢填了，今天先填第一个坑。本人参考博客1-本人参考博客2-本人参考博客3-AlexNet的翻译本人参考的博客-模型结构发展简史 AlexNet 介绍论文原文链接AlexNet是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年ImageNet图像分类竞赛中提出的一种经典的卷积神经网络。AlexNet在 Ima">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenlidbk.xyz/pic/paper_Alex_1.png">
<meta property="article:published_time" content="2024-04-21T13:47:32.000Z">
<meta property="article:modified_time" content="2024-04-22T03:20:56.111Z">
<meta property="article:author" content="chenli">
<meta property="article:tag" content="深度学习论文">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://chenlidbk.xyz/pic/paper_Alex_1.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>经典网络结构——AlexNet - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"chenlidbk.xyz","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  



  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>chenli</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="经典网络结构——AlexNet"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-04-21 21:47" pubdate>
          2024年4月21日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          10k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          84 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">经典网络结构——AlexNet</h1>
            
            
              <div class="markdown-body">
                
                <p>我给自己挖了很多坑没有去填，只能慢慢填了，今天先填第一个坑。<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/guzhao9901/article/details/118552085">本人参考博客1-</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/618545757">本人参考博客2-</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/hongbin_xu/article/details/80271291">本人参考博客3-AlexNet的翻译</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/ARYAD/article/details/107687362">本人参考的博客-模型结构发展简史</a></p>
<h1 id="AlexNet-介绍"><a href="#AlexNet-介绍" class="headerlink" title="AlexNet 介绍"></a>AlexNet 介绍</h1><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">论文原文链接</a><br>AlexNet是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年ImageNet图像分类竞赛中提出的一种经典的卷积神经网络。AlexNet在 ImageNet 大规模视觉识别竞赛中取得了优异的成绩，把深度学习模型在比赛中的正确率提升到一个前所未有的高度。因此，它的出现对深度学习发展具有里程碑式的意义。<br><a target="_blank" rel="noopener" href="https://github.com/aaron-xichen/pytorch-playground">可以参考的github仓库</a></p>
<ol>
<li>AlexNet的输入为RGB三通道大小的图像，图像的shape可以表述为（227x227x3）。AlexNet共包含5个卷积层（包含3个池化）和3个全连接层。其中每个卷积层都包含卷积核、偏置项、ReLU激活函数和局部响应归一化（LRN）模块。第1，2，5个卷积层后面都跟着一个最大池化层，后三个层为全连接层。最终的输出层为softmax（这里有一个很有意思的知识，softmax怎么将网络输出转化为概率值，后面再说。）</li>
</ol>
<p><img src="/pic/paper_Alex_1.png" srcset="/img/loading.gif" lazyload alt="AlexNet模型结构图"><br>这里需要指出的是，在网络设计上并非上图所示，上图包含了GPU通信的部分。这是因为当时的GPU内存的限制引起的，作者使用了两块GPU进行计算<br>废话不多说，直接上代码。代码来源为《动手深度学习》</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn, optim<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-meta"># 上面的部分为引入包操作，介绍一下上面引入的包的作用。</span><br><span class="hljs-meta"># time：这是python中的内置的模块，用于处理时间相关的操作。可以用来获取当前的时间，或者在程序中添加延迟。</span><br><span class="hljs-meta"># torch：这是pytorch库的主要部分，一个用于机器学习和深度学习的开源库。提高高效的张量（多维数组）计算（类似于Numpy）的方式，同时支持GPU计算（基于CUDA和CUDNN）</span><br><span class="hljs-meta"># torch.nn 是pytorch中的一个子模块，提供构建神经网络所需要的各种工具和组件。</span><br><span class="hljs-meta"># torch.optim也是pytorch中的一个子模块，提供各种优化算法，比如SGD，Adam和RMSProp等（这里给自己挖个坑）</span><br><span class="hljs-meta"># torch.torchvision，一个与PyTorch关联的库，专门用于处理图像和视频的计算机视觉任务。它提供许多预训练的模型，如ResNet，VGG和AlexNet等，同时还有常见的数据集，如ImageNet，CIFAR10/100，MNIST等。</span><br><br><span class="hljs-title">device</span> = torch.device(&#x27;cuda&#x27; <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> &#x27;cpu&#x27;)<br><span class="hljs-meta"># 这一句的作用是选取GPU训练还是CPU训练。</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">AlexNet</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">AlexNet</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.conv = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding（输出通道数，输出通道数，卷积核大小，步长，填充，这里又有坑，关于卷积后特征图应该怎么计算？）</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(), # <span class="hljs-type">ReLU</span>激活函数</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2), # kernel_size, stride 最大池化，3x3的池化层，步长为2.意思是一个3x3的二维矩阵，按照最大值来输出最大特征。</span><br><span class="hljs-class">            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(96, 256, 5, 1, 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2),</span><br><span class="hljs-class">            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span><br><span class="hljs-class">            # 前两个卷积层后不使用池化层来减小输入的高和宽</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(256, 384, 3, 1, 1), # 第三个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(384, 384, 3, 1, 1), # 第四个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(384, 256, 3, 1, 1), # 第五个卷积层</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(3, 2)</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">         # 这里全连接层的输出个数比<span class="hljs-type">LeNet</span>中的大数倍。使用丢弃层来缓解过拟合</span><br><span class="hljs-class">        self.fc = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Linear</span>(256*5*5, 4096), # 线性层，256*5*5为输入大小，4096为输出大小。</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout</span>(0.5), # 随机失活，<span class="hljs-type">AlexNet</span>的主要创新点之一。这里失活率为0.5</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(4096, 4096),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout</span>(0.5),</span><br><span class="hljs-class">            # 输出层。由于这里使用<span class="hljs-type">Fashion</span>-<span class="hljs-type">MNIST</span>，所以用类别数为10，而非论文中的1000</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(4096, 10), # 输出类为10.</span><br><span class="hljs-class">        )</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-title">img</span>): # 前向传播，forward在代码中需要自定义。在这里可以加入残差等操作。</span><br><span class="hljs-class">        feature = self.conv(<span class="hljs-title">img</span>)</span><br><span class="hljs-class">        output = self.fc(<span class="hljs-title">feature</span>.<span class="hljs-title">view</span>(<span class="hljs-title">img</span>.<span class="hljs-title">shape</span>[0], -1))</span><br><span class="hljs-class">        return output</span><br></code></pre></td></tr></table></figure>
<ol start="2">
<li>背景介绍，在AlexNet网络问世之前，大量的学者在进行图像分类、分割、识别的操作时，主要是通过对图像提取特征或特征+机器学习的方法，手工提取特征是非常难的事情，即特征工程。为了提升准确率或减少人工复杂度等种种原因。因此，学界一直认为，特征是不是可以进行学习？如果可以学习，特征之间的表示方法是什么？例如第一层为线或是点特征，第二层为线与点组成的初步特征，第三层为局部特征）？从这一思想出发，特征可学习且自动组合并给出结果，这是典型的“end-to-end” 。</li>
</ol>
<h1 id="论文阅读："><a href="#论文阅读：" class="headerlink" title="论文阅读："></a>论文阅读：</h1><p>先阐述一下论文结构</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">0</span>. 标题（title）<br><span class="hljs-attribute">0</span>.<span class="hljs-number">5</span>. 摘要（Abstract）<br><span class="hljs-attribute">1</span>. 介绍（Introduction）<br><span class="hljs-attribute">2</span>. 数据集（The Dataset）<br><span class="hljs-attribute">3</span>. 网络结构（The Architecture）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">1</span> ReLU非线性单元（ReLU Nonlinearity）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">2</span> 多GPU训练（Training <span class="hljs-literal">on</span> Multiple GPU）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">3</span> 局部响应和归一化（Local Response Normalization）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">4</span> 层叠池化（Overlapping Pooling）<br><span class="hljs-attribute">3</span>.<span class="hljs-number">5</span> 整体结构（Overall Architecture）<br><span class="hljs-attribute">4</span>. 减少过拟合（Reducing Overfitting） <br><span class="hljs-attribute">4</span>.<span class="hljs-number">1</span> 数据增强 （Data Augmentation）<br><span class="hljs-attribute">4</span>.<span class="hljs-number">2</span> 随机失活 （Dropout）<br><span class="hljs-attribute">5</span>. 学习细节 （Details of learning）<br><span class="hljs-attribute">6</span>. 结果 （Results）<br><span class="hljs-attribute">6</span>.<span class="hljs-number">1</span> 定性评估（Qualitative Evacuation）<br><span class="hljs-attribute">6</span>.<span class="hljs-number">2</span> 讨论（Discussion） <br></code></pre></td></tr></table></figure>
<p>这里需要说明的是由于markdown的限制和本人技术能力的欠缺。在这篇博文中不放公式，如果想看公式，请去看原论文，数学的公式才是最简洁的表达方式，前提是能够看懂，看懂了之后就像打开新世界的大门。感觉就像我有一双滑板鞋，我走到那就穿到哪。<br>0. 标题论文标题为<br>ImageNet Classification with Deep Convolutional Neural Networks<br>摘要： 我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。（开门见山，直接说干了什么。）在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率（这里使用的是错误率的评估指标，和我目前使用的Acc，Presion，Recall，召回率评估指标不一样。），这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成（这里挖一个坑，参数量和神经元数量的评估指标不一样。），其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。（这里挖个坑softMax的机制是怎么样的？）为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法（dropout，随机丢弃的机制是什么？），该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模式的一个变种，取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。</p>
<ol>
<li><p>介绍：目前，机器学习方法对物体识别非常重要。为了改善他们的表现（前提条件就是之前的表现不是很好），我们可以收集更大的数据集，训练更强大的模型，并使用更好的技术来防止过拟合。（这里指出减少过拟合的方法有增大数据集，更改模型结构，使用更好的优化技术。）直到最近，标记好图像的数据集相对还较小——大约上万的数量级（例如，NORB，Caltech-101&#x2F;256和CIFAR-10&#x2F;100）。使用这种规模的数据集可以很好地解决简单的识别任务，特别是如果他们增加了保留标签转换（label-preserving transformations）。例如，目前MNIST数字识别任务的最低错误率（&lt;0.3％）基本达到了人类的识别水平。但是物体在现实环境中可能表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。事实上，小图像数据集的缺点已是众所周知（例如，Pinto），但直到最近才可以收集到数百万的标记数据集。新的大型数据集包括LabelMe ，其中包含数十万个完全分割的图像，以及ImageNet ，其中包含超过15,000万个超过22,000个类别的高分辨率图像。（目前的研究的对象，研究现状。）<br>要从数百万图像中学习数千个类别，我们需要一个具有强大学习能力的模型。（这里暗示这篇文章的模型大小非常大，但是现在看来入门级把，毕竟是12年前的文章了，开山鼻祖了。）然而，物体识别任务的巨大复杂性意味着即使是像ImageNet这样大的数据集也不能完美地解决这个问题，所以我们的模型也需要使用很多先验知识来弥补我们数据集不足的问题。卷积神经网络（CNN）就构成了一类这样的模型（卷积神经网络可以获取先验的知识，来弥补数据集不足的问题，后面是卷积神经网络为什么能够实现获取先验知识。）。它们的容量可以通过改变它们的深度和宽度来控制，并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。<br>尽管CNN具有很好的质量，并且尽管其局部结构的效率相对较高，但将它们大规模应用于高分辨率图像时仍然显得非常昂贵。幸运的是，当前的GPU可以用于高度优化的二维卷积，能够加速许多大型CNN的训练，并且最近的数据集（如ImageNet）包含足够多的标记样本来训练此类模型，而不会出现严重的过度拟合。<br>本文的具体贡献如下：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。（codeing能力还是有的，我的目标就是能够实现自己的想法，Talk is cheap. Show me the code.这句话真的是令人兴奋）我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3章节。我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。（这点可能验证了特征是层级表示的，）最后，网络的规模主要受限于目前GPU上可用的内存量以及我们可接受的训练时间。我们的网络需要在两块GTX 580 3GB GPU上花费五到六天的时间来训练。我们所有的实验都表明，通过等待更快的GPU和更大的数据集出现，我们的结果可以进一步完善。</p>
</li>
<li><p>数据集：ImageNet是一个拥有超过1500万个已标记高分辨率图像的数据集，大概有22,000个类别。（对数据集有一个基本介绍，保证权威性，说明没有造假）图像都是从网上收集，并使用Amazon-Mechanical Turk群智工具人工标记（人工智能，人工越多越智能，找不到工作就去打标签，打标签的特点就是不费脑子，一坐坐一天。ImageNet是李飞飞<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%9D%8E%E9%A3%9E%E9%A3%9E/7448630">放个连接</a>。从2010年起，作为Pascal视觉对象挑战赛的一部分，这是每年举办一次的名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。 ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。<br>ILSVRC-2010是ILSVRC中的唯一可以使用测试集标签的版本，因此这也正是我们进行大部分实验的版本。由于我们也在ILSVRC-2012比赛中引入了我们的模型，因此在第6部分中，我们也会给出此版本数据集的结果，尽管这个版本的测试集标签不可用。在ImageNet上，习惯上使用两种错误率：top-1和top-5，其中top-5错误率是正确标签不在被模型认为最可能的五个标签之中的测试图像的百分率。（原来错误率来源于这个比赛）<br>ImageNet由可变分辨率的图像组成，而我们的系统需要固定的输入尺寸。因此，我们将图像下采样到256×256的固定分辨率。给定一个矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心的256×256的图片。除了将每个像素中减去训练集的像素均值之外，我们没有以任何其他方式对图像进行预处理。所以我们在像素的（中心）原始RGB值上训练了我们的网络。</p>
</li>
<li><p>图（前文放的模型结构图）概括了我们所提出网络的结构。它包含八个学习层——五个卷积层和三个全连接层。下面，我们将描述一些所提出网络框架中新颖或不寻常的地方。 3.1-3.4节按照我们对它们重要性的估计进行排序，其中最重要的是第一个。<br>RelU：对一个神经元模型的输出的常规套路是，给他接上一个激活函数：（tanh（x）的公式，）就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数（ReLU的公式f(x)&#x3D;max(0,x)注：因为ReLU的公式比较简单所以这里放一下)如慢得多。根据Nair和Hinton的说法[20]（这篇论文相当于为ReLU背书了，就相当于我的理论依据），我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。这个结果从图1中可以看出来（实验证明来了，填坑，使用了实验证明），该图展示了对于一个特定的四层CNN，CIFAR-10数据集训练中的误差率达到25%所需要的迭代次数。从这张图的结果可以看出，如果我们使用传统的饱和神经元模型来训练CNN，那么我们将无法为这项工作训练如此大型的神经网络。我们并不是第一个考虑在CNN中替换掉传统神经元模型的(继续理论证明，巨大的论文阅读量，)。例如，Jarrett等人[11]声称，非线性函数在他们的对比度归一化问题上，再接上局部均值池化单元，在Caltech-101数据集上表现的非常好。然而，在这个数据集中，主要担心的还是防止过拟合，所以他们观察到的效果与我们在使用ReLU时观察到的训练集的加速能力还是不一样。加快训练速度对大型数据集上训练的大型模型的性能有很大的影响。<br>在多个GPU上训练：单个GTX 580 GPU只有3GB内存（当时GPU的内存确实小，不过也挺厉害了。有时候真觉得自己跟不上时代了，对时间没有一点感觉，12年24年对我有什么区别？），这限制了可以在其上训练的网络的最大尺寸。事实证明，120万个训练样本足以训练那些因规模太大而不适合使用一个GPU训练的网络。因此，我们将网络分布在两个GPU上。目前的GPU很适合于跨GPU并行化操作，因为它们能够直接读写对方的内存，而无需通过主机内存。我们采用的并行化方案基本上将半个内核（或神经元）放在各个GPU上，（有种左右脑的感觉）——另外还有一个技巧：GPU只在某些层间进行通信。这意味着，例如，第3层的内核从第2层的所有内核映射（kernel maps）中获取输入。然而，第4层中的内核又仅从位于同一GPU上的第3层中的那些内核映射获取输入。选择连接模式对于交叉验证是一个不小的问题，但这使得我们能够精确调整通信量，直到它的计算量的达到可接受的程度。由此产生的架构有点类似于Cire¸san等人使用的“柱状”CNN[5]，除了我们的每列不是独立的之外（见图2）。与一个GPU上训练的每个卷积层只有一半的内核数量的网络相比，该方案分别将我们的top-1和top-5错误率分别降低了1.7％和1.2％。双GPU网络的训练时间比单GPU网络更少。<br>局部响应归一化：ReLU具有理想的属性，它们不需要对输入进行归一化来防止它们饱和。如果至少有一些训练实例为ReLU产生了正的输入，那么这个神经元就会学习。然而，我们还是发现下面的这种归一化方法有助于泛化。设aix,y表示第i个内核计算(x,y)位置的ReLU非线性单元的输出，而响应归一化（Local Response Normalization）的输出值定义为bix,y其中，（公式）求和部分公式中的 n表示同一个位置下与该位置相邻的内核映射的数量，而N表示这一层所有的内核数（即通道数）。内核映射的顺序当然是任意的，并且在训练之前就已经定好了。这种响应归一化实现了一种模仿真实神经元的横向抑制，从而在使用不同内核计算的神经元输出之间产生较大的竞争。常数k都是超参数（hyper-parameters），它们的值都由验证集决定。我们取 k&#x3D;2。我们在某些层的应用ReLU后再使用这种归一化方法（参见第3.5节）。这个方案与Jarrett等人[11]的局部对比归一化方案有些相似之处，但我们的被更准确地称为“亮度归一化”，因为我们没有减去均值。响应归一化将我们的top-1和top-5的错误率分别降低了1.4％和1.2％。我们还验证了这种方案在CIFAR-10数据集上的有效性：没有进行归一化的四层CNN实现了13％的测试错误率，而进行了归一化的则为11％。<br>层叠池化：CNN中的池化层汇集了相同内核映射中相邻神经元组的输出。在传统方法中，相邻池化单元之间互不重叠（例如[17,11,4]）。更准确地说，一个池化层可以被认为是由一些间隔为s个像素的池化单元组成的网格，每个都表示了一个以池化单元的位置为中心的大小为z×z的邻域。如果我们令s &#x3D; z，我们就可以得到CNN中常用的传统的局部池化。<br>整体结构：现在我们已经准备好描述CNN的整体架构了。如图2所示，这个网络包含了八层权重;前五个是卷积层，其余三个为全连接层。最后的全连接层的输出被送到1000维的softmax函数，其产生1000个类的预测。我们的网络最大化多项逻辑回归目标，这相当于在预测的分布下最大化训练样本中正确标签对数概率的平均值。第二，第四和第五个卷积层的内核仅与上一层存放在同一GPU上的内核映射相连（见图2）。第三个卷积层的内核连接到第二层中的所有内核映射。全连接层中的神经元连接到前一层中的所有神经元。响应归一化层紧接着第一个和第二个卷积层。 在3.4节中介绍的最大池化层，后面连接响应归一化层以及第五个卷积层。将ReLU应用于每个卷积层和全连接层的输出。第一个卷积层的输入为224×224×3的图像，对其使用96个大小为11×11×3、步长为4（步长表示内核映射中相邻神经元感受野中心之间的距离）的内核来处理输入图像。第二个卷积层将第一个卷积层的输出（响应归一化以及池化）作为输入，并使用256个内核处理图像，每个内核大小为5×5×48。第三个、第四个和第五个卷积层彼此连接而中间没有任何池化或归一化层。第三个卷积层有384个内核，每个的大小为3×3×256，其输入为第二个卷积层的输出。第四个卷积层有384个内核，每个内核大小为3×3×192。第五个卷积层有256个内核，每个内核大小为3×3×192。全连接层各有4096个神经元。</p>
</li>
<li><p>减少过拟合。我们的神经网络架构拥有6000万个参数。尽管ILSVRC的1000个类别使得每个训练样本从图像到标签的映射被限制在了10 bit之内，但这不足以保证训练这么多参数而不出现过拟合。下面，我们将介绍对付过度拟合的两个方法。<br>数据增强： 减小过拟合的最简单且最常用的方法就是，使用标签保留转换（label-preserving transformations，例如[25,4,5]），人为地放大数据集。我们采用两种不同形式的数据增强方法，它们都允许通过很少的计算就能从原始图像中生成转换图像，所以转换后的图像不需要存储在硬盘上。在我们实现过程中，转换后的图像是使用CPU上的Python代码生成的，在生成这些转换图像的同时，GPU还在训练上一批图像数据。所以这些数据增强方案实际上是很高效的。<br>数据增强的第一种形式包括平移图像和水平映射。我们通过从256×256图像中随机提取224×224的图像块（及其水平映射）并在这些提取的图像块上训练我们的网络来做到这一点。这使我们的训练集的规模增加了2048倍，尽管由此产生的训练样本当然还是高度相互依赖的。如果没有这个方案，我们的网络就可能会遭受大量的的过拟合，可能会迫使我们不得不使用更小的网络。在测试时，网络通过提取5个224×224的图像块（四个角块和中心块）以及它们的水平映射（因此总共包括10个块）来进行预测，并求网络的softmax层的上的十个预测结果的均值。第二种形式的数据增强包括改变训练图像中RGB通道的灰度。具体而言，我们在整个ImageNet训练集的图像的RGB像素值上使用PCA。对于每个训练图像，我们添加多个通过PCA找到的主成分，大小与相应的特征值成比例，乘以一个随机值，该随机值属于均值为0、标准差为0.1的高斯分布。因此，对于每个图像的RGB像素有：Ixy&#x3D;[IRxy IGxy IBxy]T（自己去看论文中的公式），我们加入如下的值：[p1 p2 p3] [α1λ1 α2λ2 α3λ3]T其中， pi和 λi分别是3x3的RGB协方差矩阵的第 i个特征向量和第i个的特征值，而 αi是前面所说的随机值。对于一张特定图像中的所有像素，每个 αi只会被抽取一次，知道这张图片再次用于训练时，才会重新提取随机变量。这个方案近似地捕捉原始图像的一些重要属性，对象的身份不受光照的强度和颜色变化影响。这个方案将top-1错误率降低了1％以上。<br>Dropout： 结合许多不同模型的预测结果是减少测试错误率的一种非常成功的方法[1,3]，但对于已经花费数天时间训练的大型神经网络来说，它似乎成本太高了。然而，有一种非常有效的模型组合方法，在训练期间，只需要消耗1&#x2F;2的参数。这个新发现的技术叫做“Dropout”[10]，它会以50%的概率将隐含层的神经元输出置为0。以这种方法被置0的神经元不参与网络的前馈和反向传播。因此，每次给网络提供了输入后，神经网络都会采用一个不同的结构，但是这些结构都共享权重。这种技术减少了神经元的复杂适应性，因为神经元无法依赖于其他特定的神经元而存在。因此，它被迫学习更强大更鲁棒的功能，使得这些神经元可以与其他神经元的许多不同的随机子集结合使用。在测试时，我们试着使用了所有的神经元，并将它们的输出乘以0.5。这与采用大量dropout的网络产生的预测结果分布的几何均值近似。我们在图2中的前两个全连接层上使用了dropout。没有dropout，我们的网络会出现严重的过拟合。Dropout大概会使达到收敛的迭代次数翻倍。</p>
</li>
<li><p>训练细节。我们使用随机梯度下降法来训练我们的模型，每个batch有128个样本，动量（momentum）为0.9，权重衰减（weight decay）为0.0005。我们发现这种较小的权重衰减对于模型的训练很重要。换句话说，权重衰减在这里不仅仅是一个正则化方法：它减少了模型的训练误差。权重ω的更新法则是：（自己看公式去）<br>我们使用标准差为0.01、均值为0的高斯分布来初始化各层的权重。我们使用常数1来初始化了网络中的第二个、第四个和第五个卷积层以及全连接层中的隐含层中的所有偏置参数。这种初始化权重的方法通过向ReLU提供了正的输入，来加速前期的训练。我们使用常数0来初始化剩余层中的偏置参数。<br>我们对所有层都使用相同的学习率，在训练过程中又手动进行了调整。我们遵循的启发式方法是：以当前的学习速率训练，验证集上的错误率停止降低时，将学习速率除以10.学习率初始时设为0.01，并且在终止前减少3次。我们使用120万张图像的训练集对网络进行了大约90次迭代的训练，这在两块NVIDIA GTX 580 3GB GPU上花费了大约5到6天的时间。（这里说明了优化函数，超参数设置。这里挖个坑，什么是超参数？）</p>
</li>
<li><p>结果：我们在ILSVRC-2010上取得的结果如表1所示。我们的网络的top-1和top-5测试集错误率分别为37.5％和17.0％。在ILSVRC-2010比赛期间取得的最佳成绩是47.1％和28.2％，其方法是对六种不同的稀疏编码模型所产生的预测结果求平均[2]。此后公布的最佳结果为45.7％、25.7％，其方法是对两种经过密集采样的特征[24]计算出来的Fisher向量（FV）训练的两个分类器取平均值。我们的网络实现了37.5％和17.0％的前1和前5个测试集错误率5。在ILSVRC-2010比赛期间取得的最佳成绩是47.1％和28.2％，其中一种方法是对六种针对不同特征进行训练的稀疏编码模型所产生的预测进行平均[2]，此后最佳公布结果为45.7％， 25.7％，其中一种方法是：对两个在不同取样密度的Fisher向量上训练的分类器取平均。（纵向对比了，相同数据集，不同模型。）<br>我们还在ILSVRC-2012竞赛中使用了我们的模型，并在表2中给出了我们的结果。由于ILSVRC-2012测试集标签未公开，因此我们无法给出我们测试过的所有模型在测试集上的错误率。在本节的其余部分中，我们将验证集和测试集的错误率互换，因为根据我们的经验，它们之间的差值不超过0.1％（见表2）。本文描述的CNN的top-5错误率达到了18.2％。对五个相似CNN的预测结果计算均值，得到的错误率为16.4％。单独一个CNN，在最后一个池化层之后，额外添加第六个卷积层，对整个ImageNet Fall 2011 release(15M images, 22K categories)进行分类，然后在ILSVRC-2012上“微调”（fine-tuning）网络，得到的错误率为16.6％。对整个ImageNet Fall 2011版本的数据集下预训练的两个CNN，求他们输出的预测值与前面提到的5个不同的CNN输出的预测值的均值，得到的错误率为15.3％。比赛的第二名达到了26.2％的top-5错误率，他们的方法是：对几个在特征取样密度不同的Fisher向量上训练的分类器的预测结果取平均的方法[7]。<br>最后，我们还在ImageNet Fall 2009版本的数据集上提交了错误率，总共有10,184个类别和890万张图像。在这个数据集中，我们遵循文献中的使用一半图像用于训练，一半图像用于测试的惯例。由于没有建立测试集，所以我们的拆分方法有必要与先前作者使用的拆分方法不同，但这并不会对结果产生显著的影响。我们在这个数据集上的top-1和top-5错误率分别是67.4％和40.9％，是通过前面描述的网络获得的，但是在最后的池化层上还有额外的第6个卷积层。该数据集此前公布的最佳结果是78.1％和60.9％[19]。<br>定性评估：图3（自己看论文去）显示了由网络的两个数据连接层学习得到的卷积内核。（网络结构还可以画出来，也是挺有意思的。）该网络已经学习到许多频率和方向提取的内核，以及各种色块。请注意两个GPU所展现的不同特性，这也是3.5节中介绍的限制互连的结果。GPU1上的内核在很大程度上与颜色无关，然而GPU2上的内核在很大程度上都于颜色有关。这种特异性在每次迭代期间都会发生，并且独立于任何特定的随机权重初始化过程（以GPU的重新编号为模）。<br>在图4（自己看论文去，图4展示了一堆实验结果）的左边，我们通过计算8张测试图像的top-5预测来定性评估网络的训练结果。请注意，即使是偏离中心的物体，如左上角的螨虫，也可以被网络识别出来。大多数top-5的标签都显得比较合理。例如，只有其他类型的猫才被认为是豹子的可能标签。在某些情况下（栅栏、樱桃），照片的关注点存在模糊性，不知道到底该关注哪个。另一个研究可视化的网络的方法是，考虑由最后一个4096维隐含层中的图像的特征的激活函数输出值。如果两幅图像产生有的欧氏距离，我们可以认为高层次的神经网络认为它们是相似的。图4显示了测试集中的5个图像和来袭训练集的6个图像，这些图像根据这种度量方法来比较它们中的哪一个与其最相似。请注意，在像素层次上，待检测的训练图像通常不会与第一列中的查询图像有较小的L2距离。例如，检索到的狗和大象有各种不同的姿势。我们在补充材料中提供了更多测试图像的结果。通过使用欧式距离来计算两个4096维实值向量的相似性，效率不高，但是通过训练自编码器可以将这些向量压缩为较短的二进制码，能够使其更高效。与应用自编码器到原始像素[14]相比，这应该是更好的图像检索方法。它不使用图像标签，因此更秦翔宇检索具有相似图案边缘的图像，不管它们的图像语义是否相似。</p>
</li>
<li><p>讨论：我们的研究结果表明，一个大的深层卷积神经网络能够在纯粹使用监督学习（这里有个概念，监督学习和无监督学习，半监督学习。挖个坑）的情况下，在极具挑战性的数据集上实现破纪录的结果。值得注意的是，如果移除任何一个卷积层，网络的性能就会下降。例如，删除任何中间层的结果会导致网络性能的top-1错误率下降2%。因此网络的深度对于实现我们的结果真的很重要。（基本上后面的深度学习的思路就是堆网络结构）<br>为了简化我们的实验，我们没有使用任何无监督的预训练方法，尽管这样可能会有所帮助，特别是如果我们获得了足够的计算能力来显著地增加网络的大小而不会相应地增加已标记数据的数量。到目前为止，我们的结果已经获得了足够的进步，因为我们已经使网络更大，并且训练了更长时间。但我们仍然有很大的空间去优化网络，使之能够像人类的视觉系统一样感知。最后，我们希望对视频序列使用非常大的深度卷积神经网路，其中时间结构提供了非常有用的信息，这些信息往往在静态图像中丢失了，或者说不太明显。</p>
</li>
</ol>
<h1 id="个人感觉"><a href="#个人感觉" class="headerlink" title="个人感觉"></a>个人感觉</h1><p>论文很短，内容很多。展现在论文中的，没有展现在论文中的。学习的过程中既有鲜花也有荆棘，这是客观的条件，我承认有人会有论语中的天生的智慧，看待世界的方式就不一样，这是现实，但是那又有什么？不管怎么样先把下面的坑填了。还有就是博客中难免有错别字，记得更改。+</p>
<h2 id="问题1，卷积是什么？作用什么？"><a href="#问题1，卷积是什么？作用什么？" class="headerlink" title="问题1，卷积是什么？作用什么？"></a>问题1，卷积是什么？作用什么？</h2><h2 id="问题2，池化是什么？作用是什么？"><a href="#问题2，池化是什么？作用是什么？" class="headerlink" title="问题2，池化是什么？作用是什么？"></a>问题2，池化是什么？作用是什么？</h2><h2 id="问题3，全连接是什么？作用是什么？"><a href="#问题3，全连接是什么？作用是什么？" class="headerlink" title="问题3，全连接是什么？作用是什么？"></a>问题3，全连接是什么？作用是什么？</h2><h2 id="问题4，AlexNet论文使用的loss函数是什么？"><a href="#问题4，AlexNet论文使用的loss函数是什么？" class="headerlink" title="问题4，AlexNet论文使用的loss函数是什么？"></a>问题4，AlexNet论文使用的loss函数是什么？</h2><h2 id="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"><a href="#问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？" class="headerlink" title="问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？"></a>问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？</h2><h2 id="问题6，AlexNet论文中使用的评价指标是什么？"><a href="#问题6，AlexNet论文中使用的评价指标是什么？" class="headerlink" title="问题6，AlexNet论文中使用的评价指标是什么？"></a>问题6，AlexNet论文中使用的评价指标是什么？</h2><h2 id="问题7，AlexNet中的创新点是什么？"><a href="#问题7，AlexNet中的创新点是什么？" class="headerlink" title="问题7，AlexNet中的创新点是什么？"></a>问题7，AlexNet中的创新点是什么？</h2><ol>
<li>ReLU激活函数的引入，采样非线性单元（ReLU）的深度卷积神经网络训练时间要比tanh单元要快几倍。而时间开销是进行模型训练过程中的很重要的因数。同时ReLU有效的防止了过拟合的现象。</li>
<li>层叠池化操作，以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize&#x3D;2×2，stride&#x3D;2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize &gt; stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。</li>
<li>Dropout操作， Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。</li>
<li>网络层数更深，与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。当然，这种思考的重要节点出现在VGG网络（下一篇博文VGG论文中将会讲到）。</li>
</ol>
<h2 id="问题8，优化函数的具体实现是什么？"><a href="#问题8，优化函数的具体实现是什么？" class="headerlink" title="问题8，优化函数的具体实现是什么？"></a>问题8，优化函数的具体实现是什么？</h2><h2 id="问题9，关于卷积后特征图应该怎么计算？"><a href="#问题9，关于卷积后特征图应该怎么计算？" class="headerlink" title="问题9，关于卷积后特征图应该怎么计算？"></a>问题9，关于卷积后特征图应该怎么计算？</h2><h2 id="问题10，什么是过拟合合和欠拟合？"><a href="#问题10，什么是过拟合合和欠拟合？" class="headerlink" title="问题10，什么是过拟合合和欠拟合？"></a>问题10，什么是过拟合合和欠拟合？</h2><h2 id="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"><a href="#问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？" class="headerlink" title="问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？"></a>问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？</h2><h2 id="问题12，-神经元数量和参数量的计算方法是什么？"><a href="#问题12，-神经元数量和参数量的计算方法是什么？" class="headerlink" title="问题12， 神经元数量和参数量的计算方法是什么？"></a>问题12， 神经元数量和参数量的计算方法是什么？</h2><h2 id="问题13，-softMax的机制是怎么样的？"><a href="#问题13，-softMax的机制是怎么样的？" class="headerlink" title="问题13， softMax的机制是怎么样的？"></a>问题13， softMax的机制是怎么样的？</h2><h2 id="问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？"><a href="#问题14，-AlexNet论文中使用的评估指标错误率的计算方法是什么？" class="headerlink" title="问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？"></a>问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？</h2><h2 id="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？"><a href="#问题15，Dropout的运行机制是什么，论文中怎么证明他们有效-理论证明和实验证明-？" class="headerlink" title="问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？"></a>问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？</h2><h2 id="问题16，-什么是超参数？"><a href="#问题16，-什么是超参数？" class="headerlink" title="问题16， 什么是超参数？"></a>问题16， 什么是超参数？</h2><h2 id="问题17，-什么是监督学习和无监督学习，半监督学习？"><a href="#问题17，-什么是监督学习和无监督学习，半监督学习？" class="headerlink" title="问题17， 什么是监督学习和无监督学习，半监督学习？"></a>问题17， 什么是监督学习和无监督学习，半监督学习？</h2><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><h3 id="关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。"><a href="#关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。" class="headerlink" title="关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。"></a>关于图像，有很多需要去填坑的部分，这里甚至可以新开一个分类去填关于图像的坑。在这篇文章中先把坑挖起来。免得忘记。</h3>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87/" class="print-no-link">#深度学习论文</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>经典网络结构——AlexNet</div>
      <div>https://chenlidbk.xyz/2024/04/21/deeplearnpaper/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>chenli</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年4月21日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/04/21/ganwu8/" title="随笔8">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">随笔8</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/04/21/deeplearnbook2/" title="PyTorch基础——Numpy">
                        <span class="hidden-mobile">PyTorch基础——Numpy</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
